### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.39 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.24 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   24.55 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.33 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    2.14 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.19 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.18 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.23 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.19 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.79 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed  171.27 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.35 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.25 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 212.24 sec*proc (27 tests)

Total Test time (real) = 212.25 sec

real	3m32.281s
user	7m11.888s
sys	0m5.754s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.46 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.33 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.06 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.12 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.08 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.07 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.24 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.05 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.08 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.04 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.07 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.27 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.18 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   14.08 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.21 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    0.90 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.17 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.18 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.18 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.22 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.28 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed   28.53 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.19 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.72 sec*proc (27 tests)

Total Test time (real) =  50.73 sec

real	0m50.743s
user	1m10.824s
sys	0m4.845s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.186 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.029.946 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.110 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.122 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.034.123 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.123 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.034.124 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.034.125 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.034.127 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.034.128 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.034.129 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.034.129 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.034.129 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.034.133 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.134 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.135 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.034.135 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.034.136 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.034.136 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.034.137 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.039.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.041.271 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.274 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.041.275 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.041.275 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.041.276 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.041.276 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.041.277 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.041.278 I llama_model_loader: - type  f32:  124 tensors
0.00.041.278 I llama_model_loader: - type  f16:   73 tensors
0.00.046.035 I llm_load_vocab: special tokens cache size = 5
0.00.048.463 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.048.469 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.470 I llm_load_print_meta: arch             = bert
0.00.048.470 I llm_load_print_meta: vocab type       = WPM
0.00.048.471 I llm_load_print_meta: n_vocab          = 30522
0.00.048.471 I llm_load_print_meta: n_merges         = 0
0.00.048.472 I llm_load_print_meta: vocab_only       = 0
0.00.048.472 I llm_load_print_meta: n_ctx_train      = 512
0.00.048.472 I llm_load_print_meta: n_embd           = 384
0.00.048.472 I llm_load_print_meta: n_layer          = 12
0.00.048.475 I llm_load_print_meta: n_head           = 12
0.00.048.476 I llm_load_print_meta: n_head_kv        = 12
0.00.048.477 I llm_load_print_meta: n_rot            = 32
0.00.048.477 I llm_load_print_meta: n_swa            = 0
0.00.048.477 I llm_load_print_meta: n_embd_head_k    = 32
0.00.048.478 I llm_load_print_meta: n_embd_head_v    = 32
0.00.048.479 I llm_load_print_meta: n_gqa            = 1
0.00.048.480 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.048.481 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.048.485 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.048.485 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.485 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.486 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.486 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.487 I llm_load_print_meta: n_ff             = 1536
0.00.048.487 I llm_load_print_meta: n_expert         = 0
0.00.048.488 I llm_load_print_meta: n_expert_used    = 0
0.00.048.488 I llm_load_print_meta: causal attn      = 0
0.00.048.488 I llm_load_print_meta: pooling type     = 2
0.00.048.488 I llm_load_print_meta: rope type        = 2
0.00.048.489 I llm_load_print_meta: rope scaling     = linear
0.00.048.489 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.490 I llm_load_print_meta: freq_scale_train = 1
0.00.048.490 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.048.491 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.491 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.491 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.491 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.492 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.492 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.506 I llm_load_print_meta: model type       = 33M
0.00.048.507 I llm_load_print_meta: model ftype      = F16
0.00.048.507 I llm_load_print_meta: model params     = 33.21 M
0.00.048.510 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.048.510 I llm_load_print_meta: general.name     = Bge Small
0.00.048.511 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.048.511 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.048.512 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.048.512 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.048.512 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.048.513 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.048.513 I llm_load_print_meta: max token length = 21
0.00.050.807 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.050.809 I llm_load_tensors: offloading output layer to GPU
0.00.050.809 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.050.837 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.050.839 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.051.504 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.506 I llama_new_context_with_model: n_ctx         = 512
0.00.051.506 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.051.507 I llama_new_context_with_model: n_batch       = 2048
0.00.051.507 I llama_new_context_with_model: n_ubatch      = 2048
0.00.051.507 I llama_new_context_with_model: flash_attn    = 0
0.00.051.508 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.509 I llama_new_context_with_model: freq_scale    = 1
0.00.051.509 I ggml_metal_init: allocating
0.00.051.514 I ggml_metal_init: found device: Apple M4
0.00.051.517 I ggml_metal_init: picking default device: Apple M4
0.00.052.465 I ggml_metal_init: using embedded metal library
0.00.062.672 I ggml_metal_init: GPU name:   Apple M4
0.00.062.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.675 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.676 I ggml_metal_init: simdgroup reduction   = true
0.00.062.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.677 I ggml_metal_init: has bfloat            = true
0.00.062.677 I ggml_metal_init: use bfloat            = true
0.00.062.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.386 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.084.389 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.084.390 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.085.497 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.085.499 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.085.500 I llama_new_context_with_model: graph nodes  = 429
0.00.085.500 I llama_new_context_with_model: graph splits = 2
0.00.085.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.095.832 I 
0.00.095.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.096.885 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.102.225 I llama_perf_context_print:        load time =      65.88 ms
0.00.102.227 I llama_perf_context_print: prompt eval time =       5.15 ms /     9 tokens (    0.57 ms per token,  1747.23 tokens per second)
0.00.102.228 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.102.229 I llama_perf_context_print:       total time =       6.39 ms /    10 tokens
0.00.102.482 I ggml_metal_free: deallocating

real	0m0.283s
user	0m0.053s
sys	0m0.042s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.073 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.427 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.432 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.433 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.433 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.433 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.434 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.435 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.435 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.436 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.436 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.436 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.438 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.439 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.012.439 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.012.439 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.012.440 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.440 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.012.440 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.187 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.189 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.189 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.189 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.190 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.016.190 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.190 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.016.191 I llama_model_loader: - type  f32:  124 tensors
0.00.016.191 I llama_model_loader: - type q8_0:   73 tensors
0.00.019.110 I llm_load_vocab: special tokens cache size = 5
0.00.020.687 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.020.690 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.020.690 I llm_load_print_meta: arch             = bert
0.00.020.691 I llm_load_print_meta: vocab type       = WPM
0.00.020.691 I llm_load_print_meta: n_vocab          = 30522
0.00.020.691 I llm_load_print_meta: n_merges         = 0
0.00.020.691 I llm_load_print_meta: vocab_only       = 0
0.00.020.691 I llm_load_print_meta: n_ctx_train      = 512
0.00.020.692 I llm_load_print_meta: n_embd           = 384
0.00.020.692 I llm_load_print_meta: n_layer          = 12
0.00.020.694 I llm_load_print_meta: n_head           = 12
0.00.020.694 I llm_load_print_meta: n_head_kv        = 12
0.00.020.695 I llm_load_print_meta: n_rot            = 32
0.00.020.695 I llm_load_print_meta: n_swa            = 0
0.00.020.695 I llm_load_print_meta: n_embd_head_k    = 32
0.00.020.696 I llm_load_print_meta: n_embd_head_v    = 32
0.00.020.697 I llm_load_print_meta: n_gqa            = 1
0.00.020.697 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.020.698 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.020.698 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.020.699 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.020.699 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.020.699 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.020.699 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.020.700 I llm_load_print_meta: n_ff             = 1536
0.00.020.700 I llm_load_print_meta: n_expert         = 0
0.00.020.700 I llm_load_print_meta: n_expert_used    = 0
0.00.020.700 I llm_load_print_meta: causal attn      = 0
0.00.020.700 I llm_load_print_meta: pooling type     = 2
0.00.020.701 I llm_load_print_meta: rope type        = 2
0.00.020.701 I llm_load_print_meta: rope scaling     = linear
0.00.020.701 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.020.702 I llm_load_print_meta: freq_scale_train = 1
0.00.020.702 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.020.702 I llm_load_print_meta: rope_finetuned   = unknown
0.00.020.702 I llm_load_print_meta: ssm_d_conv       = 0
0.00.020.702 I llm_load_print_meta: ssm_d_inner      = 0
0.00.020.702 I llm_load_print_meta: ssm_d_state      = 0
0.00.020.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.020.702 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.020.709 I llm_load_print_meta: model type       = 33M
0.00.020.709 I llm_load_print_meta: model ftype      = Q8_0
0.00.020.710 I llm_load_print_meta: model params     = 33.21 M
0.00.020.710 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.020.711 I llm_load_print_meta: general.name     = Bge Small
0.00.020.711 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.020.711 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.020.711 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.020.711 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.020.712 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.020.712 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.020.712 I llm_load_print_meta: max token length = 21
0.00.022.087 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.022.088 I llm_load_tensors: offloading output layer to GPU
0.00.022.088 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.022.096 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.022.096 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.022.549 I llama_new_context_with_model: n_seq_max     = 1
0.00.022.550 I llama_new_context_with_model: n_ctx         = 512
0.00.022.550 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.022.550 I llama_new_context_with_model: n_batch       = 2048
0.00.022.550 I llama_new_context_with_model: n_ubatch      = 2048
0.00.022.550 I llama_new_context_with_model: flash_attn    = 0
0.00.022.551 I llama_new_context_with_model: freq_base     = 10000.0
0.00.022.551 I llama_new_context_with_model: freq_scale    = 1
0.00.022.552 I ggml_metal_init: allocating
0.00.022.554 I ggml_metal_init: found device: Apple M4
0.00.022.557 I ggml_metal_init: picking default device: Apple M4
0.00.023.081 I ggml_metal_init: using embedded metal library
0.00.025.344 I ggml_metal_init: GPU name:   Apple M4
0.00.025.345 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.346 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.346 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.346 I ggml_metal_init: simdgroup reduction   = true
0.00.025.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.347 I ggml_metal_init: has bfloat            = true
0.00.025.347 I ggml_metal_init: use bfloat            = true
0.00.025.347 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.348 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.469 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.472 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.474 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.110 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.112 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.112 I llama_new_context_with_model: graph nodes  = 429
0.00.034.112 I llama_new_context_with_model: graph splits = 2
0.00.034.123 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.556 I 
0.00.040.571 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.041.148 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.262 I llama_perf_context_print:        load time =      30.48 ms
0.00.046.263 I llama_perf_context_print: prompt eval time =       4.97 ms /     9 tokens (    0.55 ms per token,  1809.77 tokens per second)
0.00.046.263 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.264 I llama_perf_context_print:       total time =       5.71 ms /    10 tokens
0.00.046.395 I ggml_metal_free: deallocating

real	0m0.060s
user	0m0.030s
sys	0m0.018s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.190 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.034.658 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.916 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.925 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.927 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.045.928 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.937 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.045.938 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.045.939 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.045.940 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.045.941 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.045.942 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.045.943 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.045.943 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.045.951 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.045.951 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.045.952 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.045.953 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.054.705 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.057.027 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.212 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.062.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.215 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.062.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.062.216 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.062.216 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.062.217 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.062.217 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.062.218 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.062.218 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.062.218 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.062.219 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.062.220 I llama_model_loader: - type  f32:   41 tensors
0.00.062.220 I llama_model_loader: - type  f16:   29 tensors
0.00.081.888 W llm_load_vocab: empty token at index 5
0.00.086.843 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.088.203 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.088.249 I llm_load_vocab: special tokens cache size = 5
0.00.331.123 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.128 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.129 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.136 I llm_load_print_meta: vocab type       = BPE
0.00.331.136 I llm_load_print_meta: n_vocab          = 61056
0.00.331.136 I llm_load_print_meta: n_merges         = 39382
0.00.331.138 I llm_load_print_meta: vocab_only       = 0
0.00.331.139 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.139 I llm_load_print_meta: n_embd           = 384
0.00.331.139 I llm_load_print_meta: n_layer          = 4
0.00.331.144 I llm_load_print_meta: n_head           = 12
0.00.331.145 I llm_load_print_meta: n_head_kv        = 12
0.00.331.145 I llm_load_print_meta: n_rot            = 32
0.00.331.146 I llm_load_print_meta: n_swa            = 0
0.00.331.146 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.146 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.146 I llm_load_print_meta: n_gqa            = 1
0.00.331.147 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.148 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.149 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.149 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.149 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.150 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.150 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.151 I llm_load_print_meta: n_ff             = 1536
0.00.331.151 I llm_load_print_meta: n_expert         = 0
0.00.331.152 I llm_load_print_meta: n_expert_used    = 0
0.00.331.152 I llm_load_print_meta: causal attn      = 0
0.00.331.152 I llm_load_print_meta: pooling type     = -1
0.00.331.152 I llm_load_print_meta: rope type        = -1
0.00.331.153 I llm_load_print_meta: rope scaling     = linear
0.00.331.153 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.153 I llm_load_print_meta: freq_scale_train = 1
0.00.331.153 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.155 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.155 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.155 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.155 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.155 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.155 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.180 I llm_load_print_meta: model type       = 33M
0.00.331.181 I llm_load_print_meta: model ftype      = F16
0.00.331.181 I llm_load_print_meta: model params     = 32.90 M
0.00.331.181 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.183 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.183 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.183 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.183 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.183 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.184 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.184 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.184 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.185 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.185 I llm_load_print_meta: max token length = 45
0.00.332.427 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.332.429 I llm_load_tensors: offloading output layer to GPU
0.00.332.429 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.332.452 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.332.452 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.474 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.475 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.475 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.475 I llama_new_context_with_model: n_batch       = 2048
0.00.333.475 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.475 I llama_new_context_with_model: flash_attn    = 0
0.00.333.476 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.476 I llama_new_context_with_model: freq_scale    = 1
0.00.333.476 I ggml_metal_init: allocating
0.00.333.479 I ggml_metal_init: found device: Apple M4
0.00.333.481 I ggml_metal_init: picking default device: Apple M4
0.00.334.388 I ggml_metal_init: using embedded metal library
0.00.341.737 I ggml_metal_init: GPU name:   Apple M4
0.00.341.738 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.341.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.341.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.341.739 I ggml_metal_init: simdgroup reduction   = true
0.00.341.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.341.739 I ggml_metal_init: has bfloat            = true
0.00.341.740 I ggml_metal_init: use bfloat            = true
0.00.341.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.341.741 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.358.922 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.358.923 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.358.925 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.359.528 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.359.529 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.359.530 I llama_new_context_with_model: graph nodes  = 154
0.00.359.530 I llama_new_context_with_model: graph splits = 2
0.00.359.547 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.400.518 I 
0.00.400.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.401.102 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.401.103 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.401.118 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.401.118 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.401.125 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.401.128 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.401.774 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.405.585 I llama_perf_context_print:        load time =     365.83 ms
0.00.405.587 I llama_perf_context_print: prompt eval time =       3.80 ms /    62 tokens (    0.06 ms per token, 16320.08 tokens per second)
0.00.405.588 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.405.589 I llama_perf_context_print:       total time =       5.07 ms /    63 tokens
0.00.405.811 I ggml_metal_free: deallocating

real	0m1.086s
user	0m0.325s
sys	0m0.055s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.111 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.278 I main: llama backend init
0.00.000.285 I main: load the model and apply lora adapter, if any
0.00.117.325 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.128.311 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.128.325 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.128.329 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.128.329 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.128.330 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.128.331 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.128.332 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.128.333 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.128.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.128.334 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.128.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.128.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.128.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.128.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.128.342 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.128.346 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.128.347 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.135.483 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.137.737 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.144.874 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.144.882 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.144.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.144.884 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.144.884 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.144.886 I llama_model_loader: - type  f32:  194 tensors
0.00.144.887 I llama_model_loader: - type  f16:   98 tensors
0.00.184.149 I llm_load_vocab: special tokens cache size = 25
0.00.191.771 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.191.775 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.191.775 I llm_load_print_meta: arch             = gptneox
0.00.191.776 I llm_load_print_meta: vocab type       = BPE
0.00.191.776 I llm_load_print_meta: n_vocab          = 50304
0.00.191.776 I llm_load_print_meta: n_merges         = 50009
0.00.191.776 I llm_load_print_meta: vocab_only       = 0
0.00.191.776 I llm_load_print_meta: n_ctx_train      = 2048
0.00.191.776 I llm_load_print_meta: n_embd           = 2048
0.00.191.777 I llm_load_print_meta: n_layer          = 24
0.00.191.781 I llm_load_print_meta: n_head           = 16
0.00.191.782 I llm_load_print_meta: n_head_kv        = 16
0.00.191.782 I llm_load_print_meta: n_rot            = 32
0.00.191.782 I llm_load_print_meta: n_swa            = 0
0.00.191.782 I llm_load_print_meta: n_embd_head_k    = 128
0.00.191.785 I llm_load_print_meta: n_embd_head_v    = 128
0.00.191.786 I llm_load_print_meta: n_gqa            = 1
0.00.191.786 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.191.787 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.191.788 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.191.788 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.191.788 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.191.788 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.191.789 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.191.789 I llm_load_print_meta: n_ff             = 8192
0.00.191.789 I llm_load_print_meta: n_expert         = 0
0.00.191.790 I llm_load_print_meta: n_expert_used    = 0
0.00.191.790 I llm_load_print_meta: causal attn      = 1
0.00.191.791 I llm_load_print_meta: pooling type     = 0
0.00.191.791 I llm_load_print_meta: rope type        = 2
0.00.191.791 I llm_load_print_meta: rope scaling     = linear
0.00.191.792 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.191.792 I llm_load_print_meta: freq_scale_train = 1
0.00.191.792 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.191.793 I llm_load_print_meta: rope_finetuned   = unknown
0.00.191.793 I llm_load_print_meta: ssm_d_conv       = 0
0.00.191.793 I llm_load_print_meta: ssm_d_inner      = 0
0.00.191.793 I llm_load_print_meta: ssm_d_state      = 0
0.00.191.793 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.191.793 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.191.800 I llm_load_print_meta: model type       = 1.4B
0.00.191.801 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.191.801 I llm_load_print_meta: model params     = 1.41 B
0.00.191.801 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.191.802 I llm_load_print_meta: general.name     = 1.4B
0.00.191.802 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.191.803 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.191.803 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.191.803 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.191.803 I llm_load_print_meta: LF token         = 128 ''
0.00.191.804 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.191.804 I llm_load_print_meta: max token length = 1024
0.00.194.007 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.194.007 I llm_load_tensors: offloading output layer to GPU
0.00.194.007 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.194.021 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.194.022 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.195.060 I llama_new_context_with_model: n_seq_max     = 1
0.00.195.061 I llama_new_context_with_model: n_ctx         = 2048
0.00.195.061 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.195.061 I llama_new_context_with_model: n_batch       = 2048
0.00.195.062 I llama_new_context_with_model: n_ubatch      = 512
0.00.195.062 I llama_new_context_with_model: flash_attn    = 0
0.00.195.062 I llama_new_context_with_model: freq_base     = 10000.0
0.00.195.062 I llama_new_context_with_model: freq_scale    = 1
0.00.195.063 I ggml_metal_init: allocating
0.00.195.066 I ggml_metal_init: found device: Apple M4
0.00.195.069 I ggml_metal_init: picking default device: Apple M4
0.00.195.744 I ggml_metal_init: using embedded metal library
0.00.250.744 I ggml_metal_init: GPU name:   Apple M4
0.00.250.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.250.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.250.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.250.748 I ggml_metal_init: simdgroup reduction   = true
0.00.250.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.250.748 I ggml_metal_init: has bfloat            = true
0.00.250.749 I ggml_metal_init: use bfloat            = true
0.00.250.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.250.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.296.596 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.296.602 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.296.620 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.297.664 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.297.666 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.297.666 I llama_new_context_with_model: graph nodes  = 967
0.00.297.666 I llama_new_context_with_model: graph splits = 2
0.00.297.687 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.269 I main: llama threadpool init, n_threads = 4
0.00.500.318 I 
0.00.500.348 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.500.348 I 
0.00.500.530 I sampler seed: 1234
0.00.500.536 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.500.559 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.500.561 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.500.561 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.382.621 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.02.382.622 I llama_perf_context_print:        load time =     382.93 ms
0.02.382.623 I llama_perf_context_print: prompt eval time =      39.77 ms /     7 tokens (    5.68 ms per token,   176.02 tokens per second)
0.02.382.624 I llama_perf_context_print:        eval time =    1839.45 ms /    63 runs   (   29.20 ms per token,    34.25 tokens per second)
0.02.382.624 I llama_perf_context_print:       total time =    1882.36 ms /    70 tokens
0.02.382.818 I ggml_metal_free: deallocating

real	0m2.678s
user	0m0.157s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.629 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.848 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.468 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.484 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.485 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.486 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.487 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.487 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.489 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.489 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.490 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.491 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.491 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.492 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.492 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.498 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.498 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.197 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.043 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.045 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.046 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.047 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.047 I llama_model_loader: - type  f32:  194 tensors
0.00.054.048 I llama_model_loader: - type  f16:   98 tensors
0.00.085.662 I llm_load_vocab: special tokens cache size = 25
0.00.092.797 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.800 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.801 I llm_load_print_meta: arch             = gptneox
0.00.092.801 I llm_load_print_meta: vocab type       = BPE
0.00.092.801 I llm_load_print_meta: n_vocab          = 50304
0.00.092.801 I llm_load_print_meta: n_merges         = 50009
0.00.092.801 I llm_load_print_meta: vocab_only       = 0
0.00.092.802 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.802 I llm_load_print_meta: n_embd           = 2048
0.00.092.802 I llm_load_print_meta: n_layer          = 24
0.00.092.805 I llm_load_print_meta: n_head           = 16
0.00.092.807 I llm_load_print_meta: n_head_kv        = 16
0.00.092.807 I llm_load_print_meta: n_rot            = 32
0.00.092.807 I llm_load_print_meta: n_swa            = 0
0.00.092.807 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.807 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.808 I llm_load_print_meta: n_gqa            = 1
0.00.092.809 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.809 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.810 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.810 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.820 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.822 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.822 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.829 I llm_load_print_meta: n_ff             = 8192
0.00.092.830 I llm_load_print_meta: n_expert         = 0
0.00.092.830 I llm_load_print_meta: n_expert_used    = 0
0.00.092.830 I llm_load_print_meta: causal attn      = 1
0.00.092.830 I llm_load_print_meta: pooling type     = 0
0.00.092.830 I llm_load_print_meta: rope type        = 2
0.00.092.830 I llm_load_print_meta: rope scaling     = linear
0.00.092.831 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.831 I llm_load_print_meta: freq_scale_train = 1
0.00.092.832 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.833 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.833 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.833 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.833 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.833 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.834 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.845 I llm_load_print_meta: model type       = 1.4B
0.00.092.846 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.847 I llm_load_print_meta: model params     = 1.41 B
0.00.092.847 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.847 I llm_load_print_meta: general.name     = 1.4B
0.00.092.848 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.848 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.849 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.849 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.850 I llm_load_print_meta: LF token         = 128 ''
0.00.092.851 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.851 I llm_load_print_meta: max token length = 1024
0.00.094.898 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.898 I llm_load_tensors: offloading output layer to GPU
0.00.094.898 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.908 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.909 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.899 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.900 I llama_new_context_with_model: n_ctx         = 128
0.00.095.900 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.900 I llama_new_context_with_model: n_batch       = 128
0.00.095.901 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.901 I llama_new_context_with_model: flash_attn    = 0
0.00.095.901 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.901 I llama_new_context_with_model: freq_scale    = 1
0.00.095.902 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.902 I ggml_metal_init: allocating
0.00.095.909 I ggml_metal_init: found device: Apple M4
0.00.095.912 I ggml_metal_init: picking default device: Apple M4
0.00.096.558 I ggml_metal_init: using embedded metal library
0.00.098.849 I ggml_metal_init: GPU name:   Apple M4
0.00.098.850 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.851 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.851 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.851 I ggml_metal_init: simdgroup reduction   = true
0.00.098.851 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.852 I ggml_metal_init: has bfloat            = true
0.00.098.852 I ggml_metal_init: use bfloat            = true
0.00.098.852 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.853 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.309 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.311 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.328 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.270 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.271 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.271 I llama_new_context_with_model: graph nodes  = 967
0.00.109.271 I llama_new_context_with_model: graph splits = 2
0.00.109.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.555.267 I 
0.01.555.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.555.356 I perplexity: tokenizing the input ..
0.01.570.446 I perplexity: tokenization took 15.087 ms
0.01.570.453 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.693.059 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.695.220 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.695.249 I llama_perf_context_print:        load time =    1531.41 ms
0.01.695.251 I llama_perf_context_print: prompt eval time =     121.64 ms /   128 tokens (    0.95 ms per token,  1052.29 tokens per second)
0.01.695.253 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.695.253 I llama_perf_context_print:       total time =     139.98 ms /   129 tokens
0.01.695.845 I ggml_metal_free: deallocating

real	0m1.881s
user	0m0.125s
sys	0m0.272s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.888 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.689 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.693 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.700 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.701 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.701 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.704 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.704 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.705 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.705 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.705 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.707 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.707 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.707 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.624 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.475 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.479 I llama_model_loader: - type  f32:  194 tensors
0.00.038.479 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.300 I llm_load_vocab: special tokens cache size = 25
0.00.071.061 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.065 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.065 I llm_load_print_meta: arch             = gptneox
0.00.071.066 I llm_load_print_meta: vocab type       = BPE
0.00.071.068 I llm_load_print_meta: n_vocab          = 50304
0.00.071.068 I llm_load_print_meta: n_merges         = 50009
0.00.071.069 I llm_load_print_meta: vocab_only       = 0
0.00.071.069 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.069 I llm_load_print_meta: n_embd           = 2048
0.00.071.069 I llm_load_print_meta: n_layer          = 24
0.00.071.074 I llm_load_print_meta: n_head           = 16
0.00.071.075 I llm_load_print_meta: n_head_kv        = 16
0.00.071.075 I llm_load_print_meta: n_rot            = 32
0.00.071.076 I llm_load_print_meta: n_swa            = 0
0.00.071.076 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.076 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.077 I llm_load_print_meta: n_gqa            = 1
0.00.071.078 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.078 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.079 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.079 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.080 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.080 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.080 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.081 I llm_load_print_meta: n_ff             = 8192
0.00.071.081 I llm_load_print_meta: n_expert         = 0
0.00.071.082 I llm_load_print_meta: n_expert_used    = 0
0.00.071.082 I llm_load_print_meta: causal attn      = 1
0.00.071.084 I llm_load_print_meta: pooling type     = 0
0.00.071.084 I llm_load_print_meta: rope type        = 2
0.00.071.084 I llm_load_print_meta: rope scaling     = linear
0.00.071.085 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.085 I llm_load_print_meta: freq_scale_train = 1
0.00.071.085 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.086 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.086 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.086 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.086 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.086 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.086 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.100 I llm_load_print_meta: model type       = 1.4B
0.00.071.100 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.100 I llm_load_print_meta: model params     = 1.41 B
0.00.071.101 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.101 I llm_load_print_meta: general.name     = 1.4B
0.00.071.101 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.101 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.102 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.102 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.103 I llm_load_print_meta: LF token         = 128 ''
0.00.071.104 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.104 I llm_load_print_meta: max token length = 1024
0.00.073.921 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.922 I llm_load_tensors: offloading output layer to GPU
0.00.073.922 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.933 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.935 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.075.183 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.185 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.185 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.185 I llama_new_context_with_model: n_batch       = 2048
0.00.075.185 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.186 I llama_new_context_with_model: flash_attn    = 0
0.00.075.186 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.186 I llama_new_context_with_model: freq_scale    = 1
0.00.075.187 I ggml_metal_init: allocating
0.00.075.191 I ggml_metal_init: found device: Apple M4
0.00.075.193 I ggml_metal_init: picking default device: Apple M4
0.00.075.992 I ggml_metal_init: using embedded metal library
0.00.082.676 I ggml_metal_init: GPU name:   Apple M4
0.00.082.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.680 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.681 I ggml_metal_init: simdgroup reduction   = true
0.00.082.681 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.681 I ggml_metal_init: has bfloat            = true
0.00.082.681 I ggml_metal_init: use bfloat            = true
0.00.082.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.683 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.123.569 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.577 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.599 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.777 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.779 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.779 I llama_new_context_with_model: graph nodes  = 967
0.00.124.779 I llama_new_context_with_model: graph splits = 2
0.00.124.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.965.960 I main: llama threadpool init, n_threads = 4
0.01.965.991 I 
0.01.966.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.966.012 I 
0.01.966.154 I sampler seed: 1234
0.01.966.159 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.966.193 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.966.194 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.966.195 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.03.061.444 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62943.26 tokens per second)
0.03.061.444 I llama_perf_context_print:        load time =    1956.07 ms
0.03.061.445 I llama_perf_context_print: prompt eval time =      33.47 ms /     7 tokens (    4.78 ms per token,   209.16 tokens per second)
0.03.061.446 I llama_perf_context_print:        eval time =    1058.96 ms /    63 runs   (   16.81 ms per token,    59.49 tokens per second)
0.03.061.446 I llama_perf_context_print:       total time =    1095.49 ms /    70 tokens
0.03.061.640 I ggml_metal_free: deallocating

real	0m3.080s
user	0m0.122s
sys	0m0.265s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.306 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.244 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.083 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.084 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.084 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.085 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.088 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.088 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.089 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.092 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.093 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.094 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.958 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.592 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.547 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.548 I llama_model_loader: - type  f32:  194 tensors
0.00.034.548 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.637 I llm_load_vocab: special tokens cache size = 25
0.00.069.759 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.762 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.763 I llm_load_print_meta: arch             = gptneox
0.00.069.763 I llm_load_print_meta: vocab type       = BPE
0.00.069.763 I llm_load_print_meta: n_vocab          = 50304
0.00.069.763 I llm_load_print_meta: n_merges         = 50009
0.00.069.763 I llm_load_print_meta: vocab_only       = 0
0.00.069.764 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.764 I llm_load_print_meta: n_embd           = 2048
0.00.069.764 I llm_load_print_meta: n_layer          = 24
0.00.069.767 I llm_load_print_meta: n_head           = 16
0.00.069.768 I llm_load_print_meta: n_head_kv        = 16
0.00.069.768 I llm_load_print_meta: n_rot            = 32
0.00.069.768 I llm_load_print_meta: n_swa            = 0
0.00.069.769 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.769 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.771 I llm_load_print_meta: n_gqa            = 1
0.00.069.772 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.773 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.773 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.774 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.774 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.774 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.774 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.775 I llm_load_print_meta: n_ff             = 8192
0.00.069.775 I llm_load_print_meta: n_expert         = 0
0.00.069.775 I llm_load_print_meta: n_expert_used    = 0
0.00.069.775 I llm_load_print_meta: causal attn      = 1
0.00.069.775 I llm_load_print_meta: pooling type     = 0
0.00.069.775 I llm_load_print_meta: rope type        = 2
0.00.069.777 I llm_load_print_meta: rope scaling     = linear
0.00.069.777 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.778 I llm_load_print_meta: freq_scale_train = 1
0.00.069.778 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.778 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.778 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.778 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.778 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.778 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.779 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.791 I llm_load_print_meta: model type       = 1.4B
0.00.069.791 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.792 I llm_load_print_meta: model params     = 1.41 B
0.00.069.792 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.792 I llm_load_print_meta: general.name     = 1.4B
0.00.069.793 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.793 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.793 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.793 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.793 I llm_load_print_meta: LF token         = 128 ''
0.00.069.794 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.794 I llm_load_print_meta: max token length = 1024
0.00.072.173 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.174 I llm_load_tensors: offloading output layer to GPU
0.00.072.174 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.184 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.072.185 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.073.203 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.204 I llama_new_context_with_model: n_ctx         = 128
0.00.073.204 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.073.205 I llama_new_context_with_model: n_batch       = 128
0.00.073.205 I llama_new_context_with_model: n_ubatch      = 128
0.00.073.205 I llama_new_context_with_model: flash_attn    = 0
0.00.073.205 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.206 I llama_new_context_with_model: freq_scale    = 1
0.00.073.206 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.073.206 I ggml_metal_init: allocating
0.00.073.209 I ggml_metal_init: found device: Apple M4
0.00.073.211 I ggml_metal_init: picking default device: Apple M4
0.00.073.758 I ggml_metal_init: using embedded metal library
0.00.076.193 I ggml_metal_init: GPU name:   Apple M4
0.00.076.194 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.195 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.195 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.195 I ggml_metal_init: simdgroup reduction   = true
0.00.076.195 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.195 I ggml_metal_init: has bfloat            = true
0.00.076.195 I ggml_metal_init: use bfloat            = true
0.00.076.196 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.144 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.086.149 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.086.163 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.129 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.087.130 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.087.130 I llama_new_context_with_model: graph nodes  = 967
0.00.087.130 I llama_new_context_with_model: graph splits = 2
0.00.087.142 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.019.360 I 
0.01.019.400 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.019.429 I perplexity: tokenizing the input ..
0.01.027.525 I perplexity: tokenization took 8.095 ms
0.01.027.528 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.149.900 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.151.165 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.151.175 I llama_perf_context_print:        load time =    1007.11 ms
0.01.151.176 I llama_perf_context_print: prompt eval time =     122.13 ms /   128 tokens (    0.95 ms per token,  1048.07 tokens per second)
0.01.151.177 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.151.178 I llama_perf_context_print:       total time =     131.82 ms /   129 tokens
0.01.151.469 I ggml_metal_free: deallocating

real	0m1.169s
user	0m0.095s
sys	0m0.171s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.012.896 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.178 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.183 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.193 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.194 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.194 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.195 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.196 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.197 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.051 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.165 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.166 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.167 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.167 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.167 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.168 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.168 I llama_model_loader: - type  f32:  194 tensors
0.00.030.169 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.169 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.534 I llm_load_vocab: special tokens cache size = 25
0.00.057.646 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.650 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.651 I llm_load_print_meta: arch             = gptneox
0.00.057.651 I llm_load_print_meta: vocab type       = BPE
0.00.057.651 I llm_load_print_meta: n_vocab          = 50304
0.00.057.651 I llm_load_print_meta: n_merges         = 50009
0.00.057.652 I llm_load_print_meta: vocab_only       = 0
0.00.057.654 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.654 I llm_load_print_meta: n_embd           = 2048
0.00.057.654 I llm_load_print_meta: n_layer          = 24
0.00.057.663 I llm_load_print_meta: n_head           = 16
0.00.057.664 I llm_load_print_meta: n_head_kv        = 16
0.00.057.664 I llm_load_print_meta: n_rot            = 32
0.00.057.664 I llm_load_print_meta: n_swa            = 0
0.00.057.665 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.665 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.666 I llm_load_print_meta: n_gqa            = 1
0.00.057.667 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.667 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.668 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.668 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.669 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.669 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.669 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.670 I llm_load_print_meta: n_ff             = 8192
0.00.057.670 I llm_load_print_meta: n_expert         = 0
0.00.057.670 I llm_load_print_meta: n_expert_used    = 0
0.00.057.670 I llm_load_print_meta: causal attn      = 1
0.00.057.670 I llm_load_print_meta: pooling type     = 0
0.00.057.670 I llm_load_print_meta: rope type        = 2
0.00.057.671 I llm_load_print_meta: rope scaling     = linear
0.00.057.671 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.671 I llm_load_print_meta: freq_scale_train = 1
0.00.057.671 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.672 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.672 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.672 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.672 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.672 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.672 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.685 I llm_load_print_meta: model type       = 1.4B
0.00.057.685 I llm_load_print_meta: model ftype      = Q4_0
0.00.057.686 I llm_load_print_meta: model params     = 1.41 B
0.00.057.686 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.057.687 I llm_load_print_meta: general.name     = 1.4B
0.00.057.687 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.687 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.687 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.687 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.688 I llm_load_print_meta: LF token         = 128 ''
0.00.057.688 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.688 I llm_load_print_meta: max token length = 1024
0.00.059.943 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.943 I llm_load_tensors: offloading output layer to GPU
0.00.059.944 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.954 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.059.956 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.060.921 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.921 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.922 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.922 I llama_new_context_with_model: n_batch       = 2048
0.00.060.922 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.922 I llama_new_context_with_model: flash_attn    = 0
0.00.060.923 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.923 I llama_new_context_with_model: freq_scale    = 1
0.00.060.923 I ggml_metal_init: allocating
0.00.060.931 I ggml_metal_init: found device: Apple M4
0.00.060.934 I ggml_metal_init: picking default device: Apple M4
0.00.061.670 I ggml_metal_init: using embedded metal library
0.00.063.840 I ggml_metal_init: GPU name:   Apple M4
0.00.063.842 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.843 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.843 I ggml_metal_init: simdgroup reduction   = true
0.00.063.843 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.843 I ggml_metal_init: has bfloat            = true
0.00.063.843 I ggml_metal_init: use bfloat            = true
0.00.063.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.844 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.617 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.626 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.647 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.884 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.886 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.886 I llama_new_context_with_model: graph nodes  = 967
0.00.106.887 I llama_new_context_with_model: graph splits = 2
0.00.106.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.842.582 I main: llama threadpool init, n_threads = 4
0.00.842.615 I 
0.00.842.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.842.634 I 
0.00.842.781 I sampler seed: 1234
0.00.842.786 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.842.826 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.842.830 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.842.830 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.523.308 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.523.309 I llama_perf_context_print:        load time =     829.68 ms
0.01.523.310 I llama_perf_context_print: prompt eval time =      32.69 ms /     7 tokens (    4.67 ms per token,   214.13 tokens per second)
0.01.523.311 I llama_perf_context_print:        eval time =     644.79 ms /    63 runs   (   10.23 ms per token,    97.71 tokens per second)
0.01.523.311 I llama_perf_context_print:       total time =     680.73 ms /    70 tokens
0.01.523.479 I ggml_metal_free: deallocating

real	0m1.543s
user	0m0.111s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.273 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.505 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.110 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.114 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.115 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.120 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.123 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.126 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.945 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.757 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.758 I llama_model_loader: - type  f32:  194 tensors
0.00.024.758 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.758 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.218 I llm_load_vocab: special tokens cache size = 25
0.00.051.213 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.215 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.216 I llm_load_print_meta: arch             = gptneox
0.00.051.216 I llm_load_print_meta: vocab type       = BPE
0.00.051.216 I llm_load_print_meta: n_vocab          = 50304
0.00.051.216 I llm_load_print_meta: n_merges         = 50009
0.00.051.217 I llm_load_print_meta: vocab_only       = 0
0.00.051.217 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.217 I llm_load_print_meta: n_embd           = 2048
0.00.051.217 I llm_load_print_meta: n_layer          = 24
0.00.051.219 I llm_load_print_meta: n_head           = 16
0.00.051.220 I llm_load_print_meta: n_head_kv        = 16
0.00.051.220 I llm_load_print_meta: n_rot            = 32
0.00.051.220 I llm_load_print_meta: n_swa            = 0
0.00.051.221 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.221 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.221 I llm_load_print_meta: n_gqa            = 1
0.00.051.222 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.223 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.223 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.224 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.224 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.224 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.224 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.225 I llm_load_print_meta: n_ff             = 8192
0.00.051.225 I llm_load_print_meta: n_expert         = 0
0.00.051.225 I llm_load_print_meta: n_expert_used    = 0
0.00.051.225 I llm_load_print_meta: causal attn      = 1
0.00.051.225 I llm_load_print_meta: pooling type     = 0
0.00.051.226 I llm_load_print_meta: rope type        = 2
0.00.051.226 I llm_load_print_meta: rope scaling     = linear
0.00.051.226 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.226 I llm_load_print_meta: freq_scale_train = 1
0.00.051.229 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.229 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.229 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.229 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.230 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.230 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.230 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.241 I llm_load_print_meta: model type       = 1.4B
0.00.051.242 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.242 I llm_load_print_meta: model params     = 1.41 B
0.00.051.243 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.243 I llm_load_print_meta: general.name     = 1.4B
0.00.051.243 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.244 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.244 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.244 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.244 I llm_load_print_meta: LF token         = 128 ''
0.00.051.245 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.245 I llm_load_print_meta: max token length = 1024
0.00.052.849 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.849 I llm_load_tensors: offloading output layer to GPU
0.00.052.849 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.859 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.860 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.712 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.713 I llama_new_context_with_model: n_ctx         = 128
0.00.053.713 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.713 I llama_new_context_with_model: n_batch       = 128
0.00.053.714 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.714 I llama_new_context_with_model: flash_attn    = 0
0.00.053.714 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.715 I llama_new_context_with_model: freq_scale    = 1
0.00.053.715 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.716 I ggml_metal_init: allocating
0.00.053.721 I ggml_metal_init: found device: Apple M4
0.00.053.723 I ggml_metal_init: picking default device: Apple M4
0.00.054.238 I ggml_metal_init: using embedded metal library
0.00.056.201 I ggml_metal_init: GPU name:   Apple M4
0.00.056.203 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.203 I ggml_metal_init: simdgroup reduction   = true
0.00.056.204 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.204 I ggml_metal_init: has bfloat            = true
0.00.056.204 I ggml_metal_init: use bfloat            = true
0.00.056.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.387 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.391 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.404 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.249 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.250 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.251 I llama_new_context_with_model: graph nodes  = 967
0.00.066.251 I llama_new_context_with_model: graph splits = 2
0.00.066.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.510 I 
0.00.644.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.644.534 I perplexity: tokenizing the input ..
0.00.652.260 I perplexity: tokenization took 7.724 ms
0.00.652.263 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.198 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.776.434 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.776.450 I llama_perf_context_print:        load time =     634.00 ms
0.00.776.451 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1043.06 tokens per second)
0.00.776.452 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.452 I llama_perf_context_print:       total time =     131.94 ms /   129 tokens
0.00.776.870 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.075s
sys	0m0.113s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.968 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.384 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.393 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.394 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.394 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.395 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.395 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.396 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.396 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.397 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.397 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.397 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.397 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.398 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.400 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.400 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.400 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.925 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.271 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.326 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.328 I llama_model_loader: - type  f32:  194 tensors
0.00.035.329 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.329 I llama_model_loader: - type q6_K:    1 tensors
0.00.067.878 I llm_load_vocab: special tokens cache size = 25
0.00.078.258 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.262 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.262 I llm_load_print_meta: arch             = gptneox
0.00.078.263 I llm_load_print_meta: vocab type       = BPE
0.00.078.263 I llm_load_print_meta: n_vocab          = 50304
0.00.078.263 I llm_load_print_meta: n_merges         = 50009
0.00.078.263 I llm_load_print_meta: vocab_only       = 0
0.00.078.264 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.264 I llm_load_print_meta: n_embd           = 2048
0.00.078.264 I llm_load_print_meta: n_layer          = 24
0.00.078.268 I llm_load_print_meta: n_head           = 16
0.00.078.269 I llm_load_print_meta: n_head_kv        = 16
0.00.078.269 I llm_load_print_meta: n_rot            = 32
0.00.078.269 I llm_load_print_meta: n_swa            = 0
0.00.078.270 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.270 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.271 I llm_load_print_meta: n_gqa            = 1
0.00.078.272 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.273 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.274 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.274 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.274 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.275 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.275 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.276 I llm_load_print_meta: n_ff             = 8192
0.00.078.276 I llm_load_print_meta: n_expert         = 0
0.00.078.276 I llm_load_print_meta: n_expert_used    = 0
0.00.078.277 I llm_load_print_meta: causal attn      = 1
0.00.078.277 I llm_load_print_meta: pooling type     = 0
0.00.078.277 I llm_load_print_meta: rope type        = 2
0.00.078.277 I llm_load_print_meta: rope scaling     = linear
0.00.078.278 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.278 I llm_load_print_meta: freq_scale_train = 1
0.00.078.279 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.279 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.279 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.279 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.279 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.280 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.280 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.287 I llm_load_print_meta: model type       = 1.4B
0.00.078.287 I llm_load_print_meta: model ftype      = Q4_1
0.00.078.288 I llm_load_print_meta: model params     = 1.41 B
0.00.078.288 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.078.289 I llm_load_print_meta: general.name     = 1.4B
0.00.078.289 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.289 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.290 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.290 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.290 I llm_load_print_meta: LF token         = 128 ''
0.00.078.290 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.291 I llm_load_print_meta: max token length = 1024
0.00.080.637 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.637 I llm_load_tensors: offloading output layer to GPU
0.00.080.637 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.643 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.080.643 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.082.092 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.093 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.093 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.093 I llama_new_context_with_model: n_batch       = 2048
0.00.082.094 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.094 I llama_new_context_with_model: flash_attn    = 0
0.00.082.095 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.095 I llama_new_context_with_model: freq_scale    = 1
0.00.082.095 I ggml_metal_init: allocating
0.00.082.100 I ggml_metal_init: found device: Apple M4
0.00.082.103 I ggml_metal_init: picking default device: Apple M4
0.00.082.910 I ggml_metal_init: using embedded metal library
0.00.085.994 I ggml_metal_init: GPU name:   Apple M4
0.00.085.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.997 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.998 I ggml_metal_init: simdgroup reduction   = true
0.00.085.998 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.998 I ggml_metal_init: has bfloat            = true
0.00.085.999 I ggml_metal_init: use bfloat            = true
0.00.085.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.511 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.519 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.536 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.473 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.474 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.474 I llama_new_context_with_model: graph nodes  = 967
0.00.118.475 I llama_new_context_with_model: graph splits = 2
0.00.118.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.156.141 I main: llama threadpool init, n_threads = 4
0.01.156.170 I 
0.01.156.191 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.156.192 I 
0.01.156.326 I sampler seed: 1234
0.01.156.330 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.156.339 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.156.340 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.156.340 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.884.893 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63167.26 tokens per second)
0.01.884.893 I llama_perf_context_print:        load time =    1147.17 ms
0.01.884.894 I llama_perf_context_print: prompt eval time =      32.72 ms /     7 tokens (    4.67 ms per token,   213.96 tokens per second)
0.01.884.895 I llama_perf_context_print:        eval time =     692.92 ms /    63 runs   (   11.00 ms per token,    90.92 tokens per second)
0.01.884.895 I llama_perf_context_print:       total time =     728.75 ms /    70 tokens
0.01.885.073 I ggml_metal_free: deallocating

real	0m1.916s
user	0m0.133s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.744 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.655 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.659 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.667 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.668 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.668 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.668 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.669 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.670 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.671 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.671 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.674 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.674 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.675 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.430 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.487 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.250 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.251 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.252 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.252 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.252 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.253 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.253 I llama_model_loader: - type  f32:  194 tensors
0.00.023.254 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.254 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.066 I llm_load_vocab: special tokens cache size = 25
0.00.050.065 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.067 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.068 I llm_load_print_meta: arch             = gptneox
0.00.050.068 I llm_load_print_meta: vocab type       = BPE
0.00.050.068 I llm_load_print_meta: n_vocab          = 50304
0.00.050.068 I llm_load_print_meta: n_merges         = 50009
0.00.050.068 I llm_load_print_meta: vocab_only       = 0
0.00.050.069 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.069 I llm_load_print_meta: n_embd           = 2048
0.00.050.069 I llm_load_print_meta: n_layer          = 24
0.00.050.072 I llm_load_print_meta: n_head           = 16
0.00.050.073 I llm_load_print_meta: n_head_kv        = 16
0.00.050.073 I llm_load_print_meta: n_rot            = 32
0.00.050.073 I llm_load_print_meta: n_swa            = 0
0.00.050.073 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.073 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.075 I llm_load_print_meta: n_gqa            = 1
0.00.050.076 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.076 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.077 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.077 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.077 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.078 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.078 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.078 I llm_load_print_meta: n_ff             = 8192
0.00.050.079 I llm_load_print_meta: n_expert         = 0
0.00.050.079 I llm_load_print_meta: n_expert_used    = 0
0.00.050.079 I llm_load_print_meta: causal attn      = 1
0.00.050.079 I llm_load_print_meta: pooling type     = 0
0.00.050.079 I llm_load_print_meta: rope type        = 2
0.00.050.079 I llm_load_print_meta: rope scaling     = linear
0.00.050.080 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.082 I llm_load_print_meta: freq_scale_train = 1
0.00.050.082 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.082 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.082 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.082 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.082 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.083 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.083 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.095 I llm_load_print_meta: model type       = 1.4B
0.00.050.095 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.095 I llm_load_print_meta: model params     = 1.41 B
0.00.050.096 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.096 I llm_load_print_meta: general.name     = 1.4B
0.00.050.096 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.096 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: LF token         = 128 ''
0.00.050.097 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.097 I llm_load_print_meta: max token length = 1024
0.00.052.026 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.027 I llm_load_tensors: offloading output layer to GPU
0.00.052.027 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.038 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.039 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.035 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.036 I llama_new_context_with_model: n_ctx         = 128
0.00.053.036 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.036 I llama_new_context_with_model: n_batch       = 128
0.00.053.036 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.036 I llama_new_context_with_model: flash_attn    = 0
0.00.053.037 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.037 I llama_new_context_with_model: freq_scale    = 1
0.00.053.037 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.038 I ggml_metal_init: allocating
0.00.053.041 I ggml_metal_init: found device: Apple M4
0.00.053.042 I ggml_metal_init: picking default device: Apple M4
0.00.053.571 I ggml_metal_init: using embedded metal library
0.00.055.499 I ggml_metal_init: GPU name:   Apple M4
0.00.055.500 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.500 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.501 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.501 I ggml_metal_init: simdgroup reduction   = true
0.00.055.501 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.501 I ggml_metal_init: has bfloat            = true
0.00.055.501 I ggml_metal_init: use bfloat            = true
0.00.055.502 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.504 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.424 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.427 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.444 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.319 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.321 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.321 I llama_new_context_with_model: graph nodes  = 967
0.00.065.321 I llama_new_context_with_model: graph splits = 2
0.00.065.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.941 I 
0.00.707.971 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.707.998 I perplexity: tokenizing the input ..
0.00.715.764 I perplexity: tokenization took 7.766 ms
0.00.715.768 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.921 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.840.168 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.840.195 I llama_perf_context_print:        load time =     699.19 ms
0.00.840.196 I llama_perf_context_print: prompt eval time =     122.91 ms /   128 tokens (    0.96 ms per token,  1041.40 tokens per second)
0.00.840.197 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.198 I llama_perf_context_print:       total time =     132.25 ms /   129 tokens
0.00.840.634 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.075s
sys	0m0.119s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.020.121 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.047.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.047.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.062 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.063 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.069 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.070 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.070 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.071 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.617 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.547 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.633 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.063.642 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.642 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.643 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.644 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.644 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.063.646 I llama_model_loader: - type  f32:  194 tensors
0.00.063.646 I llama_model_loader: - type q5_0:   97 tensors
0.00.063.647 I llama_model_loader: - type q6_K:    1 tensors
0.00.104.499 I llm_load_vocab: special tokens cache size = 25
0.00.113.099 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.113.103 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.113.103 I llm_load_print_meta: arch             = gptneox
0.00.113.103 I llm_load_print_meta: vocab type       = BPE
0.00.113.104 I llm_load_print_meta: n_vocab          = 50304
0.00.113.104 I llm_load_print_meta: n_merges         = 50009
0.00.113.104 I llm_load_print_meta: vocab_only       = 0
0.00.113.104 I llm_load_print_meta: n_ctx_train      = 2048
0.00.113.104 I llm_load_print_meta: n_embd           = 2048
0.00.113.105 I llm_load_print_meta: n_layer          = 24
0.00.113.107 I llm_load_print_meta: n_head           = 16
0.00.113.108 I llm_load_print_meta: n_head_kv        = 16
0.00.113.108 I llm_load_print_meta: n_rot            = 32
0.00.113.109 I llm_load_print_meta: n_swa            = 0
0.00.113.111 I llm_load_print_meta: n_embd_head_k    = 128
0.00.113.111 I llm_load_print_meta: n_embd_head_v    = 128
0.00.113.112 I llm_load_print_meta: n_gqa            = 1
0.00.113.113 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.113.113 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.113.114 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.113.116 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.113.116 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.113.116 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.113.116 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.113.117 I llm_load_print_meta: n_ff             = 8192
0.00.113.117 I llm_load_print_meta: n_expert         = 0
0.00.113.117 I llm_load_print_meta: n_expert_used    = 0
0.00.113.118 I llm_load_print_meta: causal attn      = 1
0.00.113.118 I llm_load_print_meta: pooling type     = 0
0.00.113.118 I llm_load_print_meta: rope type        = 2
0.00.113.118 I llm_load_print_meta: rope scaling     = linear
0.00.113.119 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.113.119 I llm_load_print_meta: freq_scale_train = 1
0.00.113.119 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.113.120 I llm_load_print_meta: rope_finetuned   = unknown
0.00.113.120 I llm_load_print_meta: ssm_d_conv       = 0
0.00.113.121 I llm_load_print_meta: ssm_d_inner      = 0
0.00.113.122 I llm_load_print_meta: ssm_d_state      = 0
0.00.113.122 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.113.122 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.113.128 I llm_load_print_meta: model type       = 1.4B
0.00.113.129 I llm_load_print_meta: model ftype      = Q5_0
0.00.113.129 I llm_load_print_meta: model params     = 1.41 B
0.00.113.130 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.113.130 I llm_load_print_meta: general.name     = 1.4B
0.00.113.131 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.113.131 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.113.131 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.113.131 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.113.131 I llm_load_print_meta: LF token         = 128 ''
0.00.113.132 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.113.132 I llm_load_print_meta: max token length = 1024
0.00.115.061 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.115.061 I llm_load_tensors: offloading output layer to GPU
0.00.115.062 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.115.067 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.115.067 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.116.206 I llama_new_context_with_model: n_seq_max     = 1
0.00.116.207 I llama_new_context_with_model: n_ctx         = 2048
0.00.116.208 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.116.208 I llama_new_context_with_model: n_batch       = 2048
0.00.116.208 I llama_new_context_with_model: n_ubatch      = 512
0.00.116.208 I llama_new_context_with_model: flash_attn    = 0
0.00.116.209 I llama_new_context_with_model: freq_base     = 10000.0
0.00.116.209 I llama_new_context_with_model: freq_scale    = 1
0.00.116.210 I ggml_metal_init: allocating
0.00.116.216 I ggml_metal_init: found device: Apple M4
0.00.116.219 I ggml_metal_init: picking default device: Apple M4
0.00.116.879 I ggml_metal_init: using embedded metal library
0.00.121.742 I ggml_metal_init: GPU name:   Apple M4
0.00.121.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.121.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.121.745 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.121.745 I ggml_metal_init: simdgroup reduction   = true
0.00.121.746 I ggml_metal_init: simdgroup matrix mul. = true
0.00.121.746 I ggml_metal_init: has bfloat            = true
0.00.121.746 I ggml_metal_init: use bfloat            = true
0.00.121.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.121.747 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.152.438 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.152.447 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.152.465 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.153.373 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.153.374 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.153.374 I llama_new_context_with_model: graph nodes  = 967
0.00.153.375 I llama_new_context_with_model: graph splits = 2
0.00.153.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.256.890 I main: llama threadpool init, n_threads = 4
0.01.256.949 I 
0.01.256.989 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.256.989 I 
0.01.257.344 I sampler seed: 1234
0.01.257.366 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.257.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.257.432 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.257.432 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.02.053.362 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.02.053.363 I llama_perf_context_print:        load time =    1236.76 ms
0.02.053.365 I llama_perf_context_print: prompt eval time =      37.23 ms /     7 tokens (    5.32 ms per token,   188.00 tokens per second)
0.02.053.365 I llama_perf_context_print:        eval time =     755.67 ms /    63 runs   (   11.99 ms per token,    83.37 tokens per second)
0.02.053.366 I llama_perf_context_print:       total time =     796.48 ms /    70 tokens
0.02.053.548 I ggml_metal_free: deallocating

real	0m2.091s
user	0m0.156s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.102 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.715 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.719 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.721 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.722 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.722 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.722 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.723 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.725 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.726 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.726 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.726 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.727 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.727 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.730 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.730 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.730 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.429 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.430 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.430 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.430 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.431 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.431 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.431 I llama_model_loader: - type  f32:  194 tensors
0.00.024.431 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.432 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.202 I llm_load_vocab: special tokens cache size = 25
0.00.051.231 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.234 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.234 I llm_load_print_meta: arch             = gptneox
0.00.051.235 I llm_load_print_meta: vocab type       = BPE
0.00.051.235 I llm_load_print_meta: n_vocab          = 50304
0.00.051.235 I llm_load_print_meta: n_merges         = 50009
0.00.051.235 I llm_load_print_meta: vocab_only       = 0
0.00.051.235 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.235 I llm_load_print_meta: n_embd           = 2048
0.00.051.236 I llm_load_print_meta: n_layer          = 24
0.00.051.238 I llm_load_print_meta: n_head           = 16
0.00.051.239 I llm_load_print_meta: n_head_kv        = 16
0.00.051.239 I llm_load_print_meta: n_rot            = 32
0.00.051.240 I llm_load_print_meta: n_swa            = 0
0.00.051.240 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.240 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.243 I llm_load_print_meta: n_gqa            = 1
0.00.051.243 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.244 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.245 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.245 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.252 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.254 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.255 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.257 I llm_load_print_meta: n_ff             = 8192
0.00.051.257 I llm_load_print_meta: n_expert         = 0
0.00.051.257 I llm_load_print_meta: n_expert_used    = 0
0.00.051.257 I llm_load_print_meta: causal attn      = 1
0.00.051.257 I llm_load_print_meta: pooling type     = 0
0.00.051.258 I llm_load_print_meta: rope type        = 2
0.00.051.259 I llm_load_print_meta: rope scaling     = linear
0.00.051.260 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.262 I llm_load_print_meta: freq_scale_train = 1
0.00.051.262 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.262 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.262 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.262 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.262 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.263 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.263 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.275 I llm_load_print_meta: model type       = 1.4B
0.00.051.275 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.275 I llm_load_print_meta: model params     = 1.41 B
0.00.051.276 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.276 I llm_load_print_meta: general.name     = 1.4B
0.00.051.276 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.277 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.277 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.277 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.277 I llm_load_print_meta: LF token         = 128 ''
0.00.051.279 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.279 I llm_load_print_meta: max token length = 1024
0.00.053.216 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.216 I llm_load_tensors: offloading output layer to GPU
0.00.053.216 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.226 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.227 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.145 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.146 I llama_new_context_with_model: n_ctx         = 128
0.00.054.146 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.146 I llama_new_context_with_model: n_batch       = 128
0.00.054.146 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.147 I llama_new_context_with_model: flash_attn    = 0
0.00.054.147 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.147 I llama_new_context_with_model: freq_scale    = 1
0.00.054.147 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.148 I ggml_metal_init: allocating
0.00.054.151 I ggml_metal_init: found device: Apple M4
0.00.054.153 I ggml_metal_init: picking default device: Apple M4
0.00.054.704 I ggml_metal_init: using embedded metal library
0.00.056.662 I ggml_metal_init: GPU name:   Apple M4
0.00.056.664 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.664 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.665 I ggml_metal_init: simdgroup reduction   = true
0.00.056.665 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.665 I ggml_metal_init: has bfloat            = true
0.00.056.665 I ggml_metal_init: use bfloat            = true
0.00.056.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.666 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.581 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.584 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.597 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.471 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.472 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.473 I llama_new_context_with_model: graph nodes  = 967
0.00.066.473 I llama_new_context_with_model: graph splits = 2
0.00.066.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.776 I 
0.00.761.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.761.844 I perplexity: tokenizing the input ..
0.00.769.615 I perplexity: tokenization took 7.77 ms
0.00.769.617 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.905.004 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.906.329 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.906.346 I llama_perf_context_print:        load time =     751.67 ms
0.00.906.347 I llama_perf_context_print: prompt eval time =     135.16 ms /   128 tokens (    1.06 ms per token,   947.03 tokens per second)
0.00.906.348 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.348 I llama_perf_context_print:       total time =     144.57 ms /   129 tokens
0.00.906.771 I ggml_metal_free: deallocating

real	0m0.921s
user	0m0.075s
sys	0m0.130s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.800 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.208 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.211 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.216 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.216 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.216 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.217 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.218 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.218 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.218 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.219 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.219 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.219 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.220 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.221 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.222 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.222 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.023 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.080 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.983 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.985 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.985 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.985 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.985 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.986 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.032.986 I llama_model_loader: - type  f32:  194 tensors
0.00.032.986 I llama_model_loader: - type q5_1:   97 tensors
0.00.032.987 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.137 I llm_load_vocab: special tokens cache size = 25
0.00.062.124 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.127 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.127 I llm_load_print_meta: arch             = gptneox
0.00.062.127 I llm_load_print_meta: vocab type       = BPE
0.00.062.127 I llm_load_print_meta: n_vocab          = 50304
0.00.062.127 I llm_load_print_meta: n_merges         = 50009
0.00.062.128 I llm_load_print_meta: vocab_only       = 0
0.00.062.128 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.128 I llm_load_print_meta: n_embd           = 2048
0.00.062.128 I llm_load_print_meta: n_layer          = 24
0.00.062.130 I llm_load_print_meta: n_head           = 16
0.00.062.131 I llm_load_print_meta: n_head_kv        = 16
0.00.062.131 I llm_load_print_meta: n_rot            = 32
0.00.062.131 I llm_load_print_meta: n_swa            = 0
0.00.062.132 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.132 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.132 I llm_load_print_meta: n_gqa            = 1
0.00.062.133 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.133 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.134 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.134 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.134 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.135 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.135 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.135 I llm_load_print_meta: n_ff             = 8192
0.00.062.136 I llm_load_print_meta: n_expert         = 0
0.00.062.136 I llm_load_print_meta: n_expert_used    = 0
0.00.062.136 I llm_load_print_meta: causal attn      = 1
0.00.062.136 I llm_load_print_meta: pooling type     = 0
0.00.062.136 I llm_load_print_meta: rope type        = 2
0.00.062.136 I llm_load_print_meta: rope scaling     = linear
0.00.062.137 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.137 I llm_load_print_meta: freq_scale_train = 1
0.00.062.137 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.137 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.137 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.137 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.137 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.138 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.138 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.144 I llm_load_print_meta: model type       = 1.4B
0.00.062.144 I llm_load_print_meta: model ftype      = Q5_1
0.00.062.144 I llm_load_print_meta: model params     = 1.41 B
0.00.062.145 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.062.145 I llm_load_print_meta: general.name     = 1.4B
0.00.062.145 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.145 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.146 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.146 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.146 I llm_load_print_meta: LF token         = 128 ''
0.00.062.146 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.146 I llm_load_print_meta: max token length = 1024
0.00.063.737 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.738 I llm_load_tensors: offloading output layer to GPU
0.00.063.738 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.747 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.063.749 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.064.674 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.675 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.675 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.675 I llama_new_context_with_model: n_batch       = 2048
0.00.064.675 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.676 I llama_new_context_with_model: flash_attn    = 0
0.00.064.676 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.676 I llama_new_context_with_model: freq_scale    = 1
0.00.064.677 I ggml_metal_init: allocating
0.00.064.682 I ggml_metal_init: found device: Apple M4
0.00.064.684 I ggml_metal_init: picking default device: Apple M4
0.00.065.224 I ggml_metal_init: using embedded metal library
0.00.067.366 I ggml_metal_init: GPU name:   Apple M4
0.00.067.368 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.369 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.369 I ggml_metal_init: simdgroup reduction   = true
0.00.067.369 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.369 I ggml_metal_init: has bfloat            = true
0.00.067.369 I ggml_metal_init: use bfloat            = true
0.00.067.370 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.327 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.332 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.349 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.335 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.336 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.337 I llama_new_context_with_model: graph nodes  = 967
0.00.095.337 I llama_new_context_with_model: graph splits = 2
0.00.095.349 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.101.087 I main: llama threadpool init, n_threads = 4
0.01.101.119 I 
0.01.101.135 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.101.135 I 
0.01.101.360 I sampler seed: 1234
0.01.101.363 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.101.384 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.101.385 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.101.386 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.945.584 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.945.584 I llama_perf_context_print:        load time =    1092.28 ms
0.01.945.585 I llama_perf_context_print: prompt eval time =      36.77 ms /     7 tokens (    5.25 ms per token,   190.36 tokens per second)
0.01.945.586 I llama_perf_context_print:        eval time =     804.51 ms /    63 runs   (   12.77 ms per token,    78.31 tokens per second)
0.01.945.586 I llama_perf_context_print:       total time =     844.50 ms /    70 tokens
0.01.945.775 I ggml_metal_free: deallocating

real	0m1.958s
user	0m0.114s
sys	0m0.247s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.612 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.239 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.242 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.243 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.243 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.243 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.245 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.246 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.246 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.246 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.247 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.249 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.249 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.249 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.012 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.814 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.816 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.817 I llama_model_loader: - type  f32:  194 tensors
0.00.022.818 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.818 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.647 I llm_load_vocab: special tokens cache size = 25
0.00.048.561 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.565 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.565 I llm_load_print_meta: arch             = gptneox
0.00.048.565 I llm_load_print_meta: vocab type       = BPE
0.00.048.565 I llm_load_print_meta: n_vocab          = 50304
0.00.048.570 I llm_load_print_meta: n_merges         = 50009
0.00.048.570 I llm_load_print_meta: vocab_only       = 0
0.00.048.570 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.570 I llm_load_print_meta: n_embd           = 2048
0.00.048.570 I llm_load_print_meta: n_layer          = 24
0.00.048.575 I llm_load_print_meta: n_head           = 16
0.00.048.575 I llm_load_print_meta: n_head_kv        = 16
0.00.048.576 I llm_load_print_meta: n_rot            = 32
0.00.048.576 I llm_load_print_meta: n_swa            = 0
0.00.048.576 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.576 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.577 I llm_load_print_meta: n_gqa            = 1
0.00.048.577 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.578 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.579 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.579 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.579 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.580 I llm_load_print_meta: n_ff             = 8192
0.00.048.581 I llm_load_print_meta: n_expert         = 0
0.00.048.581 I llm_load_print_meta: n_expert_used    = 0
0.00.048.581 I llm_load_print_meta: causal attn      = 1
0.00.048.581 I llm_load_print_meta: pooling type     = 0
0.00.048.581 I llm_load_print_meta: rope type        = 2
0.00.048.582 I llm_load_print_meta: rope scaling     = linear
0.00.048.582 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.582 I llm_load_print_meta: freq_scale_train = 1
0.00.048.582 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.583 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.583 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.583 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.583 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.583 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.583 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.595 I llm_load_print_meta: model type       = 1.4B
0.00.048.596 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.597 I llm_load_print_meta: model params     = 1.41 B
0.00.048.597 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.598 I llm_load_print_meta: general.name     = 1.4B
0.00.048.598 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.598 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.598 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.598 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.599 I llm_load_print_meta: LF token         = 128 ''
0.00.048.599 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.599 I llm_load_print_meta: max token length = 1024
0.00.050.564 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.564 I llm_load_tensors: offloading output layer to GPU
0.00.050.564 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.575 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.576 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.506 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.506 I llama_new_context_with_model: n_ctx         = 128
0.00.051.507 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.507 I llama_new_context_with_model: n_batch       = 128
0.00.051.507 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.507 I llama_new_context_with_model: flash_attn    = 0
0.00.051.508 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.508 I llama_new_context_with_model: freq_scale    = 1
0.00.051.508 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.509 I ggml_metal_init: allocating
0.00.051.514 I ggml_metal_init: found device: Apple M4
0.00.051.521 I ggml_metal_init: picking default device: Apple M4
0.00.052.081 I ggml_metal_init: using embedded metal library
0.00.054.022 I ggml_metal_init: GPU name:   Apple M4
0.00.054.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.024 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.025 I ggml_metal_init: simdgroup reduction   = true
0.00.054.025 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.025 I ggml_metal_init: has bfloat            = true
0.00.054.025 I ggml_metal_init: use bfloat            = true
0.00.054.025 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.966 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.970 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.985 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.878 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.880 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.880 I llama_new_context_with_model: graph nodes  = 967
0.00.063.880 I llama_new_context_with_model: graph splits = 2
0.00.063.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.699 I 
0.00.778.733 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.778.761 I perplexity: tokenizing the input ..
0.00.786.829 I perplexity: tokenization took 8.067 ms
0.00.786.833 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.921.877 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.923.198 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.923.209 I llama_perf_context_print:        load time =     770.08 ms
0.00.923.210 I llama_perf_context_print: prompt eval time =     134.81 ms /   128 tokens (    1.05 ms per token,   949.47 tokens per second)
0.00.923.211 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.923.211 I llama_perf_context_print:       total time =     144.51 ms /   129 tokens
0.00.923.623 I ggml_metal_free: deallocating

real	0m0.936s
user	0m0.074s
sys	0m0.146s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.018.623 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.038.269 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.271 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.271 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.272 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.275 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.276 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.277 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.278 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.285 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.630 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.152 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.154 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.154 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.155 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.155 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.155 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.050.156 I llama_model_loader: - type  f32:  194 tensors
0.00.050.156 I llama_model_loader: - type q2_K:   49 tensors
0.00.050.157 I llama_model_loader: - type q3_K:   48 tensors
0.00.050.157 I llama_model_loader: - type q6_K:    1 tensors
0.00.088.229 I llm_load_vocab: special tokens cache size = 25
0.00.098.191 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.195 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.195 I llm_load_print_meta: arch             = gptneox
0.00.098.196 I llm_load_print_meta: vocab type       = BPE
0.00.098.196 I llm_load_print_meta: n_vocab          = 50304
0.00.098.196 I llm_load_print_meta: n_merges         = 50009
0.00.098.196 I llm_load_print_meta: vocab_only       = 0
0.00.098.197 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.197 I llm_load_print_meta: n_embd           = 2048
0.00.098.197 I llm_load_print_meta: n_layer          = 24
0.00.098.200 I llm_load_print_meta: n_head           = 16
0.00.098.201 I llm_load_print_meta: n_head_kv        = 16
0.00.098.201 I llm_load_print_meta: n_rot            = 32
0.00.098.202 I llm_load_print_meta: n_swa            = 0
0.00.098.202 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.202 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.205 I llm_load_print_meta: n_gqa            = 1
0.00.098.206 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.207 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.208 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.208 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.209 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.209 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.209 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.210 I llm_load_print_meta: n_ff             = 8192
0.00.098.210 I llm_load_print_meta: n_expert         = 0
0.00.098.212 I llm_load_print_meta: n_expert_used    = 0
0.00.098.213 I llm_load_print_meta: causal attn      = 1
0.00.098.213 I llm_load_print_meta: pooling type     = 0
0.00.098.214 I llm_load_print_meta: rope type        = 2
0.00.098.214 I llm_load_print_meta: rope scaling     = linear
0.00.098.214 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.215 I llm_load_print_meta: freq_scale_train = 1
0.00.098.215 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.215 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.215 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.215 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.216 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.216 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.216 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.228 I llm_load_print_meta: model type       = 1.4B
0.00.098.228 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.098.229 I llm_load_print_meta: model params     = 1.41 B
0.00.098.230 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.098.231 I llm_load_print_meta: general.name     = 1.4B
0.00.098.231 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.231 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.231 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.232 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.232 I llm_load_print_meta: LF token         = 128 ''
0.00.098.232 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.232 I llm_load_print_meta: max token length = 1024
0.00.100.419 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.420 I llm_load_tensors: offloading output layer to GPU
0.00.100.420 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.430 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.100.432 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.101.653 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.654 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.655 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.655 I llama_new_context_with_model: n_batch       = 2048
0.00.101.655 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.655 I llama_new_context_with_model: flash_attn    = 0
0.00.101.656 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.656 I llama_new_context_with_model: freq_scale    = 1
0.00.101.657 I ggml_metal_init: allocating
0.00.101.666 I ggml_metal_init: found device: Apple M4
0.00.101.668 I ggml_metal_init: picking default device: Apple M4
0.00.102.391 I ggml_metal_init: using embedded metal library
0.00.105.220 I ggml_metal_init: GPU name:   Apple M4
0.00.105.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.105.223 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.105.223 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.105.224 I ggml_metal_init: simdgroup reduction   = true
0.00.105.224 I ggml_metal_init: simdgroup matrix mul. = true
0.00.105.226 I ggml_metal_init: has bfloat            = true
0.00.105.226 I ggml_metal_init: use bfloat            = true
0.00.105.231 I ggml_metal_init: hasUnifiedMemory      = true
0.00.105.233 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.140.846 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.140.851 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.140.870 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.141.805 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.141.806 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.141.807 I llama_new_context_with_model: graph nodes  = 967
0.00.141.807 I llama_new_context_with_model: graph splits = 2
0.00.141.820 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.932.220 I main: llama threadpool init, n_threads = 4
0.00.932.256 I 
0.00.932.279 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.932.279 I 
0.00.932.624 I sampler seed: 1234
0.00.932.628 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.932.652 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.932.654 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.932.654 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.616.031 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.616.031 I llama_perf_context_print:        load time =     913.59 ms
0.01.616.032 I llama_perf_context_print: prompt eval time =      36.10 ms /     7 tokens (    5.16 ms per token,   193.91 tokens per second)
0.01.616.033 I llama_perf_context_print:        eval time =     644.48 ms /    63 runs   (   10.23 ms per token,    97.75 tokens per second)
0.01.616.033 I llama_perf_context_print:       total time =     683.81 ms /    70 tokens
0.01.616.208 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.143s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.809 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.549 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.550 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.551 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.552 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.553 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.553 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.433 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.470 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.428 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.429 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.429 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.430 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.430 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.430 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.431 I llama_model_loader: - type  f32:  194 tensors
0.00.024.431 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.431 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.432 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.090 I llm_load_vocab: special tokens cache size = 25
0.00.051.014 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.017 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.018 I llm_load_print_meta: arch             = gptneox
0.00.051.018 I llm_load_print_meta: vocab type       = BPE
0.00.051.018 I llm_load_print_meta: n_vocab          = 50304
0.00.051.018 I llm_load_print_meta: n_merges         = 50009
0.00.051.018 I llm_load_print_meta: vocab_only       = 0
0.00.051.019 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.019 I llm_load_print_meta: n_embd           = 2048
0.00.051.019 I llm_load_print_meta: n_layer          = 24
0.00.051.022 I llm_load_print_meta: n_head           = 16
0.00.051.023 I llm_load_print_meta: n_head_kv        = 16
0.00.051.023 I llm_load_print_meta: n_rot            = 32
0.00.051.023 I llm_load_print_meta: n_swa            = 0
0.00.051.023 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.024 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.024 I llm_load_print_meta: n_gqa            = 1
0.00.051.025 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.026 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.026 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.027 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.027 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.027 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.027 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.028 I llm_load_print_meta: n_ff             = 8192
0.00.051.028 I llm_load_print_meta: n_expert         = 0
0.00.051.031 I llm_load_print_meta: n_expert_used    = 0
0.00.051.031 I llm_load_print_meta: causal attn      = 1
0.00.051.031 I llm_load_print_meta: pooling type     = 0
0.00.051.031 I llm_load_print_meta: rope type        = 2
0.00.051.031 I llm_load_print_meta: rope scaling     = linear
0.00.051.032 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.032 I llm_load_print_meta: freq_scale_train = 1
0.00.051.032 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.032 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.032 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.033 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.033 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.033 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.033 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.040 I llm_load_print_meta: model type       = 1.4B
0.00.051.040 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.040 I llm_load_print_meta: model params     = 1.41 B
0.00.051.041 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.041 I llm_load_print_meta: general.name     = 1.4B
0.00.051.041 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.041 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.041 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.042 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.042 I llm_load_print_meta: LF token         = 128 ''
0.00.051.042 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.042 I llm_load_print_meta: max token length = 1024
0.00.052.733 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.733 I llm_load_tensors: offloading output layer to GPU
0.00.052.733 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.738 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.740 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.639 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.640 I llama_new_context_with_model: n_ctx         = 128
0.00.053.640 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.640 I llama_new_context_with_model: n_batch       = 128
0.00.053.640 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.641 I llama_new_context_with_model: flash_attn    = 0
0.00.053.641 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.641 I llama_new_context_with_model: freq_scale    = 1
0.00.053.641 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.642 I ggml_metal_init: allocating
0.00.053.645 I ggml_metal_init: found device: Apple M4
0.00.053.646 I ggml_metal_init: picking default device: Apple M4
0.00.054.173 I ggml_metal_init: using embedded metal library
0.00.056.103 I ggml_metal_init: GPU name:   Apple M4
0.00.056.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.105 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.105 I ggml_metal_init: simdgroup reduction   = true
0.00.056.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.106 I ggml_metal_init: has bfloat            = true
0.00.056.106 I ggml_metal_init: use bfloat            = true
0.00.056.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.053 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.056 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.071 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.957 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.958 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.958 I llama_new_context_with_model: graph nodes  = 967
0.00.065.958 I llama_new_context_with_model: graph splits = 2
0.00.065.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.514.518 I 
0.00.514.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.514.593 I perplexity: tokenizing the input ..
0.00.522.519 I perplexity: tokenization took 7.926 ms
0.00.522.528 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.654.694 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.655.946 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.655.963 I llama_perf_context_print:        load time =     504.70 ms
0.00.655.964 I llama_perf_context_print: prompt eval time =     131.94 ms /   128 tokens (    1.03 ms per token,   970.15 tokens per second)
0.00.655.965 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.966 I llama_perf_context_print:       total time =     141.46 ms /   129 tokens
0.00.656.362 I ggml_metal_free: deallocating

real	0m0.670s
user	0m0.075s
sys	0m0.088s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.011.163 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.026.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.737 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.738 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.738 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.739 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.740 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.740 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.741 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.743 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.743 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.743 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.768 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.071 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.071 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.072 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.072 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.072 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.036.073 I llama_model_loader: - type  f32:  194 tensors
0.00.036.073 I llama_model_loader: - type q3_K:   25 tensors
0.00.036.073 I llama_model_loader: - type q4_K:   71 tensors
0.00.036.074 I llama_model_loader: - type q5_K:    1 tensors
0.00.036.074 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.699 I llm_load_vocab: special tokens cache size = 25
0.00.067.944 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.947 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.947 I llm_load_print_meta: arch             = gptneox
0.00.067.948 I llm_load_print_meta: vocab type       = BPE
0.00.067.948 I llm_load_print_meta: n_vocab          = 50304
0.00.067.948 I llm_load_print_meta: n_merges         = 50009
0.00.067.948 I llm_load_print_meta: vocab_only       = 0
0.00.067.948 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.949 I llm_load_print_meta: n_embd           = 2048
0.00.067.949 I llm_load_print_meta: n_layer          = 24
0.00.067.952 I llm_load_print_meta: n_head           = 16
0.00.067.953 I llm_load_print_meta: n_head_kv        = 16
0.00.067.953 I llm_load_print_meta: n_rot            = 32
0.00.067.953 I llm_load_print_meta: n_swa            = 0
0.00.067.953 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.953 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.955 I llm_load_print_meta: n_gqa            = 1
0.00.067.956 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.957 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.957 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.957 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.959 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.959 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.959 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.960 I llm_load_print_meta: n_ff             = 8192
0.00.067.960 I llm_load_print_meta: n_expert         = 0
0.00.067.960 I llm_load_print_meta: n_expert_used    = 0
0.00.067.960 I llm_load_print_meta: causal attn      = 1
0.00.067.960 I llm_load_print_meta: pooling type     = 0
0.00.067.961 I llm_load_print_meta: rope type        = 2
0.00.067.961 I llm_load_print_meta: rope scaling     = linear
0.00.067.961 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.961 I llm_load_print_meta: freq_scale_train = 1
0.00.067.962 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.962 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.962 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.962 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.962 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.962 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.962 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.974 I llm_load_print_meta: model type       = 1.4B
0.00.067.974 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.067.975 I llm_load_print_meta: model params     = 1.41 B
0.00.067.976 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.067.977 I llm_load_print_meta: general.name     = 1.4B
0.00.067.977 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.977 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.977 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.977 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.977 I llm_load_print_meta: LF token         = 128 ''
0.00.067.978 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.978 I llm_load_print_meta: max token length = 1024
0.00.070.130 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.130 I llm_load_tensors: offloading output layer to GPU
0.00.070.130 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.140 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.070.141 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.071.240 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.241 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.241 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.241 I llama_new_context_with_model: n_batch       = 2048
0.00.071.241 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.242 I llama_new_context_with_model: flash_attn    = 0
0.00.071.242 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.242 I llama_new_context_with_model: freq_scale    = 1
0.00.071.243 I ggml_metal_init: allocating
0.00.071.248 I ggml_metal_init: found device: Apple M4
0.00.071.251 I ggml_metal_init: picking default device: Apple M4
0.00.071.845 I ggml_metal_init: using embedded metal library
0.00.076.316 I ggml_metal_init: GPU name:   Apple M4
0.00.076.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.320 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.320 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.323 I ggml_metal_init: simdgroup reduction   = true
0.00.076.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.323 I ggml_metal_init: has bfloat            = true
0.00.076.323 I ggml_metal_init: use bfloat            = true
0.00.076.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.937 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.946 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.966 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.041 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.042 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.043 I llama_new_context_with_model: graph nodes  = 967
0.00.110.043 I llama_new_context_with_model: graph splits = 2
0.00.110.057 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.980.541 I main: llama threadpool init, n_threads = 4
0.00.980.601 I 
0.00.980.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.980.639 I 
0.00.981.162 I sampler seed: 1234
0.00.981.173 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.981.204 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.981.206 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.981.206 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.731.905 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.731.906 I llama_perf_context_print:        load time =     969.37 ms
0.01.731.907 I llama_perf_context_print: prompt eval time =      36.24 ms /     7 tokens (    5.18 ms per token,   193.16 tokens per second)
0.01.731.907 I llama_perf_context_print:        eval time =     711.57 ms /    63 runs   (   11.29 ms per token,    88.54 tokens per second)
0.01.731.908 I llama_perf_context_print:       total time =     751.37 ms /    70 tokens
0.01.732.073 I ggml_metal_free: deallocating

real	0m1.749s
user	0m0.125s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.132 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.038.983 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.071.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.071.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.071.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.071.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.071.581 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.071.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.071.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.071.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.071.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.071.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.071.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.071.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.071.589 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.071.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.071.594 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.071.594 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.071.596 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.079.614 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.081.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.088.889 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.088.899 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.088.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.088.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.088.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.088.902 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.088.909 I llama_model_loader: - type  f32:  194 tensors
0.00.088.910 I llama_model_loader: - type q3_K:   25 tensors
0.00.088.911 I llama_model_loader: - type q4_K:   71 tensors
0.00.088.911 I llama_model_loader: - type q5_K:    1 tensors
0.00.088.912 I llama_model_loader: - type q6_K:    1 tensors
0.00.131.315 I llm_load_vocab: special tokens cache size = 25
0.00.139.644 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.139.649 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.139.650 I llm_load_print_meta: arch             = gptneox
0.00.139.650 I llm_load_print_meta: vocab type       = BPE
0.00.139.650 I llm_load_print_meta: n_vocab          = 50304
0.00.139.651 I llm_load_print_meta: n_merges         = 50009
0.00.139.651 I llm_load_print_meta: vocab_only       = 0
0.00.139.651 I llm_load_print_meta: n_ctx_train      = 2048
0.00.139.651 I llm_load_print_meta: n_embd           = 2048
0.00.139.651 I llm_load_print_meta: n_layer          = 24
0.00.139.655 I llm_load_print_meta: n_head           = 16
0.00.139.656 I llm_load_print_meta: n_head_kv        = 16
0.00.139.656 I llm_load_print_meta: n_rot            = 32
0.00.139.656 I llm_load_print_meta: n_swa            = 0
0.00.139.656 I llm_load_print_meta: n_embd_head_k    = 128
0.00.139.657 I llm_load_print_meta: n_embd_head_v    = 128
0.00.139.657 I llm_load_print_meta: n_gqa            = 1
0.00.139.658 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.139.659 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.139.659 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.139.660 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.139.660 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.139.660 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.139.660 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.139.663 I llm_load_print_meta: n_ff             = 8192
0.00.139.664 I llm_load_print_meta: n_expert         = 0
0.00.139.664 I llm_load_print_meta: n_expert_used    = 0
0.00.139.664 I llm_load_print_meta: causal attn      = 1
0.00.139.664 I llm_load_print_meta: pooling type     = 0
0.00.139.664 I llm_load_print_meta: rope type        = 2
0.00.139.664 I llm_load_print_meta: rope scaling     = linear
0.00.139.665 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.139.665 I llm_load_print_meta: freq_scale_train = 1
0.00.139.665 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.139.666 I llm_load_print_meta: rope_finetuned   = unknown
0.00.139.666 I llm_load_print_meta: ssm_d_conv       = 0
0.00.139.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.139.666 I llm_load_print_meta: ssm_d_state      = 0
0.00.139.666 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.139.666 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.139.678 I llm_load_print_meta: model type       = 1.4B
0.00.139.678 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.139.679 I llm_load_print_meta: model params     = 1.41 B
0.00.139.679 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.139.679 I llm_load_print_meta: general.name     = 1.4B
0.00.139.680 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.139.680 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.139.680 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.139.680 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.139.682 I llm_load_print_meta: LF token         = 128 ''
0.00.139.682 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.139.682 I llm_load_print_meta: max token length = 1024
0.00.141.642 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.141.642 I llm_load_tensors: offloading output layer to GPU
0.00.141.643 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.141.652 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.141.653 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.142.808 I llama_new_context_with_model: n_seq_max     = 1
0.00.142.809 I llama_new_context_with_model: n_ctx         = 128
0.00.142.809 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.142.809 I llama_new_context_with_model: n_batch       = 128
0.00.142.810 I llama_new_context_with_model: n_ubatch      = 128
0.00.142.810 I llama_new_context_with_model: flash_attn    = 0
0.00.142.810 I llama_new_context_with_model: freq_base     = 10000.0
0.00.142.811 I llama_new_context_with_model: freq_scale    = 1
0.00.142.811 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.142.812 I ggml_metal_init: allocating
0.00.142.821 I ggml_metal_init: found device: Apple M4
0.00.142.824 I ggml_metal_init: picking default device: Apple M4
0.00.143.497 I ggml_metal_init: using embedded metal library
0.00.146.725 I ggml_metal_init: GPU name:   Apple M4
0.00.146.727 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.146.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.146.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.146.729 I ggml_metal_init: simdgroup reduction   = true
0.00.146.729 I ggml_metal_init: simdgroup matrix mul. = true
0.00.146.729 I ggml_metal_init: has bfloat            = true
0.00.146.729 I ggml_metal_init: use bfloat            = true
0.00.146.730 I ggml_metal_init: hasUnifiedMemory      = true
0.00.146.730 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.158.763 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.158.765 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.158.780 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.159.821 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.159.822 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.159.823 I llama_new_context_with_model: graph nodes  = 967
0.00.159.823 I llama_new_context_with_model: graph splits = 2
0.00.159.835 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.463 I 
0.00.621.560 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.621.621 I perplexity: tokenizing the input ..
0.00.637.674 I perplexity: tokenization took 16.045 ms
0.00.637.688 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.912 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.779.803 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.779.850 I llama_perf_context_print:        load time =     582.47 ms
0.00.779.854 I llama_perf_context_print: prompt eval time =     136.30 ms /   128 tokens (    1.06 ms per token,   939.08 tokens per second)
0.00.779.859 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.865 I llama_perf_context_print:       total time =     158.39 ms /   129 tokens
0.00.781.333 I ggml_metal_free: deallocating

real	0m0.859s
user	0m0.158s
sys	0m0.109s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.014.734 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.357 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.030.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.365 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.366 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.367 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.368 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.370 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.371 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.371 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.371 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.372 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.378 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.250 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.655 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.657 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.657 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.658 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.658 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.042.659 I llama_model_loader: - type  f32:  194 tensors
0.00.042.660 I llama_model_loader: - type q4_K:   61 tensors
0.00.042.660 I llama_model_loader: - type q5_K:   24 tensors
0.00.042.660 I llama_model_loader: - type q6_K:   13 tensors
0.00.079.391 I llm_load_vocab: special tokens cache size = 25
0.00.088.841 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.844 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.845 I llm_load_print_meta: arch             = gptneox
0.00.088.845 I llm_load_print_meta: vocab type       = BPE
0.00.088.845 I llm_load_print_meta: n_vocab          = 50304
0.00.088.846 I llm_load_print_meta: n_merges         = 50009
0.00.088.846 I llm_load_print_meta: vocab_only       = 0
0.00.088.846 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.846 I llm_load_print_meta: n_embd           = 2048
0.00.088.847 I llm_load_print_meta: n_layer          = 24
0.00.088.850 I llm_load_print_meta: n_head           = 16
0.00.088.851 I llm_load_print_meta: n_head_kv        = 16
0.00.088.851 I llm_load_print_meta: n_rot            = 32
0.00.088.852 I llm_load_print_meta: n_swa            = 0
0.00.088.853 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.853 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.854 I llm_load_print_meta: n_gqa            = 1
0.00.088.855 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.856 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.856 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.857 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.857 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.857 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.857 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.858 I llm_load_print_meta: n_ff             = 8192
0.00.088.858 I llm_load_print_meta: n_expert         = 0
0.00.088.860 I llm_load_print_meta: n_expert_used    = 0
0.00.088.861 I llm_load_print_meta: causal attn      = 1
0.00.088.862 I llm_load_print_meta: pooling type     = 0
0.00.088.862 I llm_load_print_meta: rope type        = 2
0.00.088.862 I llm_load_print_meta: rope scaling     = linear
0.00.088.863 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.863 I llm_load_print_meta: freq_scale_train = 1
0.00.088.863 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.864 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.864 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.864 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.864 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.864 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.864 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.877 I llm_load_print_meta: model type       = 1.4B
0.00.088.877 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.088.878 I llm_load_print_meta: model params     = 1.41 B
0.00.088.878 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.088.879 I llm_load_print_meta: general.name     = 1.4B
0.00.088.879 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.879 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.879 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.880 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.880 I llm_load_print_meta: LF token         = 128 ''
0.00.088.880 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.881 I llm_load_print_meta: max token length = 1024
0.00.091.531 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.531 I llm_load_tensors: offloading output layer to GPU
0.00.091.531 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.542 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.091.543 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.092.852 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.853 I llama_new_context_with_model: n_ctx         = 2048
0.00.092.853 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.092.853 I llama_new_context_with_model: n_batch       = 2048
0.00.092.854 I llama_new_context_with_model: n_ubatch      = 512
0.00.092.854 I llama_new_context_with_model: flash_attn    = 0
0.00.092.854 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.855 I llama_new_context_with_model: freq_scale    = 1
0.00.092.855 I ggml_metal_init: allocating
0.00.092.859 I ggml_metal_init: found device: Apple M4
0.00.092.861 I ggml_metal_init: picking default device: Apple M4
0.00.093.562 I ggml_metal_init: using embedded metal library
0.00.096.350 I ggml_metal_init: GPU name:   Apple M4
0.00.096.353 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.355 I ggml_metal_init: simdgroup reduction   = true
0.00.096.356 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.356 I ggml_metal_init: has bfloat            = true
0.00.096.357 I ggml_metal_init: use bfloat            = true
0.00.096.359 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.360 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.126.599 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.126.610 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.126.632 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.596 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.598 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.598 I llama_new_context_with_model: graph nodes  = 967
0.00.127.598 I llama_new_context_with_model: graph splits = 2
0.00.127.606 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.310.189 I main: llama threadpool init, n_threads = 4
0.01.310.228 I 
0.01.310.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.310.245 I 
0.01.310.474 I sampler seed: 1234
0.01.310.480 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.310.527 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.310.528 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.310.528 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.02.064.986 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.02.064.987 I llama_perf_context_print:        load time =    1295.45 ms
0.02.064.987 I llama_perf_context_print: prompt eval time =      36.54 ms /     7 tokens (    5.22 ms per token,   191.59 tokens per second)
0.02.064.988 I llama_perf_context_print:        eval time =     714.92 ms /    63 runs   (   11.35 ms per token,    88.12 tokens per second)
0.02.064.988 I llama_perf_context_print:       total time =     754.80 ms /    70 tokens
0.02.065.154 I ggml_metal_free: deallocating

real	0m2.100s
user	0m0.140s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.141 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.476 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.023.176 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.178 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.179 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.179 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.180 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.182 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.182 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.183 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.183 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.183 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.184 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.186 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.186 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.186 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.847 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.035.610 I llama_model_loader: - type  f32:  194 tensors
0.00.035.610 I llama_model_loader: - type q4_K:   61 tensors
0.00.035.610 I llama_model_loader: - type q5_K:   24 tensors
0.00.035.611 I llama_model_loader: - type q6_K:   13 tensors
0.00.059.872 I llm_load_vocab: special tokens cache size = 25
0.00.065.806 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.808 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.809 I llm_load_print_meta: arch             = gptneox
0.00.065.809 I llm_load_print_meta: vocab type       = BPE
0.00.065.809 I llm_load_print_meta: n_vocab          = 50304
0.00.065.810 I llm_load_print_meta: n_merges         = 50009
0.00.065.810 I llm_load_print_meta: vocab_only       = 0
0.00.065.810 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.810 I llm_load_print_meta: n_embd           = 2048
0.00.065.810 I llm_load_print_meta: n_layer          = 24
0.00.065.813 I llm_load_print_meta: n_head           = 16
0.00.065.814 I llm_load_print_meta: n_head_kv        = 16
0.00.065.814 I llm_load_print_meta: n_rot            = 32
0.00.065.814 I llm_load_print_meta: n_swa            = 0
0.00.065.814 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.814 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.815 I llm_load_print_meta: n_gqa            = 1
0.00.065.816 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.817 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.817 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.817 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.817 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.818 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.818 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.819 I llm_load_print_meta: n_ff             = 8192
0.00.065.819 I llm_load_print_meta: n_expert         = 0
0.00.065.819 I llm_load_print_meta: n_expert_used    = 0
0.00.065.819 I llm_load_print_meta: causal attn      = 1
0.00.065.819 I llm_load_print_meta: pooling type     = 0
0.00.065.819 I llm_load_print_meta: rope type        = 2
0.00.065.820 I llm_load_print_meta: rope scaling     = linear
0.00.065.820 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.820 I llm_load_print_meta: freq_scale_train = 1
0.00.065.820 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.821 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.821 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.821 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.821 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.821 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.832 I llm_load_print_meta: model type       = 1.4B
0.00.065.833 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.065.833 I llm_load_print_meta: model params     = 1.41 B
0.00.065.833 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.065.833 I llm_load_print_meta: general.name     = 1.4B
0.00.065.834 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.834 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.834 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.834 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.835 I llm_load_print_meta: LF token         = 128 ''
0.00.065.835 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.835 I llm_load_print_meta: max token length = 1024
0.00.067.411 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.411 I llm_load_tensors: offloading output layer to GPU
0.00.067.412 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.421 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.067.422 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.068.291 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.292 I llama_new_context_with_model: n_ctx         = 128
0.00.068.292 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.068.292 I llama_new_context_with_model: n_batch       = 128
0.00.068.292 I llama_new_context_with_model: n_ubatch      = 128
0.00.068.292 I llama_new_context_with_model: flash_attn    = 0
0.00.068.293 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.293 I llama_new_context_with_model: freq_scale    = 1
0.00.068.293 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.294 I ggml_metal_init: allocating
0.00.068.300 I ggml_metal_init: found device: Apple M4
0.00.068.302 I ggml_metal_init: picking default device: Apple M4
0.00.068.807 I ggml_metal_init: using embedded metal library
0.00.070.711 I ggml_metal_init: GPU name:   Apple M4
0.00.070.712 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.713 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.713 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.713 I ggml_metal_init: simdgroup reduction   = true
0.00.070.713 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.713 I ggml_metal_init: has bfloat            = true
0.00.070.714 I ggml_metal_init: use bfloat            = true
0.00.070.714 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.693 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.697 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.720 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.080.578 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.080.579 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.080.579 I llama_new_context_with_model: graph nodes  = 967
0.00.080.580 I llama_new_context_with_model: graph splits = 2
0.00.080.591 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.360 I 
0.00.632.375 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.632.386 I perplexity: tokenizing the input ..
0.00.640.461 I perplexity: tokenization took 8.074 ms
0.00.640.464 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.754 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.776.004 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.776.017 I llama_perf_context_print:        load time =     618.88 ms
0.00.776.018 I llama_perf_context_print: prompt eval time =     134.07 ms /   128 tokens (    1.05 ms per token,   954.75 tokens per second)
0.00.776.018 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.019 I llama_perf_context_print:       total time =     143.66 ms /   129 tokens
0.00.776.412 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.091s
sys	0m0.112s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.012.393 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.023.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.571 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.572 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.573 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.574 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.574 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.950 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.952 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.034.952 I llama_model_loader: - type  f32:  194 tensors
0.00.034.953 I llama_model_loader: - type q5_K:   61 tensors
0.00.034.953 I llama_model_loader: - type q6_K:   37 tensors
0.00.068.595 I llm_load_vocab: special tokens cache size = 25
0.00.078.725 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.728 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.729 I llm_load_print_meta: arch             = gptneox
0.00.078.729 I llm_load_print_meta: vocab type       = BPE
0.00.078.730 I llm_load_print_meta: n_vocab          = 50304
0.00.078.730 I llm_load_print_meta: n_merges         = 50009
0.00.078.730 I llm_load_print_meta: vocab_only       = 0
0.00.078.730 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.730 I llm_load_print_meta: n_embd           = 2048
0.00.078.731 I llm_load_print_meta: n_layer          = 24
0.00.078.734 I llm_load_print_meta: n_head           = 16
0.00.078.735 I llm_load_print_meta: n_head_kv        = 16
0.00.078.735 I llm_load_print_meta: n_rot            = 32
0.00.078.735 I llm_load_print_meta: n_swa            = 0
0.00.078.735 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.736 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.737 I llm_load_print_meta: n_gqa            = 1
0.00.078.738 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.738 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.739 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.741 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.741 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.742 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.742 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.743 I llm_load_print_meta: n_ff             = 8192
0.00.078.743 I llm_load_print_meta: n_expert         = 0
0.00.078.743 I llm_load_print_meta: n_expert_used    = 0
0.00.078.743 I llm_load_print_meta: causal attn      = 1
0.00.078.743 I llm_load_print_meta: pooling type     = 0
0.00.078.744 I llm_load_print_meta: rope type        = 2
0.00.078.744 I llm_load_print_meta: rope scaling     = linear
0.00.078.744 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.745 I llm_load_print_meta: freq_scale_train = 1
0.00.078.745 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.745 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.746 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.748 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.748 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.748 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.748 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.761 I llm_load_print_meta: model type       = 1.4B
0.00.078.761 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.078.762 I llm_load_print_meta: model params     = 1.41 B
0.00.078.763 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.078.763 I llm_load_print_meta: general.name     = 1.4B
0.00.078.763 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.764 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.764 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.764 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.765 I llm_load_print_meta: LF token         = 128 ''
0.00.078.765 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.765 I llm_load_print_meta: max token length = 1024
0.00.081.411 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.411 I llm_load_tensors: offloading output layer to GPU
0.00.081.412 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.423 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.081.424 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.082.856 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.858 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.858 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.858 I llama_new_context_with_model: n_batch       = 2048
0.00.082.859 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.859 I llama_new_context_with_model: flash_attn    = 0
0.00.082.859 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.860 I llama_new_context_with_model: freq_scale    = 1
0.00.082.860 I ggml_metal_init: allocating
0.00.082.864 I ggml_metal_init: found device: Apple M4
0.00.082.867 I ggml_metal_init: picking default device: Apple M4
0.00.083.600 I ggml_metal_init: using embedded metal library
0.00.086.603 I ggml_metal_init: GPU name:   Apple M4
0.00.086.605 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.606 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.607 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.607 I ggml_metal_init: simdgroup reduction   = true
0.00.086.607 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.607 I ggml_metal_init: has bfloat            = true
0.00.086.607 I ggml_metal_init: use bfloat            = true
0.00.086.608 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.609 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.453 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.461 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.484 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.408 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.410 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.410 I llama_new_context_with_model: graph nodes  = 967
0.00.118.410 I llama_new_context_with_model: graph splits = 2
0.00.118.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.383.732 I main: llama threadpool init, n_threads = 4
0.01.383.760 I 
0.01.383.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.383.777 I 
0.01.383.916 I sampler seed: 1234
0.01.383.920 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.383.929 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.383.930 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.383.930 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.02.226.147 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 64840.18 tokens per second)
0.02.226.148 I llama_perf_context_print:        load time =    1371.34 ms
0.02.226.149 I llama_perf_context_print: prompt eval time =      38.72 ms /     7 tokens (    5.53 ms per token,   180.80 tokens per second)
0.02.226.149 I llama_perf_context_print:        eval time =     800.62 ms /    63 runs   (   12.71 ms per token,    78.69 tokens per second)
0.02.226.150 I llama_perf_context_print:       total time =     842.42 ms /    70 tokens
0.02.226.318 I ggml_metal_free: deallocating

real	0m2.260s
user	0m0.133s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.561 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.565 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.568 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.568 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.569 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.575 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.576 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.341 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.274 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.275 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.275 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.275 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.276 I llama_model_loader: - type  f32:  194 tensors
0.00.023.276 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.277 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.887 I llm_load_vocab: special tokens cache size = 25
0.00.049.816 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.821 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.821 I llm_load_print_meta: arch             = gptneox
0.00.049.821 I llm_load_print_meta: vocab type       = BPE
0.00.049.822 I llm_load_print_meta: n_vocab          = 50304
0.00.049.822 I llm_load_print_meta: n_merges         = 50009
0.00.049.822 I llm_load_print_meta: vocab_only       = 0
0.00.049.822 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.822 I llm_load_print_meta: n_embd           = 2048
0.00.049.823 I llm_load_print_meta: n_layer          = 24
0.00.049.825 I llm_load_print_meta: n_head           = 16
0.00.049.826 I llm_load_print_meta: n_head_kv        = 16
0.00.049.826 I llm_load_print_meta: n_rot            = 32
0.00.049.826 I llm_load_print_meta: n_swa            = 0
0.00.049.828 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.828 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.828 I llm_load_print_meta: n_gqa            = 1
0.00.049.829 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.830 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.831 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.831 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.831 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.831 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.833 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.834 I llm_load_print_meta: n_ff             = 8192
0.00.049.834 I llm_load_print_meta: n_expert         = 0
0.00.049.834 I llm_load_print_meta: n_expert_used    = 0
0.00.049.834 I llm_load_print_meta: causal attn      = 1
0.00.049.834 I llm_load_print_meta: pooling type     = 0
0.00.049.835 I llm_load_print_meta: rope type        = 2
0.00.049.835 I llm_load_print_meta: rope scaling     = linear
0.00.049.835 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.836 I llm_load_print_meta: freq_scale_train = 1
0.00.049.836 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.836 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.836 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.836 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.840 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.840 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.840 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.850 I llm_load_print_meta: model type       = 1.4B
0.00.049.851 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.851 I llm_load_print_meta: model params     = 1.41 B
0.00.049.852 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.852 I llm_load_print_meta: general.name     = 1.4B
0.00.049.852 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.852 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.852 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.852 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.853 I llm_load_print_meta: LF token         = 128 ''
0.00.049.853 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.853 I llm_load_print_meta: max token length = 1024
0.00.051.878 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.878 I llm_load_tensors: offloading output layer to GPU
0.00.051.879 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.889 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.890 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.826 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.827 I llama_new_context_with_model: n_ctx         = 128
0.00.052.827 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.827 I llama_new_context_with_model: n_batch       = 128
0.00.052.828 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.828 I llama_new_context_with_model: flash_attn    = 0
0.00.052.828 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.828 I llama_new_context_with_model: freq_scale    = 1
0.00.052.829 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.829 I ggml_metal_init: allocating
0.00.052.835 I ggml_metal_init: found device: Apple M4
0.00.052.837 I ggml_metal_init: picking default device: Apple M4
0.00.053.374 I ggml_metal_init: using embedded metal library
0.00.055.328 I ggml_metal_init: GPU name:   Apple M4
0.00.055.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.330 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.330 I ggml_metal_init: simdgroup reduction   = true
0.00.055.330 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.331 I ggml_metal_init: has bfloat            = true
0.00.055.331 I ggml_metal_init: use bfloat            = true
0.00.055.331 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.353 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.357 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.374 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.282 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.283 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.283 I llama_new_context_with_model: graph nodes  = 967
0.00.065.283 I llama_new_context_with_model: graph splits = 2
0.00.065.296 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.340 I 
0.00.698.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.698.400 I perplexity: tokenizing the input ..
0.00.705.952 I perplexity: tokenization took 7.55 ms
0.00.705.957 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.846.662 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.847.954 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.847.967 I llama_perf_context_print:        load time =     689.64 ms
0.00.847.969 I llama_perf_context_print: prompt eval time =     140.47 ms /   128 tokens (    1.10 ms per token,   911.26 tokens per second)
0.00.847.970 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.970 I llama_perf_context_print:       total time =     149.63 ms /   129 tokens
0.00.848.365 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.074s
sys	0m0.130s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.002 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.323 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.329 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.330 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.332 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.332 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.332 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.333 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.335 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.336 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.340 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.151 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.035 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.036 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.036 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.036 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.037 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.037 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.038 I llama_model_loader: - type  f32:  194 tensors
0.00.026.038 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.003 I llm_load_vocab: special tokens cache size = 25
0.00.051.938 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.941 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.941 I llm_load_print_meta: arch             = gptneox
0.00.051.942 I llm_load_print_meta: vocab type       = BPE
0.00.051.942 I llm_load_print_meta: n_vocab          = 50304
0.00.051.942 I llm_load_print_meta: n_merges         = 50009
0.00.051.942 I llm_load_print_meta: vocab_only       = 0
0.00.051.943 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.943 I llm_load_print_meta: n_embd           = 2048
0.00.051.943 I llm_load_print_meta: n_layer          = 24
0.00.051.945 I llm_load_print_meta: n_head           = 16
0.00.051.946 I llm_load_print_meta: n_head_kv        = 16
0.00.051.946 I llm_load_print_meta: n_rot            = 32
0.00.051.948 I llm_load_print_meta: n_swa            = 0
0.00.051.948 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.948 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.949 I llm_load_print_meta: n_gqa            = 1
0.00.051.950 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.950 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.951 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.951 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.951 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.952 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.952 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.952 I llm_load_print_meta: n_ff             = 8192
0.00.051.953 I llm_load_print_meta: n_expert         = 0
0.00.051.953 I llm_load_print_meta: n_expert_used    = 0
0.00.051.953 I llm_load_print_meta: causal attn      = 1
0.00.051.954 I llm_load_print_meta: pooling type     = 0
0.00.051.955 I llm_load_print_meta: rope type        = 2
0.00.051.955 I llm_load_print_meta: rope scaling     = linear
0.00.051.955 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.956 I llm_load_print_meta: freq_scale_train = 1
0.00.051.956 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.956 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.956 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.956 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.956 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.958 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.958 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.969 I llm_load_print_meta: model type       = 1.4B
0.00.051.969 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.969 I llm_load_print_meta: model params     = 1.41 B
0.00.051.970 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.970 I llm_load_print_meta: general.name     = 1.4B
0.00.051.970 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.970 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.970 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.970 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.971 I llm_load_print_meta: LF token         = 128 ''
0.00.051.971 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.971 I llm_load_print_meta: max token length = 1024
0.00.053.595 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.595 I llm_load_tensors: offloading output layer to GPU
0.00.053.595 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.605 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.606 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.467 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.467 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.468 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.468 I llama_new_context_with_model: n_batch       = 2048
0.00.054.468 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.468 I llama_new_context_with_model: flash_attn    = 0
0.00.054.468 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.469 I llama_new_context_with_model: freq_scale    = 1
0.00.054.469 I ggml_metal_init: allocating
0.00.054.476 I ggml_metal_init: found device: Apple M4
0.00.054.478 I ggml_metal_init: picking default device: Apple M4
0.00.055.033 I ggml_metal_init: using embedded metal library
0.00.056.978 I ggml_metal_init: GPU name:   Apple M4
0.00.056.979 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.980 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.980 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.980 I ggml_metal_init: simdgroup reduction   = true
0.00.056.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.982 I ggml_metal_init: has bfloat            = true
0.00.056.982 I ggml_metal_init: use bfloat            = true
0.00.056.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.963 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.968 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.985 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.872 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.873 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.873 I llama_new_context_with_model: graph nodes  = 967
0.00.085.874 I llama_new_context_with_model: graph splits = 2
0.00.085.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.860.971 I main: llama threadpool init, n_threads = 4
0.00.861.002 I 
0.00.861.022 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.861.022 I 
0.00.861.252 I sampler seed: 1234
0.00.861.257 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.861.268 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.861.268 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.861.268 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.733.862 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61631.94 tokens per second)
0.01.733.863 I llama_perf_context_print:        load time =     850.97 ms
0.01.733.864 I llama_perf_context_print: prompt eval time =      38.67 ms /     7 tokens (    5.52 ms per token,   181.03 tokens per second)
0.01.733.865 I llama_perf_context_print:        eval time =     830.98 ms /    63 runs   (   13.19 ms per token,    75.81 tokens per second)
0.01.733.865 I llama_perf_context_print:       total time =     872.89 ms /    70 tokens
0.01.734.035 I ggml_metal_free: deallocating

real	0m1.751s
user	0m0.107s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4155 (96fa2c5e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.851 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.423 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.428 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.428 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.429 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.429 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.429 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.430 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.430 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.431 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.431 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.431 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.434 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.434 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.435 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.104 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.822 I llama_model_loader: - type  f32:  194 tensors
0.00.023.823 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.448 I llm_load_vocab: special tokens cache size = 25
0.00.049.086 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.089 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.089 I llm_load_print_meta: arch             = gptneox
0.00.049.089 I llm_load_print_meta: vocab type       = BPE
0.00.049.090 I llm_load_print_meta: n_vocab          = 50304
0.00.049.090 I llm_load_print_meta: n_merges         = 50009
0.00.049.090 I llm_load_print_meta: vocab_only       = 0
0.00.049.090 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.090 I llm_load_print_meta: n_embd           = 2048
0.00.049.091 I llm_load_print_meta: n_layer          = 24
0.00.049.093 I llm_load_print_meta: n_head           = 16
0.00.049.094 I llm_load_print_meta: n_head_kv        = 16
0.00.049.094 I llm_load_print_meta: n_rot            = 32
0.00.049.094 I llm_load_print_meta: n_swa            = 0
0.00.049.094 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.095 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.095 I llm_load_print_meta: n_gqa            = 1
0.00.049.096 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.097 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.097 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.097 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.098 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.098 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.098 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.099 I llm_load_print_meta: n_ff             = 8192
0.00.049.099 I llm_load_print_meta: n_expert         = 0
0.00.049.099 I llm_load_print_meta: n_expert_used    = 0
0.00.049.099 I llm_load_print_meta: causal attn      = 1
0.00.049.099 I llm_load_print_meta: pooling type     = 0
0.00.049.099 I llm_load_print_meta: rope type        = 2
0.00.049.101 I llm_load_print_meta: rope scaling     = linear
0.00.049.101 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.103 I llm_load_print_meta: freq_scale_train = 1
0.00.049.104 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.104 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.104 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.105 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.105 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.105 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.105 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.111 I llm_load_print_meta: model type       = 1.4B
0.00.049.112 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.112 I llm_load_print_meta: model params     = 1.41 B
0.00.049.114 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.114 I llm_load_print_meta: general.name     = 1.4B
0.00.049.114 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.114 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.115 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.115 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.115 I llm_load_print_meta: LF token         = 128 ''
0.00.049.115 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.115 I llm_load_print_meta: max token length = 1024
0.00.050.904 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.905 I llm_load_tensors: offloading output layer to GPU
0.00.050.905 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.910 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.910 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.890 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.891 I llama_new_context_with_model: n_ctx         = 128
0.00.051.891 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.891 I llama_new_context_with_model: n_batch       = 128
0.00.051.891 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.892 I llama_new_context_with_model: flash_attn    = 0
0.00.051.892 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.892 I llama_new_context_with_model: freq_scale    = 1
0.00.051.892 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.893 I ggml_metal_init: allocating
0.00.051.895 I ggml_metal_init: found device: Apple M4
0.00.051.897 I ggml_metal_init: picking default device: Apple M4
0.00.052.439 I ggml_metal_init: using embedded metal library
0.00.054.347 I ggml_metal_init: GPU name:   Apple M4
0.00.054.348 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.348 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.349 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.349 I ggml_metal_init: simdgroup reduction   = true
0.00.054.349 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.349 I ggml_metal_init: has bfloat            = true
0.00.054.349 I ggml_metal_init: use bfloat            = true
0.00.054.351 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.351 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.111 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.117 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.130 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.081 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.082 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.082 I llama_new_context_with_model: graph nodes  = 967
0.00.064.082 I llama_new_context_with_model: graph splits = 2
0.00.064.095 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.285 I 
0.00.612.321 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.612.351 I perplexity: tokenizing the input ..
0.00.620.285 I perplexity: tokenization took 7.934 ms
0.00.620.290 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.251 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.761.459 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.761.471 I llama_perf_context_print:        load time =     602.43 ms
0.00.761.472 I llama_perf_context_print: prompt eval time =     139.71 ms /   128 tokens (    1.09 ms per token,   916.18 tokens per second)
0.00.761.473 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.474 I llama_perf_context_print:       total time =     149.19 ms /   129 tokens
0.00.761.840 I ggml_metal_free: deallocating

real	0m0.775s
user	0m0.074s
sys	0m0.115s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4155 (96fa2c5e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15050a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15050aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15050b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15050b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15050bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15050c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15050c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15050ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15050d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15050d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15050dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15050e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15050eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15050f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15050fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150510380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x150510aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1505111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1505118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1505120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1505127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150512ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x150513610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150513eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1505145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x150514890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150514ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150515b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150516050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150516310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1505167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150516a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x150517300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150517840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150517b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150517fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150518440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1505188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150518d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150519220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1505196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150519b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15051a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15051a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15051a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15051ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15051b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15051bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15051c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15051c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15051ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15051d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15051daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15051e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15051e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15051ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15051f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15051f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15051fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1505202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1505205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150520a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150520ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150521390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x150521830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150521cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150522170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x150522610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150522ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150522f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1505233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150523890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x150523d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1505241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x150524670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150524b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150524fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x150525450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1505258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150525d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x150526230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1505266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150526b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150527010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1505274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x150527950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150527df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x150528290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x150528730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150528bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x150529070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150529510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1505299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150529e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15052a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15052a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15052ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15051b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15052b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15052b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15052bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15052c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15052c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15052c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15052ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15052d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15052d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15052dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15052e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15052e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15052ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15052eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15052f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15052f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15052fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150530120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1505305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x150530a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150530f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1505313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150531840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150531ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x150532180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x150532620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150532ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x150532f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150533400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1505338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x150533d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1505341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x150534680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150534b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150534fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x150535460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150535900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150535da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x150536240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1505366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150536b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150537020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1505374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150537960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150537e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1505382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150538740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150538be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150539080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150539520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1505399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150539e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15053a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15053a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15053ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15053b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15053b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15053bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15053c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15053c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15053ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15053d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15053d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15053dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15053e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15053ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15053ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15053f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15053f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150540010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150540560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150540ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x150541000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150541550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150541aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x150541ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x150542540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150542a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150542fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x150543530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150543a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150543fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150544520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150544a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150544fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x150545510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150545a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150545fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150546500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150546a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150546fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1505474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150547a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150547f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1505484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150548a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150548f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1505494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150549a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150549f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15054a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15054aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15054af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15054b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15054ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15054bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15054c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15054c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15054cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15054d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15054d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15054df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15054e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15054e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15054ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15054f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15054f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15054ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150550460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1505509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150550f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150551450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1505519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x150551ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150552440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150552990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150552e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1505532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150553770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150553c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1505540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150554550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1505549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150554e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150555330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1505557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150555c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150556110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1505565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150556b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150557220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150557940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150558060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150558780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150558a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150559050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x150559660 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.149.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x150549aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150549f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15054a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15054a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15054ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15054b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15054b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15054b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15054be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15054c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15054c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15054cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15054d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15054dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15054e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15054ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15054f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15054fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1505500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150550a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150551160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150551850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x150551f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150552630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150552d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x150553190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150553600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150553a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150553ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150554350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1505547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150554c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1505550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150555360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1505557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150555c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1505560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150556520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150556990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150556e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150557270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1505576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150557b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150557fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150558430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1505588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x150558d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x150559180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1505595f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15050b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15050bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15050aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15050ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x150509b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15050c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15050cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15050a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x150517830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150517af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x150517f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1505183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150518840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150518cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150519120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x150519590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150519a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150519e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15051a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15051a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15051abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15051b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15051b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15051b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15051bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15051c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15051c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15051cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15051cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15051d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15051d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15051dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15051e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15051e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15051e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15051ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15051f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15051f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15051fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x150520010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150520480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1505208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150520d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1505211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150521640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150521ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150521f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x150522390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150522800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x150522c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1505230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x150523550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1505239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150523e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1505242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150524710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150524b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150524ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150525460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1505258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150525d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1505261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x150526620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x150526a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150526f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x150527370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1505277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150527c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1505280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150528530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1505289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150528e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150529280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1505296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x150529b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150529fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15052a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15052a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15052ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15052b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15052b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15052ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15052bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15052c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15052c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15052cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15052d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15052d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15052d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15052ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15052e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15052e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15052eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15052efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15052f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15052f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15052fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150530170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1505305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150530a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150530ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150531330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1505317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150531c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150532080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1505324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150532960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150532dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150533240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1505336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150533b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150533f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150534400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x150534870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x150534ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150535150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1505355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150535a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1505361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150536620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150536a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x150536f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150537370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1505377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x150537c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1505380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150538530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1505389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x150538e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150539280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1505396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150539b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150539fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15053a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15053a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15053ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15053b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15053b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15053ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15053bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15053c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15053c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15053cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15053d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15053d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15053d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15053ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15053e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15053e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15053eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15053efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15053f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15053f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15053fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150540170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1505405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150540a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150540ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150541330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1505417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150541c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150542080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1505424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x150542960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150542dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150543240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1505436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150543b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x150543f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150544400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150544870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150544ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x150545150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1505455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150545a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150545ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150546310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150546780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150546bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150547060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1505474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x150547940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150547db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150548220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150548690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150548b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150548f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1505161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150516660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150516d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15050d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15050de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15050e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15050e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15050ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15050f270 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1504046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150404b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x150404fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x150405430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1504058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x150405d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x150406180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1504065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150406a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x150406ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x150407340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x150407a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x150408540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x150408cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150409500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150409c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15040a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15040aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15040b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15040b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15040c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15040c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15040ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15040d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15040dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15040dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15040e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15040e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15040eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15040efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15040f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15040f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15040fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150410090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150410500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150410970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150410de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150411250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1504116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150411b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150411fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150412410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150412880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150412cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150413160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1504135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x150413a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x150413eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x150414320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x150414790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x150414c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x150415070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1504154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x150415950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x150415dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x150416230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1504167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x150416ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150417110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x150417580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1504179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150417e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1504182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150418740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x150418bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150419020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150419490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x150419900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150419d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15041a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15041a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15041aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15041af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15041b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15041b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15041bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15041c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15041c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15041c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15041ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15041d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15041d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15041db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15041e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15041e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15041e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15041ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15041f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15041f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15041faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15041ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150420380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1504207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150420c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1504210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150421540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1504219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150421e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x150422290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x150422700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x150422b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x150422fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150423450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1504238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150423d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1504241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150424610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150424a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150424ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150425360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1504257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x150425c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1504260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150426520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x150426990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150426e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150427270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1504276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150427b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x150427fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150428430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1504288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x150428d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x150429180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1504295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x150429a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150429ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15042a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15042a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15042ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15042b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15042b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15042b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15042bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15042c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15042c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15042cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15042cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15042d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15042d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15042dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15042e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15042e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15042ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15042eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15042f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15042f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15042fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150430070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1504304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150430950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150430dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150431230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1504316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150431b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150431f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1504323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150432860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150432cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150433140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1504335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150433a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x150433e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x150434300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150434770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150434be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150435050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150435be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150435ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150436160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1504365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150436a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150436eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x150437320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x150437790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150437c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150438070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1504384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150438950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150438dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150439230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1504396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150439b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x150439f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15043a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15043a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15043acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15043b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15043b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15043ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15043be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15043c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15043c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15043cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15043d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15043d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15043d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15043dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15043e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15043e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15043eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15043ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15043f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15043f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15043fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150440120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150440590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150440a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150440e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1504412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150441750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150441bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x150442030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1504424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150442910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x150442d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1504431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x150443660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150443ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150443f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1504443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x150444820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150444c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150445100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150445570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1504459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150445e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1504462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150446730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150446ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x150447010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150447480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1504478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150447d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1504481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150448640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150448ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150448f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150449a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15044a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15044a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15044afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15044b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15044b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15044b9b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.842s
user	0m0.281s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4155 (96fa2c5e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f0dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f0e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f0e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f0ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f0f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f10b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f11500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f11a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f12520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f12cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f13c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f14320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f14a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f15160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f15930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f16050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f16770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f16e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f18110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f18720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f19b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f1a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f1b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f1b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f1c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f1c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f1caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f1d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f1d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f1dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f20750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f21370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f21980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f22610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f22d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f23b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f24770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f24c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f250b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f25e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f26330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f26c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f27110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f27a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f29610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f2ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f2b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f2bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f2bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f2c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f2d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f2e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f2e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f2f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f2f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f31000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f31940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f32280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f32720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f32bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f33060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f33500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f33e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f35a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f36340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f36c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f37120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f38840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f38ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f39180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f39620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f39ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f3a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f3b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f3bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f3bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f3c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f3c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f3d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f3db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f3e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f3ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f3fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f40ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f42c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f43890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f43de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f44880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f44dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f45dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f46860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f46db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f47da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f4ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f4cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f4d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f56b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131f57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131f57930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131f57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131f58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131f58710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131f58bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131f59050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131f594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131f59990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131f59e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131f5a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131f5aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131f5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131f5b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131f5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131f5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131f5c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131f5cee0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.124.840 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f0ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f0f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f0fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f10260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f106d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f0dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f4ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f50340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f50a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f51120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f51aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f52f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f53660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f53d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f54630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f54aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f54f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f55380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f56390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f56800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f57550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f57e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f58710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f58b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f59460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f5a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f5aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f5b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f5bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f5c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f5c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f5c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f5ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f37550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f38b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f39be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f3ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f3baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f3c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f3f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131f4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131f4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131f4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131f4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131f4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131f4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131f4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131f4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131f4c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131f198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131f19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131f1a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131f1ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131f113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131f11ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131f11f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131f123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131f12830 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131e07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131e080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131e08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131e08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131e08df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131e09260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131e096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131e09b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131e09fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131e0a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131e0a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131e0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131e0bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131e0c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131e0cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131e0d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131e0d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131e0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131e0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131e0ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131e0f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131e0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131e10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131e10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131e112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131e11590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131e11850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131e11cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131e12130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131e125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131e12a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131e12f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131e133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131e13670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131e13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131e13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131e143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131e14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131e14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131e15110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131e15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131e159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131e15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131e162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131e16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131e16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131e17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131e17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131e17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131e17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131e181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131e18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131e18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131e18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131e193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131e19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131e19d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131e1a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131e1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131e1ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131e1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131e1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131e1b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131e1bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131e1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131e1c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131e1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131e1cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131e1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131e1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131e1dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131e1e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131e1e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131e1e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131e1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131e1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131e1f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131e1fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131e1ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131e20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131e20890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131e20d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131e21170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131e215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131e21a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131e21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131e22330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131e227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131e22c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131e23080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131e234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131e23960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131e23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131e24240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131e246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131e24b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131e24f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131e25400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131e25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131e25ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131e26150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131e265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131e26a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131e26ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131e27310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131e27780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131e27bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131e28060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131e284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131e28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131e28db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131e29220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131e29690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131e29b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131e29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131e2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131e2a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131e2acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131e2b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131e2b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131e2ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131e2be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131e2c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131e2c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131e2cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131e2d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131e2d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131e2d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131e2dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131e2e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131e2e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131e2eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131e2ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131e2f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131e2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131e2fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131e30110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131e30580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131e309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131e30e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131e312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131e31740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131e31bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131e32020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131e32490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131e32900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131e32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131e331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131e33650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131e33ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131e33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131e343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131e34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131e34c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131e350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131e35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131e359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131e35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131e362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131e36720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131e36b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131e37000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131e37470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131e378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131e37d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131e381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131e38630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131e391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131e39480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131e39740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131e39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131e3a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131e3a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131e3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131e3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131e3b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131e3b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131e3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131e3bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131e3c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131e3c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131e3cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131e3d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131e3d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131e3d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131e3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131e3e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131e3e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131e3eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131e3f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131e3f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131e3f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131e3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131e401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131e40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131e40aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131e40f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131e41380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131e417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131e41c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131e420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131e42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131e429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131e42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131e43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131e43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131e43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131e43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131e44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131e448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131e44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131e451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131e45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131e45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131e45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131e46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131e467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131e46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131e470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131e47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131e47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131e47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131e48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131e486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131e48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131e48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131e49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131e498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131e49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131e4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131e4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131e4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131e4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131e4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131e4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131e4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131e4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131e4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131e4d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131e4d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131e4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131e4e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131e4e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131e4eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131e4ef90 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m1.034s
user	0m0.264s
sys	0m0.133s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.68 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.25 sec*proc (2 tests)

Total Test time (real) =   1.28 sec
        1.38 real         0.75 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.25 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.29 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.15 user         0.04 sys
```
