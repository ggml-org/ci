### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.34 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.63 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.92 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.96 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  193.85 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.15 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.38 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 255.13 sec*proc (29 tests)

Total Test time (real) = 255.15 sec

real	4m15.319s
user	8m32.567s
sys	0m7.194s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.82 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.45 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.88 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.40 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.83 sec*proc (29 tests)

Total Test time (real) =  54.85 sec

real	0m54.857s
user	1m17.486s
sys	0m6.306s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.202 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.153 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.645 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.655 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.027.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.656 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.027.657 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.027.657 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.659 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.659 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.660 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.661 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.661 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.664 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.665 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.666 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.666 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.667 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.667 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.668 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.054 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.213 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.215 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.216 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.216 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.217 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.217 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.033.218 I llama_model_loader: - type  f32:  124 tensors
0.00.033.218 I llama_model_loader: - type  f16:   73 tensors
0.00.033.219 I print_info: file format = GGUF V3 (latest)
0.00.033.220 I print_info: file type   = F16
0.00.033.224 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.037.566 I load: special tokens cache size = 5
0.00.039.655 I load: token to piece cache size = 0.2032 MB
0.00.039.685 I print_info: arch             = bert
0.00.039.686 I print_info: vocab_only       = 0
0.00.039.686 I print_info: n_ctx_train      = 512
0.00.039.687 I print_info: n_embd           = 384
0.00.039.687 I print_info: n_layer          = 12
0.00.039.690 I print_info: n_head           = 12
0.00.039.691 I print_info: n_head_kv        = 12
0.00.039.691 I print_info: n_rot            = 32
0.00.039.692 I print_info: n_swa            = 0
0.00.039.692 I print_info: n_embd_head_k    = 32
0.00.039.692 I print_info: n_embd_head_v    = 32
0.00.039.693 I print_info: n_gqa            = 1
0.00.039.694 I print_info: n_embd_k_gqa     = 384
0.00.039.696 I print_info: n_embd_v_gqa     = 384
0.00.039.697 I print_info: f_norm_eps       = 1.0e-12
0.00.039.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.698 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.698 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.698 I print_info: f_logit_scale    = 0.0e+00
0.00.039.699 I print_info: n_ff             = 1536
0.00.039.699 I print_info: n_expert         = 0
0.00.039.699 I print_info: n_expert_used    = 0
0.00.039.700 I print_info: causal attn      = 0
0.00.039.706 I print_info: pooling type     = 2
0.00.039.706 I print_info: rope type        = 2
0.00.039.707 I print_info: rope scaling     = linear
0.00.039.707 I print_info: freq_base_train  = 10000.0
0.00.039.708 I print_info: freq_scale_train = 1
0.00.039.708 I print_info: n_ctx_orig_yarn  = 512
0.00.039.708 I print_info: rope_finetuned   = unknown
0.00.039.711 I print_info: ssm_d_conv       = 0
0.00.039.711 I print_info: ssm_d_inner      = 0
0.00.039.711 I print_info: ssm_d_state      = 0
0.00.039.711 I print_info: ssm_dt_rank      = 0
0.00.039.711 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.712 I print_info: model type       = 33M
0.00.039.712 I print_info: model params     = 33.21 M
0.00.039.712 I print_info: general.name     = Bge Small
0.00.039.713 I print_info: vocab type       = WPM
0.00.039.713 I print_info: n_vocab          = 30522
0.00.039.714 I print_info: n_merges         = 0
0.00.039.714 I print_info: BOS token        = 101 '[CLS]'
0.00.039.714 I print_info: UNK token        = 100 '[UNK]'
0.00.039.714 I print_info: SEP token        = 102 '[SEP]'
0.00.039.715 I print_info: PAD token        = 0 '[PAD]'
0.00.039.715 I print_info: MASK token       = 103 '[MASK]'
0.00.039.715 I print_info: LF token         = 0 '[PAD]'
0.00.039.716 I print_info: max token length = 21
0.00.039.716 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.042.928 I load_tensors: offloading 12 repeating layers to GPU
0.00.042.930 I load_tensors: offloading output layer to GPU
0.00.042.930 I load_tensors: offloaded 13/13 layers to GPU
0.00.042.955 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.042.956 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.043.195 I llama_context: constructing llama_context
0.00.043.196 I llama_context: n_seq_max     = 1
0.00.043.197 I llama_context: n_ctx         = 512
0.00.043.197 I llama_context: n_ctx_per_seq = 512
0.00.043.197 I llama_context: n_batch       = 2048
0.00.043.198 I llama_context: n_ubatch      = 2048
0.00.043.198 I llama_context: flash_attn    = 0
0.00.043.198 I llama_context: freq_base     = 10000.0
0.00.043.199 I llama_context: freq_scale    = 1
0.00.043.199 I ggml_metal_init: allocating
0.00.043.204 I ggml_metal_init: found device: Apple M4
0.00.043.209 I ggml_metal_init: picking default device: Apple M4
0.00.043.986 I ggml_metal_init: using embedded metal library
0.00.047.879 I ggml_metal_init: GPU name:   Apple M4
0.00.047.882 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.047.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.047.883 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.047.883 I ggml_metal_init: simdgroup reduction   = true
0.00.047.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.047.883 I ggml_metal_init: has residency sets    = true
0.00.047.883 I ggml_metal_init: has bfloat            = true
0.00.047.884 I ggml_metal_init: use bfloat            = true
0.00.047.884 I ggml_metal_init: hasUnifiedMemory      = true
0.00.047.885 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.059.896 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.061.119 I init:      Metal compute buffer size =    16.75 MiB
0.00.061.121 I init:        CPU compute buffer size =     2.51 MiB
0.00.061.121 I init: graph nodes  = 441
0.00.061.122 I init: graph splits = 2
0.00.061.122 W get_kv_self: llama_context::get_kv_self() is not supported
0.00.061.123 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.061.123 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.061.124 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.161 W get_kv_self: llama_context::get_kv_self() is not supported
0.00.066.165 I 
0.00.066.194 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.765 W get_kv_self: llama_context::get_kv_self() is not supported
0.00.066.766 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.615 I llama_perf_context_print:        load time =      44.01 ms
0.00.071.616 I llama_perf_context_print: prompt eval time =       4.71 ms /     9 tokens (    0.52 ms per token,  1912.05 tokens per second)
0.00.071.617 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.617 I llama_perf_context_print:       total time =       5.45 ms /    10 tokens
0.00.071.767 I ggml_metal_free: deallocating

real	0m0.255s
user	0m0.049s
sys	0m0.036s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.042 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.310 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.972 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.976 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.977 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.978 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.978 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.978 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.979 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.980 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.980 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.980 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.980 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.984 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.984 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.984 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.985 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.985 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.986 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.367 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.999 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.000 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.001 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.001 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.001 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.002 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.002 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.002 I llama_model_loader: - type  f32:  124 tensors
0.00.015.003 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.003 I print_info: file format = GGUF V3 (latest)
0.00.015.003 I print_info: file type   = Q8_0
0.00.015.005 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.390 I load: special tokens cache size = 5
0.00.018.655 I load: token to piece cache size = 0.2032 MB
0.00.018.665 I print_info: arch             = bert
0.00.018.666 I print_info: vocab_only       = 0
0.00.018.666 I print_info: n_ctx_train      = 512
0.00.018.666 I print_info: n_embd           = 384
0.00.018.667 I print_info: n_layer          = 12
0.00.018.670 I print_info: n_head           = 12
0.00.018.670 I print_info: n_head_kv        = 12
0.00.018.670 I print_info: n_rot            = 32
0.00.018.670 I print_info: n_swa            = 0
0.00.018.671 I print_info: n_embd_head_k    = 32
0.00.018.671 I print_info: n_embd_head_v    = 32
0.00.018.671 I print_info: n_gqa            = 1
0.00.018.672 I print_info: n_embd_k_gqa     = 384
0.00.018.674 I print_info: n_embd_v_gqa     = 384
0.00.018.675 I print_info: f_norm_eps       = 1.0e-12
0.00.018.676 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.676 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.676 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.676 I print_info: f_logit_scale    = 0.0e+00
0.00.018.677 I print_info: n_ff             = 1536
0.00.018.677 I print_info: n_expert         = 0
0.00.018.677 I print_info: n_expert_used    = 0
0.00.018.677 I print_info: causal attn      = 0
0.00.018.677 I print_info: pooling type     = 2
0.00.018.678 I print_info: rope type        = 2
0.00.018.678 I print_info: rope scaling     = linear
0.00.018.678 I print_info: freq_base_train  = 10000.0
0.00.018.678 I print_info: freq_scale_train = 1
0.00.018.678 I print_info: n_ctx_orig_yarn  = 512
0.00.018.679 I print_info: rope_finetuned   = unknown
0.00.018.679 I print_info: ssm_d_conv       = 0
0.00.018.679 I print_info: ssm_d_inner      = 0
0.00.018.679 I print_info: ssm_d_state      = 0
0.00.018.679 I print_info: ssm_dt_rank      = 0
0.00.018.679 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.679 I print_info: model type       = 33M
0.00.018.680 I print_info: model params     = 33.21 M
0.00.018.682 I print_info: general.name     = Bge Small
0.00.018.683 I print_info: vocab type       = WPM
0.00.018.683 I print_info: n_vocab          = 30522
0.00.018.683 I print_info: n_merges         = 0
0.00.018.683 I print_info: BOS token        = 101 '[CLS]'
0.00.018.684 I print_info: UNK token        = 100 '[UNK]'
0.00.018.684 I print_info: SEP token        = 102 '[SEP]'
0.00.018.684 I print_info: PAD token        = 0 '[PAD]'
0.00.018.684 I print_info: MASK token       = 103 '[MASK]'
0.00.018.684 I print_info: LF token         = 0 '[PAD]'
0.00.018.684 I print_info: max token length = 21
0.00.018.685 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.415 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.416 I load_tensors: offloading output layer to GPU
0.00.020.417 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.422 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.423 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.594 I llama_context: constructing llama_context
0.00.020.595 I llama_context: n_seq_max     = 1
0.00.020.596 I llama_context: n_ctx         = 512
0.00.020.596 I llama_context: n_ctx_per_seq = 512
0.00.020.596 I llama_context: n_batch       = 2048
0.00.020.596 I llama_context: n_ubatch      = 2048
0.00.020.597 I llama_context: flash_attn    = 0
0.00.020.597 I llama_context: freq_base     = 10000.0
0.00.020.597 I llama_context: freq_scale    = 1
0.00.020.598 I ggml_metal_init: allocating
0.00.020.601 I ggml_metal_init: found device: Apple M4
0.00.020.605 I ggml_metal_init: picking default device: Apple M4
0.00.021.122 I ggml_metal_init: using embedded metal library
0.00.023.480 I ggml_metal_init: GPU name:   Apple M4
0.00.023.481 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.482 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.482 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.483 I ggml_metal_init: simdgroup reduction   = true
0.00.023.483 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.483 I ggml_metal_init: has residency sets    = true
0.00.023.483 I ggml_metal_init: has bfloat            = true
0.00.023.483 I ggml_metal_init: use bfloat            = true
0.00.023.484 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.747 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.034.748 I init:      Metal compute buffer size =    16.75 MiB
0.00.034.750 I init:        CPU compute buffer size =     2.51 MiB
0.00.034.750 I init: graph nodes  = 441
0.00.034.750 I init: graph splits = 2
0.00.034.751 W get_kv_self: llama_context::get_kv_self() is not supported
0.00.034.751 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.034.752 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.752 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.112 W get_kv_self: llama_context::get_kv_self() is not supported
0.00.038.115 I 
0.00.038.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.577 W get_kv_self: llama_context::get_kv_self() is not supported
0.00.038.578 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.041.749 I llama_perf_context_print:        load time =      28.80 ms
0.00.041.750 I llama_perf_context_print: prompt eval time =       3.04 ms /     9 tokens (    0.34 ms per token,  2956.64 tokens per second)
0.00.041.751 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.041.751 I llama_perf_context_print:       total time =       3.64 ms /    10 tokens
0.00.041.940 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.241 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.389 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.782 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.790 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.794 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.794 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.795 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.797 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.797 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.798 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.802 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.802 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.806 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.806 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.807 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.808 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.808 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.541 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.723 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.318 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.318 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.319 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.319 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.320 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.320 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.320 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.321 I llama_model_loader: - type  f32:   40 tensors
0.00.049.321 I llama_model_loader: - type  f16:   30 tensors
0.00.049.322 I print_info: file format = GGUF V3 (latest)
0.00.049.323 I print_info: file type   = F16
0.00.049.324 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.814 W load: empty token at index 5
0.00.059.338 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.968 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.006 I load: special tokens cache size = 5
0.00.318.893 I load: token to piece cache size = 1.5060 MB
0.00.318.925 I print_info: arch             = jina-bert-v2
0.00.318.926 I print_info: vocab_only       = 0
0.00.318.926 I print_info: n_ctx_train      = 8192
0.00.318.926 I print_info: n_embd           = 384
0.00.318.926 I print_info: n_layer          = 4
0.00.318.933 I print_info: n_head           = 12
0.00.318.934 I print_info: n_head_kv        = 12
0.00.318.934 I print_info: n_rot            = 32
0.00.318.934 I print_info: n_swa            = 0
0.00.318.934 I print_info: n_embd_head_k    = 32
0.00.318.934 I print_info: n_embd_head_v    = 32
0.00.318.935 I print_info: n_gqa            = 1
0.00.318.938 I print_info: n_embd_k_gqa     = 384
0.00.318.938 I print_info: n_embd_v_gqa     = 384
0.00.318.939 I print_info: f_norm_eps       = 1.0e-12
0.00.318.939 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.318.940 I print_info: f_clamp_kqv      = 0.0e+00
0.00.318.940 I print_info: f_max_alibi_bias = 8.0e+00
0.00.318.940 I print_info: f_logit_scale    = 0.0e+00
0.00.318.941 I print_info: n_ff             = 1536
0.00.318.941 I print_info: n_expert         = 0
0.00.318.941 I print_info: n_expert_used    = 0
0.00.318.941 I print_info: causal attn      = 0
0.00.318.941 I print_info: pooling type     = -1
0.00.318.941 I print_info: rope type        = -1
0.00.318.942 I print_info: rope scaling     = linear
0.00.318.942 I print_info: freq_base_train  = 10000.0
0.00.318.942 I print_info: freq_scale_train = 1
0.00.318.942 I print_info: n_ctx_orig_yarn  = 8192
0.00.318.943 I print_info: rope_finetuned   = unknown
0.00.318.943 I print_info: ssm_d_conv       = 0
0.00.318.943 I print_info: ssm_d_inner      = 0
0.00.318.943 I print_info: ssm_d_state      = 0
0.00.318.943 I print_info: ssm_dt_rank      = 0
0.00.318.943 I print_info: ssm_dt_b_c_rms   = 0
0.00.318.944 I print_info: model type       = 33M
0.00.318.944 I print_info: model params     = 32.90 M
0.00.318.945 I print_info: general.name     = Jina Bert Implementation
0.00.318.946 I print_info: vocab type       = BPE
0.00.318.946 I print_info: n_vocab          = 61056
0.00.318.946 I print_info: n_merges         = 39382
0.00.318.946 I print_info: BOS token        = 0 '<s>'
0.00.318.947 I print_info: EOS token        = 2 '</s>'
0.00.318.947 I print_info: UNK token        = 3 '<unk>'
0.00.318.947 I print_info: SEP token        = 2 '</s>'
0.00.318.947 I print_info: PAD token        = 1 '<pad>'
0.00.318.947 I print_info: MASK token       = 4 '<mask>'
0.00.318.948 I print_info: EOG token        = 2 '</s>'
0.00.318.948 I print_info: max token length = 45
0.00.318.949 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.320.966 I load_tensors: offloading 4 repeating layers to GPU
0.00.320.967 I load_tensors: offloading output layer to GPU
0.00.320.968 I load_tensors: offloaded 5/5 layers to GPU
0.00.320.992 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.320.993 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.321.249 I llama_context: constructing llama_context
0.00.321.250 I llama_context: n_seq_max     = 1
0.00.321.251 I llama_context: n_ctx         = 8192
0.00.321.251 I llama_context: n_ctx_per_seq = 8192
0.00.321.251 I llama_context: n_batch       = 2048
0.00.321.251 I llama_context: n_ubatch      = 2048
0.00.321.251 I llama_context: flash_attn    = 0
0.00.321.252 I llama_context: freq_base     = 10000.0
0.00.321.252 I llama_context: freq_scale    = 1
0.00.321.253 I ggml_metal_init: allocating
0.00.321.256 I ggml_metal_init: found device: Apple M4
0.00.321.259 I ggml_metal_init: picking default device: Apple M4
0.00.322.165 I ggml_metal_init: using embedded metal library
0.00.324.828 I ggml_metal_init: GPU name:   Apple M4
0.00.324.830 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.324.830 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.324.831 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.324.831 I ggml_metal_init: simdgroup reduction   = true
0.00.324.831 I ggml_metal_init: simdgroup matrix mul. = true
0.00.324.831 I ggml_metal_init: has residency sets    = true
0.00.324.831 I ggml_metal_init: has bfloat            = true
0.00.324.831 I ggml_metal_init: use bfloat            = true
0.00.324.832 I ggml_metal_init: hasUnifiedMemory      = true
0.00.324.832 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.334.342 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.341.531 I init:      Metal compute buffer size =   223.01 MiB
0.00.341.533 I init:        CPU compute buffer size =    22.02 MiB
0.00.341.533 I init: graph nodes  = 158
0.00.341.533 I init: graph splits = 2
0.00.341.534 W get_kv_self: llama_context::get_kv_self() is not supported
0.00.341.534 W common_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting
0.00.341.535 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.341.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.344.932 W get_kv_self: llama_context::get_kv_self() is not supported
0.00.344.935 I 
0.00.344.950 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.345.065 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.345.065 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.345.068 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.345.068 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.345.072 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.345.072 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.345.098 W get_kv_self: llama_context::get_kv_self() is not supported
0.00.345.098 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.348.550 I llama_perf_context_print:        load time =     323.54 ms
0.00.348.551 I llama_perf_context_print: prompt eval time =       3.44 ms /    62 tokens (    0.06 ms per token, 18002.32 tokens per second)
0.00.348.551 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.348.552 I llama_perf_context_print:       total time =       3.61 ms /    63 tokens
0.00.348.795 I ggml_metal_free: deallocating

real	0m1.114s
user	0m0.325s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.219 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.397 I main: llama backend init
0.00.000.404 I main: load the model and apply lora adapter, if any
0.00.035.143 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.047.723 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.739 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.740 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.745 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.746 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.746 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.759 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.760 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.761 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.761 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.767 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.767 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.066.975 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.066.978 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.066.979 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.066.979 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.066.980 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.066.981 I llama_model_loader: - type  f32:  194 tensors
0.00.066.981 I llama_model_loader: - type  f16:   98 tensors
0.00.066.982 I print_info: file format = GGUF V3 (latest)
0.00.066.984 I print_info: file type   = all F32 (guessed)
0.00.066.986 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.229 I load: special tokens cache size = 25
0.00.090.054 I load: token to piece cache size = 0.2984 MB
0.00.090.078 I print_info: arch             = gptneox
0.00.090.079 I print_info: vocab_only       = 0
0.00.090.079 I print_info: n_ctx_train      = 2048
0.00.090.079 I print_info: n_embd           = 2048
0.00.090.080 I print_info: n_layer          = 24
0.00.090.083 I print_info: n_head           = 16
0.00.090.084 I print_info: n_head_kv        = 16
0.00.090.084 I print_info: n_rot            = 32
0.00.090.084 I print_info: n_swa            = 0
0.00.090.085 I print_info: n_embd_head_k    = 128
0.00.090.085 I print_info: n_embd_head_v    = 128
0.00.090.086 I print_info: n_gqa            = 1
0.00.090.087 I print_info: n_embd_k_gqa     = 2048
0.00.090.089 I print_info: n_embd_v_gqa     = 2048
0.00.090.090 I print_info: f_norm_eps       = 1.0e-05
0.00.090.090 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.091 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.091 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.091 I print_info: f_logit_scale    = 0.0e+00
0.00.090.092 I print_info: n_ff             = 8192
0.00.090.092 I print_info: n_expert         = 0
0.00.090.092 I print_info: n_expert_used    = 0
0.00.090.092 I print_info: causal attn      = 1
0.00.090.092 I print_info: pooling type     = 0
0.00.090.093 I print_info: rope type        = 2
0.00.090.093 I print_info: rope scaling     = linear
0.00.090.095 I print_info: freq_base_train  = 10000.0
0.00.090.095 I print_info: freq_scale_train = 1
0.00.090.095 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.096 I print_info: rope_finetuned   = unknown
0.00.090.096 I print_info: ssm_d_conv       = 0
0.00.090.096 I print_info: ssm_d_inner      = 0
0.00.090.096 I print_info: ssm_d_state      = 0
0.00.090.096 I print_info: ssm_dt_rank      = 0
0.00.090.096 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.097 I print_info: model type       = 1.4B
0.00.090.097 I print_info: model params     = 1.41 B
0.00.090.097 I print_info: general.name     = 1.4B
0.00.090.098 I print_info: vocab type       = BPE
0.00.090.098 I print_info: n_vocab          = 50304
0.00.090.098 I print_info: n_merges         = 50009
0.00.090.098 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.098 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.098 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.099 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.099 I print_info: LF token         = 187 ''
0.00.090.103 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.103 I print_info: max token length = 1024
0.00.090.103 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.130.166 I load_tensors: offloading 24 repeating layers to GPU
0.00.130.169 I load_tensors: offloading output layer to GPU
0.00.130.170 I load_tensors: offloaded 25/25 layers to GPU
0.00.130.192 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.130.194 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.130.620 I llama_context: constructing llama_context
0.00.130.620 I llama_context: n_seq_max     = 1
0.00.130.621 I llama_context: n_ctx         = 2048
0.00.130.621 I llama_context: n_ctx_per_seq = 2048
0.00.130.621 I llama_context: n_batch       = 2048
0.00.130.621 I llama_context: n_ubatch      = 512
0.00.130.621 I llama_context: flash_attn    = 0
0.00.130.622 I llama_context: freq_base     = 10000.0
0.00.130.622 I llama_context: freq_scale    = 1
0.00.130.623 I ggml_metal_init: allocating
0.00.130.645 I ggml_metal_init: found device: Apple M4
0.00.130.651 I ggml_metal_init: picking default device: Apple M4
0.00.131.327 I ggml_metal_init: using embedded metal library
0.00.142.501 I ggml_metal_init: GPU name:   Apple M4
0.00.142.503 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.142.503 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.142.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.142.504 I ggml_metal_init: simdgroup reduction   = true
0.00.142.504 I ggml_metal_init: simdgroup matrix mul. = true
0.00.142.504 I ggml_metal_init: has residency sets    = true
0.00.142.504 I ggml_metal_init: has bfloat            = true
0.00.142.504 I ggml_metal_init: use bfloat            = true
0.00.142.505 I ggml_metal_init: hasUnifiedMemory      = true
0.00.142.505 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.166.535 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.166.537 I llama_context_kv_self: constructing llama_context_kv_self
0.00.166.538 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.197.953 I init:      Metal KV buffer size =   384.00 MiB
0.00.197.961 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.202.520 I init:      Metal compute buffer size =   102.25 MiB
0.00.202.523 I init:        CPU compute buffer size =     8.01 MiB
0.00.202.523 I init: graph nodes  = 991
0.00.202.523 I init: graph splits = 2
0.00.202.530 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.202.663 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.202.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.268.470 I main: llama threadpool init, n_threads = 4
0.00.268.515 I 
0.00.268.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.268.530 I 
0.00.268.705 I sampler seed: 1234
0.00.268.710 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.268.734 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.268.736 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.268.736 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.106.019 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61101.55 tokens per second)
0.02.106.020 I llama_perf_context_print:        load time =     232.41 ms
0.02.106.021 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.20 tokens per second)
0.02.106.022 I llama_perf_context_print:        eval time =    1790.84 ms /    63 runs   (   28.43 ms per token,    35.18 tokens per second)
0.02.106.022 I llama_perf_context_print:       total time =    1838.45 ms /    70 tokens
0.02.109.848 I ggml_metal_free: deallocating

real	0m2.391s
user	0m0.130s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.562 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.622 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.412 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.425 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.426 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.427 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.428 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.435 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.435 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.436 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.440 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.947 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.319 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.321 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.322 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.323 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.323 I llama_model_loader: - type  f32:  194 tensors
0.00.056.324 I llama_model_loader: - type  f16:   98 tensors
0.00.056.324 I print_info: file format = GGUF V3 (latest)
0.00.056.326 I print_info: file type   = all F32 (guessed)
0.00.056.327 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.164 I load: special tokens cache size = 25
0.00.077.018 I load: token to piece cache size = 0.2984 MB
0.00.077.033 I print_info: arch             = gptneox
0.00.077.035 I print_info: vocab_only       = 0
0.00.077.035 I print_info: n_ctx_train      = 2048
0.00.077.035 I print_info: n_embd           = 2048
0.00.077.035 I print_info: n_layer          = 24
0.00.077.038 I print_info: n_head           = 16
0.00.077.039 I print_info: n_head_kv        = 16
0.00.077.039 I print_info: n_rot            = 32
0.00.077.042 I print_info: n_swa            = 0
0.00.077.042 I print_info: n_embd_head_k    = 128
0.00.077.042 I print_info: n_embd_head_v    = 128
0.00.077.043 I print_info: n_gqa            = 1
0.00.077.044 I print_info: n_embd_k_gqa     = 2048
0.00.077.045 I print_info: n_embd_v_gqa     = 2048
0.00.077.046 I print_info: f_norm_eps       = 1.0e-05
0.00.077.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.047 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.047 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.047 I print_info: f_logit_scale    = 0.0e+00
0.00.077.052 I print_info: n_ff             = 8192
0.00.077.053 I print_info: n_expert         = 0
0.00.077.053 I print_info: n_expert_used    = 0
0.00.077.054 I print_info: causal attn      = 1
0.00.077.054 I print_info: pooling type     = 0
0.00.077.054 I print_info: rope type        = 2
0.00.077.056 I print_info: rope scaling     = linear
0.00.077.056 I print_info: freq_base_train  = 10000.0
0.00.077.057 I print_info: freq_scale_train = 1
0.00.077.057 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.057 I print_info: rope_finetuned   = unknown
0.00.077.057 I print_info: ssm_d_conv       = 0
0.00.077.057 I print_info: ssm_d_inner      = 0
0.00.077.058 I print_info: ssm_d_state      = 0
0.00.077.059 I print_info: ssm_dt_rank      = 0
0.00.077.059 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.060 I print_info: model type       = 1.4B
0.00.077.060 I print_info: model params     = 1.41 B
0.00.077.061 I print_info: general.name     = 1.4B
0.00.077.062 I print_info: vocab type       = BPE
0.00.077.063 I print_info: n_vocab          = 50304
0.00.077.063 I print_info: n_merges         = 50009
0.00.077.064 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.064 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.064 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.064 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.064 I print_info: LF token         = 187 ''
0.00.077.065 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.065 I print_info: max token length = 1024
0.00.077.070 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.474.167 I load_tensors: offloading 24 repeating layers to GPU
0.01.474.172 I load_tensors: offloading output layer to GPU
0.01.474.172 I load_tensors: offloaded 25/25 layers to GPU
0.01.474.199 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.474.201 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.475.352 I llama_context: constructing llama_context
0.01.475.353 I llama_context: n_seq_max     = 1
0.01.475.354 I llama_context: n_ctx         = 128
0.01.475.354 I llama_context: n_ctx_per_seq = 128
0.01.475.354 I llama_context: n_batch       = 128
0.01.475.355 I llama_context: n_ubatch      = 128
0.01.475.355 I llama_context: flash_attn    = 0
0.01.475.355 I llama_context: freq_base     = 10000.0
0.01.475.356 I llama_context: freq_scale    = 1
0.01.475.356 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.475.357 I ggml_metal_init: allocating
0.01.475.443 I ggml_metal_init: found device: Apple M4
0.01.475.449 I ggml_metal_init: picking default device: Apple M4
0.01.476.682 I ggml_metal_init: using embedded metal library
0.01.480.594 I ggml_metal_init: GPU name:   Apple M4
0.01.480.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.480.597 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.480.598 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.480.598 I ggml_metal_init: simdgroup reduction   = true
0.01.480.598 I ggml_metal_init: simdgroup matrix mul. = true
0.01.480.598 I ggml_metal_init: has residency sets    = true
0.01.480.599 I ggml_metal_init: has bfloat            = true
0.01.480.599 I ggml_metal_init: use bfloat            = true
0.01.480.600 I ggml_metal_init: hasUnifiedMemory      = true
0.01.480.601 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.491.258 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.491.260 I llama_context_kv_self: constructing llama_context_kv_self
0.01.491.261 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.492.996 I init:      Metal KV buffer size =    24.00 MiB
0.01.492.998 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.494.635 I init:      Metal compute buffer size =    25.56 MiB
0.01.494.636 I init:        CPU compute buffer size =     1.06 MiB
0.01.494.637 I init: graph nodes  = 991
0.01.494.637 I init: graph splits = 2
0.01.494.638 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.494.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.529.211 I 
0.01.529.235 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.529.240 I perplexity: tokenizing the input ..
0.01.534.252 I perplexity: tokenization took 5.011 ms
0.01.534.256 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.652.701 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.654.049 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.654.079 I llama_perf_context_print:        load time =    1504.58 ms
0.01.654.079 I llama_perf_context_print: prompt eval time =     118.16 ms /   128 tokens (    0.92 ms per token,  1083.29 tokens per second)
0.01.654.080 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.654.080 I llama_perf_context_print:       total time =     124.87 ms /   129 tokens
0.01.654.664 I ggml_metal_free: deallocating

real	0m1.892s
user	0m0.098s
sys	0m0.292s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.934 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.317 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.324 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.326 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.327 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.331 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.332 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.332 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.334 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.334 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.336 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.339 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.376 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.414 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.367 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.367 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.368 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.368 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.368 I llama_model_loader: - type  f32:  194 tensors
0.00.038.369 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.369 I print_info: file format = GGUF V3 (latest)
0.00.038.370 I print_info: file type   = Q8_0
0.00.038.371 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.109 I load: special tokens cache size = 25
0.00.054.290 I load: token to piece cache size = 0.2984 MB
0.00.054.306 I print_info: arch             = gptneox
0.00.054.307 I print_info: vocab_only       = 0
0.00.054.307 I print_info: n_ctx_train      = 2048
0.00.054.307 I print_info: n_embd           = 2048
0.00.054.308 I print_info: n_layer          = 24
0.00.054.312 I print_info: n_head           = 16
0.00.054.313 I print_info: n_head_kv        = 16
0.00.054.313 I print_info: n_rot            = 32
0.00.054.313 I print_info: n_swa            = 0
0.00.054.313 I print_info: n_embd_head_k    = 128
0.00.054.316 I print_info: n_embd_head_v    = 128
0.00.054.317 I print_info: n_gqa            = 1
0.00.054.317 I print_info: n_embd_k_gqa     = 2048
0.00.054.318 I print_info: n_embd_v_gqa     = 2048
0.00.054.319 I print_info: f_norm_eps       = 1.0e-05
0.00.054.319 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.320 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.320 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.320 I print_info: f_logit_scale    = 0.0e+00
0.00.054.321 I print_info: n_ff             = 8192
0.00.054.321 I print_info: n_expert         = 0
0.00.054.321 I print_info: n_expert_used    = 0
0.00.054.321 I print_info: causal attn      = 1
0.00.054.321 I print_info: pooling type     = 0
0.00.054.321 I print_info: rope type        = 2
0.00.054.322 I print_info: rope scaling     = linear
0.00.054.322 I print_info: freq_base_train  = 10000.0
0.00.054.322 I print_info: freq_scale_train = 1
0.00.054.322 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.323 I print_info: rope_finetuned   = unknown
0.00.054.323 I print_info: ssm_d_conv       = 0
0.00.054.323 I print_info: ssm_d_inner      = 0
0.00.054.323 I print_info: ssm_d_state      = 0
0.00.054.323 I print_info: ssm_dt_rank      = 0
0.00.054.323 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.324 I print_info: model type       = 1.4B
0.00.054.324 I print_info: model params     = 1.41 B
0.00.054.324 I print_info: general.name     = 1.4B
0.00.054.325 I print_info: vocab type       = BPE
0.00.054.325 I print_info: n_vocab          = 50304
0.00.054.325 I print_info: n_merges         = 50009
0.00.054.325 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.326 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.326 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.326 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.326 I print_info: LF token         = 187 ''
0.00.054.327 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.328 I print_info: max token length = 1024
0.00.054.328 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.141.545 I load_tensors: offloading 24 repeating layers to GPU
0.01.141.551 I load_tensors: offloading output layer to GPU
0.01.141.552 I load_tensors: offloaded 25/25 layers to GPU
0.01.141.575 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.141.578 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.142.947 I llama_context: constructing llama_context
0.01.142.949 I llama_context: n_seq_max     = 1
0.01.142.950 I llama_context: n_ctx         = 2048
0.01.142.950 I llama_context: n_ctx_per_seq = 2048
0.01.142.951 I llama_context: n_batch       = 2048
0.01.142.951 I llama_context: n_ubatch      = 512
0.01.142.951 I llama_context: flash_attn    = 0
0.01.142.952 I llama_context: freq_base     = 10000.0
0.01.142.953 I llama_context: freq_scale    = 1
0.01.142.953 I ggml_metal_init: allocating
0.01.142.966 I ggml_metal_init: found device: Apple M4
0.01.142.975 I ggml_metal_init: picking default device: Apple M4
0.01.144.265 I ggml_metal_init: using embedded metal library
0.01.149.867 I ggml_metal_init: GPU name:   Apple M4
0.01.149.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.149.871 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.149.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.149.873 I ggml_metal_init: simdgroup reduction   = true
0.01.149.873 I ggml_metal_init: simdgroup matrix mul. = true
0.01.149.873 I ggml_metal_init: has residency sets    = true
0.01.149.873 I ggml_metal_init: has bfloat            = true
0.01.149.874 I ggml_metal_init: use bfloat            = true
0.01.149.874 I ggml_metal_init: hasUnifiedMemory      = true
0.01.149.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.166.210 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.166.212 I llama_context_kv_self: constructing llama_context_kv_self
0.01.166.214 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.213.528 I init:      Metal KV buffer size =   384.00 MiB
0.01.213.537 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.217.679 I init:      Metal compute buffer size =   102.25 MiB
0.01.217.681 I init:        CPU compute buffer size =     8.01 MiB
0.01.217.682 I init: graph nodes  = 991
0.01.217.682 I init: graph splits = 2
0.01.217.688 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.217.826 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.217.826 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.270.871 I main: llama threadpool init, n_threads = 4
0.01.270.911 I 
0.01.270.925 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.270.925 I 
0.01.271.108 I sampler seed: 1234
0.01.271.113 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.271.155 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.271.158 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.271.158 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.377.140 I llama_perf_sampler_print:    sampling time =       1.53 ms /    71 runs   (    0.02 ms per token, 46557.38 tokens per second)
0.02.377.142 I llama_perf_context_print:        load time =    1260.22 ms
0.02.377.142 I llama_perf_context_print: prompt eval time =      39.95 ms /     7 tokens (    5.71 ms per token,   175.23 tokens per second)
0.02.377.143 I llama_perf_context_print:        eval time =    1063.28 ms /    63 runs   (   16.88 ms per token,    59.25 tokens per second)
0.02.377.143 I llama_perf_context_print:       total time =    1106.99 ms /    70 tokens
0.02.380.300 I ggml_metal_free: deallocating

real	0m2.398s
user	0m0.112s
sys	0m0.263s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.178 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.827 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.832 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.833 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.833 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.834 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.836 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.836 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.813 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.751 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.753 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.754 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.754 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.754 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.755 I llama_model_loader: - type  f32:  194 tensors
0.00.025.755 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.756 I print_info: file format = GGUF V3 (latest)
0.00.025.757 I print_info: file type   = Q8_0
0.00.025.758 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.180 I load: special tokens cache size = 25
0.00.040.381 I load: token to piece cache size = 0.2984 MB
0.00.040.398 I print_info: arch             = gptneox
0.00.040.399 I print_info: vocab_only       = 0
0.00.040.399 I print_info: n_ctx_train      = 2048
0.00.040.399 I print_info: n_embd           = 2048
0.00.040.399 I print_info: n_layer          = 24
0.00.040.404 I print_info: n_head           = 16
0.00.040.404 I print_info: n_head_kv        = 16
0.00.040.405 I print_info: n_rot            = 32
0.00.040.405 I print_info: n_swa            = 0
0.00.040.405 I print_info: n_embd_head_k    = 128
0.00.040.405 I print_info: n_embd_head_v    = 128
0.00.040.406 I print_info: n_gqa            = 1
0.00.040.406 I print_info: n_embd_k_gqa     = 2048
0.00.040.407 I print_info: n_embd_v_gqa     = 2048
0.00.040.407 I print_info: f_norm_eps       = 1.0e-05
0.00.040.408 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.408 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.408 I print_info: f_logit_scale    = 0.0e+00
0.00.040.409 I print_info: n_ff             = 8192
0.00.040.409 I print_info: n_expert         = 0
0.00.040.409 I print_info: n_expert_used    = 0
0.00.040.412 I print_info: causal attn      = 1
0.00.040.412 I print_info: pooling type     = 0
0.00.040.412 I print_info: rope type        = 2
0.00.040.413 I print_info: rope scaling     = linear
0.00.040.413 I print_info: freq_base_train  = 10000.0
0.00.040.413 I print_info: freq_scale_train = 1
0.00.040.413 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.413 I print_info: rope_finetuned   = unknown
0.00.040.414 I print_info: ssm_d_conv       = 0
0.00.040.414 I print_info: ssm_d_inner      = 0
0.00.040.414 I print_info: ssm_d_state      = 0
0.00.040.414 I print_info: ssm_dt_rank      = 0
0.00.040.414 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.414 I print_info: model type       = 1.4B
0.00.040.415 I print_info: model params     = 1.41 B
0.00.040.422 I print_info: general.name     = 1.4B
0.00.040.425 I print_info: vocab type       = BPE
0.00.040.425 I print_info: n_vocab          = 50304
0.00.040.425 I print_info: n_merges         = 50009
0.00.040.426 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.426 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.427 I print_info: LF token         = 187 ''
0.00.040.427 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.427 I print_info: max token length = 1024
0.00.040.427 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.924.579 I load_tensors: offloading 24 repeating layers to GPU
0.00.924.586 I load_tensors: offloading output layer to GPU
0.00.924.586 I load_tensors: offloaded 25/25 layers to GPU
0.00.924.616 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.924.618 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.926.167 I llama_context: constructing llama_context
0.00.926.170 I llama_context: n_seq_max     = 1
0.00.926.170 I llama_context: n_ctx         = 128
0.00.926.170 I llama_context: n_ctx_per_seq = 128
0.00.926.171 I llama_context: n_batch       = 128
0.00.926.171 I llama_context: n_ubatch      = 128
0.00.926.171 I llama_context: flash_attn    = 0
0.00.926.172 I llama_context: freq_base     = 10000.0
0.00.926.173 I llama_context: freq_scale    = 1
0.00.926.173 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.926.174 I ggml_metal_init: allocating
0.00.926.230 I ggml_metal_init: found device: Apple M4
0.00.926.239 I ggml_metal_init: picking default device: Apple M4
0.00.927.667 I ggml_metal_init: using embedded metal library
0.00.933.130 I ggml_metal_init: GPU name:   Apple M4
0.00.933.133 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.933.134 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.933.135 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.933.136 I ggml_metal_init: simdgroup reduction   = true
0.00.933.136 I ggml_metal_init: simdgroup matrix mul. = true
0.00.933.136 I ggml_metal_init: has residency sets    = true
0.00.933.136 I ggml_metal_init: has bfloat            = true
0.00.933.137 I ggml_metal_init: use bfloat            = true
0.00.933.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.933.142 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.948.483 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.948.484 I llama_context_kv_self: constructing llama_context_kv_self
0.00.948.486 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.951.788 I init:      Metal KV buffer size =    24.00 MiB
0.00.951.794 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.954.766 I init:      Metal compute buffer size =    25.56 MiB
0.00.954.768 I init:        CPU compute buffer size =     1.06 MiB
0.00.954.768 I init: graph nodes  = 991
0.00.954.769 I init: graph splits = 2
0.00.954.772 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.954.772 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.979.429 I 
0.00.979.470 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.979.477 I perplexity: tokenizing the input ..
0.00.986.254 I perplexity: tokenization took 6.773 ms
0.00.986.269 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.121.233 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.122.576 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.122.600 I llama_perf_context_print:        load time =     970.25 ms
0.01.122.601 I llama_perf_context_print: prompt eval time =     134.50 ms /   128 tokens (    1.05 ms per token,   951.66 tokens per second)
0.01.122.602 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.122.602 I llama_perf_context_print:       total time =     143.17 ms /   129 tokens
0.01.123.187 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.077s
sys	0m0.171s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.075 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.122 I main: llama backend init
0.00.000.124 I main: load the model and apply lora adapter, if any
0.00.016.525 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.038.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.315 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.317 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.317 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.317 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.318 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.318 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.319 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.324 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.324 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.279 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.426 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.422 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.047.423 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.424 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.424 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.424 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.425 I llama_model_loader: - type  f32:  194 tensors
0.00.047.425 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.426 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.427 I print_info: file format = GGUF V3 (latest)
0.00.047.427 I print_info: file type   = Q4_0
0.00.047.428 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.055.936 I load: special tokens cache size = 25
0.00.068.331 I load: token to piece cache size = 0.2984 MB
0.00.068.357 I print_info: arch             = gptneox
0.00.068.358 I print_info: vocab_only       = 0
0.00.068.358 I print_info: n_ctx_train      = 2048
0.00.068.359 I print_info: n_embd           = 2048
0.00.068.359 I print_info: n_layer          = 24
0.00.068.364 I print_info: n_head           = 16
0.00.068.364 I print_info: n_head_kv        = 16
0.00.068.364 I print_info: n_rot            = 32
0.00.068.365 I print_info: n_swa            = 0
0.00.068.365 I print_info: n_embd_head_k    = 128
0.00.068.365 I print_info: n_embd_head_v    = 128
0.00.068.365 I print_info: n_gqa            = 1
0.00.068.366 I print_info: n_embd_k_gqa     = 2048
0.00.068.367 I print_info: n_embd_v_gqa     = 2048
0.00.068.367 I print_info: f_norm_eps       = 1.0e-05
0.00.068.367 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.367 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.367 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.370 I print_info: f_logit_scale    = 0.0e+00
0.00.068.373 I print_info: n_ff             = 8192
0.00.068.373 I print_info: n_expert         = 0
0.00.068.373 I print_info: n_expert_used    = 0
0.00.068.374 I print_info: causal attn      = 1
0.00.068.374 I print_info: pooling type     = 0
0.00.068.374 I print_info: rope type        = 2
0.00.068.374 I print_info: rope scaling     = linear
0.00.068.374 I print_info: freq_base_train  = 10000.0
0.00.068.375 I print_info: freq_scale_train = 1
0.00.068.375 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.375 I print_info: rope_finetuned   = unknown
0.00.068.375 I print_info: ssm_d_conv       = 0
0.00.068.375 I print_info: ssm_d_inner      = 0
0.00.068.375 I print_info: ssm_d_state      = 0
0.00.068.375 I print_info: ssm_dt_rank      = 0
0.00.068.375 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.376 I print_info: model type       = 1.4B
0.00.068.376 I print_info: model params     = 1.41 B
0.00.068.376 I print_info: general.name     = 1.4B
0.00.068.377 I print_info: vocab type       = BPE
0.00.068.379 I print_info: n_vocab          = 50304
0.00.068.379 I print_info: n_merges         = 50009
0.00.068.379 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.379 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.379 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.379 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.380 I print_info: LF token         = 187 ''
0.00.068.380 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.380 I print_info: max token length = 1024
0.00.068.380 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.100.569 I load_tensors: offloading 24 repeating layers to GPU
0.01.100.574 I load_tensors: offloading output layer to GPU
0.01.100.575 I load_tensors: offloaded 25/25 layers to GPU
0.01.100.593 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.01.100.594 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.01.101.326 I llama_context: constructing llama_context
0.01.101.329 I llama_context: n_seq_max     = 1
0.01.101.330 I llama_context: n_ctx         = 2048
0.01.101.330 I llama_context: n_ctx_per_seq = 2048
0.01.101.331 I llama_context: n_batch       = 2048
0.01.101.331 I llama_context: n_ubatch      = 512
0.01.101.331 I llama_context: flash_attn    = 0
0.01.101.332 I llama_context: freq_base     = 10000.0
0.01.101.333 I llama_context: freq_scale    = 1
0.01.101.334 I ggml_metal_init: allocating
0.01.101.377 I ggml_metal_init: found device: Apple M4
0.01.101.389 I ggml_metal_init: picking default device: Apple M4
0.01.102.497 I ggml_metal_init: using embedded metal library
0.01.107.513 I ggml_metal_init: GPU name:   Apple M4
0.01.107.522 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.107.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.107.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.107.524 I ggml_metal_init: simdgroup reduction   = true
0.01.107.524 I ggml_metal_init: simdgroup matrix mul. = true
0.01.107.524 I ggml_metal_init: has residency sets    = true
0.01.107.525 I ggml_metal_init: has bfloat            = true
0.01.107.525 I ggml_metal_init: use bfloat            = true
0.01.107.526 I ggml_metal_init: hasUnifiedMemory      = true
0.01.107.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.125.351 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.125.352 I llama_context_kv_self: constructing llama_context_kv_self
0.01.125.354 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.155.793 I init:      Metal KV buffer size =   384.00 MiB
0.01.155.802 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.160.621 I init:      Metal compute buffer size =   102.25 MiB
0.01.160.624 I init:        CPU compute buffer size =     8.01 MiB
0.01.160.624 I init: graph nodes  = 991
0.01.160.624 I init: graph splits = 2
0.01.160.633 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.160.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.160.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.219.371 I main: llama threadpool init, n_threads = 4
0.01.219.415 I 
0.01.219.431 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.219.431 I 
0.01.219.611 I sampler seed: 1234
0.01.219.616 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.219.650 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.219.654 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.219.654 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.901.610 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.01.901.610 I llama_perf_context_print:        load time =    1202.10 ms
0.01.901.611 I llama_perf_context_print: prompt eval time =      49.56 ms /     7 tokens (    7.08 ms per token,   141.25 tokens per second)
0.01.901.612 I llama_perf_context_print:        eval time =     629.49 ms /    63 runs   (    9.99 ms per token,   100.08 tokens per second)
0.01.901.612 I llama_perf_context_print:       total time =     682.98 ms /    70 tokens
0.01.905.348 I ggml_metal_free: deallocating

real	0m1.949s
user	0m0.113s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.582 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.209 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.215 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.222 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.222 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.230 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.230 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.231 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.163 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.131 I llama_model_loader: - type  f32:  194 tensors
0.00.026.132 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.132 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.133 I print_info: file format = GGUF V3 (latest)
0.00.026.133 I print_info: file type   = Q4_0
0.00.026.135 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.544 I load: special tokens cache size = 25
0.00.040.786 I load: token to piece cache size = 0.2984 MB
0.00.040.804 I print_info: arch             = gptneox
0.00.040.805 I print_info: vocab_only       = 0
0.00.040.805 I print_info: n_ctx_train      = 2048
0.00.040.805 I print_info: n_embd           = 2048
0.00.040.805 I print_info: n_layer          = 24
0.00.040.810 I print_info: n_head           = 16
0.00.040.810 I print_info: n_head_kv        = 16
0.00.040.811 I print_info: n_rot            = 32
0.00.040.811 I print_info: n_swa            = 0
0.00.040.811 I print_info: n_embd_head_k    = 128
0.00.040.816 I print_info: n_embd_head_v    = 128
0.00.040.817 I print_info: n_gqa            = 1
0.00.040.818 I print_info: n_embd_k_gqa     = 2048
0.00.040.818 I print_info: n_embd_v_gqa     = 2048
0.00.040.819 I print_info: f_norm_eps       = 1.0e-05
0.00.040.819 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.819 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.819 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.819 I print_info: f_logit_scale    = 0.0e+00
0.00.040.820 I print_info: n_ff             = 8192
0.00.040.820 I print_info: n_expert         = 0
0.00.040.820 I print_info: n_expert_used    = 0
0.00.040.820 I print_info: causal attn      = 1
0.00.040.820 I print_info: pooling type     = 0
0.00.040.821 I print_info: rope type        = 2
0.00.040.821 I print_info: rope scaling     = linear
0.00.040.821 I print_info: freq_base_train  = 10000.0
0.00.040.821 I print_info: freq_scale_train = 1
0.00.040.821 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.822 I print_info: rope_finetuned   = unknown
0.00.040.822 I print_info: ssm_d_conv       = 0
0.00.040.822 I print_info: ssm_d_inner      = 0
0.00.040.822 I print_info: ssm_d_state      = 0
0.00.040.822 I print_info: ssm_dt_rank      = 0
0.00.040.822 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.822 I print_info: model type       = 1.4B
0.00.040.823 I print_info: model params     = 1.41 B
0.00.040.823 I print_info: general.name     = 1.4B
0.00.040.823 I print_info: vocab type       = BPE
0.00.040.823 I print_info: n_vocab          = 50304
0.00.040.823 I print_info: n_merges         = 50009
0.00.040.824 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.824 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.824 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.825 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.826 I print_info: LF token         = 187 ''
0.00.040.826 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.826 I print_info: max token length = 1024
0.00.040.827 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.223 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.238 I load_tensors: offloading output layer to GPU
0.00.619.238 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.272 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.619.273 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.620.951 I llama_context: constructing llama_context
0.00.620.954 I llama_context: n_seq_max     = 1
0.00.620.955 I llama_context: n_ctx         = 128
0.00.620.955 I llama_context: n_ctx_per_seq = 128
0.00.620.956 I llama_context: n_batch       = 128
0.00.620.956 I llama_context: n_ubatch      = 128
0.00.620.956 I llama_context: flash_attn    = 0
0.00.620.958 I llama_context: freq_base     = 10000.0
0.00.620.959 I llama_context: freq_scale    = 1
0.00.620.959 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.620.964 I ggml_metal_init: allocating
0.00.621.064 I ggml_metal_init: found device: Apple M4
0.00.621.080 I ggml_metal_init: picking default device: Apple M4
0.00.623.061 I ggml_metal_init: using embedded metal library
0.00.629.676 I ggml_metal_init: GPU name:   Apple M4
0.00.629.685 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.686 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.687 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.688 I ggml_metal_init: simdgroup reduction   = true
0.00.629.688 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.689 I ggml_metal_init: has residency sets    = true
0.00.629.689 I ggml_metal_init: has bfloat            = true
0.00.629.689 I ggml_metal_init: use bfloat            = true
0.00.629.691 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.834 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.647.837 I llama_context_kv_self: constructing llama_context_kv_self
0.00.647.840 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.311 I init:      Metal KV buffer size =    24.00 MiB
0.00.651.316 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.393 I init:      Metal compute buffer size =    25.56 MiB
0.00.654.395 I init:        CPU compute buffer size =     1.06 MiB
0.00.654.395 I init: graph nodes  = 991
0.00.654.396 I init: graph splits = 2
0.00.654.399 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.654.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.638 I 
0.00.683.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.709 I perplexity: tokenizing the input ..
0.00.690.895 I perplexity: tokenization took 7.184 ms
0.00.690.901 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.293 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.827.632 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.827.653 I llama_perf_context_print:        load time =     674.05 ms
0.00.827.654 I llama_perf_context_print: prompt eval time =     134.84 ms /   128 tokens (    1.05 ms per token,   949.27 tokens per second)
0.00.827.654 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.655 I llama_perf_context_print:       total time =     144.02 ms /   129 tokens
0.00.828.197 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.080s
sys	0m0.138s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.970 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.908 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.913 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.916 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.917 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.917 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.918 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.918 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.919 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.920 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.920 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.923 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.923 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.923 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.688 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.560 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.561 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.561 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.562 I llama_model_loader: - type  f32:  194 tensors
0.00.027.562 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.562 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.563 I print_info: file format = GGUF V3 (latest)
0.00.027.563 I print_info: file type   = Q4_1
0.00.027.569 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.511 I load: special tokens cache size = 25
0.00.041.501 I load: token to piece cache size = 0.2984 MB
0.00.041.515 I print_info: arch             = gptneox
0.00.041.516 I print_info: vocab_only       = 0
0.00.041.517 I print_info: n_ctx_train      = 2048
0.00.041.517 I print_info: n_embd           = 2048
0.00.041.517 I print_info: n_layer          = 24
0.00.041.520 I print_info: n_head           = 16
0.00.041.521 I print_info: n_head_kv        = 16
0.00.041.521 I print_info: n_rot            = 32
0.00.041.521 I print_info: n_swa            = 0
0.00.041.521 I print_info: n_embd_head_k    = 128
0.00.041.521 I print_info: n_embd_head_v    = 128
0.00.041.522 I print_info: n_gqa            = 1
0.00.041.523 I print_info: n_embd_k_gqa     = 2048
0.00.041.524 I print_info: n_embd_v_gqa     = 2048
0.00.041.525 I print_info: f_norm_eps       = 1.0e-05
0.00.041.525 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.525 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.526 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.527 I print_info: f_logit_scale    = 0.0e+00
0.00.041.528 I print_info: n_ff             = 8192
0.00.041.528 I print_info: n_expert         = 0
0.00.041.528 I print_info: n_expert_used    = 0
0.00.041.528 I print_info: causal attn      = 1
0.00.041.529 I print_info: pooling type     = 0
0.00.041.530 I print_info: rope type        = 2
0.00.041.530 I print_info: rope scaling     = linear
0.00.041.533 I print_info: freq_base_train  = 10000.0
0.00.041.534 I print_info: freq_scale_train = 1
0.00.041.534 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.534 I print_info: rope_finetuned   = unknown
0.00.041.534 I print_info: ssm_d_conv       = 0
0.00.041.534 I print_info: ssm_d_inner      = 0
0.00.041.534 I print_info: ssm_d_state      = 0
0.00.041.534 I print_info: ssm_dt_rank      = 0
0.00.041.535 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.535 I print_info: model type       = 1.4B
0.00.041.535 I print_info: model params     = 1.41 B
0.00.041.535 I print_info: general.name     = 1.4B
0.00.041.536 I print_info: vocab type       = BPE
0.00.041.536 I print_info: n_vocab          = 50304
0.00.041.536 I print_info: n_merges         = 50009
0.00.041.537 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.537 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.537 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.537 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.537 I print_info: LF token         = 187 ''
0.00.041.537 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.538 I print_info: max token length = 1024
0.00.041.538 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.675.084 I load_tensors: offloading 24 repeating layers to GPU
0.00.675.099 I load_tensors: offloading output layer to GPU
0.00.675.100 I load_tensors: offloaded 25/25 layers to GPU
0.00.675.136 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.675.137 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.676.251 I llama_context: constructing llama_context
0.00.676.254 I llama_context: n_seq_max     = 1
0.00.676.254 I llama_context: n_ctx         = 2048
0.00.676.256 I llama_context: n_ctx_per_seq = 2048
0.00.676.256 I llama_context: n_batch       = 2048
0.00.676.256 I llama_context: n_ubatch      = 512
0.00.676.257 I llama_context: flash_attn    = 0
0.00.676.259 I llama_context: freq_base     = 10000.0
0.00.676.259 I llama_context: freq_scale    = 1
0.00.676.261 I ggml_metal_init: allocating
0.00.676.343 I ggml_metal_init: found device: Apple M4
0.00.676.357 I ggml_metal_init: picking default device: Apple M4
0.00.678.260 I ggml_metal_init: using embedded metal library
0.00.684.898 I ggml_metal_init: GPU name:   Apple M4
0.00.684.903 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.684.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.684.904 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.684.905 I ggml_metal_init: simdgroup reduction   = true
0.00.684.905 I ggml_metal_init: simdgroup matrix mul. = true
0.00.684.905 I ggml_metal_init: has residency sets    = true
0.00.684.906 I ggml_metal_init: has bfloat            = true
0.00.684.906 I ggml_metal_init: use bfloat            = true
0.00.684.907 I ggml_metal_init: hasUnifiedMemory      = true
0.00.684.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.761 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.702.763 I llama_context_kv_self: constructing llama_context_kv_self
0.00.702.766 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.753.885 I init:      Metal KV buffer size =   384.00 MiB
0.00.753.892 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.758.298 I init:      Metal compute buffer size =   102.25 MiB
0.00.758.300 I init:        CPU compute buffer size =     8.01 MiB
0.00.758.300 I init: graph nodes  = 991
0.00.758.300 I init: graph splits = 2
0.00.758.306 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.758.421 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.758.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.920 I main: llama threadpool init, n_threads = 4
0.00.815.960 I 
0.00.815.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.974 I 
0.00.816.140 I sampler seed: 1234
0.00.816.145 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.181 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.183 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.183 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.540.489 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.540.490 I llama_perf_context_print:        load time =     804.18 ms
0.01.540.491 I llama_perf_context_print: prompt eval time =      48.82 ms /     7 tokens (    6.97 ms per token,   143.39 tokens per second)
0.01.540.492 I llama_perf_context_print:        eval time =     672.72 ms /    63 runs   (   10.68 ms per token,    93.65 tokens per second)
0.01.540.492 I llama_perf_context_print:       total time =     725.34 ms /    70 tokens
0.01.544.393 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.108s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.286 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.734 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.741 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.742 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.745 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.745 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.745 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.746 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.746 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.747 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.748 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.595 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.669 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.538 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.540 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.541 I llama_model_loader: - type  f32:  194 tensors
0.00.025.542 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.542 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.543 I print_info: file format = GGUF V3 (latest)
0.00.025.544 I print_info: file type   = Q4_1
0.00.025.545 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.131 I load: special tokens cache size = 25
0.00.040.275 I load: token to piece cache size = 0.2984 MB
0.00.040.293 I print_info: arch             = gptneox
0.00.040.294 I print_info: vocab_only       = 0
0.00.040.294 I print_info: n_ctx_train      = 2048
0.00.040.294 I print_info: n_embd           = 2048
0.00.040.295 I print_info: n_layer          = 24
0.00.040.299 I print_info: n_head           = 16
0.00.040.299 I print_info: n_head_kv        = 16
0.00.040.299 I print_info: n_rot            = 32
0.00.040.299 I print_info: n_swa            = 0
0.00.040.300 I print_info: n_embd_head_k    = 128
0.00.040.300 I print_info: n_embd_head_v    = 128
0.00.040.300 I print_info: n_gqa            = 1
0.00.040.301 I print_info: n_embd_k_gqa     = 2048
0.00.040.301 I print_info: n_embd_v_gqa     = 2048
0.00.040.302 I print_info: f_norm_eps       = 1.0e-05
0.00.040.302 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.303 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.303 I print_info: f_logit_scale    = 0.0e+00
0.00.040.303 I print_info: n_ff             = 8192
0.00.040.304 I print_info: n_expert         = 0
0.00.040.304 I print_info: n_expert_used    = 0
0.00.040.305 I print_info: causal attn      = 1
0.00.040.305 I print_info: pooling type     = 0
0.00.040.305 I print_info: rope type        = 2
0.00.040.306 I print_info: rope scaling     = linear
0.00.040.306 I print_info: freq_base_train  = 10000.0
0.00.040.306 I print_info: freq_scale_train = 1
0.00.040.307 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.307 I print_info: rope_finetuned   = unknown
0.00.040.307 I print_info: ssm_d_conv       = 0
0.00.040.307 I print_info: ssm_d_inner      = 0
0.00.040.307 I print_info: ssm_d_state      = 0
0.00.040.307 I print_info: ssm_dt_rank      = 0
0.00.040.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.308 I print_info: model type       = 1.4B
0.00.040.308 I print_info: model params     = 1.41 B
0.00.040.308 I print_info: general.name     = 1.4B
0.00.040.309 I print_info: vocab type       = BPE
0.00.040.309 I print_info: n_vocab          = 50304
0.00.040.309 I print_info: n_merges         = 50009
0.00.040.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.309 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.309 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.309 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.310 I print_info: LF token         = 187 ''
0.00.040.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.310 I print_info: max token length = 1024
0.00.040.311 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.654.508 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.522 I load_tensors: offloading output layer to GPU
0.00.654.523 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.555 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.654.558 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.656.188 I llama_context: constructing llama_context
0.00.656.191 I llama_context: n_seq_max     = 1
0.00.656.192 I llama_context: n_ctx         = 128
0.00.656.193 I llama_context: n_ctx_per_seq = 128
0.00.656.193 I llama_context: n_batch       = 128
0.00.656.193 I llama_context: n_ubatch      = 128
0.00.656.194 I llama_context: flash_attn    = 0
0.00.656.196 I llama_context: freq_base     = 10000.0
0.00.656.197 I llama_context: freq_scale    = 1
0.00.656.197 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.656.199 I ggml_metal_init: allocating
0.00.656.284 I ggml_metal_init: found device: Apple M4
0.00.656.298 I ggml_metal_init: picking default device: Apple M4
0.00.658.121 I ggml_metal_init: using embedded metal library
0.00.664.790 I ggml_metal_init: GPU name:   Apple M4
0.00.664.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.799 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.799 I ggml_metal_init: simdgroup reduction   = true
0.00.664.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.800 I ggml_metal_init: has residency sets    = true
0.00.664.800 I ggml_metal_init: has bfloat            = true
0.00.664.800 I ggml_metal_init: use bfloat            = true
0.00.664.801 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.493 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.682.496 I llama_context_kv_self: constructing llama_context_kv_self
0.00.682.498 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.977 I init:      Metal KV buffer size =    24.00 MiB
0.00.685.982 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.689.125 I init:      Metal compute buffer size =    25.56 MiB
0.00.689.127 I init:        CPU compute buffer size =     1.06 MiB
0.00.689.127 I init: graph nodes  = 991
0.00.689.128 I init: graph splits = 2
0.00.689.131 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.689.134 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.268 I 
0.00.713.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.339 I perplexity: tokenizing the input ..
0.00.720.627 I perplexity: tokenization took 7.286 ms
0.00.720.639 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.107 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.854.447 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.854.470 I llama_perf_context_print:        load time =     703.97 ms
0.00.854.471 I llama_perf_context_print: prompt eval time =     131.54 ms /   128 tokens (    1.03 ms per token,   973.07 tokens per second)
0.00.854.472 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.472 I llama_perf_context_print:       total time =     141.21 ms /   129 tokens
0.00.855.059 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.081s
sys	0m0.123s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.580 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.468 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.471 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.471 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.472 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.475 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.119 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.122 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.122 I llama_model_loader: - type  f32:  194 tensors
0.00.025.123 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.123 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.123 I print_info: file format = GGUF V3 (latest)
0.00.025.124 I print_info: file type   = Q5_0
0.00.025.125 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.010 I load: special tokens cache size = 25
0.00.039.056 I load: token to piece cache size = 0.2984 MB
0.00.039.070 I print_info: arch             = gptneox
0.00.039.072 I print_info: vocab_only       = 0
0.00.039.072 I print_info: n_ctx_train      = 2048
0.00.039.072 I print_info: n_embd           = 2048
0.00.039.072 I print_info: n_layer          = 24
0.00.039.075 I print_info: n_head           = 16
0.00.039.076 I print_info: n_head_kv        = 16
0.00.039.076 I print_info: n_rot            = 32
0.00.039.076 I print_info: n_swa            = 0
0.00.039.076 I print_info: n_embd_head_k    = 128
0.00.039.076 I print_info: n_embd_head_v    = 128
0.00.039.077 I print_info: n_gqa            = 1
0.00.039.078 I print_info: n_embd_k_gqa     = 2048
0.00.039.079 I print_info: n_embd_v_gqa     = 2048
0.00.039.079 I print_info: f_norm_eps       = 1.0e-05
0.00.039.080 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.080 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.080 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.080 I print_info: f_logit_scale    = 0.0e+00
0.00.039.082 I print_info: n_ff             = 8192
0.00.039.082 I print_info: n_expert         = 0
0.00.039.082 I print_info: n_expert_used    = 0
0.00.039.082 I print_info: causal attn      = 1
0.00.039.082 I print_info: pooling type     = 0
0.00.039.083 I print_info: rope type        = 2
0.00.039.084 I print_info: rope scaling     = linear
0.00.039.084 I print_info: freq_base_train  = 10000.0
0.00.039.084 I print_info: freq_scale_train = 1
0.00.039.084 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.085 I print_info: rope_finetuned   = unknown
0.00.039.086 I print_info: ssm_d_conv       = 0
0.00.039.087 I print_info: ssm_d_inner      = 0
0.00.039.087 I print_info: ssm_d_state      = 0
0.00.039.087 I print_info: ssm_dt_rank      = 0
0.00.039.087 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.087 I print_info: model type       = 1.4B
0.00.039.087 I print_info: model params     = 1.41 B
0.00.039.087 I print_info: general.name     = 1.4B
0.00.039.088 I print_info: vocab type       = BPE
0.00.039.088 I print_info: n_vocab          = 50304
0.00.039.088 I print_info: n_merges         = 50009
0.00.039.088 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.088 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.089 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.089 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.089 I print_info: LF token         = 187 ''
0.00.039.089 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.089 I print_info: max token length = 1024
0.00.039.090 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.721.219 I load_tensors: offloading 24 repeating layers to GPU
0.00.721.226 I load_tensors: offloading output layer to GPU
0.00.721.227 I load_tensors: offloaded 25/25 layers to GPU
0.00.721.260 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.721.261 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.722.946 I llama_context: constructing llama_context
0.00.722.949 I llama_context: n_seq_max     = 1
0.00.722.950 I llama_context: n_ctx         = 2048
0.00.722.950 I llama_context: n_ctx_per_seq = 2048
0.00.722.951 I llama_context: n_batch       = 2048
0.00.722.951 I llama_context: n_ubatch      = 512
0.00.722.951 I llama_context: flash_attn    = 0
0.00.722.954 I llama_context: freq_base     = 10000.0
0.00.722.954 I llama_context: freq_scale    = 1
0.00.722.957 I ggml_metal_init: allocating
0.00.723.009 I ggml_metal_init: found device: Apple M4
0.00.723.021 I ggml_metal_init: picking default device: Apple M4
0.00.725.206 I ggml_metal_init: using embedded metal library
0.00.731.735 I ggml_metal_init: GPU name:   Apple M4
0.00.731.738 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.731.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.731.740 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.731.740 I ggml_metal_init: simdgroup reduction   = true
0.00.731.741 I ggml_metal_init: simdgroup matrix mul. = true
0.00.731.741 I ggml_metal_init: has residency sets    = true
0.00.731.741 I ggml_metal_init: has bfloat            = true
0.00.731.741 I ggml_metal_init: use bfloat            = true
0.00.731.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.731.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.748.808 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.748.810 I llama_context_kv_self: constructing llama_context_kv_self
0.00.748.813 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.802.058 I init:      Metal KV buffer size =   384.00 MiB
0.00.802.066 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.807.685 I init:      Metal compute buffer size =   102.25 MiB
0.00.807.687 I init:        CPU compute buffer size =     8.01 MiB
0.00.807.687 I init: graph nodes  = 991
0.00.807.687 I init: graph splits = 2
0.00.807.697 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.807.820 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.807.821 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.863.201 I main: llama threadpool init, n_threads = 4
0.00.863.257 I 
0.00.863.272 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.863.272 I 
0.00.863.416 I sampler seed: 1234
0.00.863.421 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.863.432 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.863.432 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.863.432 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.641.851 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50283.29 tokens per second)
0.01.641.851 I llama_perf_context_print:        load time =     853.89 ms
0.01.641.852 I llama_perf_context_print: prompt eval time =      43.05 ms /     7 tokens (    6.15 ms per token,   162.60 tokens per second)
0.01.641.853 I llama_perf_context_print:        eval time =     732.35 ms /    63 runs   (   11.62 ms per token,    86.02 tokens per second)
0.01.641.853 I llama_perf_context_print:       total time =     779.38 ms /    70 tokens
0.01.645.748 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.108s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.907 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.382 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.395 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.395 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.396 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.397 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.399 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.402 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.248 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.180 I llama_model_loader: - type  f32:  194 tensors
0.00.025.180 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.180 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.181 I print_info: file format = GGUF V3 (latest)
0.00.025.182 I print_info: file type   = Q5_0
0.00.025.183 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.733 I load: special tokens cache size = 25
0.00.039.996 I load: token to piece cache size = 0.2984 MB
0.00.040.014 I print_info: arch             = gptneox
0.00.040.015 I print_info: vocab_only       = 0
0.00.040.015 I print_info: n_ctx_train      = 2048
0.00.040.015 I print_info: n_embd           = 2048
0.00.040.015 I print_info: n_layer          = 24
0.00.040.019 I print_info: n_head           = 16
0.00.040.021 I print_info: n_head_kv        = 16
0.00.040.021 I print_info: n_rot            = 32
0.00.040.021 I print_info: n_swa            = 0
0.00.040.022 I print_info: n_embd_head_k    = 128
0.00.040.022 I print_info: n_embd_head_v    = 128
0.00.040.022 I print_info: n_gqa            = 1
0.00.040.023 I print_info: n_embd_k_gqa     = 2048
0.00.040.024 I print_info: n_embd_v_gqa     = 2048
0.00.040.024 I print_info: f_norm_eps       = 1.0e-05
0.00.040.024 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.025 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.025 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.025 I print_info: f_logit_scale    = 0.0e+00
0.00.040.026 I print_info: n_ff             = 8192
0.00.040.026 I print_info: n_expert         = 0
0.00.040.026 I print_info: n_expert_used    = 0
0.00.040.026 I print_info: causal attn      = 1
0.00.040.026 I print_info: pooling type     = 0
0.00.040.026 I print_info: rope type        = 2
0.00.040.027 I print_info: rope scaling     = linear
0.00.040.027 I print_info: freq_base_train  = 10000.0
0.00.040.027 I print_info: freq_scale_train = 1
0.00.040.027 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.028 I print_info: rope_finetuned   = unknown
0.00.040.028 I print_info: ssm_d_conv       = 0
0.00.040.028 I print_info: ssm_d_inner      = 0
0.00.040.028 I print_info: ssm_d_state      = 0
0.00.040.028 I print_info: ssm_dt_rank      = 0
0.00.040.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.029 I print_info: model type       = 1.4B
0.00.040.029 I print_info: model params     = 1.41 B
0.00.040.029 I print_info: general.name     = 1.4B
0.00.040.030 I print_info: vocab type       = BPE
0.00.040.030 I print_info: n_vocab          = 50304
0.00.040.030 I print_info: n_merges         = 50009
0.00.040.030 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.030 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.030 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.031 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.031 I print_info: LF token         = 187 ''
0.00.040.031 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.031 I print_info: max token length = 1024
0.00.040.032 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.699.865 I load_tensors: offloading 24 repeating layers to GPU
0.00.699.877 I load_tensors: offloading output layer to GPU
0.00.699.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.699.913 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.699.915 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.701.520 I llama_context: constructing llama_context
0.00.701.523 I llama_context: n_seq_max     = 1
0.00.701.524 I llama_context: n_ctx         = 128
0.00.701.524 I llama_context: n_ctx_per_seq = 128
0.00.701.525 I llama_context: n_batch       = 128
0.00.701.525 I llama_context: n_ubatch      = 128
0.00.701.525 I llama_context: flash_attn    = 0
0.00.701.527 I llama_context: freq_base     = 10000.0
0.00.701.527 I llama_context: freq_scale    = 1
0.00.701.528 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.701.530 I ggml_metal_init: allocating
0.00.701.608 I ggml_metal_init: found device: Apple M4
0.00.701.624 I ggml_metal_init: picking default device: Apple M4
0.00.703.252 I ggml_metal_init: using embedded metal library
0.00.709.776 I ggml_metal_init: GPU name:   Apple M4
0.00.709.781 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.709.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.709.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.709.784 I ggml_metal_init: simdgroup reduction   = true
0.00.709.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.709.784 I ggml_metal_init: has residency sets    = true
0.00.709.785 I ggml_metal_init: has bfloat            = true
0.00.709.785 I ggml_metal_init: use bfloat            = true
0.00.709.786 I ggml_metal_init: hasUnifiedMemory      = true
0.00.709.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.727.397 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.727.399 I llama_context_kv_self: constructing llama_context_kv_self
0.00.727.402 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.972 I init:      Metal KV buffer size =    24.00 MiB
0.00.730.975 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.734.195 I init:      Metal compute buffer size =    25.56 MiB
0.00.734.198 I init:        CPU compute buffer size =     1.06 MiB
0.00.734.198 I init: graph nodes  = 991
0.00.734.198 I init: graph splits = 2
0.00.734.202 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.734.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.114 I 
0.00.762.180 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.191 I perplexity: tokenizing the input ..
0.00.769.517 I perplexity: tokenization took 7.322 ms
0.00.769.524 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.905.615 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.907.055 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.907.079 I llama_perf_context_print:        load time =     753.20 ms
0.00.907.080 I llama_perf_context_print: prompt eval time =     135.17 ms /   128 tokens (    1.06 ms per token,   946.94 tokens per second)
0.00.907.080 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.907.081 I llama_perf_context_print:       total time =     144.97 ms /   129 tokens
0.00.907.655 I ggml_metal_free: deallocating

real	0m0.921s
user	0m0.080s
sys	0m0.134s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.814 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.468 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.472 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.472 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.210 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.257 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.014 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.014 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.014 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.015 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.015 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.015 I llama_model_loader: - type  f32:  194 tensors
0.00.026.016 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.016 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.017 I print_info: file format = GGUF V3 (latest)
0.00.026.017 I print_info: file type   = Q5_1
0.00.026.018 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.834 I load: special tokens cache size = 25
0.00.040.056 I load: token to piece cache size = 0.2984 MB
0.00.040.071 I print_info: arch             = gptneox
0.00.040.072 I print_info: vocab_only       = 0
0.00.040.072 I print_info: n_ctx_train      = 2048
0.00.040.072 I print_info: n_embd           = 2048
0.00.040.072 I print_info: n_layer          = 24
0.00.040.075 I print_info: n_head           = 16
0.00.040.076 I print_info: n_head_kv        = 16
0.00.040.076 I print_info: n_rot            = 32
0.00.040.076 I print_info: n_swa            = 0
0.00.040.076 I print_info: n_embd_head_k    = 128
0.00.040.077 I print_info: n_embd_head_v    = 128
0.00.040.077 I print_info: n_gqa            = 1
0.00.040.078 I print_info: n_embd_k_gqa     = 2048
0.00.040.079 I print_info: n_embd_v_gqa     = 2048
0.00.040.079 I print_info: f_norm_eps       = 1.0e-05
0.00.040.079 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.081 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.081 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.081 I print_info: f_logit_scale    = 0.0e+00
0.00.040.082 I print_info: n_ff             = 8192
0.00.040.082 I print_info: n_expert         = 0
0.00.040.082 I print_info: n_expert_used    = 0
0.00.040.082 I print_info: causal attn      = 1
0.00.040.083 I print_info: pooling type     = 0
0.00.040.083 I print_info: rope type        = 2
0.00.040.083 I print_info: rope scaling     = linear
0.00.040.083 I print_info: freq_base_train  = 10000.0
0.00.040.084 I print_info: freq_scale_train = 1
0.00.040.084 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.084 I print_info: rope_finetuned   = unknown
0.00.040.086 I print_info: ssm_d_conv       = 0
0.00.040.086 I print_info: ssm_d_inner      = 0
0.00.040.086 I print_info: ssm_d_state      = 0
0.00.040.086 I print_info: ssm_dt_rank      = 0
0.00.040.086 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.086 I print_info: model type       = 1.4B
0.00.040.087 I print_info: model params     = 1.41 B
0.00.040.087 I print_info: general.name     = 1.4B
0.00.040.087 I print_info: vocab type       = BPE
0.00.040.087 I print_info: n_vocab          = 50304
0.00.040.087 I print_info: n_merges         = 50009
0.00.040.088 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.088 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.088 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.088 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: LF token         = 187 ''
0.00.040.092 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.093 I print_info: max token length = 1024
0.00.040.093 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.413 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.416 I load_tensors: offloading output layer to GPU
0.00.619.417 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.439 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.619.441 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.621.047 I llama_context: constructing llama_context
0.00.621.049 I llama_context: n_seq_max     = 1
0.00.621.049 I llama_context: n_ctx         = 2048
0.00.621.050 I llama_context: n_ctx_per_seq = 2048
0.00.621.050 I llama_context: n_batch       = 2048
0.00.621.051 I llama_context: n_ubatch      = 512
0.00.621.051 I llama_context: flash_attn    = 0
0.00.621.052 I llama_context: freq_base     = 10000.0
0.00.621.052 I llama_context: freq_scale    = 1
0.00.621.054 I ggml_metal_init: allocating
0.00.621.085 I ggml_metal_init: found device: Apple M4
0.00.621.094 I ggml_metal_init: picking default device: Apple M4
0.00.622.658 I ggml_metal_init: using embedded metal library
0.00.628.841 I ggml_metal_init: GPU name:   Apple M4
0.00.628.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.628.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.628.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.628.846 I ggml_metal_init: simdgroup reduction   = true
0.00.628.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.628.847 I ggml_metal_init: has residency sets    = true
0.00.628.847 I ggml_metal_init: has bfloat            = true
0.00.628.847 I ggml_metal_init: use bfloat            = true
0.00.628.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.628.849 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.270 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.645.272 I llama_context_kv_self: constructing llama_context_kv_self
0.00.645.274 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.982 I init:      Metal KV buffer size =   384.00 MiB
0.00.698.988 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.703.472 I init:      Metal compute buffer size =   102.25 MiB
0.00.703.474 I init:        CPU compute buffer size =     8.01 MiB
0.00.703.474 I init: graph nodes  = 991
0.00.703.475 I init: graph splits = 2
0.00.703.484 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.882 I main: llama threadpool init, n_threads = 4
0.00.754.936 I 
0.00.754.954 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.954 I 
0.00.755.123 I sampler seed: 1234
0.00.755.128 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.139 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.141 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.605.173 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.605.174 I llama_perf_context_print:        load time =     743.35 ms
0.01.605.175 I llama_perf_context_print: prompt eval time =      51.98 ms /     7 tokens (    7.43 ms per token,   134.66 tokens per second)
0.01.605.177 I llama_perf_context_print:        eval time =     795.29 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.605.177 I llama_perf_context_print:       total time =     851.01 ms /    70 tokens
0.01.608.776 I ggml_metal_free: deallocating

real	0m1.626s
user	0m0.107s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.043 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.126 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.132 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.134 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.136 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.137 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.138 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.066 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.134 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.025 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.027 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.028 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.028 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.028 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.029 I llama_model_loader: - type  f32:  194 tensors
0.00.026.029 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.030 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.030 I print_info: file format = GGUF V3 (latest)
0.00.026.031 I print_info: file type   = Q5_1
0.00.026.032 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.144 I load: special tokens cache size = 25
0.00.040.550 I load: token to piece cache size = 0.2984 MB
0.00.040.563 I print_info: arch             = gptneox
0.00.040.564 I print_info: vocab_only       = 0
0.00.040.564 I print_info: n_ctx_train      = 2048
0.00.040.564 I print_info: n_embd           = 2048
0.00.040.564 I print_info: n_layer          = 24
0.00.040.568 I print_info: n_head           = 16
0.00.040.569 I print_info: n_head_kv        = 16
0.00.040.569 I print_info: n_rot            = 32
0.00.040.569 I print_info: n_swa            = 0
0.00.040.569 I print_info: n_embd_head_k    = 128
0.00.040.571 I print_info: n_embd_head_v    = 128
0.00.040.572 I print_info: n_gqa            = 1
0.00.040.572 I print_info: n_embd_k_gqa     = 2048
0.00.040.573 I print_info: n_embd_v_gqa     = 2048
0.00.040.573 I print_info: f_norm_eps       = 1.0e-05
0.00.040.575 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.575 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.575 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.575 I print_info: f_logit_scale    = 0.0e+00
0.00.040.576 I print_info: n_ff             = 8192
0.00.040.576 I print_info: n_expert         = 0
0.00.040.576 I print_info: n_expert_used    = 0
0.00.040.576 I print_info: causal attn      = 1
0.00.040.576 I print_info: pooling type     = 0
0.00.040.576 I print_info: rope type        = 2
0.00.040.577 I print_info: rope scaling     = linear
0.00.040.577 I print_info: freq_base_train  = 10000.0
0.00.040.577 I print_info: freq_scale_train = 1
0.00.040.577 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.578 I print_info: rope_finetuned   = unknown
0.00.040.579 I print_info: ssm_d_conv       = 0
0.00.040.579 I print_info: ssm_d_inner      = 0
0.00.040.579 I print_info: ssm_d_state      = 0
0.00.040.579 I print_info: ssm_dt_rank      = 0
0.00.040.579 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.579 I print_info: model type       = 1.4B
0.00.040.580 I print_info: model params     = 1.41 B
0.00.040.580 I print_info: general.name     = 1.4B
0.00.040.580 I print_info: vocab type       = BPE
0.00.040.580 I print_info: n_vocab          = 50304
0.00.040.581 I print_info: n_merges         = 50009
0.00.040.581 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.581 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.581 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.581 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.582 I print_info: LF token         = 187 ''
0.00.040.582 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.582 I print_info: max token length = 1024
0.00.040.582 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.625.812 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.826 I load_tensors: offloading output layer to GPU
0.00.625.827 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.864 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.625.866 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.627.599 I llama_context: constructing llama_context
0.00.627.604 I llama_context: n_seq_max     = 1
0.00.627.604 I llama_context: n_ctx         = 128
0.00.627.605 I llama_context: n_ctx_per_seq = 128
0.00.627.605 I llama_context: n_batch       = 128
0.00.627.605 I llama_context: n_ubatch      = 128
0.00.627.606 I llama_context: flash_attn    = 0
0.00.627.608 I llama_context: freq_base     = 10000.0
0.00.627.608 I llama_context: freq_scale    = 1
0.00.627.609 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.627.611 I ggml_metal_init: allocating
0.00.627.644 I ggml_metal_init: found device: Apple M4
0.00.627.664 I ggml_metal_init: picking default device: Apple M4
0.00.629.114 I ggml_metal_init: using embedded metal library
0.00.635.488 I ggml_metal_init: GPU name:   Apple M4
0.00.635.492 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.493 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.495 I ggml_metal_init: simdgroup reduction   = true
0.00.635.495 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.495 I ggml_metal_init: has residency sets    = true
0.00.635.495 I ggml_metal_init: has bfloat            = true
0.00.635.496 I ggml_metal_init: use bfloat            = true
0.00.635.497 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.499 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.362 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.652.364 I llama_context_kv_self: constructing llama_context_kv_self
0.00.652.366 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.655.887 I init:      Metal KV buffer size =    24.00 MiB
0.00.655.899 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.659.069 I init:      Metal compute buffer size =    25.56 MiB
0.00.659.071 I init:        CPU compute buffer size =     1.06 MiB
0.00.659.072 I init: graph nodes  = 991
0.00.659.072 I init: graph splits = 2
0.00.659.075 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.659.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.404 I 
0.00.688.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.473 I perplexity: tokenizing the input ..
0.00.695.758 I perplexity: tokenization took 7.282 ms
0.00.695.765 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.831.572 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.832.926 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.832.957 I llama_perf_context_print:        load time =     678.35 ms
0.00.832.958 I llama_perf_context_print: prompt eval time =     134.85 ms /   128 tokens (    1.05 ms per token,   949.22 tokens per second)
0.00.832.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.832.959 I llama_perf_context_print:       total time =     144.56 ms /   129 tokens
0.00.833.557 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.080s
sys	0m0.148s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.694 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.429 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.434 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.435 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.435 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.436 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.437 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.438 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.438 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.228 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.229 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.230 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.230 I llama_model_loader: - type  f32:  194 tensors
0.00.024.231 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.231 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.231 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.232 I print_info: file format = GGUF V3 (latest)
0.00.024.232 I print_info: file type   = Q2_K - Medium
0.00.024.233 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.313 I load: special tokens cache size = 25
0.00.038.426 I load: token to piece cache size = 0.2984 MB
0.00.038.440 I print_info: arch             = gptneox
0.00.038.441 I print_info: vocab_only       = 0
0.00.038.441 I print_info: n_ctx_train      = 2048
0.00.038.441 I print_info: n_embd           = 2048
0.00.038.441 I print_info: n_layer          = 24
0.00.038.444 I print_info: n_head           = 16
0.00.038.445 I print_info: n_head_kv        = 16
0.00.038.445 I print_info: n_rot            = 32
0.00.038.445 I print_info: n_swa            = 0
0.00.038.445 I print_info: n_embd_head_k    = 128
0.00.038.446 I print_info: n_embd_head_v    = 128
0.00.038.446 I print_info: n_gqa            = 1
0.00.038.447 I print_info: n_embd_k_gqa     = 2048
0.00.038.448 I print_info: n_embd_v_gqa     = 2048
0.00.038.448 I print_info: f_norm_eps       = 1.0e-05
0.00.038.449 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.449 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.449 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.449 I print_info: f_logit_scale    = 0.0e+00
0.00.038.452 I print_info: n_ff             = 8192
0.00.038.452 I print_info: n_expert         = 0
0.00.038.452 I print_info: n_expert_used    = 0
0.00.038.452 I print_info: causal attn      = 1
0.00.038.452 I print_info: pooling type     = 0
0.00.038.452 I print_info: rope type        = 2
0.00.038.460 I print_info: rope scaling     = linear
0.00.038.462 I print_info: freq_base_train  = 10000.0
0.00.038.463 I print_info: freq_scale_train = 1
0.00.038.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.463 I print_info: rope_finetuned   = unknown
0.00.038.464 I print_info: ssm_d_conv       = 0
0.00.038.464 I print_info: ssm_d_inner      = 0
0.00.038.464 I print_info: ssm_d_state      = 0
0.00.038.464 I print_info: ssm_dt_rank      = 0
0.00.038.464 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.464 I print_info: model type       = 1.4B
0.00.038.465 I print_info: model params     = 1.41 B
0.00.038.465 I print_info: general.name     = 1.4B
0.00.038.465 I print_info: vocab type       = BPE
0.00.038.466 I print_info: n_vocab          = 50304
0.00.038.466 I print_info: n_merges         = 50009
0.00.038.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.466 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.466 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.467 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.467 I print_info: LF token         = 187 ''
0.00.038.468 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.468 I print_info: max token length = 1024
0.00.038.469 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.342.014 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.030 I load_tensors: offloading output layer to GPU
0.00.342.030 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.066 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.077 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.343.731 I llama_context: constructing llama_context
0.00.343.735 I llama_context: n_seq_max     = 1
0.00.343.736 I llama_context: n_ctx         = 2048
0.00.343.737 I llama_context: n_ctx_per_seq = 2048
0.00.343.737 I llama_context: n_batch       = 2048
0.00.343.737 I llama_context: n_ubatch      = 512
0.00.343.738 I llama_context: flash_attn    = 0
0.00.343.740 I llama_context: freq_base     = 10000.0
0.00.343.740 I llama_context: freq_scale    = 1
0.00.343.742 I ggml_metal_init: allocating
0.00.343.820 I ggml_metal_init: found device: Apple M4
0.00.343.834 I ggml_metal_init: picking default device: Apple M4
0.00.345.740 I ggml_metal_init: using embedded metal library
0.00.351.297 I ggml_metal_init: GPU name:   Apple M4
0.00.351.312 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.313 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.314 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.315 I ggml_metal_init: simdgroup reduction   = true
0.00.351.315 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.316 I ggml_metal_init: has residency sets    = true
0.00.351.316 I ggml_metal_init: has bfloat            = true
0.00.351.316 I ggml_metal_init: use bfloat            = true
0.00.351.318 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.189 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.372.191 I llama_context_kv_self: constructing llama_context_kv_self
0.00.372.194 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.428.726 I init:      Metal KV buffer size =   384.00 MiB
0.00.428.745 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.433.108 I init:      Metal compute buffer size =   102.25 MiB
0.00.433.110 I init:        CPU compute buffer size =     8.01 MiB
0.00.433.111 I init: graph nodes  = 991
0.00.433.111 I init: graph splits = 2
0.00.433.116 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.433.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.433.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.489.577 I main: llama threadpool init, n_threads = 4
0.00.489.623 I 
0.00.489.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.489.639 I 
0.00.489.810 I sampler seed: 1234
0.00.489.814 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.489.826 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.489.828 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.489.828 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.164.251 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.164.253 I llama_perf_context_print:        load time =     480.16 ms
0.01.164.254 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.45 tokens per second)
0.01.164.254 I llama_perf_context_print:        eval time =     635.77 ms /    63 runs   (   10.09 ms per token,    99.09 tokens per second)
0.01.164.255 I llama_perf_context_print:       total time =     675.40 ms /    70 tokens
0.01.167.886 I ggml_metal_free: deallocating

real	0m1.183s
user	0m0.111s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.977 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.147 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.154 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.159 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.159 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.160 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.161 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.162 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.162 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.793 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.795 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.796 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.797 I llama_model_loader: - type  f32:  194 tensors
0.00.024.797 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.798 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.798 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.798 I print_info: file format = GGUF V3 (latest)
0.00.024.803 I print_info: file type   = Q2_K - Medium
0.00.024.804 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.843 I load: special tokens cache size = 25
0.00.039.180 I load: token to piece cache size = 0.2984 MB
0.00.039.198 I print_info: arch             = gptneox
0.00.039.199 I print_info: vocab_only       = 0
0.00.039.199 I print_info: n_ctx_train      = 2048
0.00.039.199 I print_info: n_embd           = 2048
0.00.039.199 I print_info: n_layer          = 24
0.00.039.203 I print_info: n_head           = 16
0.00.039.203 I print_info: n_head_kv        = 16
0.00.039.204 I print_info: n_rot            = 32
0.00.039.204 I print_info: n_swa            = 0
0.00.039.204 I print_info: n_embd_head_k    = 128
0.00.039.207 I print_info: n_embd_head_v    = 128
0.00.039.207 I print_info: n_gqa            = 1
0.00.039.208 I print_info: n_embd_k_gqa     = 2048
0.00.039.208 I print_info: n_embd_v_gqa     = 2048
0.00.039.209 I print_info: f_norm_eps       = 1.0e-05
0.00.039.209 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.209 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.209 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.210 I print_info: f_logit_scale    = 0.0e+00
0.00.039.210 I print_info: n_ff             = 8192
0.00.039.210 I print_info: n_expert         = 0
0.00.039.211 I print_info: n_expert_used    = 0
0.00.039.211 I print_info: causal attn      = 1
0.00.039.211 I print_info: pooling type     = 0
0.00.039.211 I print_info: rope type        = 2
0.00.039.211 I print_info: rope scaling     = linear
0.00.039.211 I print_info: freq_base_train  = 10000.0
0.00.039.212 I print_info: freq_scale_train = 1
0.00.039.212 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.212 I print_info: rope_finetuned   = unknown
0.00.039.212 I print_info: ssm_d_conv       = 0
0.00.039.212 I print_info: ssm_d_inner      = 0
0.00.039.212 I print_info: ssm_d_state      = 0
0.00.039.212 I print_info: ssm_dt_rank      = 0
0.00.039.212 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.213 I print_info: model type       = 1.4B
0.00.039.213 I print_info: model params     = 1.41 B
0.00.039.213 I print_info: general.name     = 1.4B
0.00.039.214 I print_info: vocab type       = BPE
0.00.039.214 I print_info: n_vocab          = 50304
0.00.039.214 I print_info: n_merges         = 50009
0.00.039.214 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.216 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.216 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.216 I print_info: LF token         = 187 ''
0.00.039.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.216 I print_info: max token length = 1024
0.00.039.217 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.340.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.340.682 I load_tensors: offloading output layer to GPU
0.00.340.682 I load_tensors: offloaded 25/25 layers to GPU
0.00.340.721 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.340.723 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.342.399 I llama_context: constructing llama_context
0.00.342.404 I llama_context: n_seq_max     = 1
0.00.342.405 I llama_context: n_ctx         = 128
0.00.342.405 I llama_context: n_ctx_per_seq = 128
0.00.342.405 I llama_context: n_batch       = 128
0.00.342.406 I llama_context: n_ubatch      = 128
0.00.342.406 I llama_context: flash_attn    = 0
0.00.342.408 I llama_context: freq_base     = 10000.0
0.00.342.408 I llama_context: freq_scale    = 1
0.00.342.409 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.342.410 I ggml_metal_init: allocating
0.00.342.506 I ggml_metal_init: found device: Apple M4
0.00.342.521 I ggml_metal_init: picking default device: Apple M4
0.00.344.292 I ggml_metal_init: using embedded metal library
0.00.349.685 I ggml_metal_init: GPU name:   Apple M4
0.00.349.703 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.349.704 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.349.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.349.706 I ggml_metal_init: simdgroup reduction   = true
0.00.349.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.349.707 I ggml_metal_init: has residency sets    = true
0.00.349.707 I ggml_metal_init: has bfloat            = true
0.00.349.707 I ggml_metal_init: use bfloat            = true
0.00.349.709 I ggml_metal_init: hasUnifiedMemory      = true
0.00.349.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.370.868 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.370.870 I llama_context_kv_self: constructing llama_context_kv_self
0.00.370.873 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.374.597 I init:      Metal KV buffer size =    24.00 MiB
0.00.374.609 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.378.086 I init:      Metal compute buffer size =    25.56 MiB
0.00.378.088 I init:        CPU compute buffer size =     1.06 MiB
0.00.378.088 I init: graph nodes  = 991
0.00.378.089 I init: graph splits = 2
0.00.378.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.378.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.405.793 I 
0.00.405.864 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.405.875 I perplexity: tokenizing the input ..
0.00.411.687 I perplexity: tokenization took 5.81 ms
0.00.411.692 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.543.223 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.544.555 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.544.580 I llama_perf_context_print:        load time =     396.81 ms
0.00.544.581 I llama_perf_context_print: prompt eval time =     131.14 ms /   128 tokens (    1.02 ms per token,   976.03 tokens per second)
0.00.544.581 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.544.581 I llama_perf_context_print:       total time =     138.79 ms /   129 tokens
0.00.545.111 I ggml_metal_free: deallocating

real	0m0.560s
user	0m0.079s
sys	0m0.092s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.715 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.351 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.353 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.353 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.354 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.354 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.355 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.355 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.356 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.356 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.358 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.358 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.360 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.147 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.964 I llama_model_loader: - type  f32:  194 tensors
0.00.024.964 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.964 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.964 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.965 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.965 I print_info: file format = GGUF V3 (latest)
0.00.024.966 I print_info: file type   = Q3_K - Medium
0.00.024.967 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.904 I load: special tokens cache size = 25
0.00.039.043 I load: token to piece cache size = 0.2984 MB
0.00.039.056 I print_info: arch             = gptneox
0.00.039.057 I print_info: vocab_only       = 0
0.00.039.058 I print_info: n_ctx_train      = 2048
0.00.039.058 I print_info: n_embd           = 2048
0.00.039.058 I print_info: n_layer          = 24
0.00.039.061 I print_info: n_head           = 16
0.00.039.062 I print_info: n_head_kv        = 16
0.00.039.062 I print_info: n_rot            = 32
0.00.039.062 I print_info: n_swa            = 0
0.00.039.062 I print_info: n_embd_head_k    = 128
0.00.039.062 I print_info: n_embd_head_v    = 128
0.00.039.063 I print_info: n_gqa            = 1
0.00.039.064 I print_info: n_embd_k_gqa     = 2048
0.00.039.065 I print_info: n_embd_v_gqa     = 2048
0.00.039.065 I print_info: f_norm_eps       = 1.0e-05
0.00.039.066 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.066 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.068 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.068 I print_info: f_logit_scale    = 0.0e+00
0.00.039.069 I print_info: n_ff             = 8192
0.00.039.069 I print_info: n_expert         = 0
0.00.039.069 I print_info: n_expert_used    = 0
0.00.039.069 I print_info: causal attn      = 1
0.00.039.069 I print_info: pooling type     = 0
0.00.039.069 I print_info: rope type        = 2
0.00.039.069 I print_info: rope scaling     = linear
0.00.039.070 I print_info: freq_base_train  = 10000.0
0.00.039.070 I print_info: freq_scale_train = 1
0.00.039.070 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.070 I print_info: rope_finetuned   = unknown
0.00.039.070 I print_info: ssm_d_conv       = 0
0.00.039.070 I print_info: ssm_d_inner      = 0
0.00.039.074 I print_info: ssm_d_state      = 0
0.00.039.074 I print_info: ssm_dt_rank      = 0
0.00.039.074 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.075 I print_info: model type       = 1.4B
0.00.039.075 I print_info: model params     = 1.41 B
0.00.039.075 I print_info: general.name     = 1.4B
0.00.039.075 I print_info: vocab type       = BPE
0.00.039.076 I print_info: n_vocab          = 50304
0.00.039.076 I print_info: n_merges         = 50009
0.00.039.076 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.076 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.076 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.076 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.077 I print_info: LF token         = 187 ''
0.00.039.077 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.077 I print_info: max token length = 1024
0.00.039.078 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.446.544 I load_tensors: offloading 24 repeating layers to GPU
0.00.446.559 I load_tensors: offloading output layer to GPU
0.00.446.560 I load_tensors: offloaded 25/25 layers to GPU
0.00.446.595 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.446.596 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.448.231 I llama_context: constructing llama_context
0.00.448.234 I llama_context: n_seq_max     = 1
0.00.448.235 I llama_context: n_ctx         = 2048
0.00.448.235 I llama_context: n_ctx_per_seq = 2048
0.00.448.235 I llama_context: n_batch       = 2048
0.00.448.236 I llama_context: n_ubatch      = 512
0.00.448.236 I llama_context: flash_attn    = 0
0.00.448.238 I llama_context: freq_base     = 10000.0
0.00.448.239 I llama_context: freq_scale    = 1
0.00.448.241 I ggml_metal_init: allocating
0.00.448.350 I ggml_metal_init: found device: Apple M4
0.00.448.363 I ggml_metal_init: picking default device: Apple M4
0.00.450.301 I ggml_metal_init: using embedded metal library
0.00.455.966 I ggml_metal_init: GPU name:   Apple M4
0.00.455.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.982 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.983 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.984 I ggml_metal_init: simdgroup reduction   = true
0.00.455.984 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.985 I ggml_metal_init: has residency sets    = true
0.00.455.985 I ggml_metal_init: has bfloat            = true
0.00.455.985 I ggml_metal_init: use bfloat            = true
0.00.455.989 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.994 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.982 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.475.985 I llama_context_kv_self: constructing llama_context_kv_self
0.00.475.987 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.533.900 I init:      Metal KV buffer size =   384.00 MiB
0.00.533.906 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.538.459 I init:      Metal compute buffer size =   102.25 MiB
0.00.538.462 I init:        CPU compute buffer size =     8.01 MiB
0.00.538.462 I init: graph nodes  = 991
0.00.538.462 I init: graph splits = 2
0.00.538.468 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.538.585 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.538.586 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.732 I main: llama threadpool init, n_threads = 4
0.00.596.775 I 
0.00.596.790 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.790 I 
0.00.596.971 I sampler seed: 1234
0.00.596.975 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.596.994 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.596.994 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.596.994 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.339.818 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.339.819 I llama_perf_context_print:        load time =     586.25 ms
0.01.339.820 I llama_perf_context_print: prompt eval time =      46.80 ms /     7 tokens (    6.69 ms per token,   149.56 tokens per second)
0.01.339.821 I llama_perf_context_print:        eval time =     693.12 ms /    63 runs   (   11.00 ms per token,    90.89 tokens per second)
0.01.339.821 I llama_perf_context_print:       total time =     743.85 ms /    70 tokens
0.01.343.848 I ggml_metal_free: deallocating

real	0m1.361s
user	0m0.110s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.822 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.947 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.954 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.954 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.955 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.956 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.957 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.957 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.958 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.839 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.841 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.842 I llama_model_loader: - type  f32:  194 tensors
0.00.024.843 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.843 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.843 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.843 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.845 I print_info: file format = GGUF V3 (latest)
0.00.024.846 I print_info: file type   = Q3_K - Medium
0.00.024.847 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.380 I load: special tokens cache size = 25
0.00.039.760 I load: token to piece cache size = 0.2984 MB
0.00.039.777 I print_info: arch             = gptneox
0.00.039.778 I print_info: vocab_only       = 0
0.00.039.778 I print_info: n_ctx_train      = 2048
0.00.039.779 I print_info: n_embd           = 2048
0.00.039.779 I print_info: n_layer          = 24
0.00.039.783 I print_info: n_head           = 16
0.00.039.784 I print_info: n_head_kv        = 16
0.00.039.784 I print_info: n_rot            = 32
0.00.039.784 I print_info: n_swa            = 0
0.00.039.784 I print_info: n_embd_head_k    = 128
0.00.039.787 I print_info: n_embd_head_v    = 128
0.00.039.788 I print_info: n_gqa            = 1
0.00.039.788 I print_info: n_embd_k_gqa     = 2048
0.00.039.789 I print_info: n_embd_v_gqa     = 2048
0.00.039.790 I print_info: f_norm_eps       = 1.0e-05
0.00.039.790 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.790 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.790 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.790 I print_info: f_logit_scale    = 0.0e+00
0.00.039.791 I print_info: n_ff             = 8192
0.00.039.791 I print_info: n_expert         = 0
0.00.039.791 I print_info: n_expert_used    = 0
0.00.039.792 I print_info: causal attn      = 1
0.00.039.792 I print_info: pooling type     = 0
0.00.039.792 I print_info: rope type        = 2
0.00.039.792 I print_info: rope scaling     = linear
0.00.039.792 I print_info: freq_base_train  = 10000.0
0.00.039.793 I print_info: freq_scale_train = 1
0.00.039.793 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.793 I print_info: rope_finetuned   = unknown
0.00.039.793 I print_info: ssm_d_conv       = 0
0.00.039.793 I print_info: ssm_d_inner      = 0
0.00.039.794 I print_info: ssm_d_state      = 0
0.00.039.794 I print_info: ssm_dt_rank      = 0
0.00.039.794 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.794 I print_info: model type       = 1.4B
0.00.039.794 I print_info: model params     = 1.41 B
0.00.039.795 I print_info: general.name     = 1.4B
0.00.039.795 I print_info: vocab type       = BPE
0.00.039.795 I print_info: n_vocab          = 50304
0.00.039.795 I print_info: n_merges         = 50009
0.00.039.795 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.796 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.796 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.796 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.796 I print_info: LF token         = 187 ''
0.00.039.796 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.797 I print_info: max token length = 1024
0.00.039.797 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.544 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.559 I load_tensors: offloading output layer to GPU
0.00.445.560 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.596 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.597 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.447.423 I llama_context: constructing llama_context
0.00.447.427 I llama_context: n_seq_max     = 1
0.00.447.428 I llama_context: n_ctx         = 128
0.00.447.428 I llama_context: n_ctx_per_seq = 128
0.00.447.428 I llama_context: n_batch       = 128
0.00.447.429 I llama_context: n_ubatch      = 128
0.00.447.429 I llama_context: flash_attn    = 0
0.00.447.431 I llama_context: freq_base     = 10000.0
0.00.447.432 I llama_context: freq_scale    = 1
0.00.447.432 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.447.437 I ggml_metal_init: allocating
0.00.447.568 I ggml_metal_init: found device: Apple M4
0.00.447.589 I ggml_metal_init: picking default device: Apple M4
0.00.449.529 I ggml_metal_init: using embedded metal library
0.00.455.054 I ggml_metal_init: GPU name:   Apple M4
0.00.455.062 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.063 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.064 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.064 I ggml_metal_init: simdgroup reduction   = true
0.00.455.064 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.065 I ggml_metal_init: has residency sets    = true
0.00.455.065 I ggml_metal_init: has bfloat            = true
0.00.455.065 I ggml_metal_init: use bfloat            = true
0.00.455.067 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.004 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.475.006 I llama_context_kv_self: constructing llama_context_kv_self
0.00.475.008 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.478.657 I init:      Metal KV buffer size =    24.00 MiB
0.00.478.666 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.482.008 I init:      Metal compute buffer size =    25.56 MiB
0.00.482.010 I init:        CPU compute buffer size =     1.06 MiB
0.00.482.011 I init: graph nodes  = 991
0.00.482.011 I init: graph splits = 2
0.00.482.015 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.482.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.509.830 I 
0.00.509.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.509.894 I perplexity: tokenizing the input ..
0.00.516.221 I perplexity: tokenization took 6.325 ms
0.00.516.226 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.657.965 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.659.284 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.659.306 I llama_perf_context_print:        load time =     501.00 ms
0.00.659.307 I llama_perf_context_print: prompt eval time =     141.34 ms /   128 tokens (    1.10 ms per token,   905.62 tokens per second)
0.00.659.308 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.659.308 I llama_perf_context_print:       total time =     149.48 ms /   129 tokens
0.00.659.805 I ggml_metal_free: deallocating

real	0m0.673s
user	0m0.080s
sys	0m0.112s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.646 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.209 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.214 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.215 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.216 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.216 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.217 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.217 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.221 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.224 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.225 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.225 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.077 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.864 I llama_model_loader: - type  f32:  194 tensors
0.00.024.865 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.865 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.865 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.866 I print_info: file format = GGUF V3 (latest)
0.00.024.866 I print_info: file type   = Q4_K - Medium
0.00.024.868 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.819 I load: special tokens cache size = 25
0.00.039.019 I load: token to piece cache size = 0.2984 MB
0.00.039.034 I print_info: arch             = gptneox
0.00.039.035 I print_info: vocab_only       = 0
0.00.039.035 I print_info: n_ctx_train      = 2048
0.00.039.035 I print_info: n_embd           = 2048
0.00.039.036 I print_info: n_layer          = 24
0.00.039.038 I print_info: n_head           = 16
0.00.039.039 I print_info: n_head_kv        = 16
0.00.039.039 I print_info: n_rot            = 32
0.00.039.039 I print_info: n_swa            = 0
0.00.039.040 I print_info: n_embd_head_k    = 128
0.00.039.040 I print_info: n_embd_head_v    = 128
0.00.039.041 I print_info: n_gqa            = 1
0.00.039.041 I print_info: n_embd_k_gqa     = 2048
0.00.039.042 I print_info: n_embd_v_gqa     = 2048
0.00.039.043 I print_info: f_norm_eps       = 1.0e-05
0.00.039.043 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.043 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.043 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.044 I print_info: f_logit_scale    = 0.0e+00
0.00.039.044 I print_info: n_ff             = 8192
0.00.039.044 I print_info: n_expert         = 0
0.00.039.045 I print_info: n_expert_used    = 0
0.00.039.046 I print_info: causal attn      = 1
0.00.039.046 I print_info: pooling type     = 0
0.00.039.047 I print_info: rope type        = 2
0.00.039.047 I print_info: rope scaling     = linear
0.00.039.047 I print_info: freq_base_train  = 10000.0
0.00.039.047 I print_info: freq_scale_train = 1
0.00.039.047 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.048 I print_info: rope_finetuned   = unknown
0.00.039.048 I print_info: ssm_d_conv       = 0
0.00.039.048 I print_info: ssm_d_inner      = 0
0.00.039.048 I print_info: ssm_d_state      = 0
0.00.039.048 I print_info: ssm_dt_rank      = 0
0.00.039.048 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.048 I print_info: model type       = 1.4B
0.00.039.049 I print_info: model params     = 1.41 B
0.00.039.049 I print_info: general.name     = 1.4B
0.00.039.049 I print_info: vocab type       = BPE
0.00.039.050 I print_info: n_vocab          = 50304
0.00.039.050 I print_info: n_merges         = 50009
0.00.039.050 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.051 I print_info: LF token         = 187 ''
0.00.039.051 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.051 I print_info: max token length = 1024
0.00.039.052 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.159 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.167 I load_tensors: offloading output layer to GPU
0.00.538.168 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.201 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.202 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.539.701 I llama_context: constructing llama_context
0.00.539.704 I llama_context: n_seq_max     = 1
0.00.539.704 I llama_context: n_ctx         = 2048
0.00.539.705 I llama_context: n_ctx_per_seq = 2048
0.00.539.705 I llama_context: n_batch       = 2048
0.00.539.705 I llama_context: n_ubatch      = 512
0.00.539.706 I llama_context: flash_attn    = 0
0.00.539.708 I llama_context: freq_base     = 10000.0
0.00.539.708 I llama_context: freq_scale    = 1
0.00.539.710 I ggml_metal_init: allocating
0.00.539.790 I ggml_metal_init: found device: Apple M4
0.00.539.802 I ggml_metal_init: picking default device: Apple M4
0.00.541.720 I ggml_metal_init: using embedded metal library
0.00.548.266 I ggml_metal_init: GPU name:   Apple M4
0.00.548.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.548.271 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.548.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.548.272 I ggml_metal_init: simdgroup reduction   = true
0.00.548.272 I ggml_metal_init: simdgroup matrix mul. = true
0.00.548.272 I ggml_metal_init: has residency sets    = true
0.00.548.273 I ggml_metal_init: has bfloat            = true
0.00.548.273 I ggml_metal_init: use bfloat            = true
0.00.548.274 I ggml_metal_init: hasUnifiedMemory      = true
0.00.548.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.566.082 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.566.084 I llama_context_kv_self: constructing llama_context_kv_self
0.00.566.086 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.674 I init:      Metal KV buffer size =   384.00 MiB
0.00.617.681 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.622.136 I init:      Metal compute buffer size =   102.25 MiB
0.00.622.138 I init:        CPU compute buffer size =     8.01 MiB
0.00.622.138 I init: graph nodes  = 991
0.00.622.138 I init: graph splits = 2
0.00.622.145 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.622.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.622.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.362 I main: llama threadpool init, n_threads = 4
0.00.682.416 I 
0.00.682.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.435 I 
0.00.682.613 I sampler seed: 1234
0.00.682.618 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.682.639 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.682.639 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.682.639 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.449.708 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50390.35 tokens per second)
0.01.449.709 I llama_perf_context_print:        load time =     672.00 ms
0.01.449.710 I llama_perf_context_print: prompt eval time =      55.59 ms /     7 tokens (    7.94 ms per token,   125.92 tokens per second)
0.01.449.711 I llama_perf_context_print:        eval time =     708.64 ms /    63 runs   (   11.25 ms per token,    88.90 tokens per second)
0.01.449.711 I llama_perf_context_print:       total time =     768.06 ms /    70 tokens
0.01.453.593 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.109s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.038 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.729 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.738 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.742 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.745 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.696 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.660 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.661 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.662 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.662 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.662 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.663 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.663 I llama_model_loader: - type  f32:  194 tensors
0.00.025.664 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.664 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.664 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.665 I print_info: file format = GGUF V3 (latest)
0.00.025.665 I print_info: file type   = Q4_K - Medium
0.00.025.666 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.153 I load: special tokens cache size = 25
0.00.040.460 I load: token to piece cache size = 0.2984 MB
0.00.040.478 I print_info: arch             = gptneox
0.00.040.478 I print_info: vocab_only       = 0
0.00.040.479 I print_info: n_ctx_train      = 2048
0.00.040.479 I print_info: n_embd           = 2048
0.00.040.479 I print_info: n_layer          = 24
0.00.040.482 I print_info: n_head           = 16
0.00.040.483 I print_info: n_head_kv        = 16
0.00.040.485 I print_info: n_rot            = 32
0.00.040.485 I print_info: n_swa            = 0
0.00.040.485 I print_info: n_embd_head_k    = 128
0.00.040.485 I print_info: n_embd_head_v    = 128
0.00.040.486 I print_info: n_gqa            = 1
0.00.040.487 I print_info: n_embd_k_gqa     = 2048
0.00.040.487 I print_info: n_embd_v_gqa     = 2048
0.00.040.488 I print_info: f_norm_eps       = 1.0e-05
0.00.040.488 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.488 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.488 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.489 I print_info: f_logit_scale    = 0.0e+00
0.00.040.491 I print_info: n_ff             = 8192
0.00.040.491 I print_info: n_expert         = 0
0.00.040.491 I print_info: n_expert_used    = 0
0.00.040.492 I print_info: causal attn      = 1
0.00.040.492 I print_info: pooling type     = 0
0.00.040.492 I print_info: rope type        = 2
0.00.040.492 I print_info: rope scaling     = linear
0.00.040.492 I print_info: freq_base_train  = 10000.0
0.00.040.493 I print_info: freq_scale_train = 1
0.00.040.493 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.493 I print_info: rope_finetuned   = unknown
0.00.040.493 I print_info: ssm_d_conv       = 0
0.00.040.493 I print_info: ssm_d_inner      = 0
0.00.040.493 I print_info: ssm_d_state      = 0
0.00.040.493 I print_info: ssm_dt_rank      = 0
0.00.040.494 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.494 I print_info: model type       = 1.4B
0.00.040.494 I print_info: model params     = 1.41 B
0.00.040.494 I print_info: general.name     = 1.4B
0.00.040.495 I print_info: vocab type       = BPE
0.00.040.495 I print_info: n_vocab          = 50304
0.00.040.495 I print_info: n_merges         = 50009
0.00.040.495 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.495 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.495 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.496 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.496 I print_info: LF token         = 187 ''
0.00.040.496 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.496 I print_info: max token length = 1024
0.00.040.497 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.554.138 I load_tensors: offloading 24 repeating layers to GPU
0.00.554.149 I load_tensors: offloading output layer to GPU
0.00.554.150 I load_tensors: offloaded 25/25 layers to GPU
0.00.554.183 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.554.185 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.555.892 I llama_context: constructing llama_context
0.00.555.895 I llama_context: n_seq_max     = 1
0.00.555.896 I llama_context: n_ctx         = 128
0.00.555.897 I llama_context: n_ctx_per_seq = 128
0.00.555.897 I llama_context: n_batch       = 128
0.00.555.898 I llama_context: n_ubatch      = 128
0.00.555.898 I llama_context: flash_attn    = 0
0.00.555.900 I llama_context: freq_base     = 10000.0
0.00.555.901 I llama_context: freq_scale    = 1
0.00.555.901 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.555.903 I ggml_metal_init: allocating
0.00.555.982 I ggml_metal_init: found device: Apple M4
0.00.555.996 I ggml_metal_init: picking default device: Apple M4
0.00.557.803 I ggml_metal_init: using embedded metal library
0.00.564.336 I ggml_metal_init: GPU name:   Apple M4
0.00.564.345 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.564.346 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.564.347 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.564.347 I ggml_metal_init: simdgroup reduction   = true
0.00.564.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.564.348 I ggml_metal_init: has residency sets    = true
0.00.564.348 I ggml_metal_init: has bfloat            = true
0.00.564.348 I ggml_metal_init: use bfloat            = true
0.00.564.349 I ggml_metal_init: hasUnifiedMemory      = true
0.00.564.359 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.583.120 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.583.123 I llama_context_kv_self: constructing llama_context_kv_self
0.00.583.125 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.586.672 I init:      Metal KV buffer size =    24.00 MiB
0.00.586.675 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.590.030 I init:      Metal compute buffer size =    25.56 MiB
0.00.590.032 I init:        CPU compute buffer size =     1.06 MiB
0.00.590.032 I init: graph nodes  = 991
0.00.590.033 I init: graph splits = 2
0.00.590.036 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.590.036 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.432 I 
0.00.618.490 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.496 I perplexity: tokenizing the input ..
0.00.625.295 I perplexity: tokenization took 6.794 ms
0.00.625.301 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.768.675 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.770.022 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.770.052 I llama_perf_context_print:        load time =     608.39 ms
0.00.770.052 I llama_perf_context_print: prompt eval time =     142.43 ms /   128 tokens (    1.11 ms per token,   898.71 tokens per second)
0.00.770.053 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.053 I llama_perf_context_print:       total time =     151.62 ms /   129 tokens
0.00.770.652 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.081s
sys	0m0.151s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.779 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.457 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.458 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.458 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.459 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.460 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.461 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.463 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.224 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.270 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.038 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.039 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.040 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.041 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.041 I llama_model_loader: - type  f32:  194 tensors
0.00.024.041 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.041 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.042 I print_info: file format = GGUF V3 (latest)
0.00.024.043 I print_info: file type   = Q5_K - Medium
0.00.024.043 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.202 I load: special tokens cache size = 25
0.00.038.388 I load: token to piece cache size = 0.2984 MB
0.00.038.403 I print_info: arch             = gptneox
0.00.038.404 I print_info: vocab_only       = 0
0.00.038.404 I print_info: n_ctx_train      = 2048
0.00.038.404 I print_info: n_embd           = 2048
0.00.038.404 I print_info: n_layer          = 24
0.00.038.407 I print_info: n_head           = 16
0.00.038.408 I print_info: n_head_kv        = 16
0.00.038.408 I print_info: n_rot            = 32
0.00.038.408 I print_info: n_swa            = 0
0.00.038.408 I print_info: n_embd_head_k    = 128
0.00.038.408 I print_info: n_embd_head_v    = 128
0.00.038.409 I print_info: n_gqa            = 1
0.00.038.410 I print_info: n_embd_k_gqa     = 2048
0.00.038.410 I print_info: n_embd_v_gqa     = 2048
0.00.038.411 I print_info: f_norm_eps       = 1.0e-05
0.00.038.412 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.412 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.412 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.412 I print_info: f_logit_scale    = 0.0e+00
0.00.038.413 I print_info: n_ff             = 8192
0.00.038.413 I print_info: n_expert         = 0
0.00.038.413 I print_info: n_expert_used    = 0
0.00.038.413 I print_info: causal attn      = 1
0.00.038.414 I print_info: pooling type     = 0
0.00.038.414 I print_info: rope type        = 2
0.00.038.414 I print_info: rope scaling     = linear
0.00.038.414 I print_info: freq_base_train  = 10000.0
0.00.038.415 I print_info: freq_scale_train = 1
0.00.038.415 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.415 I print_info: rope_finetuned   = unknown
0.00.038.415 I print_info: ssm_d_conv       = 0
0.00.038.415 I print_info: ssm_d_inner      = 0
0.00.038.415 I print_info: ssm_d_state      = 0
0.00.038.415 I print_info: ssm_dt_rank      = 0
0.00.038.416 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.416 I print_info: model type       = 1.4B
0.00.038.416 I print_info: model params     = 1.41 B
0.00.038.416 I print_info: general.name     = 1.4B
0.00.038.417 I print_info: vocab type       = BPE
0.00.038.417 I print_info: n_vocab          = 50304
0.00.038.417 I print_info: n_merges         = 50009
0.00.038.417 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.418 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.418 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.419 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.419 I print_info: LF token         = 187 ''
0.00.038.419 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.419 I print_info: max token length = 1024
0.00.038.420 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.620.850 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.860 I load_tensors: offloading output layer to GPU
0.00.620.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.895 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.620.900 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.622.715 I llama_context: constructing llama_context
0.00.622.723 I llama_context: n_seq_max     = 1
0.00.622.724 I llama_context: n_ctx         = 2048
0.00.622.724 I llama_context: n_ctx_per_seq = 2048
0.00.622.724 I llama_context: n_batch       = 2048
0.00.622.725 I llama_context: n_ubatch      = 512
0.00.622.725 I llama_context: flash_attn    = 0
0.00.622.727 I llama_context: freq_base     = 10000.0
0.00.622.728 I llama_context: freq_scale    = 1
0.00.622.735 I ggml_metal_init: allocating
0.00.622.821 I ggml_metal_init: found device: Apple M4
0.00.622.834 I ggml_metal_init: picking default device: Apple M4
0.00.624.599 I ggml_metal_init: using embedded metal library
0.00.631.283 I ggml_metal_init: GPU name:   Apple M4
0.00.631.287 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.287 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.288 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.289 I ggml_metal_init: simdgroup reduction   = true
0.00.631.289 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.290 I ggml_metal_init: has residency sets    = true
0.00.631.290 I ggml_metal_init: has bfloat            = true
0.00.631.290 I ggml_metal_init: use bfloat            = true
0.00.631.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.648.956 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.648.958 I llama_context_kv_self: constructing llama_context_kv_self
0.00.648.961 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.487 I init:      Metal KV buffer size =   384.00 MiB
0.00.705.493 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.708 I init:      Metal compute buffer size =   102.25 MiB
0.00.710.710 I init:        CPU compute buffer size =     8.01 MiB
0.00.710.711 I init: graph nodes  = 991
0.00.710.711 I init: graph splits = 2
0.00.710.718 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.710.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.710.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.151 I main: llama threadpool init, n_threads = 4
0.00.773.194 I 
0.00.773.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.208 I 
0.00.773.374 I sampler seed: 1234
0.00.773.378 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.389 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.389 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.389 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.616.252 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.616.253 I llama_perf_context_print:        load time =     763.62 ms
0.01.616.255 I llama_perf_context_print: prompt eval time =      52.48 ms /     7 tokens (    7.50 ms per token,   133.38 tokens per second)
0.01.616.255 I llama_perf_context_print:        eval time =     787.72 ms /    63 runs   (   12.50 ms per token,    79.98 tokens per second)
0.01.616.256 I llama_perf_context_print:       total time =     843.85 ms /    70 tokens
0.01.620.203 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.110s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.056 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.993 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.999 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.001 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.001 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.003 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.004 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.004 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.004 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.005 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.005 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.007 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.961 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.051 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.958 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.962 I llama_model_loader: - type  f32:  194 tensors
0.00.024.962 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.962 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.963 I print_info: file format = GGUF V3 (latest)
0.00.024.964 I print_info: file type   = Q5_K - Medium
0.00.024.965 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.559 I load: special tokens cache size = 25
0.00.039.643 I load: token to piece cache size = 0.2984 MB
0.00.039.661 I print_info: arch             = gptneox
0.00.039.662 I print_info: vocab_only       = 0
0.00.039.662 I print_info: n_ctx_train      = 2048
0.00.039.663 I print_info: n_embd           = 2048
0.00.039.663 I print_info: n_layer          = 24
0.00.039.667 I print_info: n_head           = 16
0.00.039.667 I print_info: n_head_kv        = 16
0.00.039.667 I print_info: n_rot            = 32
0.00.039.667 I print_info: n_swa            = 0
0.00.039.668 I print_info: n_embd_head_k    = 128
0.00.039.668 I print_info: n_embd_head_v    = 128
0.00.039.668 I print_info: n_gqa            = 1
0.00.039.669 I print_info: n_embd_k_gqa     = 2048
0.00.039.670 I print_info: n_embd_v_gqa     = 2048
0.00.039.670 I print_info: f_norm_eps       = 1.0e-05
0.00.039.671 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.671 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.671 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.671 I print_info: f_logit_scale    = 0.0e+00
0.00.039.672 I print_info: n_ff             = 8192
0.00.039.672 I print_info: n_expert         = 0
0.00.039.672 I print_info: n_expert_used    = 0
0.00.039.672 I print_info: causal attn      = 1
0.00.039.672 I print_info: pooling type     = 0
0.00.039.672 I print_info: rope type        = 2
0.00.039.673 I print_info: rope scaling     = linear
0.00.039.673 I print_info: freq_base_train  = 10000.0
0.00.039.673 I print_info: freq_scale_train = 1
0.00.039.673 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.674 I print_info: rope_finetuned   = unknown
0.00.039.674 I print_info: ssm_d_conv       = 0
0.00.039.674 I print_info: ssm_d_inner      = 0
0.00.039.674 I print_info: ssm_d_state      = 0
0.00.039.674 I print_info: ssm_dt_rank      = 0
0.00.039.674 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.674 I print_info: model type       = 1.4B
0.00.039.675 I print_info: model params     = 1.41 B
0.00.039.675 I print_info: general.name     = 1.4B
0.00.039.675 I print_info: vocab type       = BPE
0.00.039.676 I print_info: n_vocab          = 50304
0.00.039.676 I print_info: n_merges         = 50009
0.00.039.676 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.676 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.676 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.676 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.677 I print_info: LF token         = 187 ''
0.00.039.680 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.680 I print_info: max token length = 1024
0.00.039.680 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.586.937 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.952 I load_tensors: offloading output layer to GPU
0.00.586.952 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.008 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.025 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.588.548 I llama_context: constructing llama_context
0.00.588.550 I llama_context: n_seq_max     = 1
0.00.588.551 I llama_context: n_ctx         = 128
0.00.588.551 I llama_context: n_ctx_per_seq = 128
0.00.588.551 I llama_context: n_batch       = 128
0.00.588.552 I llama_context: n_ubatch      = 128
0.00.588.552 I llama_context: flash_attn    = 0
0.00.588.553 I llama_context: freq_base     = 10000.0
0.00.588.553 I llama_context: freq_scale    = 1
0.00.588.554 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.555 I ggml_metal_init: allocating
0.00.588.578 I ggml_metal_init: found device: Apple M4
0.00.588.589 I ggml_metal_init: picking default device: Apple M4
0.00.590.056 I ggml_metal_init: using embedded metal library
0.00.596.207 I ggml_metal_init: GPU name:   Apple M4
0.00.596.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.211 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.212 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.213 I ggml_metal_init: simdgroup reduction   = true
0.00.596.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.214 I ggml_metal_init: has residency sets    = true
0.00.596.214 I ggml_metal_init: has bfloat            = true
0.00.596.214 I ggml_metal_init: use bfloat            = true
0.00.596.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.017 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.613.018 I llama_context_kv_self: constructing llama_context_kv_self
0.00.613.021 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.443 I init:      Metal KV buffer size =    24.00 MiB
0.00.616.450 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.626 I init:      Metal compute buffer size =    25.56 MiB
0.00.619.628 I init:        CPU compute buffer size =     1.06 MiB
0.00.619.628 I init: graph nodes  = 991
0.00.619.628 I init: graph splits = 2
0.00.619.631 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.012 I 
0.00.653.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.081 I perplexity: tokenizing the input ..
0.00.658.528 I perplexity: tokenization took 5.446 ms
0.00.658.533 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.488 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.795.831 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.795.858 I llama_perf_context_print:        load time =     643.95 ms
0.00.795.859 I llama_perf_context_print: prompt eval time =     135.72 ms /   128 tokens (    1.06 ms per token,   943.13 tokens per second)
0.00.795.860 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.860 I llama_perf_context_print:       total time =     142.85 ms /   129 tokens
0.00.796.401 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.077s
sys	0m0.131s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.624 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.483 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.483 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.484 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.484 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.486 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.487 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.487 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.488 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.490 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.490 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.341 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.194 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.196 I llama_model_loader: - type  f32:  194 tensors
0.00.024.197 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.197 I print_info: file format = GGUF V3 (latest)
0.00.024.197 I print_info: file type   = Q6_K
0.00.024.198 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.380 I load: special tokens cache size = 25
0.00.038.474 I load: token to piece cache size = 0.2984 MB
0.00.038.488 I print_info: arch             = gptneox
0.00.038.489 I print_info: vocab_only       = 0
0.00.038.490 I print_info: n_ctx_train      = 2048
0.00.038.490 I print_info: n_embd           = 2048
0.00.038.490 I print_info: n_layer          = 24
0.00.038.493 I print_info: n_head           = 16
0.00.038.494 I print_info: n_head_kv        = 16
0.00.038.494 I print_info: n_rot            = 32
0.00.038.494 I print_info: n_swa            = 0
0.00.038.495 I print_info: n_embd_head_k    = 128
0.00.038.495 I print_info: n_embd_head_v    = 128
0.00.038.496 I print_info: n_gqa            = 1
0.00.038.496 I print_info: n_embd_k_gqa     = 2048
0.00.038.497 I print_info: n_embd_v_gqa     = 2048
0.00.038.498 I print_info: f_norm_eps       = 1.0e-05
0.00.038.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.498 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.498 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.499 I print_info: f_logit_scale    = 0.0e+00
0.00.038.499 I print_info: n_ff             = 8192
0.00.038.499 I print_info: n_expert         = 0
0.00.038.499 I print_info: n_expert_used    = 0
0.00.038.500 I print_info: causal attn      = 1
0.00.038.500 I print_info: pooling type     = 0
0.00.038.500 I print_info: rope type        = 2
0.00.038.500 I print_info: rope scaling     = linear
0.00.038.501 I print_info: freq_base_train  = 10000.0
0.00.038.502 I print_info: freq_scale_train = 1
0.00.038.502 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.502 I print_info: rope_finetuned   = unknown
0.00.038.502 I print_info: ssm_d_conv       = 0
0.00.038.502 I print_info: ssm_d_inner      = 0
0.00.038.502 I print_info: ssm_d_state      = 0
0.00.038.502 I print_info: ssm_dt_rank      = 0
0.00.038.503 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.503 I print_info: model type       = 1.4B
0.00.038.503 I print_info: model params     = 1.41 B
0.00.038.503 I print_info: general.name     = 1.4B
0.00.038.504 I print_info: vocab type       = BPE
0.00.038.505 I print_info: n_vocab          = 50304
0.00.038.506 I print_info: n_merges         = 50009
0.00.038.506 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.506 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.506 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.506 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.506 I print_info: LF token         = 187 ''
0.00.038.507 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.507 I print_info: max token length = 1024
0.00.038.507 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.662.843 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.858 I load_tensors: offloading output layer to GPU
0.00.662.859 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.895 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.662.896 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.664.405 I llama_context: constructing llama_context
0.00.664.408 I llama_context: n_seq_max     = 1
0.00.664.408 I llama_context: n_ctx         = 2048
0.00.664.409 I llama_context: n_ctx_per_seq = 2048
0.00.664.409 I llama_context: n_batch       = 2048
0.00.664.409 I llama_context: n_ubatch      = 512
0.00.664.410 I llama_context: flash_attn    = 0
0.00.664.411 I llama_context: freq_base     = 10000.0
0.00.664.412 I llama_context: freq_scale    = 1
0.00.664.413 I ggml_metal_init: allocating
0.00.664.447 I ggml_metal_init: found device: Apple M4
0.00.664.457 I ggml_metal_init: picking default device: Apple M4
0.00.665.998 I ggml_metal_init: using embedded metal library
0.00.672.183 I ggml_metal_init: GPU name:   Apple M4
0.00.672.187 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.672.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.672.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.672.190 I ggml_metal_init: simdgroup reduction   = true
0.00.672.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.672.190 I ggml_metal_init: has residency sets    = true
0.00.672.191 I ggml_metal_init: has bfloat            = true
0.00.672.191 I ggml_metal_init: use bfloat            = true
0.00.672.192 I ggml_metal_init: hasUnifiedMemory      = true
0.00.672.193 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.690.104 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.690.106 I llama_context_kv_self: constructing llama_context_kv_self
0.00.690.108 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.249 I init:      Metal KV buffer size =   384.00 MiB
0.00.743.255 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.747.291 I init:      Metal compute buffer size =   102.25 MiB
0.00.747.293 I init:        CPU compute buffer size =     8.01 MiB
0.00.747.293 I init: graph nodes  = 991
0.00.747.293 I init: graph splits = 2
0.00.747.300 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.747.425 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.425 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.844 I main: llama threadpool init, n_threads = 4
0.00.817.890 I 
0.00.817.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.908 I 
0.00.818.082 I sampler seed: 1234
0.00.818.087 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.098 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.098 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.098 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.691.730 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55252.92 tokens per second)
0.01.691.731 I llama_perf_context_print:        load time =     808.46 ms
0.01.691.732 I llama_perf_context_print: prompt eval time =      57.47 ms /     7 tokens (    8.21 ms per token,   121.80 tokens per second)
0.01.691.733 I llama_perf_context_print:        eval time =     813.35 ms /    63 runs   (   12.91 ms per token,    77.46 tokens per second)
0.01.691.733 I llama_perf_context_print:       total time =     874.64 ms /    70 tokens
0.01.695.551 I ggml_metal_free: deallocating

real	0m1.711s
user	0m0.110s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4814 (d9f8cec2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.010 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.736 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.019.742 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.744 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.751 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.751 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.752 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.753 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.754 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.609 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.650 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.485 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.485 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.485 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.486 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.028.486 I llama_model_loader: - type  f32:  194 tensors
0.00.028.487 I llama_model_loader: - type q6_K:   98 tensors
0.00.028.487 I print_info: file format = GGUF V3 (latest)
0.00.028.488 I print_info: file type   = Q6_K
0.00.028.493 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.036.558 I load: special tokens cache size = 25
0.00.042.869 I load: token to piece cache size = 0.2984 MB
0.00.042.888 I print_info: arch             = gptneox
0.00.042.888 I print_info: vocab_only       = 0
0.00.042.889 I print_info: n_ctx_train      = 2048
0.00.042.889 I print_info: n_embd           = 2048
0.00.042.889 I print_info: n_layer          = 24
0.00.042.893 I print_info: n_head           = 16
0.00.042.893 I print_info: n_head_kv        = 16
0.00.042.894 I print_info: n_rot            = 32
0.00.042.894 I print_info: n_swa            = 0
0.00.042.894 I print_info: n_embd_head_k    = 128
0.00.042.894 I print_info: n_embd_head_v    = 128
0.00.042.895 I print_info: n_gqa            = 1
0.00.042.895 I print_info: n_embd_k_gqa     = 2048
0.00.042.896 I print_info: n_embd_v_gqa     = 2048
0.00.042.896 I print_info: f_norm_eps       = 1.0e-05
0.00.042.897 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.897 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.897 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.897 I print_info: f_logit_scale    = 0.0e+00
0.00.042.898 I print_info: n_ff             = 8192
0.00.042.898 I print_info: n_expert         = 0
0.00.042.898 I print_info: n_expert_used    = 0
0.00.042.898 I print_info: causal attn      = 1
0.00.042.898 I print_info: pooling type     = 0
0.00.042.898 I print_info: rope type        = 2
0.00.042.899 I print_info: rope scaling     = linear
0.00.042.899 I print_info: freq_base_train  = 10000.0
0.00.042.899 I print_info: freq_scale_train = 1
0.00.042.899 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.900 I print_info: rope_finetuned   = unknown
0.00.042.900 I print_info: ssm_d_conv       = 0
0.00.042.900 I print_info: ssm_d_inner      = 0
0.00.042.900 I print_info: ssm_d_state      = 0
0.00.042.900 I print_info: ssm_dt_rank      = 0
0.00.042.900 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.901 I print_info: model type       = 1.4B
0.00.042.901 I print_info: model params     = 1.41 B
0.00.042.901 I print_info: general.name     = 1.4B
0.00.042.902 I print_info: vocab type       = BPE
0.00.042.902 I print_info: n_vocab          = 50304
0.00.042.902 I print_info: n_merges         = 50009
0.00.042.902 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.902 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.903 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.903 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.903 I print_info: LF token         = 187 ''
0.00.042.903 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.903 I print_info: max token length = 1024
0.00.042.904 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.597.798 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.802 I load_tensors: offloading output layer to GPU
0.00.597.802 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.828 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.597.831 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.599.194 I llama_context: constructing llama_context
0.00.599.196 I llama_context: n_seq_max     = 1
0.00.599.197 I llama_context: n_ctx         = 128
0.00.599.197 I llama_context: n_ctx_per_seq = 128
0.00.599.197 I llama_context: n_batch       = 128
0.00.599.197 I llama_context: n_ubatch      = 128
0.00.599.198 I llama_context: flash_attn    = 0
0.00.599.198 I llama_context: freq_base     = 10000.0
0.00.599.199 I llama_context: freq_scale    = 1
0.00.599.200 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.201 I ggml_metal_init: allocating
0.00.599.226 I ggml_metal_init: found device: Apple M4
0.00.599.233 I ggml_metal_init: picking default device: Apple M4
0.00.600.614 I ggml_metal_init: using embedded metal library
0.00.606.332 I ggml_metal_init: GPU name:   Apple M4
0.00.606.335 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.336 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.336 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.337 I ggml_metal_init: simdgroup reduction   = true
0.00.606.337 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.337 I ggml_metal_init: has residency sets    = true
0.00.606.337 I ggml_metal_init: has bfloat            = true
0.00.606.338 I ggml_metal_init: use bfloat            = true
0.00.606.339 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.340 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.191 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.622.193 I llama_context_kv_self: constructing llama_context_kv_self
0.00.622.196 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.634 I init:      Metal KV buffer size =    24.00 MiB
0.00.625.636 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.628.975 I init:      Metal compute buffer size =    25.56 MiB
0.00.628.976 I init:        CPU compute buffer size =     1.06 MiB
0.00.628.977 I init: graph nodes  = 991
0.00.628.977 I init: graph splits = 2
0.00.628.981 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.628.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.992 I 
0.00.662.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.079 I perplexity: tokenizing the input ..
0.00.669.285 I perplexity: tokenization took 7.202 ms
0.00.669.293 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.400 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.801.812 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.801.839 I llama_perf_context_print:        load time =     648.97 ms
0.00.801.840 I llama_perf_context_print: prompt eval time =     130.73 ms /   128 tokens (    1.02 ms per token,   979.13 tokens per second)
0.00.801.840 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.841 I llama_perf_context_print:       total time =     139.85 ms /   129 tokens
0.00.802.430 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.076s
sys	0m0.137s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4814 (d9f8cec2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f3055a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f305d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f306300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f3068b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f306e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f307410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f3079c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f307f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f308520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f308a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f308f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f309420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f309f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f30a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f30af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f30b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f30bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f30c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f30cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f30d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f30da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f30e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f30e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f30f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f30f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f30fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f310140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f310db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f3112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f3115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f311a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f311d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f3125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f312ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f312da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f313240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f3136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f313b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f314020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f3144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f314960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f314e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f3152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f315740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f315a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f316010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f316620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f316f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f317550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f317b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f318170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f318780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f318d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f3193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f319b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f31a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f31a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f31a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f31ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f31b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f31b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f31bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f31c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f31c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f31cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f31cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f31d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f31d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f31dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f31e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f31e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f31eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f31efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f31f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f31fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f31ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f320510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f320a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f320fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f321500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f321a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f321fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f3224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f322a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f322f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f3234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f323a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f323f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f3244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f324a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f324f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f3254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f325a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f325f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f3264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f326a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f326f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f316c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f3273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f327b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f3280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f328610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f328b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f3290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f329600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f329b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f32a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f32a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f32ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f32b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f32b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f32bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f32c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f32c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f32c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f32ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f32d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f32d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f32dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f32e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f32e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f32ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f32eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f32f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f32f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f32fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f330140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f3305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f330a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f330f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f3313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f331860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f331d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f3321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f332640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f332ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f332f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f333420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f3338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f333d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f334200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f3346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f334b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f334fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f335480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f335920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f335dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f336260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f336700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f336ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f337040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f3374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f337980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f337e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f3382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f338760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f338c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f3390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f339540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f3399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f339e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f33a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f33a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f33ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f33b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f33b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f33ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f33bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f33c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f33c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f33ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f33d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f33d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f33daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f33df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f33e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f33e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f33ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f33f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f33f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f33fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f33ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f340440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f3408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f340d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f341220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f3416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f341b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f342000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f3424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f342940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f342de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f343280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f3437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f343d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f344270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f3447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f344a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f345090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f3456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f345cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f3464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f346940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f346c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f347210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f347820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f348010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f3484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f348950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f348df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f3495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f349af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f34a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f34a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f34aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f34b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f34b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f34bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f34c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f34c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f34cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f34d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f34d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f34dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f34e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f34e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f34eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f34eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f34f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f34fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f34ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f350530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f350a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f350fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f351520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f351a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f351fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f352510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f352a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f352fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f353500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f353a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f353fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f3544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f354a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f354f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f3554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f355a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f355f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f3564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f356a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f356f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f3574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f357a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f357f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f3584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f358a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f358f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f3594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f3599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f359f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f35a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f35a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f35af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f35b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f35b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f35bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f35c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f35c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f35cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f35d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f35d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f35dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f35df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f35e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f35e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f35ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f35f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f35f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f35fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f35ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f360480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f3609d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f3610f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f361810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f361f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f362650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f362910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f363100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f3633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f3639d0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 991
init: graph splits = 2
0.00.751.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.751.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f50b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f50b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f50be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f50c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f50c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f50cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f50d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f50d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f50d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f50dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f50e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f50e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f50f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f50fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f5103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f510af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f511210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f511930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f512050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f512780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f512ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f5135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f513ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f514400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f514b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f514de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f5150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f515510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f515980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f515df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f5162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f516800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f516c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f516f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f5173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f517810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f517d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f518270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f518770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f518c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f519170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f519670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f519b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f51a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f51a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f51a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f51ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f51b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f51b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f51bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f51c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f51c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f51c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f51cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f51d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f51d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f51de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f51e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f51e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f51ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f51f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f51f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f51fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f520180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f520620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f520ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f520f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f521400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f5218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f521d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f5221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f522680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f522b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f523070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f5235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f523b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f524060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f5245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f524b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f525050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f5255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f525af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f526040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f526590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f526ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f527030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f527580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f527ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f528020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f528570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f528ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f529010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f529560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f529ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f52a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f52a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f52aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f52aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f52b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f52ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f52bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f52c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f52ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f52cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f52d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f52da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f52dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f52e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f52ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f52efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f52f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f52fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f52ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f530440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f5308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f530d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f531220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f5316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f531b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f532000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f5324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f532940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f532de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f533280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f533720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f533bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f534060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f534500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f5349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f534e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f5352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f535780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f535c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f5360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f536560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f536a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f536ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f537340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f5377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f537c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f538120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f5385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f538a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f538f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f5393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f539840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f539ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f53a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f53a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f53aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f53af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f53b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f53b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f53bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f53c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f53c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f53cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f53cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f53d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f53d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f53dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f53e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f53e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f53eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f53f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f53f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f53f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f53fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f5402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f540740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f540be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f541080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f541520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f5419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f541e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f542300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f5427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f542c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f5430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f543580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f543a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f543ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f544360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f544800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f544ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f545140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f5455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f545a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f545f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f5463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f546860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f546d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f5471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f5476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f547c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f548190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f5486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f5489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f548fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f5495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f549bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f54a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f54a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f54ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f54b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f54b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f54bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f54c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f54c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f54cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f54d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f54da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f54df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f54e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f54ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f54ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f54f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f54f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f54ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f550490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f5509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f550f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f551480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f5519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f551f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f552470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f5529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f552f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f553460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f5539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f553f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f554450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f5549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f554ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f555440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f555990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f555ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f556430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f556980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f556ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f557420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f557970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f557ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f558410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f558960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f558eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f559400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f559950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f559ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f55a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f55a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f55ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f55b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f55b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f55be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f55c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f55c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f55ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f55d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f55d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f55de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f55e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f55e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f55ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f55f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f55f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f55fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f5602e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f560780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f560c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f5610c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f561560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f561a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f561ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f562340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f5627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f562c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f563120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f5635c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f563a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f563f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f5643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f5648f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f565010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f565730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f565e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f566570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f566830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f567020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f5672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f5678f0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 991
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f405e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f406300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f406770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f406be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f407050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f4074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f407930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f407da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f408210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f408680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f408af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f409170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f409c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f40a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f40ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f40b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f40ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f40c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f40c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f40d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f40d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f40dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f40e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f40ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f40f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f40f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f40f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f40fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f4102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f410710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13f410b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13f4110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13f411520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13f4117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13f411c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13f4120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13f412530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13f4129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13f412e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13f413280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13f4136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13f413b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13f413fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13f414440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13f4148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13f414d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13f415190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13f415600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13f415a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13f415ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13f416350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13f4167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13f416c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13f4170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13f417510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13f417980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13f417ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13f4183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13f418860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13f418cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13f419140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13f4195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13f419a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13f419e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13f41a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13f41a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13f41abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13f41b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13f41b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13f41b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13f41bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13f41c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13f41c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13f41caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13f41cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13f41d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13f41d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13f41dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13f41e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13f41e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13f41ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13f41ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13f41f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13f41f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13f41fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13f420030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13f4204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13f420910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13f420d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13f4211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13f421660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13f421ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13f421f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13f4223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13f422820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13f422c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13f423100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13f423570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13f4239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13f423e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13f4242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13f424730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13f424ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13f425010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13f425980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13f425c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13f4260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13f426520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13f426990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13f426e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13f427270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13f4276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13f427b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13f427fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13f428430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13f4288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13f428d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13f429180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13f4295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13f429a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13f429ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13f42a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13f42a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13f42ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13f42b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13f42b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13f42b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13f42bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13f42c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13f42c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13f42cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13f42cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13f42d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13f42d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13f42dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13f42e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13f42e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13f42ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13f42eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13f42f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13f42f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13f42fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13f430070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13f4304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13f430950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13f430dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13f431230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13f4316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13f431b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13f431f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13f4323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13f432860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13f432cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13f433140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13f4335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13f433a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13f433e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13f434300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13f434770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13f434be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13f435050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13f4354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13f435930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13f435da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13f436210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13f436680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13f436af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13f436f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13f4373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13f437840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13f437cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13f438120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13f438590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13f438a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13f438e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13f4392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13f439750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13f439bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13f43a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13f43a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13f43a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13f43ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13f43b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13f43b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13f43bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13f43bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13f43c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13f43c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13f43cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13f43d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13f43d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13f43d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13f43de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13f43e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13f43e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13f43eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13f43f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13f43f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13f43f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13f43fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13f4401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13f440640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13f440ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13f440f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13f441390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13f441800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13f441c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13f442200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13f442670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13f442ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13f443630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13f4438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13f443bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13f444020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13f444490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13f444900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13f444d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13f4451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13f445650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13f445ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13f445f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13f4463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13f446810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13f446c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13f4470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13f447560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13f4479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13f447e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13f4482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13f448720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13f448b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13f449000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13f449470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13f4498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13f449d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13f44a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13f44a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13f44aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13f44af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13f44b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13f44b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13f44bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13f44c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13f44c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13f44c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13f44ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13f44d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13f44d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13f44db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13f44dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13f44e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13f44e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13f44ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13f44f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13f44f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13f44fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13f44fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13f450360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13f4507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13f450c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13f4510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13f451520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13f451990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13f451e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13f452270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13f4526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13f452b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13f452fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13f453430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13f4538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13f453d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13f454180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13f4545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13f454a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13f454ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13f455340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13f4557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13f455c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13f456090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13f456500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13f456970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13f456de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13f457250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13f457cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13f4583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13f458b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13f459220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13f4594e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13f459950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13f459f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13f45a560 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 991
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.802s
user	0m0.274s
sys	0m0.343s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4814 (d9f8cec2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143e0d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143e0d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143e0dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143e0e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143e0e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143e0ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143e0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143e0f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143e0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143e104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143e109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143e10ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143e119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143e12170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143e12980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143e130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143e137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143e13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143e14600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143e14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143e154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143e15c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143e16330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143e16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143e172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143e175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143e17bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143e18830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143e18d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143e19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143e194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143e19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143e1a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143e1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143e1a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143e1acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143e1b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143e1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143e1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143e1bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143e1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143e1c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143e1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143e1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143e1d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143e1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143e1e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143e1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143e1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143e1f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143e1fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143e20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143e20810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143e20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143e21610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143e21ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143e21f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143e22210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143e22820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143e23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143e232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143e23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143e23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143e240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143e24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143e249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143e24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143e25330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143e257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143e25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143e26110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143e265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143e26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143e26fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143e274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143e27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143e27f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143e284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143e28a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143e28f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143e294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143e29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143e29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143e2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143e2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143e2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143e2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143e2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143e2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143e2c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143e2c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143e2cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143e2d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143e2d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143e2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143e2e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143e2e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143e1e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143e2ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143e2f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143e2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143e30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143e305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143e30b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143e31080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143e315d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143e31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143e32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143e325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143e32b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143e33060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143e335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143e33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143e33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143e34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143e348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143e34d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143e35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143e356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143e35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143e36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143e364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143e36940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143e36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143e37280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143e37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143e37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143e38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143e38500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143e389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143e38e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143e392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143e39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143e39c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143e3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143e3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143e3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143e3aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143e3b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143e3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143e3bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143e3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143e3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143e3ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143e3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143e3d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143e3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143e3dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143e3e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143e3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143e3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143e3ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143e3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143e3f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143e3fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143e401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143e40680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143e40b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143e40fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143e41460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143e41900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143e41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143e42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143e426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143e42b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143e43020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143e434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143e43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143e43e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143e442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143e44740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143e44be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143e45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143e45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143e459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143e45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143e46300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143e467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143e46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143e470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143e47580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143e47a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143e47ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143e48360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143e48800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143e48ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143e49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143e495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143e49a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143e49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143e4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143e4a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143e4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143e4b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143e4b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143e4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143e4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143e4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143e4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143e4d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143e4d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143e4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143e4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143e4e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143e4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143e4f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143e4fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143e4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143e503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143e50870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143e51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143e51570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143e51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143e52010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143e52560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143e52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143e53000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143e53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143e53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143e53ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143e54540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143e54a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143e54fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143e55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143e55a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143e55fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143e56520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143e56a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143e56fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143e57510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143e57a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143e57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143e58500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143e58a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143e58fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143e594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143e59a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143e59f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143e5a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143e5aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143e5af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143e5b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143e5ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143e5bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143e5c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143e5ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143e5cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143e5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143e5da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143e5df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143e5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143e5e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143e5ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143e5f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143e5f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143e5ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143e60480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143e609d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143e60f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143e61470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143e619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143e61f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143e62460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143e629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143e62f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143e63450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143e639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143e63e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143e642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143e64780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143e64c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143e650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143e65560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143e65a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143e65ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143e66340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143e667e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143e66c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143e67120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143e675c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143e67a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143e67f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143e68450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143e68b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143e69290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143e699b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143e6a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143e6a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143e6ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143e6ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143e6b450 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 896
init: graph splits = 2
0.00.101.766 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143e6b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143e4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143e4c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143e4d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143e204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143e1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143e224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143e4ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143e17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143e1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143e1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143e1f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143e1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143e1f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143e16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143e0c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143e210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143e22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143e2f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143e6a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143e19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143e19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143e4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143e4d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143e17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143e18140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143e18400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143e6b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143e6bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143e6be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143e6c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143e6c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143e6c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143e6c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143e6cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143e6ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143e6d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143e6d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143e6d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143e6d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143e6dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143e6df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143e6e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143e6e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143e6e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143e6ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143e6ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143e6efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143e6f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143e6f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143e6f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143e6fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143e6fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143e70030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143e702f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143e705b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143e70870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143e70b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143e70df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143e710b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143e71370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143e71630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143e718f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143e71bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143e71e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143e72130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143e723f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143e726b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143e72970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143e72c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143e72ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143e731b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143e73470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143e73730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143e739f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143e73cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143e73f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143e74230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143e744f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143e747b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143e74a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143e74d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143e74ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143e752b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143e75570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143e75830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143e75af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143e75db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143e76070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143e76330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143e765f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143e768b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143e76b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143e76e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143e770f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143e773b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143e77670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143e77930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143e77bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143e77eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143e78170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143e78430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143e786f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143e789b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143e78c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143e78f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143e791f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143e794b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143e79770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143e79a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143e79cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143e79fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143e7a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143e7a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143e7a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143e7aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143e7ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143e7b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143e7b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143e7b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143e7b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143e7bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143e7bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143e7c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143e7c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143e7c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143e7c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143e7cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143e7ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143e7d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143e7d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143e7d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143e7d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143e7dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143e7def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143e7e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143e7e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143e7e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143e7e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143e7ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143e7ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143e7f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143e7f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143e7f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143e7fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143e7fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143e7fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143e802b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143e80570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143e80830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143e80af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143e80db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143e81070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143e81330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143e815f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143e818b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143e81b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143e81e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143e820f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143e823b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143e82670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143e82930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143e82bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143e82eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143e83170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143e83430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143e836f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143e839b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143e83c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143e83f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143e841f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143e844b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143e84770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143e84a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143e84cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143e84fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143e85270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143e85530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143e857f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143e85ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143e85d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143e86030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143e862f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143e865b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143e86870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143e86b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143e86df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143e870b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143e87370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143e87630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143e878f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143e87bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143e87e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143e88130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143e883f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143e886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143e88970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143e88c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143e88ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143e891b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143e89470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143e89730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143e899f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143e89cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143e89f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143e8a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143e8a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143e8a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143e8aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143e8ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143e8b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143e8b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143e8b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143e8bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143e8be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143e8c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143e8c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143e8c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143e8c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143e8cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143e8ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143e8d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143e8d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143e8d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143e8d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143e8dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143e8df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143e8e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143e8e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143e8e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143e8ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143e8ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143e8ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143e8f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143e8f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143e8f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143e8fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143e8fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143e90000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143e902c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143e90580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143e90840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143e90b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143e91050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143e915a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143e91af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143e92040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143e92590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143e92ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143e93030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143e93580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143e93ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143e94020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143e94570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143e94ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143e95010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143e95560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143e95ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143e96000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143e96550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143e96aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143e96ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143e97540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143e97a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143e97fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143e98530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143e98a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143e98d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143e99000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143e99500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143e99a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143e99f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143e9a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143e9a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143e9ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143e9b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143e9b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143e9bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143e9c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143e9c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143e9cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143e9d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143e9d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143e9e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143e9e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143e9ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143e9f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143e9f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143ea0020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143ea02e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143ea08f0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 896
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145108d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145106e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145109370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145109920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145109ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14510a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14510aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14510afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14510b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14510ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14510bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14510c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14510cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14510d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14510df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14510e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14510edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14510f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14510fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1451103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145110ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145111200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145111920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145112040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145112760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145112a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145113030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145113640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145113c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145114440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1451148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145114ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145115430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145115970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145115c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1451160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145116570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145116a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145116eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145117350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1451177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145117c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145118130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1451185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145118890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145118ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1451194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145119ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14511a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14511a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14511acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14511b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14511b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14511bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14511c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14511cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14511d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14511d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14511d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14511e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14511e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14511ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14511eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14511f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14511f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14511fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145120170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145120610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145120ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145120f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143e9d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143e4e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143ea05a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143e9faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143ea0d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143ea1010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143ea12d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143ea1590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143ea1850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143ea1b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143ea1dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143ea2090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143ea2350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143ea2610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143ea28d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143ea2b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143ea2e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143ea3110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143ea33d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143ea3690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143ea3950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143ea3c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143ea3ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143ea4190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143ea4450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143ea4710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143ea49d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143ea4c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143ea4f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143ea5210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143ea54d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143ea5790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143ea5a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143ea5d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143ea5fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143ea6290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143ea6550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143ea6810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143ea6ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143ea6d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143ea7050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143ea7310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143ea75d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143ea7890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143ea7b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143ea7e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143ea80d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143ea8390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143ea8650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143ea8910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143ea8bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143ea8e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143ea9150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143ea9410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143ea96d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143ea9990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143ea9c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143ea9f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143eaa1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143eaa490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143eaa750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143eaaa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143eaacd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143eaaf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143eab250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143eab510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143eab7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143eaba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143eabd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143eac010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143eac2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143eac590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143eac850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143eacb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143eacdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143ead090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143ead350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143ead610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143ead8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143eadb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143eade50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143eae110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143eae3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143eae690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143eae950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143eaec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143eaeed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143eaf190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143eaf450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143eaf710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143eaf9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143eafc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143eaff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143eb0210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143eb04d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143eb0790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143eb0a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143eb0d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143eb0fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143eb1290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143eb1550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143eb1810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143eb1ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143eb1d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143eb2050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143eb2310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143eb25d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143eb2890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143eb2b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143eb2e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143eb30d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143eb3390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143eb3650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143eb3910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143eb3bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143eb3e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143eb4150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143eb4410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143eb46d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143eb4990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143eb4c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143eb4f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143eb51d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143eb5490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143eb5750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143eb5a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143eb5cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143eb5f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143eb6250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143eb6510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143eb67d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143eb6a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143eb6d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143eb7010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143eb72d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143eb7590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143eb7850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143eb7b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143eb7dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143eb8090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143eb8660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143eb8920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143eb8be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143eb8ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143eb9160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143eb9420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143eb96e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143eb99a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143eb9c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143eb9f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143eba1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143eba4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143eba760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143ebaa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143ebace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143ebafa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143ebb260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143ebb520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143ebb7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143ebbaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143ebbd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143ebc020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143ebc2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143ebc5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143ebc860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143ebcb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143ebcde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143ebd0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143ebd360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143ebd620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143ebd8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143ebdba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143ebde60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143ebe120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143ebe3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143ebe6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143ebe960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143ebec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143ebeee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143ebf1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143ebf460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143ebf720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143ebf9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143ebfca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143ebff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143ec0220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143ec04e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143ec07a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143ec0a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143ec0d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143ec0fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143ec12a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143ec1560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143ec1820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143ec1ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143ec1da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143ec2060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143ec2320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143ec25e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143ec28a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143ec2b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143ec2e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143ec30e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143ec33a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143ec3660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143ec3920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143ec3be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143ec3ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143ec4160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143ec4420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143ec46e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143ec49a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143ec4c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143ec4f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143ec51e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143ec54a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143ec5760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143ec5a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143ec5ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143ec5fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143ec6260 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: constructing llama_context_kv_self
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 896
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.952s
user	0m0.229s
sys	0m0.180s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.08 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.52 sec*proc (2 tests)

Total Test time (real) =   1.53 sec
        1.56 real         0.52 user         0.19 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.58 sec
        0.58 real         0.13 user         0.09 sys
```
