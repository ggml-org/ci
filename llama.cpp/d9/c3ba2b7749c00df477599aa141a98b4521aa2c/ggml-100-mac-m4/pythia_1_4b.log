Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:303 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.644s
user	0m0.689s
sys	0m0.986s
++ nproc
+ make -j10
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target xxhash
[  6%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 23%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Linking CXX executable ../../bin/llama-simple-chat
[ 26%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-run
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Built target llava
[ 33%] Linking CXX static library libcommon.a
[ 33%] Built target llama-simple-chat
[ 33%] Built target llama-run
[ 33%] Built target llama-simple
[ 33%] Built target test-c
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Linking CXX shared library libllava_shared.dylib
[ 34%] Built target llama-quantize-stats
[ 34%] Built target common
[ 34%] Built target llava_static
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 34%] Built target llava_shared
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 45%] Linking CXX executable ../bin/test-chat-template
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Built target test-chat-template
[ 49%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-grammar-integration
[ 51%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Built target test-sampling
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Built target test-llama-grammar
[ 60%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-barrier
[ 64%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 64%] Built target test-rope
[ 64%] Built target test-backend-ops
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Built target test-json-schema-to-grammar
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-batched
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-embedding
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookup
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Built target llama-bench
[ 76%] Built target llama-infill
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Built target llama-lookahead
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookup
[ 81%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-stats
[ 83%] Generating index.html.hpp
[ 83%] Built target llama-parallel
[ 83%] Built target llama-cli
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 87%] Built target llama-perplexity
[ 88%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 92%] Built target llama-retrieval
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Built target llama-speculative
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-save-load-state
[ 95%] Built target llama-speculative-simple
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-tokenize
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-export-lora
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.407s
user	0m5.116s
sys	0m8.407s

main: quantize time =  5503.39 ms
main:    total time =  5503.39 ms

main: quantize time =  1864.62 ms
main:    total time =  1864.62 ms

main: quantize time =  1980.26 ms
main:    total time =  1980.26 ms

main: quantize time =  3154.18 ms
main:    total time =  3154.19 ms

main: quantize time =  2926.66 ms
main:    total time =  2926.66 ms

main: quantize time =  5218.29 ms
main:    total time =  5218.29 ms

main: quantize time =  5611.40 ms
main:    total time =  5611.40 ms

main: quantize time =  6932.83 ms
main:    total time =  6932.83 ms

main: quantize time =  5871.49 ms
main:    total time =  5871.49 ms

main: quantize time =  4657.05 ms
main:    total time =  4657.05 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.103 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.205 I main: llama backend init
0.00.000.210 I main: load the model and apply lora adapter, if any
0.00.031.959 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.533 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.559 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.563 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.565 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.566 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.567 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.574 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.574 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.575 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.576 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.577 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.581 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.581 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.641 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.954 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.080 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.080 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.081 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.082 I llama_model_loader: - type  f32:  194 tensors
0.00.064.082 I llama_model_loader: - type  f16:   98 tensors
0.00.096.641 I llm_load_vocab: special tokens cache size = 25
0.00.103.757 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.103.760 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.103.760 I llm_load_print_meta: arch             = gptneox
0.00.103.760 I llm_load_print_meta: vocab type       = BPE
0.00.103.761 I llm_load_print_meta: n_vocab          = 50304
0.00.103.761 I llm_load_print_meta: n_merges         = 50009
0.00.103.761 I llm_load_print_meta: vocab_only       = 0
0.00.103.761 I llm_load_print_meta: n_ctx_train      = 2048
0.00.103.761 I llm_load_print_meta: n_embd           = 2048
0.00.103.762 I llm_load_print_meta: n_layer          = 24
0.00.103.783 I llm_load_print_meta: n_head           = 16
0.00.103.785 I llm_load_print_meta: n_head_kv        = 16
0.00.103.785 I llm_load_print_meta: n_rot            = 32
0.00.103.785 I llm_load_print_meta: n_swa            = 0
0.00.103.785 I llm_load_print_meta: n_embd_head_k    = 128
0.00.103.785 I llm_load_print_meta: n_embd_head_v    = 128
0.00.103.786 I llm_load_print_meta: n_gqa            = 1
0.00.103.787 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.103.787 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.103.788 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.103.788 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.103.788 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.103.788 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.103.789 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.103.789 I llm_load_print_meta: n_ff             = 8192
0.00.103.790 I llm_load_print_meta: n_expert         = 0
0.00.103.790 I llm_load_print_meta: n_expert_used    = 0
0.00.103.790 I llm_load_print_meta: causal attn      = 1
0.00.103.790 I llm_load_print_meta: pooling type     = 0
0.00.103.790 I llm_load_print_meta: rope type        = 2
0.00.103.790 I llm_load_print_meta: rope scaling     = linear
0.00.103.793 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.103.793 I llm_load_print_meta: freq_scale_train = 1
0.00.103.793 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.103.794 I llm_load_print_meta: rope_finetuned   = unknown
0.00.103.794 I llm_load_print_meta: ssm_d_conv       = 0
0.00.103.794 I llm_load_print_meta: ssm_d_inner      = 0
0.00.103.794 I llm_load_print_meta: ssm_d_state      = 0
0.00.103.794 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.103.795 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.103.804 I llm_load_print_meta: model type       = 1.4B
0.00.103.805 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.103.805 I llm_load_print_meta: model params     = 1.41 B
0.00.103.806 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.103.806 I llm_load_print_meta: general.name     = 1.4B
0.00.103.806 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.103.806 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.103.807 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.103.807 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.103.807 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.103.807 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.103.808 I llm_load_print_meta: max token length = 1024
0.00.106.318 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.106.318 I llm_load_tensors: offloading output layer to GPU
0.00.106.318 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.106.337 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.106.338 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.107.313 I llama_new_context_with_model: n_seq_max     = 1
0.00.107.314 I llama_new_context_with_model: n_ctx         = 2048
0.00.107.314 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.107.314 I llama_new_context_with_model: n_batch       = 2048
0.00.107.314 I llama_new_context_with_model: n_ubatch      = 512
0.00.107.314 I llama_new_context_with_model: flash_attn    = 0
0.00.107.315 I llama_new_context_with_model: freq_base     = 10000.0
0.00.107.315 I llama_new_context_with_model: freq_scale    = 1
0.00.107.316 I ggml_metal_init: allocating
0.00.107.326 I ggml_metal_init: found device: Apple M4
0.00.107.329 I ggml_metal_init: picking default device: Apple M4
0.00.108.025 I ggml_metal_init: using embedded metal library
0.00.117.048 I ggml_metal_init: GPU name:   Apple M4
0.00.117.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.117.050 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.117.051 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.117.051 I ggml_metal_init: simdgroup reduction   = true
0.00.117.051 I ggml_metal_init: simdgroup matrix mul. = true
0.00.117.051 I ggml_metal_init: has bfloat            = true
0.00.117.051 I ggml_metal_init: use bfloat            = true
0.00.117.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.117.052 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.160.821 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.160.827 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.160.855 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.161.804 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.161.806 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.161.807 I llama_new_context_with_model: graph nodes  = 967
0.00.161.807 I llama_new_context_with_model: graph splits = 2
0.00.161.831 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.239.716 I main: llama threadpool init, n_threads = 4
0.00.239.748 I 
0.00.239.800 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.239.802 I 
0.00.239.884 I sampler seed: 1234
0.00.239.889 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.239.911 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.239.913 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.239.913 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.089.172 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.02.089.173 I llama_perf_context_print:        load time =     207.75 ms
0.02.089.174 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.92 tokens per second)
0.02.089.178 I llama_perf_context_print:        eval time =    1802.53 ms /    63 runs   (   28.61 ms per token,    34.95 tokens per second)
0.02.089.179 I llama_perf_context_print:       total time =    1849.46 ms /    70 tokens
0.02.089.371 I ggml_metal_free: deallocating

real	0m2.464s
user	0m0.147s
sys	0m0.099s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.829 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.714 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.721 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.723 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.726 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.568 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.619 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.621 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.621 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.621 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.622 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.622 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.623 I llama_model_loader: - type  f32:  194 tensors
0.00.033.623 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.763 I llm_load_vocab: special tokens cache size = 25
0.00.061.608 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.611 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.612 I llm_load_print_meta: arch             = gptneox
0.00.061.612 I llm_load_print_meta: vocab type       = BPE
0.00.061.612 I llm_load_print_meta: n_vocab          = 50304
0.00.061.613 I llm_load_print_meta: n_merges         = 50009
0.00.061.613 I llm_load_print_meta: vocab_only       = 0
0.00.061.613 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.613 I llm_load_print_meta: n_embd           = 2048
0.00.061.613 I llm_load_print_meta: n_layer          = 24
0.00.061.631 I llm_load_print_meta: n_head           = 16
0.00.061.632 I llm_load_print_meta: n_head_kv        = 16
0.00.061.632 I llm_load_print_meta: n_rot            = 32
0.00.061.632 I llm_load_print_meta: n_swa            = 0
0.00.061.633 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.633 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.633 I llm_load_print_meta: n_gqa            = 1
0.00.061.634 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.634 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.635 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.635 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.636 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.636 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.636 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.637 I llm_load_print_meta: n_ff             = 8192
0.00.061.637 I llm_load_print_meta: n_expert         = 0
0.00.061.637 I llm_load_print_meta: n_expert_used    = 0
0.00.061.637 I llm_load_print_meta: causal attn      = 1
0.00.061.637 I llm_load_print_meta: pooling type     = 0
0.00.061.637 I llm_load_print_meta: rope type        = 2
0.00.061.638 I llm_load_print_meta: rope scaling     = linear
0.00.061.638 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.638 I llm_load_print_meta: freq_scale_train = 1
0.00.061.638 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.639 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.639 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.639 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.639 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.639 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.639 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.649 I llm_load_print_meta: model type       = 1.4B
0.00.061.649 I llm_load_print_meta: model ftype      = Q8_0
0.00.061.649 I llm_load_print_meta: model params     = 1.41 B
0.00.061.650 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.061.650 I llm_load_print_meta: general.name     = 1.4B
0.00.061.650 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.650 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.650 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.651 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.651 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.061.651 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.651 I llm_load_print_meta: max token length = 1024
0.00.063.484 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.484 I llm_load_tensors: offloading output layer to GPU
0.00.063.484 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.495 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.496 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.402 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.403 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.403 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.403 I llama_new_context_with_model: n_batch       = 2048
0.00.064.403 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.403 I llama_new_context_with_model: flash_attn    = 0
0.00.064.404 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.404 I llama_new_context_with_model: freq_scale    = 1
0.00.064.405 I ggml_metal_init: allocating
0.00.064.409 I ggml_metal_init: found device: Apple M4
0.00.064.412 I ggml_metal_init: picking default device: Apple M4
0.00.065.143 I ggml_metal_init: using embedded metal library
0.00.067.712 I ggml_metal_init: GPU name:   Apple M4
0.00.067.713 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.714 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.714 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.714 I ggml_metal_init: simdgroup reduction   = true
0.00.067.715 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.715 I ggml_metal_init: has bfloat            = true
0.00.067.715 I ggml_metal_init: use bfloat            = true
0.00.067.715 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.716 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.096 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.111 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.329 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.331 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.331 I llama_new_context_with_model: graph nodes  = 967
0.00.104.331 I llama_new_context_with_model: graph splits = 2
0.00.104.348 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.458.341 I main: llama threadpool init, n_threads = 4
0.01.458.380 I 
0.01.458.416 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.458.419 I 
0.01.458.654 I sampler seed: 1234
0.01.458.658 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.458.686 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.458.688 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.458.688 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.554.567 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.02.554.568 I llama_perf_context_print:        load time =    1448.51 ms
0.02.554.568 I llama_perf_context_print: prompt eval time =      46.93 ms /     7 tokens (    6.70 ms per token,   149.16 tokens per second)
0.02.554.569 I llama_perf_context_print:        eval time =    1045.96 ms /    63 runs   (   16.60 ms per token,    60.23 tokens per second)
0.02.554.569 I llama_perf_context_print:       total time =    1096.23 ms /    70 tokens
0.02.554.775 I ggml_metal_free: deallocating

real	0m2.574s
user	0m0.112s
sys	0m0.228s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.016.410 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.038.718 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.722 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.723 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.723 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.724 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.724 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.726 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.726 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.727 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.727 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.727 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.728 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.728 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.733 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.734 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.734 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.050.384 I llama_model_loader: - type  f32:  194 tensors
0.00.050.384 I llama_model_loader: - type q4_0:   97 tensors
0.00.050.384 I llama_model_loader: - type q6_K:    1 tensors
0.00.086.411 I llm_load_vocab: special tokens cache size = 25
0.00.096.273 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.096.277 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.096.277 I llm_load_print_meta: arch             = gptneox
0.00.096.278 I llm_load_print_meta: vocab type       = BPE
0.00.096.278 I llm_load_print_meta: n_vocab          = 50304
0.00.096.278 I llm_load_print_meta: n_merges         = 50009
0.00.096.278 I llm_load_print_meta: vocab_only       = 0
0.00.096.278 I llm_load_print_meta: n_ctx_train      = 2048
0.00.096.279 I llm_load_print_meta: n_embd           = 2048
0.00.096.279 I llm_load_print_meta: n_layer          = 24
0.00.096.295 I llm_load_print_meta: n_head           = 16
0.00.096.296 I llm_load_print_meta: n_head_kv        = 16
0.00.096.297 I llm_load_print_meta: n_rot            = 32
0.00.096.297 I llm_load_print_meta: n_swa            = 0
0.00.096.299 I llm_load_print_meta: n_embd_head_k    = 128
0.00.096.299 I llm_load_print_meta: n_embd_head_v    = 128
0.00.096.300 I llm_load_print_meta: n_gqa            = 1
0.00.096.301 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.096.302 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.096.302 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.096.303 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.096.303 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.096.303 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.096.303 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.096.304 I llm_load_print_meta: n_ff             = 8192
0.00.096.305 I llm_load_print_meta: n_expert         = 0
0.00.096.305 I llm_load_print_meta: n_expert_used    = 0
0.00.096.307 I llm_load_print_meta: causal attn      = 1
0.00.096.307 I llm_load_print_meta: pooling type     = 0
0.00.096.307 I llm_load_print_meta: rope type        = 2
0.00.096.308 I llm_load_print_meta: rope scaling     = linear
0.00.096.308 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.096.309 I llm_load_print_meta: freq_scale_train = 1
0.00.096.309 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.096.309 I llm_load_print_meta: rope_finetuned   = unknown
0.00.096.309 I llm_load_print_meta: ssm_d_conv       = 0
0.00.096.309 I llm_load_print_meta: ssm_d_inner      = 0
0.00.096.310 I llm_load_print_meta: ssm_d_state      = 0
0.00.096.310 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.096.310 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.096.321 I llm_load_print_meta: model type       = 1.4B
0.00.096.321 I llm_load_print_meta: model ftype      = Q4_0
0.00.096.321 I llm_load_print_meta: model params     = 1.41 B
0.00.096.322 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.096.322 I llm_load_print_meta: general.name     = 1.4B
0.00.096.323 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.096.323 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.096.323 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.096.323 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.096.324 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.096.325 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.096.326 I llm_load_print_meta: max token length = 1024
0.00.099.150 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.099.151 I llm_load_tensors: offloading output layer to GPU
0.00.099.151 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.099.163 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.099.164 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.100.605 I llama_new_context_with_model: n_seq_max     = 1
0.00.100.607 I llama_new_context_with_model: n_ctx         = 2048
0.00.100.607 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.100.607 I llama_new_context_with_model: n_batch       = 2048
0.00.100.608 I llama_new_context_with_model: n_ubatch      = 512
0.00.100.608 I llama_new_context_with_model: flash_attn    = 0
0.00.100.609 I llama_new_context_with_model: freq_base     = 10000.0
0.00.100.609 I llama_new_context_with_model: freq_scale    = 1
0.00.100.610 I ggml_metal_init: allocating
0.00.100.617 I ggml_metal_init: found device: Apple M4
0.00.100.620 I ggml_metal_init: picking default device: Apple M4
0.00.101.507 I ggml_metal_init: using embedded metal library
0.00.105.184 I ggml_metal_init: GPU name:   Apple M4
0.00.105.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.105.187 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.105.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.105.187 I ggml_metal_init: simdgroup reduction   = true
0.00.105.187 I ggml_metal_init: simdgroup matrix mul. = true
0.00.105.188 I ggml_metal_init: has bfloat            = true
0.00.105.188 I ggml_metal_init: use bfloat            = true
0.00.105.188 I ggml_metal_init: hasUnifiedMemory      = true
0.00.105.189 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.140.698 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.140.707 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.140.732 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.141.896 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.141.897 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.141.898 I llama_new_context_with_model: graph nodes  = 967
0.00.141.898 I llama_new_context_with_model: graph splits = 2
0.00.141.909 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.812.610 I main: llama threadpool init, n_threads = 4
0.00.812.690 I 
0.00.812.758 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.812.760 I 
0.00.813.269 I sampler seed: 1234
0.00.813.276 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.813.347 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.813.352 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.813.352 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.494.742 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.494.743 I llama_perf_context_print:        load time =     796.19 ms
0.01.494.744 I llama_perf_context_print: prompt eval time =      40.40 ms /     7 tokens (    5.77 ms per token,   173.28 tokens per second)
0.01.494.745 I llama_perf_context_print:        eval time =     638.00 ms /    63 runs   (   10.13 ms per token,    98.75 tokens per second)
0.01.494.746 I llama_perf_context_print:       total time =     682.14 ms /    70 tokens
0.01.494.930 I ggml_metal_free: deallocating

real	0m1.530s
user	0m0.147s
sys	0m0.193s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.465 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.625 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.638 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.640 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.640 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.641 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.641 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.655 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.656 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.656 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.656 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.657 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.657 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.657 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.659 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.659 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.792 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.943 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.944 I llama_model_loader: - type  f32:  194 tensors
0.00.037.944 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.944 I llama_model_loader: - type q6_K:    1 tensors
0.00.063.174 I llm_load_vocab: special tokens cache size = 25
0.00.071.498 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.501 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.501 I llm_load_print_meta: arch             = gptneox
0.00.071.501 I llm_load_print_meta: vocab type       = BPE
0.00.071.502 I llm_load_print_meta: n_vocab          = 50304
0.00.071.502 I llm_load_print_meta: n_merges         = 50009
0.00.071.502 I llm_load_print_meta: vocab_only       = 0
0.00.071.502 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.502 I llm_load_print_meta: n_embd           = 2048
0.00.071.503 I llm_load_print_meta: n_layer          = 24
0.00.071.517 I llm_load_print_meta: n_head           = 16
0.00.071.518 I llm_load_print_meta: n_head_kv        = 16
0.00.071.518 I llm_load_print_meta: n_rot            = 32
0.00.071.520 I llm_load_print_meta: n_swa            = 0
0.00.071.521 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.521 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.522 I llm_load_print_meta: n_gqa            = 1
0.00.071.523 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.523 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.524 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.524 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.525 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.525 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.525 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.526 I llm_load_print_meta: n_ff             = 8192
0.00.071.526 I llm_load_print_meta: n_expert         = 0
0.00.071.526 I llm_load_print_meta: n_expert_used    = 0
0.00.071.526 I llm_load_print_meta: causal attn      = 1
0.00.071.527 I llm_load_print_meta: pooling type     = 0
0.00.071.527 I llm_load_print_meta: rope type        = 2
0.00.071.527 I llm_load_print_meta: rope scaling     = linear
0.00.071.528 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.528 I llm_load_print_meta: freq_scale_train = 1
0.00.071.528 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.528 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.530 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.530 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.530 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.530 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.531 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.540 I llm_load_print_meta: model type       = 1.4B
0.00.071.541 I llm_load_print_meta: model ftype      = Q4_1
0.00.071.541 I llm_load_print_meta: model params     = 1.41 B
0.00.071.543 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.071.543 I llm_load_print_meta: general.name     = 1.4B
0.00.071.543 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.544 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.544 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.544 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.544 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.071.544 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.545 I llm_load_print_meta: max token length = 1024
0.00.073.952 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.953 I llm_load_tensors: offloading output layer to GPU
0.00.073.953 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.964 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.073.965 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.075.564 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.566 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.566 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.567 I llama_new_context_with_model: n_batch       = 2048
0.00.075.567 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.567 I llama_new_context_with_model: flash_attn    = 0
0.00.075.568 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.568 I llama_new_context_with_model: freq_scale    = 1
0.00.075.569 I ggml_metal_init: allocating
0.00.075.574 I ggml_metal_init: found device: Apple M4
0.00.075.577 I ggml_metal_init: picking default device: Apple M4
0.00.076.539 I ggml_metal_init: using embedded metal library
0.00.080.652 I ggml_metal_init: GPU name:   Apple M4
0.00.080.655 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.656 I ggml_metal_init: simdgroup reduction   = true
0.00.080.656 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.656 I ggml_metal_init: has bfloat            = true
0.00.080.657 I ggml_metal_init: use bfloat            = true
0.00.080.657 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.658 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.116.820 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.829 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.848 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.896 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.897 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.898 I llama_new_context_with_model: graph nodes  = 967
0.00.117.898 I llama_new_context_with_model: graph splits = 2
0.00.117.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.605 I main: llama threadpool init, n_threads = 4
0.00.749.643 I 
0.00.749.673 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.673 I 
0.00.749.898 I sampler seed: 1234
0.00.749.902 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.924 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.925 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.925 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.480.162 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64137.31 tokens per second)
0.01.480.164 I llama_perf_context_print:        load time =     740.13 ms
0.01.480.166 I llama_perf_context_print: prompt eval time =      43.60 ms /     7 tokens (    6.23 ms per token,   160.56 tokens per second)
0.01.480.167 I llama_perf_context_print:        eval time =     683.76 ms /    63 runs   (   10.85 ms per token,    92.14 tokens per second)
0.01.480.167 I llama_perf_context_print:       total time =     730.56 ms /    70 tokens
0.01.480.355 I ggml_metal_free: deallocating

real	0m1.499s
user	0m0.124s
sys	0m0.171s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.653 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.486 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.488 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.489 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.489 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.490 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.490 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.491 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.491 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.491 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.492 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.492 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.492 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.527 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.528 I llama_model_loader: - type  f32:  194 tensors
0.00.024.528 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.528 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.465 I llm_load_vocab: special tokens cache size = 25
0.00.051.330 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.333 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.333 I llm_load_print_meta: arch             = gptneox
0.00.051.334 I llm_load_print_meta: vocab type       = BPE
0.00.051.334 I llm_load_print_meta: n_vocab          = 50304
0.00.051.334 I llm_load_print_meta: n_merges         = 50009
0.00.051.334 I llm_load_print_meta: vocab_only       = 0
0.00.051.335 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.335 I llm_load_print_meta: n_embd           = 2048
0.00.051.335 I llm_load_print_meta: n_layer          = 24
0.00.051.350 I llm_load_print_meta: n_head           = 16
0.00.051.351 I llm_load_print_meta: n_head_kv        = 16
0.00.051.351 I llm_load_print_meta: n_rot            = 32
0.00.051.352 I llm_load_print_meta: n_swa            = 0
0.00.051.352 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.352 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.353 I llm_load_print_meta: n_gqa            = 1
0.00.051.353 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.354 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.355 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.355 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.355 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.355 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.355 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.360 I llm_load_print_meta: n_ff             = 8192
0.00.051.361 I llm_load_print_meta: n_expert         = 0
0.00.051.361 I llm_load_print_meta: n_expert_used    = 0
0.00.051.362 I llm_load_print_meta: causal attn      = 1
0.00.051.364 I llm_load_print_meta: pooling type     = 0
0.00.051.364 I llm_load_print_meta: rope type        = 2
0.00.051.364 I llm_load_print_meta: rope scaling     = linear
0.00.051.364 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.364 I llm_load_print_meta: freq_scale_train = 1
0.00.051.364 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.365 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.365 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.366 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.366 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.367 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.367 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.376 I llm_load_print_meta: model type       = 1.4B
0.00.051.377 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.377 I llm_load_print_meta: model params     = 1.41 B
0.00.051.378 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.378 I llm_load_print_meta: general.name     = 1.4B
0.00.051.378 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.378 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.378 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.378 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.379 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.379 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.379 I llm_load_print_meta: max token length = 1024
0.00.053.367 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.367 I llm_load_tensors: offloading output layer to GPU
0.00.053.367 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.378 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.379 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.344 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.345 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.345 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.345 I llama_new_context_with_model: n_batch       = 2048
0.00.054.345 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.345 I llama_new_context_with_model: flash_attn    = 0
0.00.054.346 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.346 I llama_new_context_with_model: freq_scale    = 1
0.00.054.347 I ggml_metal_init: allocating
0.00.054.353 I ggml_metal_init: found device: Apple M4
0.00.054.357 I ggml_metal_init: picking default device: Apple M4
0.00.054.962 I ggml_metal_init: using embedded metal library
0.00.057.310 I ggml_metal_init: GPU name:   Apple M4
0.00.057.312 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.312 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.312 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.313 I ggml_metal_init: simdgroup reduction   = true
0.00.057.313 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.313 I ggml_metal_init: has bfloat            = true
0.00.057.313 I ggml_metal_init: use bfloat            = true
0.00.057.314 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.316 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.520 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.526 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.545 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.522 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.524 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.524 I llama_new_context_with_model: graph nodes  = 967
0.00.086.524 I llama_new_context_with_model: graph splits = 2
0.00.086.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.003 I main: llama threadpool init, n_threads = 4
0.00.780.040 I 
0.00.780.077 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.080 I 
0.00.780.311 I sampler seed: 1234
0.00.780.316 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.360 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.361 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.361 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.567.075 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.567.075 I llama_perf_context_print:        load time =     771.35 ms
0.01.567.076 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.37 tokens per second)
0.01.567.077 I llama_perf_context_print:        eval time =     740.61 ms /    63 runs   (   11.76 ms per token,    85.06 tokens per second)
0.01.567.078 I llama_perf_context_print:       total time =     787.08 ms /    70 tokens
0.01.567.272 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.692 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.378 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.388 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.390 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.391 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.391 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.392 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.392 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.396 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.396 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.396 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.397 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.398 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.399 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.399 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.402 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.403 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.403 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.404 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.404 I llama_model_loader: - type  f32:  194 tensors
0.00.025.404 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.404 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.342 I llm_load_vocab: special tokens cache size = 25
0.00.052.407 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.409 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.409 I llm_load_print_meta: arch             = gptneox
0.00.052.410 I llm_load_print_meta: vocab type       = BPE
0.00.052.410 I llm_load_print_meta: n_vocab          = 50304
0.00.052.410 I llm_load_print_meta: n_merges         = 50009
0.00.052.410 I llm_load_print_meta: vocab_only       = 0
0.00.052.411 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.411 I llm_load_print_meta: n_embd           = 2048
0.00.052.411 I llm_load_print_meta: n_layer          = 24
0.00.052.425 I llm_load_print_meta: n_head           = 16
0.00.052.426 I llm_load_print_meta: n_head_kv        = 16
0.00.052.427 I llm_load_print_meta: n_rot            = 32
0.00.052.427 I llm_load_print_meta: n_swa            = 0
0.00.052.427 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.427 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.428 I llm_load_print_meta: n_gqa            = 1
0.00.052.433 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.433 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.434 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.434 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.434 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.436 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.436 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.436 I llm_load_print_meta: n_ff             = 8192
0.00.052.437 I llm_load_print_meta: n_expert         = 0
0.00.052.437 I llm_load_print_meta: n_expert_used    = 0
0.00.052.437 I llm_load_print_meta: causal attn      = 1
0.00.052.437 I llm_load_print_meta: pooling type     = 0
0.00.052.437 I llm_load_print_meta: rope type        = 2
0.00.052.437 I llm_load_print_meta: rope scaling     = linear
0.00.052.438 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.438 I llm_load_print_meta: freq_scale_train = 1
0.00.052.438 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.439 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.439 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.439 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.439 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.439 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.439 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.449 I llm_load_print_meta: model type       = 1.4B
0.00.052.449 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.449 I llm_load_print_meta: model params     = 1.41 B
0.00.052.450 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.450 I llm_load_print_meta: general.name     = 1.4B
0.00.052.450 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.450 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.451 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.451 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.451 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.452 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.453 I llm_load_print_meta: max token length = 1024
0.00.054.497 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.497 I llm_load_tensors: offloading output layer to GPU
0.00.054.497 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.508 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.509 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.449 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.450 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.450 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.450 I llama_new_context_with_model: n_batch       = 2048
0.00.055.450 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.451 I llama_new_context_with_model: flash_attn    = 0
0.00.055.451 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.451 I llama_new_context_with_model: freq_scale    = 1
0.00.055.452 I ggml_metal_init: allocating
0.00.055.455 I ggml_metal_init: found device: Apple M4
0.00.055.457 I ggml_metal_init: picking default device: Apple M4
0.00.056.062 I ggml_metal_init: using embedded metal library
0.00.058.421 I ggml_metal_init: GPU name:   Apple M4
0.00.058.422 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.423 I ggml_metal_init: simdgroup reduction   = true
0.00.058.423 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.424 I ggml_metal_init: has bfloat            = true
0.00.058.424 I ggml_metal_init: use bfloat            = true
0.00.058.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.473 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.479 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.497 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.563 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.565 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.565 I llama_new_context_with_model: graph nodes  = 967
0.00.089.565 I llama_new_context_with_model: graph splits = 2
0.00.089.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.995 I main: llama threadpool init, n_threads = 4
0.00.803.033 I 
0.00.803.080 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.081 I 
0.00.803.307 I sampler seed: 1234
0.00.803.312 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.323 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.324 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.324 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.641.881 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.641.882 I llama_perf_context_print:        load time =     793.30 ms
0.01.641.882 I llama_perf_context_print: prompt eval time =      42.29 ms /     7 tokens (    6.04 ms per token,   165.54 tokens per second)
0.01.641.883 I llama_perf_context_print:        eval time =     793.35 ms /    63 runs   (   12.59 ms per token,    79.41 tokens per second)
0.01.641.883 I llama_perf_context_print:       total time =     838.89 ms /    70 tokens
0.01.642.091 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.110s
sys	0m0.173s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.220 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.648 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.653 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.658 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.659 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.659 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.660 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.660 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.662 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.662 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.355 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.357 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.357 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.358 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.358 I llama_model_loader: - type  f32:  194 tensors
0.00.023.359 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.359 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.359 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.454 I llm_load_vocab: special tokens cache size = 25
0.00.049.450 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.452 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.453 I llm_load_print_meta: arch             = gptneox
0.00.049.453 I llm_load_print_meta: vocab type       = BPE
0.00.049.453 I llm_load_print_meta: n_vocab          = 50304
0.00.049.454 I llm_load_print_meta: n_merges         = 50009
0.00.049.454 I llm_load_print_meta: vocab_only       = 0
0.00.049.454 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.454 I llm_load_print_meta: n_embd           = 2048
0.00.049.454 I llm_load_print_meta: n_layer          = 24
0.00.049.469 I llm_load_print_meta: n_head           = 16
0.00.049.471 I llm_load_print_meta: n_head_kv        = 16
0.00.049.471 I llm_load_print_meta: n_rot            = 32
0.00.049.471 I llm_load_print_meta: n_swa            = 0
0.00.049.471 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.471 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.472 I llm_load_print_meta: n_gqa            = 1
0.00.049.474 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.474 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.475 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.475 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.475 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.475 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.475 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.476 I llm_load_print_meta: n_ff             = 8192
0.00.049.476 I llm_load_print_meta: n_expert         = 0
0.00.049.476 I llm_load_print_meta: n_expert_used    = 0
0.00.049.477 I llm_load_print_meta: causal attn      = 1
0.00.049.477 I llm_load_print_meta: pooling type     = 0
0.00.049.477 I llm_load_print_meta: rope type        = 2
0.00.049.477 I llm_load_print_meta: rope scaling     = linear
0.00.049.477 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.478 I llm_load_print_meta: freq_scale_train = 1
0.00.049.478 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.478 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.478 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.478 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.478 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.478 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.479 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.488 I llm_load_print_meta: model type       = 1.4B
0.00.049.489 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.489 I llm_load_print_meta: model params     = 1.41 B
0.00.049.490 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.490 I llm_load_print_meta: general.name     = 1.4B
0.00.049.491 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.491 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.492 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.492 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.492 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.492 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.492 I llm_load_print_meta: max token length = 1024
0.00.051.326 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.326 I llm_load_tensors: offloading output layer to GPU
0.00.051.327 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.337 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.338 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.206 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.207 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.207 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.207 I llama_new_context_with_model: n_batch       = 2048
0.00.052.207 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.208 I llama_new_context_with_model: flash_attn    = 0
0.00.052.208 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.208 I llama_new_context_with_model: freq_scale    = 1
0.00.052.209 I ggml_metal_init: allocating
0.00.052.212 I ggml_metal_init: found device: Apple M4
0.00.052.214 I ggml_metal_init: picking default device: Apple M4
0.00.052.796 I ggml_metal_init: using embedded metal library
0.00.055.111 I ggml_metal_init: GPU name:   Apple M4
0.00.055.112 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.113 I ggml_metal_init: simdgroup reduction   = true
0.00.055.113 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.114 I ggml_metal_init: has bfloat            = true
0.00.055.114 I ggml_metal_init: use bfloat            = true
0.00.055.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.239 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.244 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.262 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.342 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.344 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.345 I llama_new_context_with_model: graph nodes  = 967
0.00.085.345 I llama_new_context_with_model: graph splits = 2
0.00.085.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.848 I main: llama threadpool init, n_threads = 4
0.00.508.892 I 
0.00.508.925 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.926 I 
0.00.509.163 I sampler seed: 1234
0.00.509.169 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.509.180 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.509.180 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.509.180 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.188.820 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.01.188.820 I llama_perf_context_print:        load time =     499.62 ms
0.01.188.823 I llama_perf_context_print: prompt eval time =      35.87 ms /     7 tokens (    5.12 ms per token,   195.14 tokens per second)
0.01.188.824 I llama_perf_context_print:        eval time =     640.81 ms /    63 runs   (   10.17 ms per token,    98.31 tokens per second)
0.01.188.825 I llama_perf_context_print:       total time =     679.98 ms /    70 tokens
0.01.189.007 I ggml_metal_free: deallocating

real	0m1.214s
user	0m0.109s
sys	0m0.126s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.535 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.103 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.107 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.109 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.110 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.110 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.111 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.112 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.112 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.113 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.113 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.115 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.115 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.116 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.964 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.859 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.860 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.861 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.861 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.862 I llama_model_loader: - type  f32:  194 tensors
0.00.022.862 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.862 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.863 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.863 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.922 I llm_load_vocab: special tokens cache size = 25
0.00.048.776 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.779 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.779 I llm_load_print_meta: arch             = gptneox
0.00.048.780 I llm_load_print_meta: vocab type       = BPE
0.00.048.780 I llm_load_print_meta: n_vocab          = 50304
0.00.048.780 I llm_load_print_meta: n_merges         = 50009
0.00.048.780 I llm_load_print_meta: vocab_only       = 0
0.00.048.780 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.781 I llm_load_print_meta: n_embd           = 2048
0.00.048.781 I llm_load_print_meta: n_layer          = 24
0.00.048.795 I llm_load_print_meta: n_head           = 16
0.00.048.796 I llm_load_print_meta: n_head_kv        = 16
0.00.048.796 I llm_load_print_meta: n_rot            = 32
0.00.048.796 I llm_load_print_meta: n_swa            = 0
0.00.048.797 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.797 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.798 I llm_load_print_meta: n_gqa            = 1
0.00.048.800 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.800 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.801 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.801 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.801 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.801 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.802 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.802 I llm_load_print_meta: n_ff             = 8192
0.00.048.803 I llm_load_print_meta: n_expert         = 0
0.00.048.803 I llm_load_print_meta: n_expert_used    = 0
0.00.048.803 I llm_load_print_meta: causal attn      = 1
0.00.048.803 I llm_load_print_meta: pooling type     = 0
0.00.048.803 I llm_load_print_meta: rope type        = 2
0.00.048.803 I llm_load_print_meta: rope scaling     = linear
0.00.048.804 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.804 I llm_load_print_meta: freq_scale_train = 1
0.00.048.804 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.804 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.804 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.804 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.804 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.805 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.805 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.814 I llm_load_print_meta: model type       = 1.4B
0.00.048.815 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.817 I llm_load_print_meta: model params     = 1.41 B
0.00.048.817 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.817 I llm_load_print_meta: general.name     = 1.4B
0.00.048.817 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.819 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.819 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.819 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.819 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.819 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.820 I llm_load_print_meta: max token length = 1024
0.00.050.704 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.704 I llm_load_tensors: offloading output layer to GPU
0.00.050.705 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.715 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.716 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.597 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.598 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.598 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.598 I llama_new_context_with_model: n_batch       = 2048
0.00.051.598 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.598 I llama_new_context_with_model: flash_attn    = 0
0.00.051.599 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.599 I llama_new_context_with_model: freq_scale    = 1
0.00.051.599 I ggml_metal_init: allocating
0.00.051.603 I ggml_metal_init: found device: Apple M4
0.00.051.605 I ggml_metal_init: picking default device: Apple M4
0.00.052.181 I ggml_metal_init: using embedded metal library
0.00.054.480 I ggml_metal_init: GPU name:   Apple M4
0.00.054.481 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.482 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.482 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.482 I ggml_metal_init: simdgroup reduction   = true
0.00.054.483 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.483 I ggml_metal_init: has bfloat            = true
0.00.054.483 I ggml_metal_init: use bfloat            = true
0.00.054.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.484 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.516 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.522 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.542 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.558 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.559 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.560 I llama_new_context_with_model: graph nodes  = 967
0.00.084.560 I llama_new_context_with_model: graph splits = 2
0.00.084.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.535.514 I main: llama threadpool init, n_threads = 4
0.00.535.557 I 
0.00.535.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.535.603 I 
0.00.535.837 I sampler seed: 1234
0.00.535.841 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.535.879 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.535.880 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.535.880 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.282.010 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.282.010 I llama_perf_context_print:        load time =     526.97 ms
0.01.282.011 I llama_perf_context_print: prompt eval time =      40.60 ms /     7 tokens (    5.80 ms per token,   172.43 tokens per second)
0.01.282.012 I llama_perf_context_print:        eval time =     702.56 ms /    63 runs   (   11.15 ms per token,    89.67 tokens per second)
0.01.282.012 I llama_perf_context_print:       total time =     746.50 ms /    70 tokens
0.01.282.199 I ggml_metal_free: deallocating

real	0m1.297s
user	0m0.109s
sys	0m0.125s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.012.413 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.912 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.916 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.922 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.923 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.923 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.924 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.924 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.925 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.925 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.926 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.926 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.926 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.927 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.927 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.928 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.929 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.929 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.944 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.956 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.957 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.959 I llama_model_loader: - type  f32:  194 tensors
0.00.027.959 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.959 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.960 I llama_model_loader: - type q6_K:   13 tensors
0.00.048.875 I llm_load_vocab: special tokens cache size = 25
0.00.054.753 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.756 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.757 I llm_load_print_meta: arch             = gptneox
0.00.054.757 I llm_load_print_meta: vocab type       = BPE
0.00.054.757 I llm_load_print_meta: n_vocab          = 50304
0.00.054.758 I llm_load_print_meta: n_merges         = 50009
0.00.054.758 I llm_load_print_meta: vocab_only       = 0
0.00.054.758 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.758 I llm_load_print_meta: n_embd           = 2048
0.00.054.758 I llm_load_print_meta: n_layer          = 24
0.00.054.773 I llm_load_print_meta: n_head           = 16
0.00.054.774 I llm_load_print_meta: n_head_kv        = 16
0.00.054.774 I llm_load_print_meta: n_rot            = 32
0.00.054.775 I llm_load_print_meta: n_swa            = 0
0.00.054.775 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.775 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.776 I llm_load_print_meta: n_gqa            = 1
0.00.054.776 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.777 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.778 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.778 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.778 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.778 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.778 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.779 I llm_load_print_meta: n_ff             = 8192
0.00.054.779 I llm_load_print_meta: n_expert         = 0
0.00.054.779 I llm_load_print_meta: n_expert_used    = 0
0.00.054.780 I llm_load_print_meta: causal attn      = 1
0.00.054.780 I llm_load_print_meta: pooling type     = 0
0.00.054.780 I llm_load_print_meta: rope type        = 2
0.00.054.780 I llm_load_print_meta: rope scaling     = linear
0.00.054.780 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.781 I llm_load_print_meta: freq_scale_train = 1
0.00.054.781 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.781 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.781 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.781 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.781 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.781 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.782 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.792 I llm_load_print_meta: model type       = 1.4B
0.00.054.792 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.054.792 I llm_load_print_meta: model params     = 1.41 B
0.00.054.793 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.054.795 I llm_load_print_meta: general.name     = 1.4B
0.00.054.795 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.795 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.795 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.795 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.796 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.796 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.796 I llm_load_print_meta: max token length = 1024
0.00.056.825 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.826 I llm_load_tensors: offloading output layer to GPU
0.00.056.826 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.836 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.056.838 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.057.803 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.804 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.804 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.804 I llama_new_context_with_model: n_batch       = 2048
0.00.057.804 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.805 I llama_new_context_with_model: flash_attn    = 0
0.00.057.805 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.805 I llama_new_context_with_model: freq_scale    = 1
0.00.057.806 I ggml_metal_init: allocating
0.00.057.812 I ggml_metal_init: found device: Apple M4
0.00.057.814 I ggml_metal_init: picking default device: Apple M4
0.00.058.425 I ggml_metal_init: using embedded metal library
0.00.060.744 I ggml_metal_init: GPU name:   Apple M4
0.00.060.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.746 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.746 I ggml_metal_init: simdgroup reduction   = true
0.00.060.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.747 I ggml_metal_init: has bfloat            = true
0.00.060.747 I ggml_metal_init: use bfloat            = true
0.00.060.747 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.167 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.175 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.191 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.179 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.180 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.180 I llama_new_context_with_model: graph nodes  = 967
0.00.090.181 I llama_new_context_with_model: graph splits = 2
0.00.090.189 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.259 I main: llama threadpool init, n_threads = 4
0.00.608.302 I 
0.00.608.332 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.333 I 
0.00.608.485 I sampler seed: 1234
0.00.608.490 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.608.523 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.608.526 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.608.526 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.371.355 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.371.356 I llama_perf_context_print:        load time =     595.84 ms
0.01.371.356 I llama_perf_context_print: prompt eval time =      46.96 ms /     7 tokens (    6.71 ms per token,   149.05 tokens per second)
0.01.371.357 I llama_perf_context_print:        eval time =     712.90 ms /    63 runs   (   11.32 ms per token,    88.37 tokens per second)
0.01.371.357 I llama_perf_context_print:       total time =     763.10 ms /    70 tokens
0.01.371.553 I ggml_metal_free: deallocating

real	0m1.390s
user	0m0.110s
sys	0m0.133s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.654 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.046 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.058 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.058 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.061 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.067 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.052 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.141 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.084 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.085 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.085 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.086 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.086 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.086 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.087 I llama_model_loader: - type  f32:  194 tensors
0.00.025.087 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.087 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.224 I llm_load_vocab: special tokens cache size = 25
0.00.052.108 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.111 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.111 I llm_load_print_meta: arch             = gptneox
0.00.052.112 I llm_load_print_meta: vocab type       = BPE
0.00.052.112 I llm_load_print_meta: n_vocab          = 50304
0.00.052.112 I llm_load_print_meta: n_merges         = 50009
0.00.052.112 I llm_load_print_meta: vocab_only       = 0
0.00.052.112 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.112 I llm_load_print_meta: n_embd           = 2048
0.00.052.113 I llm_load_print_meta: n_layer          = 24
0.00.052.128 I llm_load_print_meta: n_head           = 16
0.00.052.129 I llm_load_print_meta: n_head_kv        = 16
0.00.052.130 I llm_load_print_meta: n_rot            = 32
0.00.052.130 I llm_load_print_meta: n_swa            = 0
0.00.052.130 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.130 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.131 I llm_load_print_meta: n_gqa            = 1
0.00.052.132 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.132 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.133 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.133 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.133 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.133 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.133 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.134 I llm_load_print_meta: n_ff             = 8192
0.00.052.134 I llm_load_print_meta: n_expert         = 0
0.00.052.134 I llm_load_print_meta: n_expert_used    = 0
0.00.052.136 I llm_load_print_meta: causal attn      = 1
0.00.052.138 I llm_load_print_meta: pooling type     = 0
0.00.052.138 I llm_load_print_meta: rope type        = 2
0.00.052.138 I llm_load_print_meta: rope scaling     = linear
0.00.052.138 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.139 I llm_load_print_meta: freq_scale_train = 1
0.00.052.139 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.139 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.139 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.139 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.139 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.139 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.140 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.149 I llm_load_print_meta: model type       = 1.4B
0.00.052.149 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.150 I llm_load_print_meta: model params     = 1.41 B
0.00.052.150 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.150 I llm_load_print_meta: general.name     = 1.4B
0.00.052.151 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.151 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.151 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.151 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.151 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.152 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.152 I llm_load_print_meta: max token length = 1024
0.00.054.245 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.245 I llm_load_tensors: offloading output layer to GPU
0.00.054.246 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.256 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.258 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.177 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.178 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.178 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.178 I llama_new_context_with_model: n_batch       = 2048
0.00.055.178 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.178 I llama_new_context_with_model: flash_attn    = 0
0.00.055.179 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.179 I llama_new_context_with_model: freq_scale    = 1
0.00.055.179 I ggml_metal_init: allocating
0.00.055.185 I ggml_metal_init: found device: Apple M4
0.00.055.187 I ggml_metal_init: picking default device: Apple M4
0.00.055.767 I ggml_metal_init: using embedded metal library
0.00.058.069 I ggml_metal_init: GPU name:   Apple M4
0.00.058.071 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.073 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.073 I ggml_metal_init: simdgroup reduction   = true
0.00.058.074 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.074 I ggml_metal_init: has bfloat            = true
0.00.058.074 I ggml_metal_init: use bfloat            = true
0.00.058.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.772 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.780 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.799 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.792 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.793 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.794 I llama_new_context_with_model: graph nodes  = 967
0.00.087.794 I llama_new_context_with_model: graph splits = 2
0.00.087.808 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.645 I main: llama threadpool init, n_threads = 4
0.00.701.683 I 
0.00.701.737 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.738 I 
0.00.701.972 I sampler seed: 1234
0.00.701.976 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.701.996 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.701.997 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.701.997 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.554.458 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.554.459 I llama_perf_context_print:        load time =     692.99 ms
0.01.554.460 I llama_perf_context_print: prompt eval time =      51.57 ms /     7 tokens (    7.37 ms per token,   135.74 tokens per second)
0.01.554.460 I llama_perf_context_print:        eval time =     797.93 ms /    63 runs   (   12.67 ms per token,    78.95 tokens per second)
0.01.554.461 I llama_perf_context_print:       total time =     852.82 ms /    70 tokens
0.01.554.652 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.824 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.673 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.679 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.680 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.680 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.680 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.681 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.682 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.682 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.682 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.683 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.683 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.684 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.687 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.687 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.667 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.710 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.606 I llama_model_loader: - type  f32:  194 tensors
0.00.025.606 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.720 I llm_load_vocab: special tokens cache size = 25
0.00.052.813 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.816 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.816 I llm_load_print_meta: arch             = gptneox
0.00.052.816 I llm_load_print_meta: vocab type       = BPE
0.00.052.817 I llm_load_print_meta: n_vocab          = 50304
0.00.052.817 I llm_load_print_meta: n_merges         = 50009
0.00.052.817 I llm_load_print_meta: vocab_only       = 0
0.00.052.817 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.817 I llm_load_print_meta: n_embd           = 2048
0.00.052.818 I llm_load_print_meta: n_layer          = 24
0.00.052.832 I llm_load_print_meta: n_head           = 16
0.00.052.834 I llm_load_print_meta: n_head_kv        = 16
0.00.052.834 I llm_load_print_meta: n_rot            = 32
0.00.052.834 I llm_load_print_meta: n_swa            = 0
0.00.052.834 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.834 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.835 I llm_load_print_meta: n_gqa            = 1
0.00.052.836 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.836 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.837 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.837 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.837 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.838 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.838 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.838 I llm_load_print_meta: n_ff             = 8192
0.00.052.839 I llm_load_print_meta: n_expert         = 0
0.00.052.839 I llm_load_print_meta: n_expert_used    = 0
0.00.052.839 I llm_load_print_meta: causal attn      = 1
0.00.052.840 I llm_load_print_meta: pooling type     = 0
0.00.052.842 I llm_load_print_meta: rope type        = 2
0.00.052.842 I llm_load_print_meta: rope scaling     = linear
0.00.052.842 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.843 I llm_load_print_meta: freq_scale_train = 1
0.00.052.844 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.844 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.844 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.845 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.845 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.845 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.845 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.854 I llm_load_print_meta: model type       = 1.4B
0.00.052.855 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.855 I llm_load_print_meta: model params     = 1.41 B
0.00.052.855 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.855 I llm_load_print_meta: general.name     = 1.4B
0.00.052.856 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.856 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.856 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.856 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.856 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.857 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.857 I llm_load_print_meta: max token length = 1024
0.00.054.918 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.918 I llm_load_tensors: offloading output layer to GPU
0.00.054.918 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.929 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.930 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.908 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.909 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.909 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.909 I llama_new_context_with_model: n_batch       = 2048
0.00.055.909 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.909 I llama_new_context_with_model: flash_attn    = 0
0.00.055.910 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.910 I llama_new_context_with_model: freq_scale    = 1
0.00.055.910 I ggml_metal_init: allocating
0.00.055.914 I ggml_metal_init: found device: Apple M4
0.00.055.916 I ggml_metal_init: picking default device: Apple M4
0.00.056.512 I ggml_metal_init: using embedded metal library
0.00.058.860 I ggml_metal_init: GPU name:   Apple M4
0.00.058.861 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.861 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.862 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.862 I ggml_metal_init: simdgroup reduction   = true
0.00.058.863 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.864 I ggml_metal_init: has bfloat            = true
0.00.058.864 I ggml_metal_init: use bfloat            = true
0.00.058.864 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.865 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.796 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.804 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.833 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.921 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.923 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.923 I llama_new_context_with_model: graph nodes  = 967
0.00.089.923 I llama_new_context_with_model: graph splits = 2
0.00.089.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.400 I main: llama threadpool init, n_threads = 4
0.00.743.445 I 
0.00.743.471 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.472 I 
0.00.743.625 I sampler seed: 1234
0.00.743.630 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.678 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.680 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.680 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.631.370 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54281.35 tokens per second)
0.01.631.371 I llama_perf_context_print:        load time =     733.57 ms
0.01.631.372 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.78 tokens per second)
0.01.631.376 I llama_perf_context_print:        eval time =     830.17 ms /    63 runs   (   13.18 ms per token,    75.89 tokens per second)
0.01.631.376 I llama_perf_context_print:       total time =     887.97 ms /    70 tokens
0.01.631.541 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.523 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.286 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.730 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.747 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.764 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.764 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.768 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.772 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.773 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.778 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.779 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.779 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.171 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.594 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.597 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.598 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.599 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.600 I llama_model_loader: - type  f32:  194 tensors
0.00.054.600 I llama_model_loader: - type  f16:   98 tensors
0.00.087.142 I llm_load_vocab: special tokens cache size = 25
0.00.094.088 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.091 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.091 I llm_load_print_meta: arch             = gptneox
0.00.094.091 I llm_load_print_meta: vocab type       = BPE
0.00.094.091 I llm_load_print_meta: n_vocab          = 50304
0.00.094.092 I llm_load_print_meta: n_merges         = 50009
0.00.094.092 I llm_load_print_meta: vocab_only       = 0
0.00.094.092 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.092 I llm_load_print_meta: n_embd           = 2048
0.00.094.092 I llm_load_print_meta: n_layer          = 24
0.00.094.107 I llm_load_print_meta: n_head           = 16
0.00.094.108 I llm_load_print_meta: n_head_kv        = 16
0.00.094.108 I llm_load_print_meta: n_rot            = 32
0.00.094.108 I llm_load_print_meta: n_swa            = 0
0.00.094.109 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.109 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.109 I llm_load_print_meta: n_gqa            = 1
0.00.094.110 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.110 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.111 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.111 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.111 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.112 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.112 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.115 I llm_load_print_meta: n_ff             = 8192
0.00.094.115 I llm_load_print_meta: n_expert         = 0
0.00.094.115 I llm_load_print_meta: n_expert_used    = 0
0.00.094.115 I llm_load_print_meta: causal attn      = 1
0.00.094.116 I llm_load_print_meta: pooling type     = 0
0.00.094.116 I llm_load_print_meta: rope type        = 2
0.00.094.116 I llm_load_print_meta: rope scaling     = linear
0.00.094.116 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.116 I llm_load_print_meta: freq_scale_train = 1
0.00.094.117 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.117 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.117 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.118 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.118 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.119 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.119 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.128 I llm_load_print_meta: model type       = 1.4B
0.00.094.129 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.094.129 I llm_load_print_meta: model params     = 1.41 B
0.00.094.130 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.094.130 I llm_load_print_meta: general.name     = 1.4B
0.00.094.130 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.130 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.130 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.131 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.131 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.094.131 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.131 I llm_load_print_meta: max token length = 1024
0.00.096.701 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.701 I llm_load_tensors: offloading output layer to GPU
0.00.096.701 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.713 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.714 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.720 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.721 I llama_new_context_with_model: n_ctx         = 128
0.00.097.721 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.097.721 I llama_new_context_with_model: n_batch       = 128
0.00.097.721 I llama_new_context_with_model: n_ubatch      = 128
0.00.097.721 I llama_new_context_with_model: flash_attn    = 0
0.00.097.722 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.722 I llama_new_context_with_model: freq_scale    = 1
0.00.097.723 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.097.723 I ggml_metal_init: allocating
0.00.097.730 I ggml_metal_init: found device: Apple M4
0.00.097.732 I ggml_metal_init: picking default device: Apple M4
0.00.098.366 I ggml_metal_init: using embedded metal library
0.00.101.032 I ggml_metal_init: GPU name:   Apple M4
0.00.101.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.034 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.035 I ggml_metal_init: simdgroup reduction   = true
0.00.101.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.035 I ggml_metal_init: has bfloat            = true
0.00.101.035 I ggml_metal_init: use bfloat            = true
0.00.101.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.630 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.111.632 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.111.645 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.492 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.112.493 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.112.494 I llama_new_context_with_model: graph nodes  = 967
0.00.112.494 I llama_new_context_with_model: graph splits = 2
0.00.112.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.965.190 I 
0.00.965.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.965.271 I perplexity: tokenizing the input ..
0.00.977.384 I perplexity: tokenization took 12.109 ms
0.00.977.411 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.098.625 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.100.300 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.100.324 I llama_perf_context_print:        load time =     941.89 ms
0.01.100.326 I llama_perf_context_print: prompt eval time =     120.80 ms /   128 tokens (    0.94 ms per token,  1059.57 tokens per second)
0.01.100.327 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.100.328 I llama_perf_context_print:       total time =     135.14 ms /   129 tokens
0.01.101.085 I ggml_metal_free: deallocating

real	0m1.289s
user	0m0.126s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.133 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.949 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.332 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.340 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.341 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.342 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.343 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.343 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.344 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.344 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.345 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.345 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.345 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.349 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.349 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.350 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.859 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.773 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.773 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.774 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.774 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.774 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.775 I llama_model_loader: - type  f32:  194 tensors
0.00.032.775 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.565 I llm_load_vocab: special tokens cache size = 25
0.00.064.748 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.751 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.751 I llm_load_print_meta: arch             = gptneox
0.00.064.752 I llm_load_print_meta: vocab type       = BPE
0.00.064.752 I llm_load_print_meta: n_vocab          = 50304
0.00.064.752 I llm_load_print_meta: n_merges         = 50009
0.00.064.752 I llm_load_print_meta: vocab_only       = 0
0.00.064.752 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.753 I llm_load_print_meta: n_embd           = 2048
0.00.064.753 I llm_load_print_meta: n_layer          = 24
0.00.064.769 I llm_load_print_meta: n_head           = 16
0.00.064.770 I llm_load_print_meta: n_head_kv        = 16
0.00.064.770 I llm_load_print_meta: n_rot            = 32
0.00.064.770 I llm_load_print_meta: n_swa            = 0
0.00.064.770 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.771 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.771 I llm_load_print_meta: n_gqa            = 1
0.00.064.772 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.772 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.773 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.773 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.773 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.773 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.774 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.774 I llm_load_print_meta: n_ff             = 8192
0.00.064.774 I llm_load_print_meta: n_expert         = 0
0.00.064.775 I llm_load_print_meta: n_expert_used    = 0
0.00.064.775 I llm_load_print_meta: causal attn      = 1
0.00.064.775 I llm_load_print_meta: pooling type     = 0
0.00.064.775 I llm_load_print_meta: rope type        = 2
0.00.064.775 I llm_load_print_meta: rope scaling     = linear
0.00.064.775 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.778 I llm_load_print_meta: freq_scale_train = 1
0.00.064.778 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.778 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.778 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.778 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.778 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.780 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.780 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.789 I llm_load_print_meta: model type       = 1.4B
0.00.064.790 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.790 I llm_load_print_meta: model params     = 1.41 B
0.00.064.790 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.791 I llm_load_print_meta: general.name     = 1.4B
0.00.064.791 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.791 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.791 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.791 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.792 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.792 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.792 I llm_load_print_meta: max token length = 1024
0.00.067.100 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.100 I llm_load_tensors: offloading output layer to GPU
0.00.067.100 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.112 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.113 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.148 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.149 I llama_new_context_with_model: n_ctx         = 128
0.00.068.150 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.068.150 I llama_new_context_with_model: n_batch       = 128
0.00.068.150 I llama_new_context_with_model: n_ubatch      = 128
0.00.068.150 I llama_new_context_with_model: flash_attn    = 0
0.00.068.151 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.151 I llama_new_context_with_model: freq_scale    = 1
0.00.068.151 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.152 I ggml_metal_init: allocating
0.00.068.158 I ggml_metal_init: found device: Apple M4
0.00.068.161 I ggml_metal_init: picking default device: Apple M4
0.00.068.835 I ggml_metal_init: using embedded metal library
0.00.071.502 I ggml_metal_init: GPU name:   Apple M4
0.00.071.503 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.504 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.505 I ggml_metal_init: simdgroup reduction   = true
0.00.071.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.505 I ggml_metal_init: has bfloat            = true
0.00.071.505 I ggml_metal_init: use bfloat            = true
0.00.071.505 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.284 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.287 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.304 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.249 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.250 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.251 I llama_new_context_with_model: graph nodes  = 967
0.00.083.251 I llama_new_context_with_model: graph splits = 2
0.00.083.264 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.849.053 I 
0.00.849.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.849.114 I perplexity: tokenizing the input ..
0.00.857.216 I perplexity: tokenization took 8.1 ms
0.00.857.226 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.981.742 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.982.898 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.982.909 I llama_perf_context_print:        load time =     837.10 ms
0.00.982.911 I llama_perf_context_print: prompt eval time =     124.26 ms /   128 tokens (    0.97 ms per token,  1030.14 tokens per second)
0.00.982.912 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.982.920 I llama_perf_context_print:       total time =     133.86 ms /   129 tokens
0.00.983.406 I ggml_metal_free: deallocating

real	0m1.001s
user	0m0.093s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.416 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.187 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.191 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.197 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.198 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.198 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.198 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.199 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.200 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.200 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.201 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.203 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.204 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.950 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.699 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.699 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.700 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.700 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.700 I llama_model_loader: - type  f32:  194 tensors
0.00.023.701 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.701 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.682 I llm_load_vocab: special tokens cache size = 25
0.00.049.679 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.681 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.682 I llm_load_print_meta: arch             = gptneox
0.00.049.682 I llm_load_print_meta: vocab type       = BPE
0.00.049.682 I llm_load_print_meta: n_vocab          = 50304
0.00.049.683 I llm_load_print_meta: n_merges         = 50009
0.00.049.683 I llm_load_print_meta: vocab_only       = 0
0.00.049.683 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.683 I llm_load_print_meta: n_embd           = 2048
0.00.049.683 I llm_load_print_meta: n_layer          = 24
0.00.049.698 I llm_load_print_meta: n_head           = 16
0.00.049.699 I llm_load_print_meta: n_head_kv        = 16
0.00.049.699 I llm_load_print_meta: n_rot            = 32
0.00.049.699 I llm_load_print_meta: n_swa            = 0
0.00.049.699 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.700 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.700 I llm_load_print_meta: n_gqa            = 1
0.00.049.701 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.702 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.702 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.703 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.703 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.703 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.703 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.704 I llm_load_print_meta: n_ff             = 8192
0.00.049.704 I llm_load_print_meta: n_expert         = 0
0.00.049.704 I llm_load_print_meta: n_expert_used    = 0
0.00.049.704 I llm_load_print_meta: causal attn      = 1
0.00.049.704 I llm_load_print_meta: pooling type     = 0
0.00.049.704 I llm_load_print_meta: rope type        = 2
0.00.049.705 I llm_load_print_meta: rope scaling     = linear
0.00.049.705 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.705 I llm_load_print_meta: freq_scale_train = 1
0.00.049.705 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.705 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.706 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.706 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.706 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.706 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.706 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.716 I llm_load_print_meta: model type       = 1.4B
0.00.049.716 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.716 I llm_load_print_meta: model params     = 1.41 B
0.00.049.717 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.717 I llm_load_print_meta: general.name     = 1.4B
0.00.049.717 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.717 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.717 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.718 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.718 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.718 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.718 I llm_load_print_meta: max token length = 1024
0.00.051.596 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.596 I llm_load_tensors: offloading output layer to GPU
0.00.051.597 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.607 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.608 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.518 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.518 I llama_new_context_with_model: n_ctx         = 128
0.00.052.519 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.519 I llama_new_context_with_model: n_batch       = 128
0.00.052.519 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.519 I llama_new_context_with_model: flash_attn    = 0
0.00.052.520 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.520 I llama_new_context_with_model: freq_scale    = 1
0.00.052.520 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.520 I ggml_metal_init: allocating
0.00.052.524 I ggml_metal_init: found device: Apple M4
0.00.052.526 I ggml_metal_init: picking default device: Apple M4
0.00.053.086 I ggml_metal_init: using embedded metal library
0.00.055.389 I ggml_metal_init: GPU name:   Apple M4
0.00.055.391 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.392 I ggml_metal_init: simdgroup reduction   = true
0.00.055.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.392 I ggml_metal_init: has bfloat            = true
0.00.055.392 I ggml_metal_init: use bfloat            = true
0.00.055.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.076 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.078 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.091 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.976 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.977 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.978 I llama_new_context_with_model: graph nodes  = 967
0.00.066.978 I llama_new_context_with_model: graph splits = 2
0.00.066.990 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.425 I 
0.00.601.455 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.462 I perplexity: tokenizing the input ..
0.00.609.508 I perplexity: tokenization took 8.044 ms
0.00.609.522 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.969 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.733.124 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.733.151 I llama_perf_context_print:        load time =     592.00 ms
0.00.733.153 I llama_perf_context_print: prompt eval time =     122.22 ms /   128 tokens (    0.95 ms per token,  1047.32 tokens per second)
0.00.733.153 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.154 I llama_perf_context_print:       total time =     131.72 ms /   129 tokens
0.00.733.686 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.077s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.879 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.916 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.922 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.923 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.923 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.923 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.929 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.929 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.930 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.931 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.744 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.745 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.745 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.746 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.746 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.746 I llama_model_loader: - type  f32:  194 tensors
0.00.023.747 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.747 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.541 I llm_load_vocab: special tokens cache size = 25
0.00.050.597 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.600 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.600 I llm_load_print_meta: arch             = gptneox
0.00.050.601 I llm_load_print_meta: vocab type       = BPE
0.00.050.601 I llm_load_print_meta: n_vocab          = 50304
0.00.050.601 I llm_load_print_meta: n_merges         = 50009
0.00.050.601 I llm_load_print_meta: vocab_only       = 0
0.00.050.601 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.602 I llm_load_print_meta: n_embd           = 2048
0.00.050.602 I llm_load_print_meta: n_layer          = 24
0.00.050.616 I llm_load_print_meta: n_head           = 16
0.00.050.617 I llm_load_print_meta: n_head_kv        = 16
0.00.050.617 I llm_load_print_meta: n_rot            = 32
0.00.050.617 I llm_load_print_meta: n_swa            = 0
0.00.050.617 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.617 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.618 I llm_load_print_meta: n_gqa            = 1
0.00.050.619 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.620 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.620 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.621 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.621 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.623 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.623 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.624 I llm_load_print_meta: n_ff             = 8192
0.00.050.624 I llm_load_print_meta: n_expert         = 0
0.00.050.624 I llm_load_print_meta: n_expert_used    = 0
0.00.050.624 I llm_load_print_meta: causal attn      = 1
0.00.050.625 I llm_load_print_meta: pooling type     = 0
0.00.050.625 I llm_load_print_meta: rope type        = 2
0.00.050.625 I llm_load_print_meta: rope scaling     = linear
0.00.050.626 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.626 I llm_load_print_meta: freq_scale_train = 1
0.00.050.626 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.627 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.627 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.627 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.627 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.627 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.628 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.637 I llm_load_print_meta: model type       = 1.4B
0.00.050.637 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.638 I llm_load_print_meta: model params     = 1.41 B
0.00.050.638 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.638 I llm_load_print_meta: general.name     = 1.4B
0.00.050.639 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.640 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: max token length = 1024
0.00.052.630 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.631 I llm_load_tensors: offloading output layer to GPU
0.00.052.631 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.641 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.642 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.538 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.538 I llama_new_context_with_model: n_ctx         = 128
0.00.053.538 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.539 I llama_new_context_with_model: n_batch       = 128
0.00.053.539 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.539 I llama_new_context_with_model: flash_attn    = 0
0.00.053.539 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.540 I llama_new_context_with_model: freq_scale    = 1
0.00.053.540 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.540 I ggml_metal_init: allocating
0.00.053.546 I ggml_metal_init: found device: Apple M4
0.00.053.548 I ggml_metal_init: picking default device: Apple M4
0.00.054.090 I ggml_metal_init: using embedded metal library
0.00.056.431 I ggml_metal_init: GPU name:   Apple M4
0.00.056.432 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.432 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.433 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.433 I ggml_metal_init: simdgroup reduction   = true
0.00.056.433 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.433 I ggml_metal_init: has bfloat            = true
0.00.056.433 I ggml_metal_init: use bfloat            = true
0.00.056.434 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.435 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.224 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.230 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.245 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.155 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.156 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.156 I llama_new_context_with_model: graph nodes  = 967
0.00.068.157 I llama_new_context_with_model: graph splits = 2
0.00.068.169 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.508 I 
0.00.644.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.644.555 I perplexity: tokenizing the input ..
0.00.652.704 I perplexity: tokenization took 8.147 ms
0.00.652.714 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.837 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.777.113 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.777.136 I llama_perf_context_print:        load time =     635.62 ms
0.00.777.137 I llama_perf_context_print: prompt eval time =     122.88 ms /   128 tokens (    0.96 ms per token,  1041.64 tokens per second)
0.00.777.138 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.139 I llama_perf_context_print:       total time =     132.63 ms /   129 tokens
0.00.777.634 I ggml_metal_free: deallocating

real	0m0.791s
user	0m0.078s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.706 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.482 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.483 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.483 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.484 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.485 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.486 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.486 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.487 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.488 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.489 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.192 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.193 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.194 I llama_model_loader: - type  f32:  194 tensors
0.00.024.194 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.195 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.331 I llm_load_vocab: special tokens cache size = 25
0.00.050.265 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.268 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.268 I llm_load_print_meta: arch             = gptneox
0.00.050.268 I llm_load_print_meta: vocab type       = BPE
0.00.050.268 I llm_load_print_meta: n_vocab          = 50304
0.00.050.269 I llm_load_print_meta: n_merges         = 50009
0.00.050.269 I llm_load_print_meta: vocab_only       = 0
0.00.050.269 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.269 I llm_load_print_meta: n_embd           = 2048
0.00.050.269 I llm_load_print_meta: n_layer          = 24
0.00.050.284 I llm_load_print_meta: n_head           = 16
0.00.050.285 I llm_load_print_meta: n_head_kv        = 16
0.00.050.285 I llm_load_print_meta: n_rot            = 32
0.00.050.286 I llm_load_print_meta: n_swa            = 0
0.00.050.286 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.286 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.287 I llm_load_print_meta: n_gqa            = 1
0.00.050.287 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.288 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.289 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.289 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.289 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.290 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.290 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.291 I llm_load_print_meta: n_ff             = 8192
0.00.050.291 I llm_load_print_meta: n_expert         = 0
0.00.050.291 I llm_load_print_meta: n_expert_used    = 0
0.00.050.292 I llm_load_print_meta: causal attn      = 1
0.00.050.292 I llm_load_print_meta: pooling type     = 0
0.00.050.292 I llm_load_print_meta: rope type        = 2
0.00.050.292 I llm_load_print_meta: rope scaling     = linear
0.00.050.292 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.293 I llm_load_print_meta: freq_scale_train = 1
0.00.050.293 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.293 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.294 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.294 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.295 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.295 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.295 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.304 I llm_load_print_meta: model type       = 1.4B
0.00.050.304 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.305 I llm_load_print_meta: model params     = 1.41 B
0.00.050.305 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.305 I llm_load_print_meta: general.name     = 1.4B
0.00.050.306 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.306 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.306 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.306 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.306 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.307 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.307 I llm_load_print_meta: max token length = 1024
0.00.052.188 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.189 I llm_load_tensors: offloading output layer to GPU
0.00.052.189 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.199 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.200 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.072 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.073 I llama_new_context_with_model: n_ctx         = 128
0.00.053.073 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.073 I llama_new_context_with_model: n_batch       = 128
0.00.053.073 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.073 I llama_new_context_with_model: flash_attn    = 0
0.00.053.074 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.074 I llama_new_context_with_model: freq_scale    = 1
0.00.053.074 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.075 I ggml_metal_init: allocating
0.00.053.078 I ggml_metal_init: found device: Apple M4
0.00.053.080 I ggml_metal_init: picking default device: Apple M4
0.00.053.636 I ggml_metal_init: using embedded metal library
0.00.055.950 I ggml_metal_init: GPU name:   Apple M4
0.00.055.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.953 I ggml_metal_init: simdgroup reduction   = true
0.00.055.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.953 I ggml_metal_init: has bfloat            = true
0.00.055.953 I ggml_metal_init: use bfloat            = true
0.00.055.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.561 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.564 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.578 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.518 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.519 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.519 I llama_new_context_with_model: graph nodes  = 967
0.00.067.519 I llama_new_context_with_model: graph splits = 2
0.00.067.531 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.245 I 
0.00.706.293 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.303 I perplexity: tokenizing the input ..
0.00.714.276 I perplexity: tokenization took 7.974 ms
0.00.714.286 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.569 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.850.836 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.850.856 I llama_perf_context_print:        load time =     696.53 ms
0.00.850.860 I llama_perf_context_print: prompt eval time =     135.04 ms /   128 tokens (    1.05 ms per token,   947.87 tokens per second)
0.00.850.862 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.863 I llama_perf_context_print:       total time =     144.61 ms /   129 tokens
0.00.851.390 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.077s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.831 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.840 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.847 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.847 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.848 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.849 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.849 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.850 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.850 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.851 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.855 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.855 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.855 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.731 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.768 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.706 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.707 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.707 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.707 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.708 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.709 I llama_model_loader: - type  f32:  194 tensors
0.00.023.709 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.709 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.577 I llm_load_vocab: special tokens cache size = 25
0.00.050.554 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.557 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.557 I llm_load_print_meta: arch             = gptneox
0.00.050.558 I llm_load_print_meta: vocab type       = BPE
0.00.050.558 I llm_load_print_meta: n_vocab          = 50304
0.00.050.558 I llm_load_print_meta: n_merges         = 50009
0.00.050.558 I llm_load_print_meta: vocab_only       = 0
0.00.050.558 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.559 I llm_load_print_meta: n_embd           = 2048
0.00.050.559 I llm_load_print_meta: n_layer          = 24
0.00.050.568 I llm_load_print_meta: n_head           = 16
0.00.050.569 I llm_load_print_meta: n_head_kv        = 16
0.00.050.569 I llm_load_print_meta: n_rot            = 32
0.00.050.569 I llm_load_print_meta: n_swa            = 0
0.00.050.570 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.570 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.573 I llm_load_print_meta: n_gqa            = 1
0.00.050.574 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.574 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.575 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.579 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.579 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.579 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.580 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.581 I llm_load_print_meta: n_ff             = 8192
0.00.050.581 I llm_load_print_meta: n_expert         = 0
0.00.050.581 I llm_load_print_meta: n_expert_used    = 0
0.00.050.581 I llm_load_print_meta: causal attn      = 1
0.00.050.581 I llm_load_print_meta: pooling type     = 0
0.00.050.581 I llm_load_print_meta: rope type        = 2
0.00.050.582 I llm_load_print_meta: rope scaling     = linear
0.00.050.583 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.584 I llm_load_print_meta: freq_scale_train = 1
0.00.050.585 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.585 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.585 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.585 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.585 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.585 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.586 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.591 I llm_load_print_meta: model type       = 1.4B
0.00.050.591 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.592 I llm_load_print_meta: model params     = 1.41 B
0.00.050.592 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.592 I llm_load_print_meta: general.name     = 1.4B
0.00.050.593 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.593 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.593 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.593 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.594 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.595 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.595 I llm_load_print_meta: max token length = 1024
0.00.052.561 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.561 I llm_load_tensors: offloading output layer to GPU
0.00.052.562 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.572 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.573 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.480 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.480 I llama_new_context_with_model: n_ctx         = 128
0.00.053.481 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.481 I llama_new_context_with_model: n_batch       = 128
0.00.053.481 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.481 I llama_new_context_with_model: flash_attn    = 0
0.00.053.482 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.482 I llama_new_context_with_model: freq_scale    = 1
0.00.053.482 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.483 I ggml_metal_init: allocating
0.00.053.488 I ggml_metal_init: found device: Apple M4
0.00.053.490 I ggml_metal_init: picking default device: Apple M4
0.00.054.049 I ggml_metal_init: using embedded metal library
0.00.056.339 I ggml_metal_init: GPU name:   Apple M4
0.00.056.340 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.341 I ggml_metal_init: simdgroup reduction   = true
0.00.056.341 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.342 I ggml_metal_init: has bfloat            = true
0.00.056.342 I ggml_metal_init: use bfloat            = true
0.00.056.342 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.343 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.926 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.934 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.950 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.789 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.790 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.791 I llama_new_context_with_model: graph nodes  = 967
0.00.067.791 I llama_new_context_with_model: graph splits = 2
0.00.067.803 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.200 I 
0.00.729.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.240 I perplexity: tokenizing the input ..
0.00.737.510 I perplexity: tokenization took 8.268 ms
0.00.737.524 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.872.350 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.873.592 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.873.607 I llama_perf_context_print:        load time =     720.37 ms
0.00.873.608 I llama_perf_context_print: prompt eval time =     134.58 ms /   128 tokens (    1.05 ms per token,   951.08 tokens per second)
0.00.873.609 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.873.610 I llama_perf_context_print:       total time =     144.41 ms /   129 tokens
0.00.874.083 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.078s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.965 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.473 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.480 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.483 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.483 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.485 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.485 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.134 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.136 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.136 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.137 I llama_model_loader: - type  f32:  194 tensors
0.00.024.137 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.137 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.137 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.250 I llm_load_vocab: special tokens cache size = 25
0.00.050.171 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.175 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.176 I llm_load_print_meta: arch             = gptneox
0.00.050.176 I llm_load_print_meta: vocab type       = BPE
0.00.050.176 I llm_load_print_meta: n_vocab          = 50304
0.00.050.177 I llm_load_print_meta: n_merges         = 50009
0.00.050.178 I llm_load_print_meta: vocab_only       = 0
0.00.050.179 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.179 I llm_load_print_meta: n_embd           = 2048
0.00.050.179 I llm_load_print_meta: n_layer          = 24
0.00.050.193 I llm_load_print_meta: n_head           = 16
0.00.050.194 I llm_load_print_meta: n_head_kv        = 16
0.00.050.194 I llm_load_print_meta: n_rot            = 32
0.00.050.194 I llm_load_print_meta: n_swa            = 0
0.00.050.194 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.195 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.195 I llm_load_print_meta: n_gqa            = 1
0.00.050.196 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.197 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.197 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.197 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.198 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.198 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.198 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.199 I llm_load_print_meta: n_ff             = 8192
0.00.050.203 I llm_load_print_meta: n_expert         = 0
0.00.050.203 I llm_load_print_meta: n_expert_used    = 0
0.00.050.203 I llm_load_print_meta: causal attn      = 1
0.00.050.203 I llm_load_print_meta: pooling type     = 0
0.00.050.204 I llm_load_print_meta: rope type        = 2
0.00.050.204 I llm_load_print_meta: rope scaling     = linear
0.00.050.204 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.205 I llm_load_print_meta: freq_scale_train = 1
0.00.050.205 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.205 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.205 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.205 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.205 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.205 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.206 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.216 I llm_load_print_meta: model type       = 1.4B
0.00.050.216 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.217 I llm_load_print_meta: model params     = 1.41 B
0.00.050.217 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.217 I llm_load_print_meta: general.name     = 1.4B
0.00.050.218 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.218 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.218 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.218 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.218 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.219 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.219 I llm_load_print_meta: max token length = 1024
0.00.052.126 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.126 I llm_load_tensors: offloading output layer to GPU
0.00.052.127 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.137 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.138 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.098 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.099 I llama_new_context_with_model: n_ctx         = 128
0.00.053.099 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.099 I llama_new_context_with_model: n_batch       = 128
0.00.053.099 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.100 I llama_new_context_with_model: flash_attn    = 0
0.00.053.100 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.100 I llama_new_context_with_model: freq_scale    = 1
0.00.053.100 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.101 I ggml_metal_init: allocating
0.00.053.104 I ggml_metal_init: found device: Apple M4
0.00.053.106 I ggml_metal_init: picking default device: Apple M4
0.00.053.660 I ggml_metal_init: using embedded metal library
0.00.055.936 I ggml_metal_init: GPU name:   Apple M4
0.00.055.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.937 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.938 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.938 I ggml_metal_init: simdgroup reduction   = true
0.00.055.938 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.938 I ggml_metal_init: has bfloat            = true
0.00.055.938 I ggml_metal_init: use bfloat            = true
0.00.055.939 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.645 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.647 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.660 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.600 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.601 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.601 I llama_new_context_with_model: graph nodes  = 967
0.00.067.602 I llama_new_context_with_model: graph splits = 2
0.00.067.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.228 I 
0.00.439.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.263 I perplexity: tokenizing the input ..
0.00.447.113 I perplexity: tokenization took 7.848 ms
0.00.447.123 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.579.682 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.580.928 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.580.950 I llama_perf_context_print:        load time =     429.26 ms
0.00.580.951 I llama_perf_context_print: prompt eval time =     132.33 ms /   128 tokens (    1.03 ms per token,   967.29 tokens per second)
0.00.580.952 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.580.952 I llama_perf_context_print:       total time =     141.72 ms /   129 tokens
0.00.581.494 I ggml_metal_free: deallocating

real	0m0.597s
user	0m0.077s
sys	0m0.079s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.742 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.308 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.313 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.315 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.316 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.317 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.317 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.319 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.319 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.320 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.320 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.322 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.322 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.196 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.275 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.147 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.147 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.148 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.149 I llama_model_loader: - type  f32:  194 tensors
0.00.023.149 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.149 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.150 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.150 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.991 I llm_load_vocab: special tokens cache size = 25
0.00.048.976 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.979 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.979 I llm_load_print_meta: arch             = gptneox
0.00.048.980 I llm_load_print_meta: vocab type       = BPE
0.00.048.980 I llm_load_print_meta: n_vocab          = 50304
0.00.048.980 I llm_load_print_meta: n_merges         = 50009
0.00.048.980 I llm_load_print_meta: vocab_only       = 0
0.00.048.980 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.981 I llm_load_print_meta: n_embd           = 2048
0.00.048.981 I llm_load_print_meta: n_layer          = 24
0.00.048.990 I llm_load_print_meta: n_head           = 16
0.00.048.991 I llm_load_print_meta: n_head_kv        = 16
0.00.048.991 I llm_load_print_meta: n_rot            = 32
0.00.048.993 I llm_load_print_meta: n_swa            = 0
0.00.048.993 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.993 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.994 I llm_load_print_meta: n_gqa            = 1
0.00.048.995 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.995 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.996 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.996 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.997 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.997 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.997 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.998 I llm_load_print_meta: n_ff             = 8192
0.00.048.998 I llm_load_print_meta: n_expert         = 0
0.00.048.998 I llm_load_print_meta: n_expert_used    = 0
0.00.048.998 I llm_load_print_meta: causal attn      = 1
0.00.048.998 I llm_load_print_meta: pooling type     = 0
0.00.048.998 I llm_load_print_meta: rope type        = 2
0.00.048.999 I llm_load_print_meta: rope scaling     = linear
0.00.049.000 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.001 I llm_load_print_meta: freq_scale_train = 1
0.00.049.001 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.001 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.001 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.001 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.002 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.002 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.002 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.007 I llm_load_print_meta: model type       = 1.4B
0.00.049.007 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.008 I llm_load_print_meta: model params     = 1.41 B
0.00.049.008 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.008 I llm_load_print_meta: general.name     = 1.4B
0.00.049.009 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.009 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.009 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.009 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.009 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.010 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.010 I llm_load_print_meta: max token length = 1024
0.00.051.436 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.436 I llm_load_tensors: offloading output layer to GPU
0.00.051.436 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.442 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.442 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.458 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.459 I llama_new_context_with_model: n_ctx         = 128
0.00.052.459 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.459 I llama_new_context_with_model: n_batch       = 128
0.00.052.459 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.459 I llama_new_context_with_model: flash_attn    = 0
0.00.052.460 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.460 I llama_new_context_with_model: freq_scale    = 1
0.00.052.460 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.461 I ggml_metal_init: allocating
0.00.052.466 I ggml_metal_init: found device: Apple M4
0.00.052.468 I ggml_metal_init: picking default device: Apple M4
0.00.053.036 I ggml_metal_init: using embedded metal library
0.00.055.305 I ggml_metal_init: GPU name:   Apple M4
0.00.055.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.309 I ggml_metal_init: simdgroup reduction   = true
0.00.055.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.309 I ggml_metal_init: has bfloat            = true
0.00.055.310 I ggml_metal_init: use bfloat            = true
0.00.055.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.922 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.925 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.939 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.830 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.832 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.832 I llama_new_context_with_model: graph nodes  = 967
0.00.066.832 I llama_new_context_with_model: graph splits = 2
0.00.066.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.451 I 
0.00.491.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.491.503 I perplexity: tokenizing the input ..
0.00.499.675 I perplexity: tokenization took 8.171 ms
0.00.499.686 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.631.519 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.632.701 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.632.720 I llama_perf_context_print:        load time =     482.71 ms
0.00.632.721 I llama_perf_context_print: prompt eval time =     131.61 ms /   128 tokens (    1.03 ms per token,   972.60 tokens per second)
0.00.632.722 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.632.722 I llama_perf_context_print:       total time =     141.27 ms /   129 tokens
0.00.633.128 I ggml_metal_free: deallocating

real	0m0.646s
user	0m0.077s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.301 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.115 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.122 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.123 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.123 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.124 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.126 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.127 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.127 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.127 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.127 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.128 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.128 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.919 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.976 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.875 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.876 I llama_model_loader: - type  f32:  194 tensors
0.00.023.876 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.876 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.877 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.596 I llm_load_vocab: special tokens cache size = 25
0.00.050.644 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.647 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.647 I llm_load_print_meta: arch             = gptneox
0.00.050.647 I llm_load_print_meta: vocab type       = BPE
0.00.050.648 I llm_load_print_meta: n_vocab          = 50304
0.00.050.648 I llm_load_print_meta: n_merges         = 50009
0.00.050.648 I llm_load_print_meta: vocab_only       = 0
0.00.050.648 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.648 I llm_load_print_meta: n_embd           = 2048
0.00.050.649 I llm_load_print_meta: n_layer          = 24
0.00.050.663 I llm_load_print_meta: n_head           = 16
0.00.050.664 I llm_load_print_meta: n_head_kv        = 16
0.00.050.664 I llm_load_print_meta: n_rot            = 32
0.00.050.664 I llm_load_print_meta: n_swa            = 0
0.00.050.664 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.664 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.665 I llm_load_print_meta: n_gqa            = 1
0.00.050.666 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.666 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.667 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.667 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.668 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.668 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.668 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.669 I llm_load_print_meta: n_ff             = 8192
0.00.050.669 I llm_load_print_meta: n_expert         = 0
0.00.050.669 I llm_load_print_meta: n_expert_used    = 0
0.00.050.669 I llm_load_print_meta: causal attn      = 1
0.00.050.669 I llm_load_print_meta: pooling type     = 0
0.00.050.670 I llm_load_print_meta: rope type        = 2
0.00.050.670 I llm_load_print_meta: rope scaling     = linear
0.00.050.670 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.670 I llm_load_print_meta: freq_scale_train = 1
0.00.050.671 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.671 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.671 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.671 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.671 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.672 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.672 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.681 I llm_load_print_meta: model type       = 1.4B
0.00.050.681 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.682 I llm_load_print_meta: model params     = 1.41 B
0.00.050.682 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.682 I llm_load_print_meta: general.name     = 1.4B
0.00.050.683 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.683 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.683 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.683 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.683 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.684 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.684 I llm_load_print_meta: max token length = 1024
0.00.052.639 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.639 I llm_load_tensors: offloading output layer to GPU
0.00.052.639 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.650 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.651 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.553 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.554 I llama_new_context_with_model: n_ctx         = 128
0.00.053.554 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.554 I llama_new_context_with_model: n_batch       = 128
0.00.053.554 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.555 I llama_new_context_with_model: flash_attn    = 0
0.00.053.555 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.555 I llama_new_context_with_model: freq_scale    = 1
0.00.053.556 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.556 I ggml_metal_init: allocating
0.00.053.559 I ggml_metal_init: found device: Apple M4
0.00.053.561 I ggml_metal_init: picking default device: Apple M4
0.00.054.118 I ggml_metal_init: using embedded metal library
0.00.056.415 I ggml_metal_init: GPU name:   Apple M4
0.00.056.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.417 I ggml_metal_init: simdgroup reduction   = true
0.00.056.417 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.417 I ggml_metal_init: has bfloat            = true
0.00.056.417 I ggml_metal_init: use bfloat            = true
0.00.056.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.418 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.387 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.389 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.404 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.298 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.299 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.300 I llama_new_context_with_model: graph nodes  = 967
0.00.068.300 I llama_new_context_with_model: graph splits = 2
0.00.068.312 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.368 I 
0.00.576.405 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.576.414 I perplexity: tokenizing the input ..
0.00.584.418 I perplexity: tokenization took 8.003 ms
0.00.584.429 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.718.852 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.720.044 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.720.073 I llama_perf_context_print:        load time =     567.06 ms
0.00.720.074 I llama_perf_context_print: prompt eval time =     134.19 ms /   128 tokens (    1.05 ms per token,   953.84 tokens per second)
0.00.720.075 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.720.075 I llama_perf_context_print:       total time =     143.71 ms /   129 tokens
0.00.720.523 I ggml_metal_free: deallocating

real	0m0.736s
user	0m0.079s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.539 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.355 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.355 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.357 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.357 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.357 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.358 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.359 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.360 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.360 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.361 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.136 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.211 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.091 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.091 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.091 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.091 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.092 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.092 I llama_model_loader: - type  f32:  194 tensors
0.00.023.093 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.093 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.043 I llm_load_vocab: special tokens cache size = 25
0.00.048.958 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.963 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.963 I llm_load_print_meta: arch             = gptneox
0.00.048.963 I llm_load_print_meta: vocab type       = BPE
0.00.048.964 I llm_load_print_meta: n_vocab          = 50304
0.00.048.965 I llm_load_print_meta: n_merges         = 50009
0.00.048.970 I llm_load_print_meta: vocab_only       = 0
0.00.048.970 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.970 I llm_load_print_meta: n_embd           = 2048
0.00.048.971 I llm_load_print_meta: n_layer          = 24
0.00.048.985 I llm_load_print_meta: n_head           = 16
0.00.048.986 I llm_load_print_meta: n_head_kv        = 16
0.00.048.986 I llm_load_print_meta: n_rot            = 32
0.00.048.986 I llm_load_print_meta: n_swa            = 0
0.00.048.987 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.987 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.988 I llm_load_print_meta: n_gqa            = 1
0.00.048.988 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.989 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.989 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.990 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.990 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.990 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.990 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.991 I llm_load_print_meta: n_ff             = 8192
0.00.048.991 I llm_load_print_meta: n_expert         = 0
0.00.048.991 I llm_load_print_meta: n_expert_used    = 0
0.00.048.991 I llm_load_print_meta: causal attn      = 1
0.00.048.991 I llm_load_print_meta: pooling type     = 0
0.00.048.991 I llm_load_print_meta: rope type        = 2
0.00.048.992 I llm_load_print_meta: rope scaling     = linear
0.00.048.992 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.993 I llm_load_print_meta: freq_scale_train = 1
0.00.048.993 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.994 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.994 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.994 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.994 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.995 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.995 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.004 I llm_load_print_meta: model type       = 1.4B
0.00.049.004 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.005 I llm_load_print_meta: model params     = 1.41 B
0.00.049.006 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.006 I llm_load_print_meta: general.name     = 1.4B
0.00.049.007 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.007 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.008 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.009 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.009 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.009 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.009 I llm_load_print_meta: max token length = 1024
0.00.050.916 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.916 I llm_load_tensors: offloading output layer to GPU
0.00.050.916 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.926 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.927 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.835 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.836 I llama_new_context_with_model: n_ctx         = 128
0.00.051.836 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.836 I llama_new_context_with_model: n_batch       = 128
0.00.051.836 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.836 I llama_new_context_with_model: flash_attn    = 0
0.00.051.837 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.837 I llama_new_context_with_model: freq_scale    = 1
0.00.051.837 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.838 I ggml_metal_init: allocating
0.00.051.843 I ggml_metal_init: found device: Apple M4
0.00.051.846 I ggml_metal_init: picking default device: Apple M4
0.00.052.413 I ggml_metal_init: using embedded metal library
0.00.054.739 I ggml_metal_init: GPU name:   Apple M4
0.00.054.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.742 I ggml_metal_init: simdgroup reduction   = true
0.00.054.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.742 I ggml_metal_init: has bfloat            = true
0.00.054.742 I ggml_metal_init: use bfloat            = true
0.00.054.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.356 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.358 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.373 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.292 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.293 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.294 I llama_new_context_with_model: graph nodes  = 967
0.00.066.294 I llama_new_context_with_model: graph splits = 2
0.00.066.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.302 I 
0.00.657.338 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.347 I perplexity: tokenizing the input ..
0.00.665.391 I perplexity: tokenization took 8.043 ms
0.00.665.406 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.391 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.807.545 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.807.557 I llama_perf_context_print:        load time =     648.76 ms
0.00.807.558 I llama_perf_context_print: prompt eval time =     140.73 ms /   128 tokens (    1.10 ms per token,   909.57 tokens per second)
0.00.807.559 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.559 I llama_perf_context_print:       total time =     150.26 ms /   129 tokens
0.00.808.044 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.077s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.435 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.190 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.194 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.195 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.196 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.196 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.196 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.199 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.200 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.201 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.201 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.203 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.203 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.012 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.114 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.093 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.094 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.094 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.095 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.096 I llama_model_loader: - type  f32:  194 tensors
0.00.024.096 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.998 I llm_load_vocab: special tokens cache size = 25
0.00.051.136 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.139 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.140 I llm_load_print_meta: arch             = gptneox
0.00.051.140 I llm_load_print_meta: vocab type       = BPE
0.00.051.140 I llm_load_print_meta: n_vocab          = 50304
0.00.051.140 I llm_load_print_meta: n_merges         = 50009
0.00.051.141 I llm_load_print_meta: vocab_only       = 0
0.00.051.141 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.141 I llm_load_print_meta: n_embd           = 2048
0.00.051.141 I llm_load_print_meta: n_layer          = 24
0.00.051.156 I llm_load_print_meta: n_head           = 16
0.00.051.156 I llm_load_print_meta: n_head_kv        = 16
0.00.051.159 I llm_load_print_meta: n_rot            = 32
0.00.051.160 I llm_load_print_meta: n_swa            = 0
0.00.051.160 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.160 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.161 I llm_load_print_meta: n_gqa            = 1
0.00.051.162 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.162 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.163 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.163 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.163 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.163 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.163 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.164 I llm_load_print_meta: n_ff             = 8192
0.00.051.164 I llm_load_print_meta: n_expert         = 0
0.00.051.164 I llm_load_print_meta: n_expert_used    = 0
0.00.051.164 I llm_load_print_meta: causal attn      = 1
0.00.051.164 I llm_load_print_meta: pooling type     = 0
0.00.051.164 I llm_load_print_meta: rope type        = 2
0.00.051.165 I llm_load_print_meta: rope scaling     = linear
0.00.051.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.165 I llm_load_print_meta: freq_scale_train = 1
0.00.051.165 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.166 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.166 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.166 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.166 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.166 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.166 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.176 I llm_load_print_meta: model type       = 1.4B
0.00.051.176 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.176 I llm_load_print_meta: model params     = 1.41 B
0.00.051.177 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.177 I llm_load_print_meta: general.name     = 1.4B
0.00.051.177 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.177 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.178 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.178 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.178 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.178 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.178 I llm_load_print_meta: max token length = 1024
0.00.053.183 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.183 I llm_load_tensors: offloading output layer to GPU
0.00.053.183 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.194 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.195 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.087 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.088 I llama_new_context_with_model: n_ctx         = 128
0.00.054.088 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.088 I llama_new_context_with_model: n_batch       = 128
0.00.054.089 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.089 I llama_new_context_with_model: flash_attn    = 0
0.00.054.089 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.090 I llama_new_context_with_model: freq_scale    = 1
0.00.054.090 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.090 I ggml_metal_init: allocating
0.00.054.093 I ggml_metal_init: found device: Apple M4
0.00.054.095 I ggml_metal_init: picking default device: Apple M4
0.00.054.655 I ggml_metal_init: using embedded metal library
0.00.056.985 I ggml_metal_init: GPU name:   Apple M4
0.00.056.987 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.988 I ggml_metal_init: simdgroup reduction   = true
0.00.056.988 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.988 I ggml_metal_init: has bfloat            = true
0.00.056.988 I ggml_metal_init: use bfloat            = true
0.00.056.989 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.989 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.888 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.890 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.914 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.837 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.838 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.838 I llama_new_context_with_model: graph nodes  = 967
0.00.068.838 I llama_new_context_with_model: graph splits = 2
0.00.068.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.271.913 I 
0.00.271.945 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.271.952 I perplexity: tokenizing the input ..
0.00.279.640 I perplexity: tokenization took 7.686 ms
0.00.279.651 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.420.166 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.421.401 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.421.419 I llama_perf_context_print:        load time =     262.47 ms
0.00.421.420 I llama_perf_context_print: prompt eval time =     140.29 ms /   128 tokens (    1.10 ms per token,   912.39 tokens per second)
0.00.421.421 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.421.422 I llama_perf_context_print:       total time =     149.51 ms /   129 tokens
0.00.421.868 I ggml_metal_free: deallocating

real	0m0.437s
user	0m0.079s
sys	0m0.060s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.272 I build: 4284 (d9c3ba2b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.570 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.366 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.370 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.373 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.373 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.373 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.380 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.380 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.380 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.381 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.381 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.383 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.384 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.384 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.979 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.962 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.975 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.976 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.976 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.976 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.977 I llama_model_loader: - type  f32:  194 tensors
0.00.048.977 I llama_model_loader: - type  f16:   98 tensors
0.00.076.947 I llm_load_vocab: special tokens cache size = 25
0.00.083.437 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.440 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.440 I llm_load_print_meta: arch             = gptneox
0.00.083.441 I llm_load_print_meta: vocab type       = BPE
0.00.083.441 I llm_load_print_meta: n_vocab          = 50304
0.00.083.441 I llm_load_print_meta: n_merges         = 50009
0.00.083.441 I llm_load_print_meta: vocab_only       = 0
0.00.083.441 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.441 I llm_load_print_meta: n_embd           = 2048
0.00.083.441 I llm_load_print_meta: n_layer          = 24
0.00.083.456 I llm_load_print_meta: n_head           = 16
0.00.083.457 I llm_load_print_meta: n_head_kv        = 16
0.00.083.457 I llm_load_print_meta: n_rot            = 32
0.00.083.457 I llm_load_print_meta: n_swa            = 0
0.00.083.457 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.457 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.458 I llm_load_print_meta: n_gqa            = 1
0.00.083.459 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.459 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.460 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.460 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.460 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.460 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.460 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.461 I llm_load_print_meta: n_ff             = 8192
0.00.083.461 I llm_load_print_meta: n_expert         = 0
0.00.083.461 I llm_load_print_meta: n_expert_used    = 0
0.00.083.462 I llm_load_print_meta: causal attn      = 1
0.00.083.462 I llm_load_print_meta: pooling type     = 0
0.00.083.462 I llm_load_print_meta: rope type        = 2
0.00.083.462 I llm_load_print_meta: rope scaling     = linear
0.00.083.462 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.463 I llm_load_print_meta: freq_scale_train = 1
0.00.083.463 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.463 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.463 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.463 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.463 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.463 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.464 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.473 I llm_load_print_meta: model type       = 1.4B
0.00.083.474 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.474 I llm_load_print_meta: model params     = 1.41 B
0.00.083.475 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.475 I llm_load_print_meta: general.name     = 1.4B
0.00.083.475 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.475 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.475 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.475 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.476 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.083.476 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.476 I llm_load_print_meta: max token length = 1024
0.00.086.105 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.086.105 I llm_load_tensors: offloading output layer to GPU
0.00.086.105 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.086.116 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.117 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.087.102 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.103 I llama_new_context_with_model: n_ctx         = 128
0.00.087.104 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.087.104 I llama_new_context_with_model: n_batch       = 128
0.00.087.104 I llama_new_context_with_model: n_ubatch      = 128
0.00.087.104 I llama_new_context_with_model: flash_attn    = 0
0.00.087.105 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.105 I llama_new_context_with_model: freq_scale    = 1
0.00.087.105 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.106 I ggml_metal_init: allocating
0.00.087.112 I ggml_metal_init: found device: Apple M4
0.00.087.115 I ggml_metal_init: picking default device: Apple M4
0.00.087.705 I ggml_metal_init: using embedded metal library
0.00.090.271 I ggml_metal_init: GPU name:   Apple M4
0.00.090.272 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.273 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.273 I ggml_metal_init: simdgroup reduction   = true
0.00.090.274 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.274 I ggml_metal_init: has bfloat            = true
0.00.090.274 I ggml_metal_init: use bfloat            = true
0.00.090.274 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.209 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.218 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.232 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.049 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.050 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.050 I llama_new_context_with_model: graph nodes  = 967
0.00.101.051 I llama_new_context_with_model: graph splits = 2
0.00.101.063 I 
0.00.101.096 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.101.097 I compute_imatrix: tokenizing the input ..
0.00.108.036 I compute_imatrix: tokenization took 6.938 ms
0.00.108.038 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.519.123 I compute_imatrix: 1.41 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.521.565 I llama_perf_context_print:        load time =    1498.55 ms
0.01.521.566 I llama_perf_context_print: prompt eval time =    1410.44 ms /   128 tokens (   11.02 ms per token,    90.75 tokens per second)
0.01.521.567 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.521.568 I llama_perf_context_print:       total time =    1500.99 ms /   129 tokens
0.01.522.167 I ggml_metal_free: deallocating

real	0m1.707s
user	0m0.164s
sys	0m0.240s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4284 (d9c3ba2b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d60a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d60a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d60aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d60b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d60ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d60bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d60c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d60cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d60d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d60d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d60daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d60dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d60eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d60f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d60fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d6101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d611030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d611750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d611f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d612640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d612d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d613480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d614440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d615980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d615ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d616180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d6168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d617170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d6176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d617e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d6182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d618750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d619090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d6199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d619e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d61a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d61a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d61abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d61b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d61bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d61c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d61c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d61cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d61d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d61d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d61df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d61e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d61ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d61f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d61f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d61f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d620160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d620420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d6208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d620d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d6216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d621b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d621fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d622920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d622dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d623260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d623700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d623ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d6240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d6250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d625630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d625b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d6260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d626620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d6270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d627610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d6280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d628600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d6290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d6295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d629b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d62a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d62a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d62ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d62b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d62b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d62bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d61b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d62bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d62c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d62cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d62d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d62d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d62dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d62e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d62e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d62ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d62f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d62f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d62fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d6301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d630700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d6310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d631ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d632370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d632810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d6335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d633a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d6343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d634870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d6351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d6368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d6376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d637b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d639710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d63a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d63a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d63a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d63ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d63b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d63b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d63bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d63c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d63c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d63c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d63ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d63d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d63d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d63dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d63e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d63e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d63ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d63eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d63f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d63f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d63fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d640610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d640f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d6413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d641d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d6421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d642670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d642fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d643450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d6438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d6446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d644b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d645010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d6454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d645950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d645df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d646730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d646bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d647070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d647510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d6479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d647e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d6483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d6488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d648e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d649390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d649650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d64a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d64a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10d64b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d64b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d64b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d64bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10d64c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d64cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d64d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d64d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d64d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d64e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d64e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d64ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d64f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d64f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d64fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d650150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d6506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d650bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d651690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d651be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d652130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d652680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d652bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d653120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d653670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d653bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d654110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d654660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d654bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d655100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d655650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d655ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d6560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d656640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d656b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d6570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d657630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d657b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d6580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d658620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d658b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d6590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d659610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d65a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d65a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d65ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d65b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d65b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d65bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d65c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d65c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d65cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d65d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d65d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d65db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d65e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d65e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d65eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d65f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d65f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d65fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d660050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d6605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d660af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10d660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10d661430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d6618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d661d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d662210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d6626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d662b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d662ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d663490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d663930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d663dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d664270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d664710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d664bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d6655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d665cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d6663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d666b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d667220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d6674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10d667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d667f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d6685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.148.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x119304bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x119305040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1193054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x119305920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x119305d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x119306200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x119306670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x119306ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x119306f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1193073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x119307830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x119307f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x119308a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1193091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x119309a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11930a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11930a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11930af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11930b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11930bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11930c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11930cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11930d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11930da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11930e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11930e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11930e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11930eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11930efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11930f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11930f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11930fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x119310230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1193104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119310960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x119310dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119311240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1193116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119311b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119311f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x119312400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119312870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x119312ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x119313150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1193135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x119313a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x119313ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x119314310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119314780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119314bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119315060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1193154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119315940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x119315db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x119316220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x119316690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x119316c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119317100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x119317570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1193179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x119317e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1193182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x119318730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x119318ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x119319010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x119319480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1193198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119319d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11931a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11931a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11931aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11931af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11931b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11931b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11931bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11931c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11931c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11931c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11931ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11931d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11931d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11931db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11931dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11931e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11931e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11931ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11931f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11931f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11931fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11931ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119320370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1193207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119320c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1193210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x119321530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1193219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x119321e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x119322280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1193226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x119322b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x119322fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x119323440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1193238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x119323d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x119324190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x119324600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x119324a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x119324ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x119325350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1193257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119325c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1193260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x119326510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x119326980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119326df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x119327260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1193276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x119327b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x119327fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x119328420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119328890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x119328d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x119329170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1193295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x119329a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x119329ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11932a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11932a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11932ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11932b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11932b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11932b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11932bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11932c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11932c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11932cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11932cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11932d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11932d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11932dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11932e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11932e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11932ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11932eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11932f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11932f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11932fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x119330060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1193304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x119330940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x119330db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x119331220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x119331690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x119331b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x119331f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1193323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x119332850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x119332cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119333130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1193335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x119333a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x119333e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1193342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x119334760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x119334bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x119335040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1193354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x119335920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x119335d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x119336200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x119336670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x119336ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x119336f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1193373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x119337830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x119337ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119338110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x119338580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1193389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x119338e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1193392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119339740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x119339bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11933a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11933a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11933a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11933ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11933b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11933b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11933bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11933bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11933c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11933c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11933cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11933d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11933d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11933d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11933de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11933e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11933e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11933eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11933f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11933f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11933f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11933fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1193401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119340630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x119340bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x119341030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1193414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119341ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1193422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x119342570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1193429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119342e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1193432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119343730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x119343ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x119344010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x119344480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1193448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x119344d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1193451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x119345640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x119345ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119345f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x119346390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x119346800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119346c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1193470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x119347550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1193479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x119347e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1193482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119348710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119348b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x119348ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119349460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1193498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x119349d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11934a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11934a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11934aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11934af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11934b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11934b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11934bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11934c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11934c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11934c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11934ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11934d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11934d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11934db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11934dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11934e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11934e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11934ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11934f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11934f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11934fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11934fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119350350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1193507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x119350c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1193510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119351510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x119351980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x119351df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x119352260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1193526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x119352b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119352fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119353420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x119353890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x119353d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x119354170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1193545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119354a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x119354ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119355330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1193557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x119355c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x119356680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x119356da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1193574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x119357be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x119357ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x119358310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x119358910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x119358f20 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d624cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d625120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d625590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d625a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d625e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d6262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d626750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d626bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d627030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d6274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d627910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d627ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d6287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d628f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d629740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d629e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d62a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d62ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d62b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d62bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d62c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d62ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d62d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d62d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d62df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d62e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d62e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d62ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d62f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d62f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d62f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d62fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d6302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d630570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d6309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d630e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d6312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d631730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d631ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d632010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d632480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d6328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d632d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d6331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d633640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d633f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d634390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d634800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d634c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d6350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d635550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d6359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d635e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d6362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d636b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d636ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d637460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d6378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d637d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d6381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d638620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d638a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d638f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d639370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d6397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d639c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d63a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d63a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d63a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d63ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d63b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d63b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d63bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d63bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d63c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d63c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d63cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d63d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d63d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d63da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d63dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d63e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d63e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d63ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d63f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d63f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d63f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d63fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d640260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d6406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d640fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d641420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d641d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d642170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d6425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d642a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d642ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d643330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d6437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d643c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d644080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d6444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d644960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d644dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d645240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d6456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d646400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d646870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d646ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d647150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d6475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d647a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d647ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d648780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d648bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d649060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d6494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d649940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d649db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d64a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d64a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d64ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d64af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d64b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d64b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d64bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d64c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d64c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d64ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d64ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d64d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d64d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d64dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d64e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d64e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d64e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d64ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d64f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d64f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d64fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d64ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d6503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d650830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d650ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d651110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d651580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d6519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d651e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d6522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d652740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d652bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d653020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d653490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d653900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d653d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d6541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d654650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d654ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d654f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d6553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d655810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d655c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d6560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d656560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d6569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d656e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d6572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d657720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d657b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d658000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d658470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d6588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d658d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d6591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d659630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d659aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d659f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d65a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d65a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d65ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d65b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d65b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d65b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d65be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d65c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d65c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d65cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d65cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d65d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d65d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d65dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d65e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d65e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d65ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10d65eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d65f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d65f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d65fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10d6600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d660520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d660990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d660e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d661270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d6619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d661e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d6622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d662740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d662bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d663020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d663490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d663900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d663d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d6641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d664650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d664ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d664f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d6653a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d665810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d665c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d6660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d666560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d6669d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d666e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d6672b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d667720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d667b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d668000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d668470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d60b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d60ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d609840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d60a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d617850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d617cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d618130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d6185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d618a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d618e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d6192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d619760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d619bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d61a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d61a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d61a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d61ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d61b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d61b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d61bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d61bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d61c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d61c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d61cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d61d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d61d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d61d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d61de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d61e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d61e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d61ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d61f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10d61f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10d61f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d61fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d6201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d620650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d620ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d620f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d6213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d621810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d621c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d6220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d622560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d6229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d6232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d623e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d6162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d6169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d616e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10d60d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d60da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d60de90 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.791s
user	0m0.310s
sys	0m0.270s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4284 (d9c3ba2b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13270d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13270d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13270df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13270e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13270ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13270f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13270f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13270fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132710130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x132710630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132710b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132711030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132711b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x132712300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132712b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132713230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132713950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132714070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132714790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132714f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132715680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132715da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1327164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132716d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132717480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132717740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132717d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1327189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132718f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1327191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132719660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132719920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13271a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13271a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13271a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13271ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13271b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13271b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13271bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13271c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13271c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13271ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13271ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13271d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13271d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13271dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13271e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13271eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13271f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13271f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13271fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132720390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1327209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132720fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1327217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132721c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1327220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1327223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1327229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1327231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132723460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132723900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132723da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132724240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1327246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132724b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132725020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1327254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132725960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132725e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1327262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132726740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132726be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132727130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132727680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132727bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132728120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132728670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132728bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132729110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132729660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132729bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13272a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13272a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13272aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13272b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13272b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13272bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13272c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13272c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13272cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13272d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13272d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13272db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13272e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13272e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13272eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13271e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13272efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13272f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13272fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x132730220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132730770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132730cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x132731210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132731760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132731cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x132732200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x132732750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x132732ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1327331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132733740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132733c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132734130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1327345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132734a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x132734f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1327353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132735850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132735cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132736190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132736630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132736ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132736f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x132737410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1327378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132737d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1327381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132738690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x132738b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x132738fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132739470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132739910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132739db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13273a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13273a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13273ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13273b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13273b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13273b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13273be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13273c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13273c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13273cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13273d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13273d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13273d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13273de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13273e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13273e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13273ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13273f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13273f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13273fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13273fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132740370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x132740810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132740cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132741150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1327415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132741a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132741f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1327423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132742870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132742d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1327431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132743650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132743af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132743f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132744430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1327448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132745210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1327456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132745b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132745ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132746490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132746930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132746dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132747270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132747710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132747bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132748050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1327484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132748990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132748e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1327492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132749770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132749c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13274a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13274a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13274a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13274ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13274b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13274b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13274be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13274c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13274c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13274cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13274d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13274d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13274e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13274e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13274e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13274ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13274f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13274fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1327500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132750560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x132750a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1327511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x132751700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132751c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1327521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1327526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x132752c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132753190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1327536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132753c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132754180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1327546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132754c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132755170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1327556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132755c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132756160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1327566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132756c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132757150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1327576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132757bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132758140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132758690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132758be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132759130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132759680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132759bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13275a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13275a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13275abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13275b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13275b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13275bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13275c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13275c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13275cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13275d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13275d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13275db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13275e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13275e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13275eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13275f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13275f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13275fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1327600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x132760610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132760b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1327610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x132761600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132761b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1327620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1327625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132762b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132763090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1327635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132763b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x132763fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132764470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x132764910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132764db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132765250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1327656f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132765b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132766030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1327664d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132766970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132766e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1327672b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132767750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132767bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132768090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1327685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132768d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132769420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132769b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13276a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13276a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13276ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13276afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13276b5e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.090.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132608f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1326093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x132609850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132609cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13260a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13260a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13260aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13260ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13260b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13260b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13260bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13260c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13260ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13260d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13260ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13260e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13260ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13260f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13260fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132610210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132610930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132611050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132611770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132611e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1326125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132612870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132612b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132612fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132613410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132613880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132613d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132614290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x132614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1326149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1326152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x132615800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x132615d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x132616700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x132616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132617100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x132617600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x132617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x132618000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x132618470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1326188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132618d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1326191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132619630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x132619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132619f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13261a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13261a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13261ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13261b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13261b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13261bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13261c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13261c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136e04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136e046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136e04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136e04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136e053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136e05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136e05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136e06140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136e065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136e06a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136e06e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136e07300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136e07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136e07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136e08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136e084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136e08930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136e08da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136e09210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136e09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136e09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136e09f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136e0a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136e0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136e0acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136e0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136e0b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136e0ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136e0be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136e0c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136e0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136e0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136e0d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136e0d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136e0d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136e0dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136e0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136e0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136e0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136e0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136e0f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136e0f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136e0fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136e10100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136e10570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136e109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136e10e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136e112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136e11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136e11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136e12010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136e12480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136e128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136e12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136e131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136e13640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136e13ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136e13f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136e14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136e14800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136e14c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136e150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136e15550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136e159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136e15e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136e162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136e16710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136e16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136e16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136e17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136e178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136e17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136e181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136e18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136e18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136e18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136e19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136e197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136e19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136e1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136e1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136e1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136e1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136e1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136e1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136e1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136e1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136e1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136e1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136e1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136e1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136e1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136e1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136e1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136e1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136e1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136e1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136e1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136e1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136e1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136e1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136e20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136e206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136e20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136e20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136e21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136e21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136e21d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136e22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136e225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136e22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136e22ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136e23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136e237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136e23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136e24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136e244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136e24960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136e24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136e25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136e256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136e25b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136e25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136e26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136e26870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136e26ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136e27150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136e275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136e27a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136e27ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136e28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136e28780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136e28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136e29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136e294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136e29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136e29db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136e2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136e2a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136e2ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136e2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136e2b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136e2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136e2bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136e2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136e2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136e2ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136e2cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136e2d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136e2d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136e2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136e2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136e2e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136e2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136e2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136e2f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136e2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136e2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136e303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136e30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136e30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136e31140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136e315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136e31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136e31e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136e32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136e32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136e32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136e33050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136e334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136e33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136e33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136e34210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136e34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136e34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136e34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136e353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136e35840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136e35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136e36120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136e36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136e36a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136e36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136e372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136e37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136e37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136e38030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136e384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136e38910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136e38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136e391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136e39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136e39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136e39f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136e3a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136e3a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136e3ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136e3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136e3b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136e3b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136e3be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136e3c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136e3c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136e3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136e3d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136e3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136e3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136e3dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136e3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136e3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136e3eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136e3ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136e3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136e3f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136e3fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136e400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136e40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136e409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136e40e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136e412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136e41710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136e41b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136e41ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136e42a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136e43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136e438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136e43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136e44280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136e446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136e44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136e45300 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132608f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1326093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x132609850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132609cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13260a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13260a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13260aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13260ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13260b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13260b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13260bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13260c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13260caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13260d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13260da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13260e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13260e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13260eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13260f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13260ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132610630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132610d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132611410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132611b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1326121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132612660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132612ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132612f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1326133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132613820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132613c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132614100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x132614570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x132614830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132614ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x132615110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x132615580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1326159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132615e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1326162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x132616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132616bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x132617020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x132617490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x132617900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x132617d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1326181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132618650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x132618ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132618f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1326193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132619810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132619c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13261a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13261a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13261a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13261ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13261b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13261b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13261bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13261c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13261c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13261c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13261ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13261d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13261d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13261dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13261e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13261e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13261eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13261f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13261f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13261fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13261ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132620580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132620b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132621100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1326216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132621c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132622240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132622800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132622dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x132623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132623940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132623f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1326244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x132624a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132625040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x132625600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132625bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132626180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x132626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x132626d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1326272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x132627880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x132627e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1326289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x132628f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132629540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132629b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13262a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13262a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13262ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13262b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13262b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13262bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13262c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13262c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13262cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13262d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13262da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13262e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13262e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13262ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13262efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13262f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13262fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13262ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132630420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132630930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132630e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132631350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132631860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x132631d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132632280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132632790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x132632ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1326331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1326336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x132633bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1326340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1326345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132634b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x132635010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x132635520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x132635a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x132635f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x132636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x132636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x132636e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x132637380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x132637890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132637da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1326382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1326387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x132638cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1326391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1326396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132639de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13263a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13263a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13263aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13263af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13263b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13263b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13263be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13263c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13263c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13263cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13263d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13263d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13263dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13263e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13263e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13263ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13263f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13263f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13263fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132640060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132640570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132640a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132640f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1326414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1326419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132641ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1326423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1326428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132642df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132643300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132643810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132643d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x132644740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132644c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132645160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x132645670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132645b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132646090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1326465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x132646ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132646fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1326474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x132647c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1326481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132648710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x132648c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132648f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x132649530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x132649b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13264a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13264a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13264ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13264b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13264b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13264bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13264c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13264c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13264cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13264d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13264da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13264df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13264e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13264ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13264ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13264f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13264fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13264ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1326504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132650a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x132650f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1326514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132651a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132651f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1326524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1326529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132652f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132653490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1326539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132653f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132654480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1326549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132654f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132655470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1326559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132655f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132656460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1326569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132656f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132657450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1326579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132657ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132658440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132658990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132658ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132659430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x132659980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x132659ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13265a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13265a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13265aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13265b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13265b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13265beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13265c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13265c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13265cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13265d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13265d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13265de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13265e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13265e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13265ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13265f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13265f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13265fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1326603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x132660860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132660d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1326611a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132661640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132661ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x132661f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132662420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1326628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x132662d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132663200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1326636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132663b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132663fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132664480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132664920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x132664e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132665590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132665cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1326663d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132666af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x132666db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1326675a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x132667860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132667e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.944s
user	0m0.244s
sys	0m0.138s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
