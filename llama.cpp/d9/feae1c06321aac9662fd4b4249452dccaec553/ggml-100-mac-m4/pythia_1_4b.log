Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.546s
user	0m0.882s
sys	0m1.208s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-cpu
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Built target test-log
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Built target test-gguf
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-backend-ops
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-chat-template
[ 63%] Built target test-barrier
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Built target llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-quantize-perf
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-imatrix
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-infill
[ 78%] Built target llama-bench
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-cli
[ 83%] Built target llama-parallel
[ 84%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-passkey
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-run
[ 87%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Built target llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-save-load-state
[ 92%] Built target llama-run
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.908s
user	0m6.088s
sys	0m9.677s

main: quantize time =  2614.88 ms
main:    total time =  2614.88 ms

main: quantize time =  1339.89 ms
main:    total time =  1339.89 ms

main: quantize time =  1306.09 ms
main:    total time =  1306.09 ms

main: quantize time =  1835.07 ms
main:    total time =  1835.07 ms

main: quantize time =  2306.33 ms
main:    total time =  2306.33 ms

main: quantize time =  4951.05 ms
main:    total time =  4951.05 ms

main: quantize time =  5897.40 ms
main:    total time =  5897.40 ms

main: quantize time =  7017.08 ms
main:    total time =  7017.08 ms

main: quantize time =  5915.16 ms
main:    total time =  5915.16 ms

main: quantize time =  4534.75 ms
main:    total time =  4534.75 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.154 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.262 I main: llama backend init
0.00.000.268 I main: load the model and apply lora adapter, if any
0.00.084.799 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.097.079 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.097.091 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.097.095 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.097.096 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.097.097 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.097.097 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.097.098 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.097.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.097.102 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.097.103 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.097.104 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.097.104 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.097.105 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.097.106 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.097.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.097.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.097.113 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.103.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.106.061 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.113.055 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.113.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.113.064 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.113.065 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.113.065 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.113.067 I llama_model_loader: - type  f32:  194 tensors
0.00.113.067 I llama_model_loader: - type  f16:   98 tensors
0.00.151.818 I llm_load_vocab: special tokens cache size = 25
0.00.159.225 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.159.228 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.159.228 I llm_load_print_meta: arch             = gptneox
0.00.159.229 I llm_load_print_meta: vocab type       = BPE
0.00.159.229 I llm_load_print_meta: n_vocab          = 50304
0.00.159.229 I llm_load_print_meta: n_merges         = 50009
0.00.159.229 I llm_load_print_meta: vocab_only       = 0
0.00.159.229 I llm_load_print_meta: n_ctx_train      = 2048
0.00.159.229 I llm_load_print_meta: n_embd           = 2048
0.00.159.230 I llm_load_print_meta: n_layer          = 24
0.00.159.233 I llm_load_print_meta: n_head           = 16
0.00.159.234 I llm_load_print_meta: n_head_kv        = 16
0.00.159.234 I llm_load_print_meta: n_rot            = 32
0.00.159.234 I llm_load_print_meta: n_swa            = 0
0.00.159.236 I llm_load_print_meta: n_embd_head_k    = 128
0.00.159.236 I llm_load_print_meta: n_embd_head_v    = 128
0.00.159.237 I llm_load_print_meta: n_gqa            = 1
0.00.159.237 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.159.238 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.159.239 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.159.239 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.159.239 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.159.239 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.159.240 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.159.242 I llm_load_print_meta: n_ff             = 8192
0.00.159.242 I llm_load_print_meta: n_expert         = 0
0.00.159.242 I llm_load_print_meta: n_expert_used    = 0
0.00.159.243 I llm_load_print_meta: causal attn      = 1
0.00.159.243 I llm_load_print_meta: pooling type     = 0
0.00.159.243 I llm_load_print_meta: rope type        = 2
0.00.159.243 I llm_load_print_meta: rope scaling     = linear
0.00.159.243 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.159.244 I llm_load_print_meta: freq_scale_train = 1
0.00.159.244 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.159.244 I llm_load_print_meta: rope_finetuned   = unknown
0.00.159.244 I llm_load_print_meta: ssm_d_conv       = 0
0.00.159.245 I llm_load_print_meta: ssm_d_inner      = 0
0.00.159.245 I llm_load_print_meta: ssm_d_state      = 0
0.00.159.245 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.159.245 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.159.245 I llm_load_print_meta: model type       = 1.4B
0.00.159.246 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.159.246 I llm_load_print_meta: model params     = 1.41 B
0.00.159.247 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.159.247 I llm_load_print_meta: general.name     = 1.4B
0.00.159.248 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.159.248 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.159.248 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.159.248 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.159.249 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.159.249 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.159.249 I llm_load_print_meta: max token length = 1024
0.00.161.979 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.161.979 I llm_load_tensors: offloading output layer to GPU
0.00.161.980 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.161.999 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.162.000 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.162.361 I llama_new_context_with_model: n_seq_max     = 1
0.00.162.362 I llama_new_context_with_model: n_ctx         = 2048
0.00.162.362 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.162.363 I llama_new_context_with_model: n_batch       = 2048
0.00.162.363 I llama_new_context_with_model: n_ubatch      = 512
0.00.162.363 I llama_new_context_with_model: flash_attn    = 0
0.00.162.363 I llama_new_context_with_model: freq_base     = 10000.0
0.00.162.364 I llama_new_context_with_model: freq_scale    = 1
0.00.162.364 I ggml_metal_init: allocating
0.00.162.368 I ggml_metal_init: found device: Apple M4
0.00.162.370 I ggml_metal_init: picking default device: Apple M4
0.00.163.072 I ggml_metal_init: using embedded metal library
0.00.346.199 I ggml_metal_init: GPU name:   Apple M4
0.00.346.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.217 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.218 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.219 I ggml_metal_init: simdgroup reduction   = true
0.00.346.219 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.219 I ggml_metal_init: has bfloat            = true
0.00.346.220 I ggml_metal_init: use bfloat            = true
0.00.346.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.227 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.390.518 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.423.908 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.423.926 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.423.979 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.426.244 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.426.248 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.426.249 I llama_new_context_with_model: graph nodes  = 967
0.00.426.250 I llama_new_context_with_model: graph splits = 2
0.00.426.254 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.426.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.426.606 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.353 I main: llama threadpool init, n_threads = 4
0.00.516.389 I 
0.00.516.412 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.516.415 I 
0.00.516.498 I sampler seed: 1234
0.00.516.504 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.516.551 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.516.553 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.516.553 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.362.380 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.02.362.381 I llama_perf_context_print:        load time =     431.54 ms
0.02.362.382 I llama_perf_context_print: prompt eval time =      44.05 ms /     7 tokens (    6.29 ms per token,   158.93 tokens per second)
0.02.362.384 I llama_perf_context_print:        eval time =    1798.85 ms /    63 runs   (   28.55 ms per token,    35.02 tokens per second)
0.02.362.385 I llama_perf_context_print:       total time =    1846.03 ms /    70 tokens
0.02.362.592 I ggml_metal_free: deallocating

real	0m2.668s
user	0m0.169s
sys	0m0.128s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.915 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.316 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.316 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.317 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.317 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.318 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.319 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.319 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.321 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.321 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.322 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.322 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.327 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.120 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.164 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.053 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.054 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.054 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.056 I llama_model_loader: - type  f32:  194 tensors
0.00.028.056 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.588 I llm_load_vocab: special tokens cache size = 25
0.00.055.690 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.695 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.695 I llm_load_print_meta: arch             = gptneox
0.00.055.695 I llm_load_print_meta: vocab type       = BPE
0.00.055.698 I llm_load_print_meta: n_vocab          = 50304
0.00.055.698 I llm_load_print_meta: n_merges         = 50009
0.00.055.698 I llm_load_print_meta: vocab_only       = 0
0.00.055.698 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.698 I llm_load_print_meta: n_embd           = 2048
0.00.055.699 I llm_load_print_meta: n_layer          = 24
0.00.055.704 I llm_load_print_meta: n_head           = 16
0.00.055.705 I llm_load_print_meta: n_head_kv        = 16
0.00.055.706 I llm_load_print_meta: n_rot            = 32
0.00.055.706 I llm_load_print_meta: n_swa            = 0
0.00.055.707 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.707 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.708 I llm_load_print_meta: n_gqa            = 1
0.00.055.709 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.710 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.710 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.711 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.714 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.714 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.714 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.715 I llm_load_print_meta: n_ff             = 8192
0.00.055.715 I llm_load_print_meta: n_expert         = 0
0.00.055.715 I llm_load_print_meta: n_expert_used    = 0
0.00.055.715 I llm_load_print_meta: causal attn      = 1
0.00.055.715 I llm_load_print_meta: pooling type     = 0
0.00.055.715 I llm_load_print_meta: rope type        = 2
0.00.055.716 I llm_load_print_meta: rope scaling     = linear
0.00.055.716 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.718 I llm_load_print_meta: freq_scale_train = 1
0.00.055.718 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.718 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.719 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.719 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.719 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.719 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.719 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.719 I llm_load_print_meta: model type       = 1.4B
0.00.055.720 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.720 I llm_load_print_meta: model params     = 1.41 B
0.00.055.721 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.721 I llm_load_print_meta: general.name     = 1.4B
0.00.055.721 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.721 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.721 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.722 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.722 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.722 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.722 I llm_load_print_meta: max token length = 1024
0.00.058.127 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.128 I llm_load_tensors: offloading output layer to GPU
0.00.058.128 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.139 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.141 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.058.528 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.529 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.529 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.529 I llama_new_context_with_model: n_batch       = 2048
0.00.058.529 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.529 I llama_new_context_with_model: flash_attn    = 0
0.00.058.530 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.530 I llama_new_context_with_model: freq_scale    = 1
0.00.058.531 I ggml_metal_init: allocating
0.00.058.534 I ggml_metal_init: found device: Apple M4
0.00.058.536 I ggml_metal_init: picking default device: Apple M4
0.00.059.259 I ggml_metal_init: using embedded metal library
0.00.061.774 I ggml_metal_init: GPU name:   Apple M4
0.00.061.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.776 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.776 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.777 I ggml_metal_init: simdgroup reduction   = true
0.00.061.777 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.777 I ggml_metal_init: has bfloat            = true
0.00.061.777 I ggml_metal_init: use bfloat            = true
0.00.061.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.778 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.253 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.773 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.787 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.811 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.942 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.945 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.945 I llama_new_context_with_model: graph nodes  = 967
0.00.098.945 I llama_new_context_with_model: graph splits = 2
0.00.098.949 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.079 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.080 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.419.581 I main: llama threadpool init, n_threads = 4
0.01.419.616 I 
0.01.419.641 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.419.641 I 
0.01.419.872 I sampler seed: 1234
0.01.419.877 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.419.892 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.419.892 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.419.892 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.502.936 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.02.502.937 I llama_perf_context_print:        load time =    1409.66 ms
0.02.502.938 I llama_perf_context_print: prompt eval time =      39.82 ms /     7 tokens (    5.69 ms per token,   175.80 tokens per second)
0.02.502.938 I llama_perf_context_print:        eval time =    1040.38 ms /    63 runs   (   16.51 ms per token,    60.56 tokens per second)
0.02.502.939 I llama_perf_context_print:       total time =    1083.36 ms /    70 tokens
0.02.503.171 I ggml_metal_free: deallocating

real	0m2.523s
user	0m0.114s
sys	0m0.227s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.012.635 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.281 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.281 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.282 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.282 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.283 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.284 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.285 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.286 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.288 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.288 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.117 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.128 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.089 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.091 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.091 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.092 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.092 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.092 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.093 I llama_model_loader: - type  f32:  194 tensors
0.00.039.093 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.094 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.187 I llm_load_vocab: special tokens cache size = 25
0.00.070.772 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.777 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.777 I llm_load_print_meta: arch             = gptneox
0.00.070.778 I llm_load_print_meta: vocab type       = BPE
0.00.070.778 I llm_load_print_meta: n_vocab          = 50304
0.00.070.778 I llm_load_print_meta: n_merges         = 50009
0.00.070.778 I llm_load_print_meta: vocab_only       = 0
0.00.070.779 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.779 I llm_load_print_meta: n_embd           = 2048
0.00.070.779 I llm_load_print_meta: n_layer          = 24
0.00.070.784 I llm_load_print_meta: n_head           = 16
0.00.070.785 I llm_load_print_meta: n_head_kv        = 16
0.00.070.785 I llm_load_print_meta: n_rot            = 32
0.00.070.785 I llm_load_print_meta: n_swa            = 0
0.00.070.785 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.785 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.786 I llm_load_print_meta: n_gqa            = 1
0.00.070.787 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.787 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.788 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.788 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.789 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.789 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.789 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.790 I llm_load_print_meta: n_ff             = 8192
0.00.070.790 I llm_load_print_meta: n_expert         = 0
0.00.070.790 I llm_load_print_meta: n_expert_used    = 0
0.00.070.790 I llm_load_print_meta: causal attn      = 1
0.00.070.790 I llm_load_print_meta: pooling type     = 0
0.00.070.791 I llm_load_print_meta: rope type        = 2
0.00.070.791 I llm_load_print_meta: rope scaling     = linear
0.00.070.791 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.792 I llm_load_print_meta: freq_scale_train = 1
0.00.070.792 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.792 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.792 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.793 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.793 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.793 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.793 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.794 I llm_load_print_meta: model type       = 1.4B
0.00.070.795 I llm_load_print_meta: model ftype      = Q4_0
0.00.070.795 I llm_load_print_meta: model params     = 1.41 B
0.00.070.795 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.070.796 I llm_load_print_meta: general.name     = 1.4B
0.00.070.796 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.796 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.796 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.796 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.797 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.070.797 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.797 I llm_load_print_meta: max token length = 1024
0.00.073.053 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.054 I llm_load_tensors: offloading output layer to GPU
0.00.073.054 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.066 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.073.068 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.073.542 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.543 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.543 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.543 I llama_new_context_with_model: n_batch       = 2048
0.00.073.543 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.544 I llama_new_context_with_model: flash_attn    = 0
0.00.073.544 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.544 I llama_new_context_with_model: freq_scale    = 1
0.00.073.545 I ggml_metal_init: allocating
0.00.073.549 I ggml_metal_init: found device: Apple M4
0.00.073.551 I ggml_metal_init: picking default device: Apple M4
0.00.074.391 I ggml_metal_init: using embedded metal library
0.00.077.421 I ggml_metal_init: GPU name:   Apple M4
0.00.077.423 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.424 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.424 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.424 I ggml_metal_init: simdgroup reduction   = true
0.00.077.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.425 I ggml_metal_init: has bfloat            = true
0.00.077.425 I ggml_metal_init: use bfloat            = true
0.00.077.425 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.426 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.058 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.123.773 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.781 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.805 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.814 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.815 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.816 I llama_new_context_with_model: graph nodes  = 967
0.00.124.816 I llama_new_context_with_model: graph splits = 2
0.00.124.822 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.124.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.124.960 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.123.206 I main: llama threadpool init, n_threads = 4
0.01.123.287 I 
0.01.123.348 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.123.349 I 
0.01.123.591 I sampler seed: 1234
0.01.123.598 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.123.616 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.123.628 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.123.629 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.850.821 I llama_perf_sampler_print:    sampling time =       2.01 ms /    71 runs   (    0.03 ms per token, 35253.23 tokens per second)
0.01.850.822 I llama_perf_context_print:        load time =    1110.56 ms
0.01.850.823 I llama_perf_context_print: prompt eval time =      50.38 ms /     7 tokens (    7.20 ms per token,   138.94 tokens per second)
0.01.850.824 I llama_perf_context_print:        eval time =     673.16 ms /    63 runs   (   10.69 ms per token,    93.59 tokens per second)
0.01.850.825 I llama_perf_context_print:       total time =     727.62 ms /    70 tokens
0.01.851.050 I ggml_metal_free: deallocating

real	0m1.869s
user	0m0.147s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.007.063 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.041 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.047 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.047 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.048 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.048 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.048 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.051 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.052 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.052 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.053 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.053 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.053 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.054 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.056 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.057 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.057 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.815 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.786 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.541 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.542 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.543 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.543 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.544 I llama_model_loader: - type  f32:  194 tensors
0.00.023.544 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.544 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.332 I llm_load_vocab: special tokens cache size = 25
0.00.050.368 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.371 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.371 I llm_load_print_meta: arch             = gptneox
0.00.050.372 I llm_load_print_meta: vocab type       = BPE
0.00.050.372 I llm_load_print_meta: n_vocab          = 50304
0.00.050.372 I llm_load_print_meta: n_merges         = 50009
0.00.050.372 I llm_load_print_meta: vocab_only       = 0
0.00.050.372 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.373 I llm_load_print_meta: n_embd           = 2048
0.00.050.373 I llm_load_print_meta: n_layer          = 24
0.00.050.376 I llm_load_print_meta: n_head           = 16
0.00.050.378 I llm_load_print_meta: n_head_kv        = 16
0.00.050.378 I llm_load_print_meta: n_rot            = 32
0.00.050.378 I llm_load_print_meta: n_swa            = 0
0.00.050.378 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.378 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.379 I llm_load_print_meta: n_gqa            = 1
0.00.050.380 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.380 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.381 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.381 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.381 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.382 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.382 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.382 I llm_load_print_meta: n_ff             = 8192
0.00.050.383 I llm_load_print_meta: n_expert         = 0
0.00.050.383 I llm_load_print_meta: n_expert_used    = 0
0.00.050.384 I llm_load_print_meta: causal attn      = 1
0.00.050.386 I llm_load_print_meta: pooling type     = 0
0.00.050.386 I llm_load_print_meta: rope type        = 2
0.00.050.386 I llm_load_print_meta: rope scaling     = linear
0.00.050.387 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.387 I llm_load_print_meta: freq_scale_train = 1
0.00.050.387 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.388 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.388 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.388 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.388 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.388 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.388 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.389 I llm_load_print_meta: model type       = 1.4B
0.00.050.389 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.389 I llm_load_print_meta: model params     = 1.41 B
0.00.050.390 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.390 I llm_load_print_meta: general.name     = 1.4B
0.00.050.390 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.390 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.390 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.394 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.394 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.395 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.395 I llm_load_print_meta: max token length = 1024
0.00.052.012 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.012 I llm_load_tensors: offloading output layer to GPU
0.00.052.012 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.023 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.024 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.349 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.349 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.350 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.350 I llama_new_context_with_model: n_batch       = 2048
0.00.052.350 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.350 I llama_new_context_with_model: flash_attn    = 0
0.00.052.350 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.351 I llama_new_context_with_model: freq_scale    = 1
0.00.052.351 I ggml_metal_init: allocating
0.00.052.355 I ggml_metal_init: found device: Apple M4
0.00.052.357 I ggml_metal_init: picking default device: Apple M4
0.00.052.957 I ggml_metal_init: using embedded metal library
0.00.055.326 I ggml_metal_init: GPU name:   Apple M4
0.00.055.327 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.328 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.328 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.328 I ggml_metal_init: simdgroup reduction   = true
0.00.055.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.329 I ggml_metal_init: has bfloat            = true
0.00.055.330 I ggml_metal_init: use bfloat            = true
0.00.055.331 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.102 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.038 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.045 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.073 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.089 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.091 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.091 I llama_new_context_with_model: graph nodes  = 967
0.00.087.091 I llama_new_context_with_model: graph splits = 2
0.00.087.094 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.235 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.236 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.481 I main: llama threadpool init, n_threads = 4
0.00.748.521 I 
0.00.748.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.550 I 
0.00.748.713 I sampler seed: 1234
0.00.748.718 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.782 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.782 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.512.647 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64195.30 tokens per second)
0.01.512.647 I llama_perf_context_print:        load time =     741.41 ms
0.01.512.648 I llama_perf_context_print: prompt eval time =      46.49 ms /     7 tokens (    6.64 ms per token,   150.57 tokens per second)
0.01.512.649 I llama_perf_context_print:        eval time =     714.50 ms /    63 runs   (   11.34 ms per token,    88.17 tokens per second)
0.01.512.649 I llama_perf_context_print:       total time =     764.17 ms /    70 tokens
0.01.512.861 I ggml_metal_free: deallocating

real	0m1.530s
user	0m0.112s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.821 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.576 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.576 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.577 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.577 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.577 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.580 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.581 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.581 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.582 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.583 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.584 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.585 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.308 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.355 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.071 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.072 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.072 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.073 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.073 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.074 I llama_model_loader: - type  f32:  194 tensors
0.00.027.074 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.074 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.976 I llm_load_vocab: special tokens cache size = 25
0.00.052.862 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.865 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.865 I llm_load_print_meta: arch             = gptneox
0.00.052.866 I llm_load_print_meta: vocab type       = BPE
0.00.052.866 I llm_load_print_meta: n_vocab          = 50304
0.00.052.866 I llm_load_print_meta: n_merges         = 50009
0.00.052.866 I llm_load_print_meta: vocab_only       = 0
0.00.052.866 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.867 I llm_load_print_meta: n_embd           = 2048
0.00.052.867 I llm_load_print_meta: n_layer          = 24
0.00.052.870 I llm_load_print_meta: n_head           = 16
0.00.052.871 I llm_load_print_meta: n_head_kv        = 16
0.00.052.871 I llm_load_print_meta: n_rot            = 32
0.00.052.871 I llm_load_print_meta: n_swa            = 0
0.00.052.872 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.872 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.872 I llm_load_print_meta: n_gqa            = 1
0.00.052.873 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.874 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.874 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.875 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.875 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.875 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.876 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.877 I llm_load_print_meta: n_ff             = 8192
0.00.052.877 I llm_load_print_meta: n_expert         = 0
0.00.052.877 I llm_load_print_meta: n_expert_used    = 0
0.00.052.877 I llm_load_print_meta: causal attn      = 1
0.00.052.877 I llm_load_print_meta: pooling type     = 0
0.00.052.878 I llm_load_print_meta: rope type        = 2
0.00.052.878 I llm_load_print_meta: rope scaling     = linear
0.00.052.880 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.880 I llm_load_print_meta: freq_scale_train = 1
0.00.052.880 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.880 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.881 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.881 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.881 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.882 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.882 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.883 I llm_load_print_meta: model type       = 1.4B
0.00.052.883 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.883 I llm_load_print_meta: model params     = 1.41 B
0.00.052.884 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.884 I llm_load_print_meta: general.name     = 1.4B
0.00.052.884 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.885 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.885 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.885 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.885 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.886 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.886 I llm_load_print_meta: max token length = 1024
0.00.054.507 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.507 I llm_load_tensors: offloading output layer to GPU
0.00.054.507 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.517 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.518 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.894 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.895 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.895 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.895 I llama_new_context_with_model: n_batch       = 2048
0.00.054.895 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.896 I llama_new_context_with_model: flash_attn    = 0
0.00.054.896 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.896 I llama_new_context_with_model: freq_scale    = 1
0.00.054.897 I ggml_metal_init: allocating
0.00.054.904 I ggml_metal_init: found device: Apple M4
0.00.054.906 I ggml_metal_init: picking default device: Apple M4
0.00.055.513 I ggml_metal_init: using embedded metal library
0.00.057.850 I ggml_metal_init: GPU name:   Apple M4
0.00.057.851 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.852 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.852 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.852 I ggml_metal_init: simdgroup reduction   = true
0.00.057.852 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.852 I ggml_metal_init: has bfloat            = true
0.00.057.853 I ggml_metal_init: use bfloat            = true
0.00.057.853 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.854 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.439 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.408 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.414 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.433 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.552 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.554 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.554 I llama_new_context_with_model: graph nodes  = 967
0.00.088.555 I llama_new_context_with_model: graph splits = 2
0.00.088.557 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.710 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.001 I main: llama threadpool init, n_threads = 4
0.00.788.048 I 
0.00.788.080 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.080 I 
0.00.788.247 I sampler seed: 1234
0.00.788.252 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.266 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.268 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.268 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.577.664 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.577.664 I llama_perf_context_print:        load time =     777.17 ms
0.01.577.665 I llama_perf_context_print: prompt eval time =      43.16 ms /     7 tokens (    6.17 ms per token,   162.20 tokens per second)
0.01.577.667 I llama_perf_context_print:        eval time =     743.33 ms /    63 runs   (   11.80 ms per token,    84.75 tokens per second)
0.01.577.667 I llama_perf_context_print:       total time =     789.67 ms /    70 tokens
0.01.577.934 I ggml_metal_free: deallocating

real	0m1.596s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.985 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.610 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.615 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.622 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.622 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.623 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.623 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.623 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.624 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.625 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.625 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.625 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.631 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.387 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.164 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.165 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.166 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.166 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.166 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.167 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.167 I llama_model_loader: - type  f32:  194 tensors
0.00.026.168 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.168 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.954 I llm_load_vocab: special tokens cache size = 25
0.00.053.001 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.003 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.004 I llm_load_print_meta: arch             = gptneox
0.00.053.004 I llm_load_print_meta: vocab type       = BPE
0.00.053.004 I llm_load_print_meta: n_vocab          = 50304
0.00.053.004 I llm_load_print_meta: n_merges         = 50009
0.00.053.004 I llm_load_print_meta: vocab_only       = 0
0.00.053.005 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.005 I llm_load_print_meta: n_embd           = 2048
0.00.053.005 I llm_load_print_meta: n_layer          = 24
0.00.053.008 I llm_load_print_meta: n_head           = 16
0.00.053.008 I llm_load_print_meta: n_head_kv        = 16
0.00.053.009 I llm_load_print_meta: n_rot            = 32
0.00.053.009 I llm_load_print_meta: n_swa            = 0
0.00.053.009 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.009 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.010 I llm_load_print_meta: n_gqa            = 1
0.00.053.011 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.011 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.012 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.012 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.012 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.013 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.013 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.013 I llm_load_print_meta: n_ff             = 8192
0.00.053.013 I llm_load_print_meta: n_expert         = 0
0.00.053.014 I llm_load_print_meta: n_expert_used    = 0
0.00.053.014 I llm_load_print_meta: causal attn      = 1
0.00.053.014 I llm_load_print_meta: pooling type     = 0
0.00.053.014 I llm_load_print_meta: rope type        = 2
0.00.053.014 I llm_load_print_meta: rope scaling     = linear
0.00.053.015 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.016 I llm_load_print_meta: freq_scale_train = 1
0.00.053.017 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.017 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.017 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.017 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.017 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.017 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.019 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.019 I llm_load_print_meta: model type       = 1.4B
0.00.053.019 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.020 I llm_load_print_meta: model params     = 1.41 B
0.00.053.020 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.021 I llm_load_print_meta: general.name     = 1.4B
0.00.053.021 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.021 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.021 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.021 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.022 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.022 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.022 I llm_load_print_meta: max token length = 1024
0.00.054.998 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.998 I llm_load_tensors: offloading output layer to GPU
0.00.054.998 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.009 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.010 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.342 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.343 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.343 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.343 I llama_new_context_with_model: n_batch       = 2048
0.00.055.344 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.344 I llama_new_context_with_model: flash_attn    = 0
0.00.055.344 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.344 I llama_new_context_with_model: freq_scale    = 1
0.00.055.345 I ggml_metal_init: allocating
0.00.055.348 I ggml_metal_init: found device: Apple M4
0.00.055.350 I ggml_metal_init: picking default device: Apple M4
0.00.055.934 I ggml_metal_init: using embedded metal library
0.00.058.275 I ggml_metal_init: GPU name:   Apple M4
0.00.058.277 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.277 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.278 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.278 I ggml_metal_init: simdgroup reduction   = true
0.00.058.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.278 I ggml_metal_init: has bfloat            = true
0.00.058.278 I ggml_metal_init: use bfloat            = true
0.00.058.279 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.279 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.974 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.412 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.422 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.440 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.449 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.451 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.451 I llama_new_context_with_model: graph nodes  = 967
0.00.088.452 I llama_new_context_with_model: graph splits = 2
0.00.088.454 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.901 I main: llama threadpool init, n_threads = 4
0.00.716.945 I 
0.00.716.974 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.975 I 
0.00.717.190 I sampler seed: 1234
0.00.717.196 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.234 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.236 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.236 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.544.475 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.01.544.476 I llama_perf_context_print:        load time =     706.91 ms
0.01.544.476 I llama_perf_context_print: prompt eval time =      42.23 ms /     7 tokens (    6.03 ms per token,   165.77 tokens per second)
0.01.544.477 I llama_perf_context_print:        eval time =     782.01 ms /    63 runs   (   12.41 ms per token,    80.56 tokens per second)
0.01.544.477 I llama_perf_context_print:       total time =     827.58 ms /    70 tokens
0.01.544.716 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.110s
sys	0m0.163s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.751 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.249 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.254 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.255 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.256 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.256 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.257 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.260 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.260 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.261 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.261 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.262 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.263 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.264 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.265 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.265 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.673 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.674 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.674 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.674 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.675 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.675 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.675 I llama_model_loader: - type  f32:  194 tensors
0.00.023.676 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.676 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.676 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.646 I llm_load_vocab: special tokens cache size = 25
0.00.049.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.604 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.604 I llm_load_print_meta: arch             = gptneox
0.00.049.605 I llm_load_print_meta: vocab type       = BPE
0.00.049.605 I llm_load_print_meta: n_vocab          = 50304
0.00.049.605 I llm_load_print_meta: n_merges         = 50009
0.00.049.605 I llm_load_print_meta: vocab_only       = 0
0.00.049.605 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.606 I llm_load_print_meta: n_embd           = 2048
0.00.049.606 I llm_load_print_meta: n_layer          = 24
0.00.049.609 I llm_load_print_meta: n_head           = 16
0.00.049.609 I llm_load_print_meta: n_head_kv        = 16
0.00.049.610 I llm_load_print_meta: n_rot            = 32
0.00.049.610 I llm_load_print_meta: n_swa            = 0
0.00.049.610 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.610 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.612 I llm_load_print_meta: n_gqa            = 1
0.00.049.613 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.614 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.614 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.615 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.615 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.615 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.615 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.616 I llm_load_print_meta: n_ff             = 8192
0.00.049.616 I llm_load_print_meta: n_expert         = 0
0.00.049.616 I llm_load_print_meta: n_expert_used    = 0
0.00.049.616 I llm_load_print_meta: causal attn      = 1
0.00.049.616 I llm_load_print_meta: pooling type     = 0
0.00.049.617 I llm_load_print_meta: rope type        = 2
0.00.049.617 I llm_load_print_meta: rope scaling     = linear
0.00.049.617 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.623 I llm_load_print_meta: freq_scale_train = 1
0.00.049.624 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.624 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.624 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.624 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.625 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.626 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.626 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.627 I llm_load_print_meta: model type       = 1.4B
0.00.049.627 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.628 I llm_load_print_meta: model params     = 1.41 B
0.00.049.629 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.629 I llm_load_print_meta: general.name     = 1.4B
0.00.049.629 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.629 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.630 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.630 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.630 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.631 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.631 I llm_load_print_meta: max token length = 1024
0.00.051.333 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.334 I llm_load_tensors: offloading output layer to GPU
0.00.051.334 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.339 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.340 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.640 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.641 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.641 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.641 I llama_new_context_with_model: n_batch       = 2048
0.00.051.641 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.641 I llama_new_context_with_model: flash_attn    = 0
0.00.051.642 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.642 I llama_new_context_with_model: freq_scale    = 1
0.00.051.642 I ggml_metal_init: allocating
0.00.051.645 I ggml_metal_init: found device: Apple M4
0.00.051.647 I ggml_metal_init: picking default device: Apple M4
0.00.052.217 I ggml_metal_init: using embedded metal library
0.00.054.532 I ggml_metal_init: GPU name:   Apple M4
0.00.054.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.534 I ggml_metal_init: simdgroup reduction   = true
0.00.054.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.535 I ggml_metal_init: has bfloat            = true
0.00.054.535 I ggml_metal_init: use bfloat            = true
0.00.054.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.035 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.491 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.500 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.523 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.410 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.411 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.411 I llama_new_context_with_model: graph nodes  = 967
0.00.085.411 I llama_new_context_with_model: graph splits = 2
0.00.085.414 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.555 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.556 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.438.999 I main: llama threadpool init, n_threads = 4
0.00.439.041 I 
0.00.439.093 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.095 I 
0.00.439.339 I sampler seed: 1234
0.00.439.345 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.439.379 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.439.380 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.439.380 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.116.562 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63963.96 tokens per second)
0.01.116.563 I llama_perf_context_print:        load time =     430.24 ms
0.01.116.563 I llama_perf_context_print: prompt eval time =      35.82 ms /     7 tokens (    5.12 ms per token,   195.42 tokens per second)
0.01.116.564 I llama_perf_context_print:        eval time =     638.54 ms /    63 runs   (   10.14 ms per token,    98.66 tokens per second)
0.01.116.567 I llama_perf_context_print:       total time =     677.57 ms /    70 tokens
0.01.116.764 I ggml_metal_free: deallocating

real	0m1.132s
user	0m0.109s
sys	0m0.111s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.011.604 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.906 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.911 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.917 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.919 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.920 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.920 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.921 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.921 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.921 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.922 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.922 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.922 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.923 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.924 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.925 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.925 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.872 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.702 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.702 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.703 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.703 I llama_model_loader: - type  f32:  194 tensors
0.00.027.704 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.704 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.704 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.704 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.647 I llm_load_vocab: special tokens cache size = 25
0.00.054.585 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.588 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.588 I llm_load_print_meta: arch             = gptneox
0.00.054.588 I llm_load_print_meta: vocab type       = BPE
0.00.054.589 I llm_load_print_meta: n_vocab          = 50304
0.00.054.589 I llm_load_print_meta: n_merges         = 50009
0.00.054.589 I llm_load_print_meta: vocab_only       = 0
0.00.054.589 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.589 I llm_load_print_meta: n_embd           = 2048
0.00.054.589 I llm_load_print_meta: n_layer          = 24
0.00.054.592 I llm_load_print_meta: n_head           = 16
0.00.054.593 I llm_load_print_meta: n_head_kv        = 16
0.00.054.593 I llm_load_print_meta: n_rot            = 32
0.00.054.593 I llm_load_print_meta: n_swa            = 0
0.00.054.594 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.594 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.595 I llm_load_print_meta: n_gqa            = 1
0.00.054.595 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.596 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.596 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.597 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.597 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.597 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.597 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.598 I llm_load_print_meta: n_ff             = 8192
0.00.054.598 I llm_load_print_meta: n_expert         = 0
0.00.054.598 I llm_load_print_meta: n_expert_used    = 0
0.00.054.599 I llm_load_print_meta: causal attn      = 1
0.00.054.599 I llm_load_print_meta: pooling type     = 0
0.00.054.599 I llm_load_print_meta: rope type        = 2
0.00.054.599 I llm_load_print_meta: rope scaling     = linear
0.00.054.599 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.600 I llm_load_print_meta: freq_scale_train = 1
0.00.054.600 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.600 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.600 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.601 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.601 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.601 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.601 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.601 I llm_load_print_meta: model type       = 1.4B
0.00.054.602 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.054.602 I llm_load_print_meta: model params     = 1.41 B
0.00.054.603 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.054.603 I llm_load_print_meta: general.name     = 1.4B
0.00.054.603 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.603 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.604 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.604 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.604 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.604 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.605 I llm_load_print_meta: max token length = 1024
0.00.056.584 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.585 I llm_load_tensors: offloading output layer to GPU
0.00.056.585 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.596 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.056.597 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.057.016 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.016 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.016 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.017 I llama_new_context_with_model: n_batch       = 2048
0.00.057.017 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.017 I llama_new_context_with_model: flash_attn    = 0
0.00.057.017 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.018 I llama_new_context_with_model: freq_scale    = 1
0.00.057.018 I ggml_metal_init: allocating
0.00.057.023 I ggml_metal_init: found device: Apple M4
0.00.057.026 I ggml_metal_init: picking default device: Apple M4
0.00.057.587 I ggml_metal_init: using embedded metal library
0.00.059.933 I ggml_metal_init: GPU name:   Apple M4
0.00.059.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.935 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.935 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.936 I ggml_metal_init: simdgroup reduction   = true
0.00.059.936 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.936 I ggml_metal_init: has bfloat            = true
0.00.059.936 I ggml_metal_init: use bfloat            = true
0.00.059.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.510 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.134 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.140 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.159 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.120 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.121 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.122 I llama_new_context_with_model: graph nodes  = 967
0.00.090.122 I llama_new_context_with_model: graph splits = 2
0.00.090.124 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.242 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.243 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.531.413 I main: llama threadpool init, n_threads = 4
0.00.531.463 I 
0.00.531.512 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.531.513 I 
0.00.531.738 I sampler seed: 1234
0.00.531.744 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.531.778 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.531.779 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.531.779 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.274.144 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.274.145 I llama_perf_context_print:        load time =     519.80 ms
0.01.274.146 I llama_perf_context_print: prompt eval time =      40.47 ms /     7 tokens (    5.78 ms per token,   172.96 tokens per second)
0.01.274.146 I llama_perf_context_print:        eval time =     698.93 ms /    63 runs   (   11.09 ms per token,    90.14 tokens per second)
0.01.274.146 I llama_perf_context_print:       total time =     742.74 ms /    70 tokens
0.01.274.394 I ggml_metal_free: deallocating

real	0m1.292s
user	0m0.110s
sys	0m0.121s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.676 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.510 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.516 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.517 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.518 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.524 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.524 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.531 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.532 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.341 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.089 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.089 I llama_model_loader: - type  f32:  194 tensors
0.00.025.090 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.090 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.090 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.203 I llm_load_vocab: special tokens cache size = 25
0.00.051.307 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.309 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.310 I llm_load_print_meta: arch             = gptneox
0.00.051.310 I llm_load_print_meta: vocab type       = BPE
0.00.051.311 I llm_load_print_meta: n_vocab          = 50304
0.00.051.311 I llm_load_print_meta: n_merges         = 50009
0.00.051.311 I llm_load_print_meta: vocab_only       = 0
0.00.051.311 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.311 I llm_load_print_meta: n_embd           = 2048
0.00.051.312 I llm_load_print_meta: n_layer          = 24
0.00.051.314 I llm_load_print_meta: n_head           = 16
0.00.051.314 I llm_load_print_meta: n_head_kv        = 16
0.00.051.314 I llm_load_print_meta: n_rot            = 32
0.00.051.315 I llm_load_print_meta: n_swa            = 0
0.00.051.315 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.315 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.316 I llm_load_print_meta: n_gqa            = 1
0.00.051.317 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.317 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.318 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.318 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.318 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.320 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.320 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.321 I llm_load_print_meta: n_ff             = 8192
0.00.051.321 I llm_load_print_meta: n_expert         = 0
0.00.051.323 I llm_load_print_meta: n_expert_used    = 0
0.00.051.324 I llm_load_print_meta: causal attn      = 1
0.00.051.324 I llm_load_print_meta: pooling type     = 0
0.00.051.324 I llm_load_print_meta: rope type        = 2
0.00.051.325 I llm_load_print_meta: rope scaling     = linear
0.00.051.325 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.325 I llm_load_print_meta: freq_scale_train = 1
0.00.051.325 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.326 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.326 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.326 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.326 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.326 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.327 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.327 I llm_load_print_meta: model type       = 1.4B
0.00.051.327 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.328 I llm_load_print_meta: model params     = 1.41 B
0.00.051.329 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.330 I llm_load_print_meta: general.name     = 1.4B
0.00.051.330 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.330 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.330 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.330 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.332 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.332 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.332 I llm_load_print_meta: max token length = 1024
0.00.053.266 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.266 I llm_load_tensors: offloading output layer to GPU
0.00.053.266 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.277 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.278 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.621 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.621 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.621 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.622 I llama_new_context_with_model: n_batch       = 2048
0.00.053.622 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.622 I llama_new_context_with_model: flash_attn    = 0
0.00.053.622 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.623 I llama_new_context_with_model: freq_scale    = 1
0.00.053.623 I ggml_metal_init: allocating
0.00.053.629 I ggml_metal_init: found device: Apple M4
0.00.053.631 I ggml_metal_init: picking default device: Apple M4
0.00.054.234 I ggml_metal_init: using embedded metal library
0.00.056.573 I ggml_metal_init: GPU name:   Apple M4
0.00.056.575 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.575 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.575 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.577 I ggml_metal_init: simdgroup reduction   = true
0.00.056.577 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.578 I ggml_metal_init: has bfloat            = true
0.00.056.578 I ggml_metal_init: use bfloat            = true
0.00.056.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.001 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.025 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.035 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.056 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.916 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.918 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.918 I llama_new_context_with_model: graph nodes  = 967
0.00.085.918 I llama_new_context_with_model: graph splits = 2
0.00.085.921 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.067 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.315 I main: llama threadpool init, n_threads = 4
0.00.609.359 I 
0.00.609.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.386 I 
0.00.609.611 I sampler seed: 1234
0.00.609.615 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.645 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.646 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.646 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.373.941 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.373.942 I llama_perf_context_print:        load time =     600.63 ms
0.01.373.942 I llama_perf_context_print: prompt eval time =      51.04 ms /     7 tokens (    7.29 ms per token,   137.14 tokens per second)
0.01.373.943 I llama_perf_context_print:        eval time =     710.10 ms /    63 runs   (   11.27 ms per token,    88.72 tokens per second)
0.01.373.943 I llama_perf_context_print:       total time =     764.63 ms /    70 tokens
0.01.374.174 I ggml_metal_free: deallocating

real	0m1.390s
user	0m0.110s
sys	0m0.134s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.535 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.231 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.237 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.237 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.237 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.240 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.241 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.241 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.241 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.242 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.248 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.088 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.126 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.938 I llama_model_loader: - type  f32:  194 tensors
0.00.025.939 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.939 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.690 I llm_load_vocab: special tokens cache size = 25
0.00.052.636 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.639 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.639 I llm_load_print_meta: arch             = gptneox
0.00.052.640 I llm_load_print_meta: vocab type       = BPE
0.00.052.640 I llm_load_print_meta: n_vocab          = 50304
0.00.052.640 I llm_load_print_meta: n_merges         = 50009
0.00.052.640 I llm_load_print_meta: vocab_only       = 0
0.00.052.640 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.641 I llm_load_print_meta: n_embd           = 2048
0.00.052.641 I llm_load_print_meta: n_layer          = 24
0.00.052.644 I llm_load_print_meta: n_head           = 16
0.00.052.645 I llm_load_print_meta: n_head_kv        = 16
0.00.052.645 I llm_load_print_meta: n_rot            = 32
0.00.052.645 I llm_load_print_meta: n_swa            = 0
0.00.052.647 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.647 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.648 I llm_load_print_meta: n_gqa            = 1
0.00.052.648 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.649 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.650 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.650 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.650 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.650 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.650 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.651 I llm_load_print_meta: n_ff             = 8192
0.00.052.651 I llm_load_print_meta: n_expert         = 0
0.00.052.651 I llm_load_print_meta: n_expert_used    = 0
0.00.052.653 I llm_load_print_meta: causal attn      = 1
0.00.052.655 I llm_load_print_meta: pooling type     = 0
0.00.052.655 I llm_load_print_meta: rope type        = 2
0.00.052.655 I llm_load_print_meta: rope scaling     = linear
0.00.052.655 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.656 I llm_load_print_meta: freq_scale_train = 1
0.00.052.656 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.656 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.656 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.656 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.657 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.661 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.661 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.661 I llm_load_print_meta: model type       = 1.4B
0.00.052.662 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.662 I llm_load_print_meta: model params     = 1.41 B
0.00.052.663 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.663 I llm_load_print_meta: general.name     = 1.4B
0.00.052.664 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.664 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.664 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.665 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.665 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.665 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.666 I llm_load_print_meta: max token length = 1024
0.00.054.658 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.658 I llm_load_tensors: offloading output layer to GPU
0.00.054.658 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.669 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.670 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.034 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.034 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.034 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.035 I llama_new_context_with_model: n_batch       = 2048
0.00.055.035 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.035 I llama_new_context_with_model: flash_attn    = 0
0.00.055.035 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.036 I llama_new_context_with_model: freq_scale    = 1
0.00.055.036 I ggml_metal_init: allocating
0.00.055.041 I ggml_metal_init: found device: Apple M4
0.00.055.044 I ggml_metal_init: picking default device: Apple M4
0.00.055.628 I ggml_metal_init: using embedded metal library
0.00.057.951 I ggml_metal_init: GPU name:   Apple M4
0.00.057.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.955 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.955 I ggml_metal_init: simdgroup reduction   = true
0.00.057.955 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.955 I ggml_metal_init: has bfloat            = true
0.00.057.956 I ggml_metal_init: use bfloat            = true
0.00.057.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.318 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.488 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.504 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.531 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.611 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.612 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.612 I llama_new_context_with_model: graph nodes  = 967
0.00.088.612 I llama_new_context_with_model: graph splits = 2
0.00.088.615 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.758 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.759 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.156 I main: llama threadpool init, n_threads = 4
0.00.698.197 I 
0.00.698.228 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.229 I 
0.00.698.475 I sampler seed: 1234
0.00.698.480 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.698.496 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.698.496 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.698.496 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.556.446 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.01.556.447 I llama_perf_context_print:        load time =     688.62 ms
0.01.556.448 I llama_perf_context_print: prompt eval time =      58.40 ms /     7 tokens (    8.34 ms per token,   119.87 tokens per second)
0.01.556.448 I llama_perf_context_print:        eval time =     796.75 ms /    63 runs   (   12.65 ms per token,    79.07 tokens per second)
0.01.556.449 I llama_perf_context_print:       total time =     858.29 ms /    70 tokens
0.01.556.699 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.111s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.840 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.356 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.361 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.363 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.363 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.366 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.369 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.369 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.370 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.373 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.376 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.376 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.377 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.166 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.168 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.168 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.168 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.169 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.169 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.169 I llama_model_loader: - type  f32:  194 tensors
0.00.026.170 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.359 I llm_load_vocab: special tokens cache size = 25
0.00.052.277 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.280 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.280 I llm_load_print_meta: arch             = gptneox
0.00.052.281 I llm_load_print_meta: vocab type       = BPE
0.00.052.281 I llm_load_print_meta: n_vocab          = 50304
0.00.052.281 I llm_load_print_meta: n_merges         = 50009
0.00.052.281 I llm_load_print_meta: vocab_only       = 0
0.00.052.281 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.281 I llm_load_print_meta: n_embd           = 2048
0.00.052.282 I llm_load_print_meta: n_layer          = 24
0.00.052.285 I llm_load_print_meta: n_head           = 16
0.00.052.286 I llm_load_print_meta: n_head_kv        = 16
0.00.052.286 I llm_load_print_meta: n_rot            = 32
0.00.052.286 I llm_load_print_meta: n_swa            = 0
0.00.052.287 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.289 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.290 I llm_load_print_meta: n_gqa            = 1
0.00.052.291 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.291 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.292 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.292 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.292 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.293 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.293 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.293 I llm_load_print_meta: n_ff             = 8192
0.00.052.294 I llm_load_print_meta: n_expert         = 0
0.00.052.294 I llm_load_print_meta: n_expert_used    = 0
0.00.052.294 I llm_load_print_meta: causal attn      = 1
0.00.052.296 I llm_load_print_meta: pooling type     = 0
0.00.052.297 I llm_load_print_meta: rope type        = 2
0.00.052.298 I llm_load_print_meta: rope scaling     = linear
0.00.052.298 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.298 I llm_load_print_meta: freq_scale_train = 1
0.00.052.298 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.299 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.299 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.299 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.299 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.299 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.299 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.300 I llm_load_print_meta: model type       = 1.4B
0.00.052.300 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.304 I llm_load_print_meta: model params     = 1.41 B
0.00.052.305 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.305 I llm_load_print_meta: general.name     = 1.4B
0.00.052.305 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.305 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.305 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.305 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.306 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.306 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.306 I llm_load_print_meta: max token length = 1024
0.00.054.352 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.353 I llm_load_tensors: offloading output layer to GPU
0.00.054.353 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.363 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.364 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.717 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.718 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.718 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.718 I llama_new_context_with_model: n_batch       = 2048
0.00.054.718 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.718 I llama_new_context_with_model: flash_attn    = 0
0.00.054.719 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.719 I llama_new_context_with_model: freq_scale    = 1
0.00.054.720 I ggml_metal_init: allocating
0.00.054.726 I ggml_metal_init: found device: Apple M4
0.00.054.728 I ggml_metal_init: picking default device: Apple M4
0.00.055.351 I ggml_metal_init: using embedded metal library
0.00.057.776 I ggml_metal_init: GPU name:   Apple M4
0.00.057.777 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.778 I ggml_metal_init: simdgroup reduction   = true
0.00.057.780 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.780 I ggml_metal_init: has bfloat            = true
0.00.057.780 I ggml_metal_init: use bfloat            = true
0.00.057.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.328 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.416 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.422 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.448 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.462 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.464 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.465 I llama_new_context_with_model: graph nodes  = 967
0.00.089.465 I llama_new_context_with_model: graph splits = 2
0.00.089.467 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.612 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.866 I main: llama threadpool init, n_threads = 4
0.00.760.907 I 
0.00.760.944 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.945 I 
0.00.761.170 I sampler seed: 1234
0.00.761.175 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.218 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.220 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.220 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.632.161 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.01.632.162 I llama_perf_context_print:        load time =     751.02 ms
0.01.632.162 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.50 tokens per second)
0.01.632.163 I llama_perf_context_print:        eval time =     813.54 ms /    63 runs   (   12.91 ms per token,    77.44 tokens per second)
0.01.632.163 I llama_perf_context_print:       total time =     871.30 ms /    70 tokens
0.01.632.397 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.112s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.786 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.028.718 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.243 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.251 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.254 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.255 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.256 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.257 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.259 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.259 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.260 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.261 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.261 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.262 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.263 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.266 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.266 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.713 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.717 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.441 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.442 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.443 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.443 I llama_model_loader: - type  f32:  194 tensors
0.00.060.444 I llama_model_loader: - type  f16:   98 tensors
0.00.088.451 I llm_load_vocab: special tokens cache size = 25
0.00.095.211 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.214 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.214 I llm_load_print_meta: arch             = gptneox
0.00.095.215 I llm_load_print_meta: vocab type       = BPE
0.00.095.215 I llm_load_print_meta: n_vocab          = 50304
0.00.095.215 I llm_load_print_meta: n_merges         = 50009
0.00.095.215 I llm_load_print_meta: vocab_only       = 0
0.00.095.215 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.216 I llm_load_print_meta: n_embd           = 2048
0.00.095.216 I llm_load_print_meta: n_layer          = 24
0.00.095.218 I llm_load_print_meta: n_head           = 16
0.00.095.219 I llm_load_print_meta: n_head_kv        = 16
0.00.095.219 I llm_load_print_meta: n_rot            = 32
0.00.095.220 I llm_load_print_meta: n_swa            = 0
0.00.095.220 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.220 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.220 I llm_load_print_meta: n_gqa            = 1
0.00.095.221 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.222 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.222 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.222 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.223 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.223 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.223 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.223 I llm_load_print_meta: n_ff             = 8192
0.00.095.223 I llm_load_print_meta: n_expert         = 0
0.00.095.224 I llm_load_print_meta: n_expert_used    = 0
0.00.095.224 I llm_load_print_meta: causal attn      = 1
0.00.095.224 I llm_load_print_meta: pooling type     = 0
0.00.095.224 I llm_load_print_meta: rope type        = 2
0.00.095.224 I llm_load_print_meta: rope scaling     = linear
0.00.095.226 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.226 I llm_load_print_meta: freq_scale_train = 1
0.00.095.227 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.227 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.227 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.227 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.227 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.227 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.227 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.228 I llm_load_print_meta: model type       = 1.4B
0.00.095.228 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.229 I llm_load_print_meta: model params     = 1.41 B
0.00.095.229 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.229 I llm_load_print_meta: general.name     = 1.4B
0.00.095.230 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.230 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.230 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.230 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.230 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.095.231 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.231 I llm_load_print_meta: max token length = 1024
0.00.100.608 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.608 I llm_load_tensors: offloading output layer to GPU
0.00.100.608 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.619 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.620 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.100.937 I llama_new_context_with_model: n_seq_max     = 1
0.00.100.938 I llama_new_context_with_model: n_ctx         = 128
0.00.100.938 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.100.938 I llama_new_context_with_model: n_batch       = 128
0.00.100.938 I llama_new_context_with_model: n_ubatch      = 128
0.00.100.938 I llama_new_context_with_model: flash_attn    = 0
0.00.100.939 I llama_new_context_with_model: freq_base     = 10000.0
0.00.100.939 I llama_new_context_with_model: freq_scale    = 1
0.00.100.939 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.100.940 I ggml_metal_init: allocating
0.00.100.943 I ggml_metal_init: found device: Apple M4
0.00.100.945 I ggml_metal_init: picking default device: Apple M4
0.00.101.559 I ggml_metal_init: using embedded metal library
0.00.104.397 I ggml_metal_init: GPU name:   Apple M4
0.00.104.399 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.104.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.104.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.104.400 I ggml_metal_init: simdgroup reduction   = true
0.00.104.400 I ggml_metal_init: simdgroup matrix mul. = true
0.00.104.400 I ggml_metal_init: has bfloat            = true
0.00.104.400 I ggml_metal_init: use bfloat            = true
0.00.104.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.104.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.115.277 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.116.661 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.116.665 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.116.682 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.602 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.117.604 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.117.604 I llama_new_context_with_model: graph nodes  = 967
0.00.117.604 I llama_new_context_with_model: graph splits = 2
0.00.117.605 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.117.605 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.410.860 I 
0.01.410.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.410.935 I perplexity: tokenizing the input ..
0.01.424.308 I perplexity: tokenization took 13.364 ms
0.01.424.314 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.560.552 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.562.071 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.562.116 I llama_perf_context_print:        load time =    1382.13 ms
0.01.562.117 I llama_perf_context_print: prompt eval time =     135.35 ms /   128 tokens (    1.06 ms per token,   945.70 tokens per second)
0.01.562.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.562.118 I llama_perf_context_print:       total time =     151.26 ms /   129 tokens
0.01.562.853 I ggml_metal_free: deallocating

real	0m1.754s
user	0m0.122s
sys	0m0.238s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.332 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.513 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.378 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.385 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.389 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.390 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.392 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.393 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.393 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.394 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.394 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.395 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.398 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.148 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.571 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.763 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.763 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.764 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.765 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.765 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.767 I llama_model_loader: - type  f32:  194 tensors
0.00.036.767 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.306 I llm_load_vocab: special tokens cache size = 25
0.00.068.575 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.578 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.579 I llm_load_print_meta: arch             = gptneox
0.00.068.579 I llm_load_print_meta: vocab type       = BPE
0.00.068.579 I llm_load_print_meta: n_vocab          = 50304
0.00.068.579 I llm_load_print_meta: n_merges         = 50009
0.00.068.579 I llm_load_print_meta: vocab_only       = 0
0.00.068.580 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.580 I llm_load_print_meta: n_embd           = 2048
0.00.068.580 I llm_load_print_meta: n_layer          = 24
0.00.068.584 I llm_load_print_meta: n_head           = 16
0.00.068.584 I llm_load_print_meta: n_head_kv        = 16
0.00.068.584 I llm_load_print_meta: n_rot            = 32
0.00.068.586 I llm_load_print_meta: n_swa            = 0
0.00.068.587 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.587 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.587 I llm_load_print_meta: n_gqa            = 1
0.00.068.588 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.589 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.589 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.589 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.589 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.590 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.590 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.590 I llm_load_print_meta: n_ff             = 8192
0.00.068.591 I llm_load_print_meta: n_expert         = 0
0.00.068.591 I llm_load_print_meta: n_expert_used    = 0
0.00.068.591 I llm_load_print_meta: causal attn      = 1
0.00.068.591 I llm_load_print_meta: pooling type     = 0
0.00.068.591 I llm_load_print_meta: rope type        = 2
0.00.068.591 I llm_load_print_meta: rope scaling     = linear
0.00.068.592 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.593 I llm_load_print_meta: freq_scale_train = 1
0.00.068.593 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.594 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.594 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.594 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.594 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.594 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.594 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.594 I llm_load_print_meta: model type       = 1.4B
0.00.068.595 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.595 I llm_load_print_meta: model params     = 1.41 B
0.00.068.595 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.596 I llm_load_print_meta: general.name     = 1.4B
0.00.068.596 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.596 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.596 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.596 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.596 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.597 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.597 I llm_load_print_meta: max token length = 1024
0.00.071.030 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.031 I llm_load_tensors: offloading output layer to GPU
0.00.071.031 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.042 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.043 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.385 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.386 I llama_new_context_with_model: n_ctx         = 128
0.00.071.386 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.071.386 I llama_new_context_with_model: n_batch       = 128
0.00.071.387 I llama_new_context_with_model: n_ubatch      = 128
0.00.071.387 I llama_new_context_with_model: flash_attn    = 0
0.00.071.387 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.387 I llama_new_context_with_model: freq_scale    = 1
0.00.071.388 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.388 I ggml_metal_init: allocating
0.00.071.391 I ggml_metal_init: found device: Apple M4
0.00.071.393 I ggml_metal_init: picking default device: Apple M4
0.00.072.080 I ggml_metal_init: using embedded metal library
0.00.075.129 I ggml_metal_init: GPU name:   Apple M4
0.00.075.131 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.131 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.132 I ggml_metal_init: simdgroup reduction   = true
0.00.075.132 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.132 I ggml_metal_init: has bfloat            = true
0.00.075.132 I ggml_metal_init: use bfloat            = true
0.00.075.133 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.133 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.614 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.153 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.087.156 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.087.183 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.356 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.088.357 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.088.357 I llama_new_context_with_model: graph nodes  = 967
0.00.088.357 I llama_new_context_with_model: graph splits = 2
0.00.088.359 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.088.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.856.205 I 
0.00.856.261 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.856.291 I perplexity: tokenizing the input ..
0.00.864.438 I perplexity: tokenization took 8.145 ms
0.00.864.441 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.989.099 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.990.279 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.990.310 I llama_perf_context_print:        load time =     842.69 ms
0.00.990.311 I llama_perf_context_print: prompt eval time =     124.43 ms /   128 tokens (    0.97 ms per token,  1028.68 tokens per second)
0.00.990.312 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.990.312 I llama_perf_context_print:       total time =     134.11 ms /   129 tokens
0.00.990.823 I ggml_metal_free: deallocating

real	0m1.011s
user	0m0.096s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.264 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.862 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.981 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.991 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.992 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.993 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.993 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.994 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.994 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.995 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.995 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.996 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.996 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.996 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.997 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.997 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.999 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.999 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.000 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.676 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.318 I llama_model_loader: - type  f32:  194 tensors
0.00.025.319 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.319 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.221 I llm_load_vocab: special tokens cache size = 25
0.00.051.083 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.086 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.086 I llm_load_print_meta: arch             = gptneox
0.00.051.087 I llm_load_print_meta: vocab type       = BPE
0.00.051.087 I llm_load_print_meta: n_vocab          = 50304
0.00.051.087 I llm_load_print_meta: n_merges         = 50009
0.00.051.087 I llm_load_print_meta: vocab_only       = 0
0.00.051.087 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.088 I llm_load_print_meta: n_embd           = 2048
0.00.051.088 I llm_load_print_meta: n_layer          = 24
0.00.051.091 I llm_load_print_meta: n_head           = 16
0.00.051.092 I llm_load_print_meta: n_head_kv        = 16
0.00.051.092 I llm_load_print_meta: n_rot            = 32
0.00.051.092 I llm_load_print_meta: n_swa            = 0
0.00.051.093 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.093 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.093 I llm_load_print_meta: n_gqa            = 1
0.00.051.094 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.095 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.095 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.096 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.096 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.096 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.096 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.097 I llm_load_print_meta: n_ff             = 8192
0.00.051.097 I llm_load_print_meta: n_expert         = 0
0.00.051.097 I llm_load_print_meta: n_expert_used    = 0
0.00.051.097 I llm_load_print_meta: causal attn      = 1
0.00.051.100 I llm_load_print_meta: pooling type     = 0
0.00.051.100 I llm_load_print_meta: rope type        = 2
0.00.051.101 I llm_load_print_meta: rope scaling     = linear
0.00.051.101 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.101 I llm_load_print_meta: freq_scale_train = 1
0.00.051.101 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.102 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.102 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.102 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.102 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.102 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.102 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.103 I llm_load_print_meta: model type       = 1.4B
0.00.051.103 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.103 I llm_load_print_meta: model params     = 1.41 B
0.00.051.104 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.104 I llm_load_print_meta: general.name     = 1.4B
0.00.051.104 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.104 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.105 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.107 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.107 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.107 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.108 I llm_load_print_meta: max token length = 1024
0.00.052.923 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.923 I llm_load_tensors: offloading output layer to GPU
0.00.052.923 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.934 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.935 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.313 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.314 I llama_new_context_with_model: n_ctx         = 128
0.00.053.314 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.314 I llama_new_context_with_model: n_batch       = 128
0.00.053.314 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.315 I llama_new_context_with_model: flash_attn    = 0
0.00.053.315 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.315 I llama_new_context_with_model: freq_scale    = 1
0.00.053.316 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.316 I ggml_metal_init: allocating
0.00.053.323 I ggml_metal_init: found device: Apple M4
0.00.053.325 I ggml_metal_init: picking default device: Apple M4
0.00.053.899 I ggml_metal_init: using embedded metal library
0.00.056.265 I ggml_metal_init: GPU name:   Apple M4
0.00.056.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.268 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.268 I ggml_metal_init: simdgroup reduction   = true
0.00.056.268 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.268 I ggml_metal_init: has bfloat            = true
0.00.056.268 I ggml_metal_init: use bfloat            = true
0.00.056.269 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.153 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.391 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.393 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.407 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.314 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.315 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.315 I llama_new_context_with_model: graph nodes  = 967
0.00.068.315 I llama_new_context_with_model: graph splits = 2
0.00.068.316 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.711 I 
0.00.597.784 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.818 I perplexity: tokenizing the input ..
0.00.605.599 I perplexity: tokenization took 7.78 ms
0.00.605.608 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.728.705 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.730.035 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.730.053 I llama_perf_context_print:        load time =     587.84 ms
0.00.730.055 I llama_perf_context_print: prompt eval time =     122.87 ms /   128 tokens (    0.96 ms per token,  1041.79 tokens per second)
0.00.730.055 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.730.056 I llama_perf_context_print:       total time =     132.35 ms /   129 tokens
0.00.730.394 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.077s
sys	0m0.090s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.166 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.993 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.997 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.003 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.004 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.005 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.006 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.006 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.007 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.007 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.009 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.009 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.009 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.695 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.390 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.391 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.392 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.392 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.392 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.393 I llama_model_loader: - type  f32:  194 tensors
0.00.024.393 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.394 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.154 I llm_load_vocab: special tokens cache size = 25
0.00.049.843 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.846 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.846 I llm_load_print_meta: arch             = gptneox
0.00.049.846 I llm_load_print_meta: vocab type       = BPE
0.00.049.847 I llm_load_print_meta: n_vocab          = 50304
0.00.049.847 I llm_load_print_meta: n_merges         = 50009
0.00.049.847 I llm_load_print_meta: vocab_only       = 0
0.00.049.847 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.847 I llm_load_print_meta: n_embd           = 2048
0.00.049.847 I llm_load_print_meta: n_layer          = 24
0.00.049.850 I llm_load_print_meta: n_head           = 16
0.00.049.851 I llm_load_print_meta: n_head_kv        = 16
0.00.049.851 I llm_load_print_meta: n_rot            = 32
0.00.049.851 I llm_load_print_meta: n_swa            = 0
0.00.049.852 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.852 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.852 I llm_load_print_meta: n_gqa            = 1
0.00.049.853 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.854 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.854 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.855 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.855 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.855 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.855 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.856 I llm_load_print_meta: n_ff             = 8192
0.00.049.856 I llm_load_print_meta: n_expert         = 0
0.00.049.856 I llm_load_print_meta: n_expert_used    = 0
0.00.049.856 I llm_load_print_meta: causal attn      = 1
0.00.049.857 I llm_load_print_meta: pooling type     = 0
0.00.049.857 I llm_load_print_meta: rope type        = 2
0.00.049.859 I llm_load_print_meta: rope scaling     = linear
0.00.049.859 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.859 I llm_load_print_meta: freq_scale_train = 1
0.00.049.859 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.860 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.860 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.860 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.860 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.860 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.860 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.861 I llm_load_print_meta: model type       = 1.4B
0.00.049.861 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.861 I llm_load_print_meta: model params     = 1.41 B
0.00.049.862 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.862 I llm_load_print_meta: general.name     = 1.4B
0.00.049.862 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.862 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.863 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.863 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.864 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.868 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.868 I llm_load_print_meta: max token length = 1024
0.00.051.552 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.552 I llm_load_tensors: offloading output layer to GPU
0.00.051.552 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.558 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.558 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.906 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.907 I llama_new_context_with_model: n_ctx         = 128
0.00.051.907 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.907 I llama_new_context_with_model: n_batch       = 128
0.00.051.907 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.907 I llama_new_context_with_model: flash_attn    = 0
0.00.051.908 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.908 I llama_new_context_with_model: freq_scale    = 1
0.00.051.908 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.909 I ggml_metal_init: allocating
0.00.051.912 I ggml_metal_init: found device: Apple M4
0.00.051.914 I ggml_metal_init: picking default device: Apple M4
0.00.052.472 I ggml_metal_init: using embedded metal library
0.00.054.803 I ggml_metal_init: GPU name:   Apple M4
0.00.054.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.805 I ggml_metal_init: simdgroup reduction   = true
0.00.054.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.805 I ggml_metal_init: has bfloat            = true
0.00.054.806 I ggml_metal_init: use bfloat            = true
0.00.054.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.807 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.169 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.518 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.522 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.538 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.433 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.434 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.435 I llama_new_context_with_model: graph nodes  = 967
0.00.066.435 I llama_new_context_with_model: graph splits = 2
0.00.066.436 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.436 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.372 I 
0.00.663.413 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.424 I perplexity: tokenizing the input ..
0.00.671.490 I perplexity: tokenization took 8.064 ms
0.00.671.493 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.566 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.795.732 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.795.756 I llama_perf_context_print:        load time =     654.20 ms
0.00.795.757 I llama_perf_context_print: prompt eval time =     122.84 ms /   128 tokens (    0.96 ms per token,  1042.03 tokens per second)
0.00.795.758 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.758 I llama_perf_context_print:       total time =     132.39 ms /   129 tokens
0.00.796.254 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.078s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.926 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.796 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.803 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.803 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.804 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.804 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.804 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.807 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.807 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.808 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.814 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.646 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.406 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.407 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.408 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.408 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.409 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.409 I llama_model_loader: - type  f32:  194 tensors
0.00.025.409 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.410 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.994 I llm_load_vocab: special tokens cache size = 25
0.00.052.004 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.006 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.007 I llm_load_print_meta: arch             = gptneox
0.00.052.007 I llm_load_print_meta: vocab type       = BPE
0.00.052.007 I llm_load_print_meta: n_vocab          = 50304
0.00.052.007 I llm_load_print_meta: n_merges         = 50009
0.00.052.008 I llm_load_print_meta: vocab_only       = 0
0.00.052.008 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.008 I llm_load_print_meta: n_embd           = 2048
0.00.052.008 I llm_load_print_meta: n_layer          = 24
0.00.052.011 I llm_load_print_meta: n_head           = 16
0.00.052.012 I llm_load_print_meta: n_head_kv        = 16
0.00.052.012 I llm_load_print_meta: n_rot            = 32
0.00.052.012 I llm_load_print_meta: n_swa            = 0
0.00.052.012 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.012 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.013 I llm_load_print_meta: n_gqa            = 1
0.00.052.014 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.014 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.015 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.016 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.016 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.016 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.016 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.017 I llm_load_print_meta: n_ff             = 8192
0.00.052.017 I llm_load_print_meta: n_expert         = 0
0.00.052.017 I llm_load_print_meta: n_expert_used    = 0
0.00.052.017 I llm_load_print_meta: causal attn      = 1
0.00.052.017 I llm_load_print_meta: pooling type     = 0
0.00.052.018 I llm_load_print_meta: rope type        = 2
0.00.052.018 I llm_load_print_meta: rope scaling     = linear
0.00.052.018 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.019 I llm_load_print_meta: freq_scale_train = 1
0.00.052.019 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.019 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.021 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.021 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.021 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.022 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.022 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.022 I llm_load_print_meta: model type       = 1.4B
0.00.052.022 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.023 I llm_load_print_meta: model params     = 1.41 B
0.00.052.023 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.024 I llm_load_print_meta: general.name     = 1.4B
0.00.052.024 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.024 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.024 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.028 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.028 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.029 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.029 I llm_load_print_meta: max token length = 1024
0.00.053.986 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.986 I llm_load_tensors: offloading output layer to GPU
0.00.053.987 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.997 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.998 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.323 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.324 I llama_new_context_with_model: n_ctx         = 128
0.00.054.324 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.324 I llama_new_context_with_model: n_batch       = 128
0.00.054.324 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.324 I llama_new_context_with_model: flash_attn    = 0
0.00.054.325 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.325 I llama_new_context_with_model: freq_scale    = 1
0.00.054.326 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.326 I ggml_metal_init: allocating
0.00.054.332 I ggml_metal_init: found device: Apple M4
0.00.054.334 I ggml_metal_init: picking default device: Apple M4
0.00.054.899 I ggml_metal_init: using embedded metal library
0.00.057.231 I ggml_metal_init: GPU name:   Apple M4
0.00.057.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.233 I ggml_metal_init: simdgroup reduction   = true
0.00.057.233 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.233 I ggml_metal_init: has bfloat            = true
0.00.057.233 I ggml_metal_init: use bfloat            = true
0.00.057.234 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.542 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.835 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.840 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.855 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.715 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.716 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.716 I llama_new_context_with_model: graph nodes  = 967
0.00.068.717 I llama_new_context_with_model: graph splits = 2
0.00.068.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.718 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.544.372 I 
0.00.544.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.544.414 I perplexity: tokenizing the input ..
0.00.552.667 I perplexity: tokenization took 8.251 ms
0.00.552.671 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.687.412 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.688.663 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.688.689 I llama_perf_context_print:        load time =     534.44 ms
0.00.688.690 I llama_perf_context_print: prompt eval time =     134.51 ms /   128 tokens (    1.05 ms per token,   951.60 tokens per second)
0.00.688.691 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.688.691 I llama_perf_context_print:       total time =     144.32 ms /   129 tokens
0.00.689.240 I ggml_metal_free: deallocating

real	0m0.704s
user	0m0.079s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.890 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.572 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.582 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.583 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.583 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.583 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.587 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.587 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.179 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.147 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.727 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.728 I llama_model_loader: - type  f32:  194 tensors
0.00.023.728 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.728 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.497 I llm_load_vocab: special tokens cache size = 25
0.00.049.501 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.503 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.504 I llm_load_print_meta: arch             = gptneox
0.00.049.504 I llm_load_print_meta: vocab type       = BPE
0.00.049.504 I llm_load_print_meta: n_vocab          = 50304
0.00.049.504 I llm_load_print_meta: n_merges         = 50009
0.00.049.505 I llm_load_print_meta: vocab_only       = 0
0.00.049.505 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.505 I llm_load_print_meta: n_embd           = 2048
0.00.049.505 I llm_load_print_meta: n_layer          = 24
0.00.049.508 I llm_load_print_meta: n_head           = 16
0.00.049.508 I llm_load_print_meta: n_head_kv        = 16
0.00.049.509 I llm_load_print_meta: n_rot            = 32
0.00.049.510 I llm_load_print_meta: n_swa            = 0
0.00.049.510 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.511 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.511 I llm_load_print_meta: n_gqa            = 1
0.00.049.512 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.513 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.513 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.514 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.514 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.514 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.514 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.515 I llm_load_print_meta: n_ff             = 8192
0.00.049.515 I llm_load_print_meta: n_expert         = 0
0.00.049.515 I llm_load_print_meta: n_expert_used    = 0
0.00.049.515 I llm_load_print_meta: causal attn      = 1
0.00.049.515 I llm_load_print_meta: pooling type     = 0
0.00.049.517 I llm_load_print_meta: rope type        = 2
0.00.049.518 I llm_load_print_meta: rope scaling     = linear
0.00.049.518 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.518 I llm_load_print_meta: freq_scale_train = 1
0.00.049.519 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.519 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.519 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.519 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.519 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.519 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.519 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.520 I llm_load_print_meta: model type       = 1.4B
0.00.049.520 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.520 I llm_load_print_meta: model params     = 1.41 B
0.00.049.524 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.524 I llm_load_print_meta: general.name     = 1.4B
0.00.049.526 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.526 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.526 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.526 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.527 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.527 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.527 I llm_load_print_meta: max token length = 1024
0.00.051.489 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.490 I llm_load_tensors: offloading output layer to GPU
0.00.051.490 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.501 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.502 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.834 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.835 I llama_new_context_with_model: n_ctx         = 128
0.00.051.835 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.835 I llama_new_context_with_model: n_batch       = 128
0.00.051.835 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.835 I llama_new_context_with_model: flash_attn    = 0
0.00.051.836 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.836 I llama_new_context_with_model: freq_scale    = 1
0.00.051.836 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.837 I ggml_metal_init: allocating
0.00.051.840 I ggml_metal_init: found device: Apple M4
0.00.051.842 I ggml_metal_init: picking default device: Apple M4
0.00.052.392 I ggml_metal_init: using embedded metal library
0.00.054.703 I ggml_metal_init: GPU name:   Apple M4
0.00.054.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.705 I ggml_metal_init: simdgroup reduction   = true
0.00.054.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.706 I ggml_metal_init: has bfloat            = true
0.00.054.706 I ggml_metal_init: use bfloat            = true
0.00.054.706 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.995 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.348 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.352 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.369 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.229 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.230 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.230 I llama_new_context_with_model: graph nodes  = 967
0.00.066.231 I llama_new_context_with_model: graph splits = 2
0.00.066.231 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.232 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.801 I 
0.00.641.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.844 I perplexity: tokenizing the input ..
0.00.649.599 I perplexity: tokenization took 7.753 ms
0.00.649.603 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.440 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.785.596 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.785.623 I llama_perf_context_print:        load time =     632.90 ms
0.00.785.624 I llama_perf_context_print: prompt eval time =     134.61 ms /   128 tokens (    1.05 ms per token,   950.89 tokens per second)
0.00.785.624 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.625 I llama_perf_context_print:       total time =     143.83 ms /   129 tokens
0.00.786.121 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.077s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.971 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.466 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.467 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.470 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.471 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.471 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.472 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.474 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.179 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.210 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.882 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.883 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.884 I llama_model_loader: - type  f32:  194 tensors
0.00.024.884 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.884 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.885 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.664 I llm_load_vocab: special tokens cache size = 25
0.00.050.464 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.466 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.467 I llm_load_print_meta: arch             = gptneox
0.00.050.467 I llm_load_print_meta: vocab type       = BPE
0.00.050.467 I llm_load_print_meta: n_vocab          = 50304
0.00.050.468 I llm_load_print_meta: n_merges         = 50009
0.00.050.468 I llm_load_print_meta: vocab_only       = 0
0.00.050.468 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.468 I llm_load_print_meta: n_embd           = 2048
0.00.050.468 I llm_load_print_meta: n_layer          = 24
0.00.050.470 I llm_load_print_meta: n_head           = 16
0.00.050.471 I llm_load_print_meta: n_head_kv        = 16
0.00.050.471 I llm_load_print_meta: n_rot            = 32
0.00.050.472 I llm_load_print_meta: n_swa            = 0
0.00.050.472 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.472 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.473 I llm_load_print_meta: n_gqa            = 1
0.00.050.473 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.474 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.474 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.475 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.475 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.475 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.475 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.476 I llm_load_print_meta: n_ff             = 8192
0.00.050.476 I llm_load_print_meta: n_expert         = 0
0.00.050.476 I llm_load_print_meta: n_expert_used    = 0
0.00.050.476 I llm_load_print_meta: causal attn      = 1
0.00.050.476 I llm_load_print_meta: pooling type     = 0
0.00.050.476 I llm_load_print_meta: rope type        = 2
0.00.050.476 I llm_load_print_meta: rope scaling     = linear
0.00.050.477 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.477 I llm_load_print_meta: freq_scale_train = 1
0.00.050.477 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.478 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.478 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.478 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.478 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.478 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.480 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.481 I llm_load_print_meta: model type       = 1.4B
0.00.050.481 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.481 I llm_load_print_meta: model params     = 1.41 B
0.00.050.482 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.482 I llm_load_print_meta: general.name     = 1.4B
0.00.050.482 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.482 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.483 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.483 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.483 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.483 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.483 I llm_load_print_meta: max token length = 1024
0.00.052.341 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.342 I llm_load_tensors: offloading output layer to GPU
0.00.052.342 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.352 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.354 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.686 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.686 I llama_new_context_with_model: n_ctx         = 128
0.00.052.687 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.687 I llama_new_context_with_model: n_batch       = 128
0.00.052.687 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.687 I llama_new_context_with_model: flash_attn    = 0
0.00.052.688 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.688 I llama_new_context_with_model: freq_scale    = 1
0.00.052.688 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.689 I ggml_metal_init: allocating
0.00.052.692 I ggml_metal_init: found device: Apple M4
0.00.052.693 I ggml_metal_init: picking default device: Apple M4
0.00.053.260 I ggml_metal_init: using embedded metal library
0.00.055.563 I ggml_metal_init: GPU name:   Apple M4
0.00.055.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.566 I ggml_metal_init: simdgroup reduction   = true
0.00.055.566 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.566 I ggml_metal_init: has bfloat            = true
0.00.055.566 I ggml_metal_init: use bfloat            = true
0.00.055.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.080 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.326 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.341 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.209 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.210 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.210 I llama_new_context_with_model: graph nodes  = 967
0.00.067.211 I llama_new_context_with_model: graph splits = 2
0.00.067.212 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.385.367 I 
0.00.385.398 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.385.411 I perplexity: tokenizing the input ..
0.00.393.471 I perplexity: tokenization took 8.059 ms
0.00.393.474 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.525.813 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.527.269 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.527.288 I llama_perf_context_print:        load time =     375.39 ms
0.00.527.289 I llama_perf_context_print: prompt eval time =     132.09 ms /   128 tokens (    1.03 ms per token,   969.06 tokens per second)
0.00.527.290 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.527.290 I llama_perf_context_print:       total time =     141.93 ms /   129 tokens
0.00.527.628 I ggml_metal_free: deallocating

real	0m0.543s
user	0m0.078s
sys	0m0.071s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.046 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.825 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.831 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.833 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.834 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.834 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.834 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.834 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.836 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.836 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.837 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.837 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.837 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.838 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.840 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.840 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.590 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.527 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.528 I llama_model_loader: - type  f32:  194 tensors
0.00.024.528 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.528 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.529 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.529 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.689 I llm_load_vocab: special tokens cache size = 25
0.00.051.796 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.799 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.799 I llm_load_print_meta: arch             = gptneox
0.00.051.800 I llm_load_print_meta: vocab type       = BPE
0.00.051.800 I llm_load_print_meta: n_vocab          = 50304
0.00.051.800 I llm_load_print_meta: n_merges         = 50009
0.00.051.800 I llm_load_print_meta: vocab_only       = 0
0.00.051.801 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.801 I llm_load_print_meta: n_embd           = 2048
0.00.051.801 I llm_load_print_meta: n_layer          = 24
0.00.051.805 I llm_load_print_meta: n_head           = 16
0.00.051.805 I llm_load_print_meta: n_head_kv        = 16
0.00.051.805 I llm_load_print_meta: n_rot            = 32
0.00.051.806 I llm_load_print_meta: n_swa            = 0
0.00.051.806 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.806 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.807 I llm_load_print_meta: n_gqa            = 1
0.00.051.808 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.808 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.809 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.809 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.809 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.810 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.810 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.810 I llm_load_print_meta: n_ff             = 8192
0.00.051.811 I llm_load_print_meta: n_expert         = 0
0.00.051.811 I llm_load_print_meta: n_expert_used    = 0
0.00.051.811 I llm_load_print_meta: causal attn      = 1
0.00.051.811 I llm_load_print_meta: pooling type     = 0
0.00.051.811 I llm_load_print_meta: rope type        = 2
0.00.051.811 I llm_load_print_meta: rope scaling     = linear
0.00.051.812 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.812 I llm_load_print_meta: freq_scale_train = 1
0.00.051.812 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.812 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.812 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.813 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.813 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.813 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.813 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.813 I llm_load_print_meta: model type       = 1.4B
0.00.051.814 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.814 I llm_load_print_meta: model params     = 1.41 B
0.00.051.814 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.814 I llm_load_print_meta: general.name     = 1.4B
0.00.051.815 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.815 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.815 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.815 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.815 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.816 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.816 I llm_load_print_meta: max token length = 1024
0.00.053.783 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.783 I llm_load_tensors: offloading output layer to GPU
0.00.053.784 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.794 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.796 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.120 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.121 I llama_new_context_with_model: n_ctx         = 128
0.00.054.121 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.121 I llama_new_context_with_model: n_batch       = 128
0.00.054.122 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.122 I llama_new_context_with_model: flash_attn    = 0
0.00.054.122 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.123 I llama_new_context_with_model: freq_scale    = 1
0.00.054.123 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.123 I ggml_metal_init: allocating
0.00.054.127 I ggml_metal_init: found device: Apple M4
0.00.054.129 I ggml_metal_init: picking default device: Apple M4
0.00.054.732 I ggml_metal_init: using embedded metal library
0.00.057.268 I ggml_metal_init: GPU name:   Apple M4
0.00.057.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.270 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.271 I ggml_metal_init: simdgroup reduction   = true
0.00.057.271 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.271 I ggml_metal_init: has bfloat            = true
0.00.057.271 I ggml_metal_init: use bfloat            = true
0.00.057.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.480 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.047 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.052 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.067 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.042 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.043 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.043 I llama_new_context_with_model: graph nodes  = 967
0.00.070.044 I llama_new_context_with_model: graph splits = 2
0.00.070.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.045 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.461.279 I 
0.00.461.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.461.335 I perplexity: tokenizing the input ..
0.00.468.326 I perplexity: tokenization took 6.988 ms
0.00.468.329 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.599.351 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.600.680 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.600.704 I llama_perf_context_print:        load time =     452.22 ms
0.00.600.705 I llama_perf_context_print: prompt eval time =     130.80 ms /   128 tokens (    1.02 ms per token,   978.59 tokens per second)
0.00.600.705 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.600.706 I llama_perf_context_print:       total time =     139.43 ms /   129 tokens
0.00.601.040 I ggml_metal_free: deallocating

real	0m0.615s
user	0m0.080s
sys	0m0.063s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.763 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.617 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.624 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.625 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.627 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.627 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.627 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.628 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.628 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.628 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.631 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.547 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.546 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.550 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.551 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.552 I llama_model_loader: - type  f32:  194 tensors
0.00.028.552 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.552 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.553 I llama_model_loader: - type q6_K:   13 tensors
0.00.049.790 I llm_load_vocab: special tokens cache size = 25
0.00.055.798 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.802 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.802 I llm_load_print_meta: arch             = gptneox
0.00.055.803 I llm_load_print_meta: vocab type       = BPE
0.00.055.803 I llm_load_print_meta: n_vocab          = 50304
0.00.055.803 I llm_load_print_meta: n_merges         = 50009
0.00.055.803 I llm_load_print_meta: vocab_only       = 0
0.00.055.804 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.804 I llm_load_print_meta: n_embd           = 2048
0.00.055.804 I llm_load_print_meta: n_layer          = 24
0.00.055.808 I llm_load_print_meta: n_head           = 16
0.00.055.809 I llm_load_print_meta: n_head_kv        = 16
0.00.055.809 I llm_load_print_meta: n_rot            = 32
0.00.055.809 I llm_load_print_meta: n_swa            = 0
0.00.055.809 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.809 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.810 I llm_load_print_meta: n_gqa            = 1
0.00.055.811 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.812 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.812 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.813 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.813 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.813 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.813 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.814 I llm_load_print_meta: n_ff             = 8192
0.00.055.814 I llm_load_print_meta: n_expert         = 0
0.00.055.814 I llm_load_print_meta: n_expert_used    = 0
0.00.055.814 I llm_load_print_meta: causal attn      = 1
0.00.055.814 I llm_load_print_meta: pooling type     = 0
0.00.055.814 I llm_load_print_meta: rope type        = 2
0.00.055.816 I llm_load_print_meta: rope scaling     = linear
0.00.055.816 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.816 I llm_load_print_meta: freq_scale_train = 1
0.00.055.816 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.817 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.817 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.817 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.817 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.817 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.817 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.818 I llm_load_print_meta: model type       = 1.4B
0.00.055.818 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.055.820 I llm_load_print_meta: model params     = 1.41 B
0.00.055.821 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.055.821 I llm_load_print_meta: general.name     = 1.4B
0.00.055.821 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.821 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.821 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.822 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.822 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.822 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.822 I llm_load_print_meta: max token length = 1024
0.00.057.799 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.799 I llm_load_tensors: offloading output layer to GPU
0.00.057.799 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.810 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.057.811 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.058.157 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.157 I llama_new_context_with_model: n_ctx         = 128
0.00.058.157 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.058.158 I llama_new_context_with_model: n_batch       = 128
0.00.058.158 I llama_new_context_with_model: n_ubatch      = 128
0.00.058.158 I llama_new_context_with_model: flash_attn    = 0
0.00.058.159 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.159 I llama_new_context_with_model: freq_scale    = 1
0.00.058.159 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.160 I ggml_metal_init: allocating
0.00.058.164 I ggml_metal_init: found device: Apple M4
0.00.058.167 I ggml_metal_init: picking default device: Apple M4
0.00.058.732 I ggml_metal_init: using embedded metal library
0.00.061.131 I ggml_metal_init: GPU name:   Apple M4
0.00.061.132 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.133 I ggml_metal_init: simdgroup reduction   = true
0.00.061.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.134 I ggml_metal_init: has bfloat            = true
0.00.061.134 I ggml_metal_init: use bfloat            = true
0.00.061.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.276 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.539 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.547 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.562 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.396 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.397 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.398 I llama_new_context_with_model: graph nodes  = 967
0.00.073.398 I llama_new_context_with_model: graph splits = 2
0.00.073.399 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.573.958 I 
0.00.573.992 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.004 I perplexity: tokenizing the input ..
0.00.581.891 I perplexity: tokenization took 7.885 ms
0.00.581.895 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.715.339 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.716.486 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.716.506 I llama_perf_context_print:        load time =     564.19 ms
0.00.716.507 I llama_perf_context_print: prompt eval time =     133.19 ms /   128 tokens (    1.04 ms per token,   961.06 tokens per second)
0.00.716.508 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.716.508 I llama_perf_context_print:       total time =     142.55 ms /   129 tokens
0.00.716.880 I ggml_metal_free: deallocating

real	0m0.733s
user	0m0.081s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.579 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.622 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.622 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.623 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.623 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.623 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.626 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.626 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.627 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.627 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.628 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.631 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.632 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.632 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.260 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.262 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.263 I llama_model_loader: - type  f32:  194 tensors
0.00.024.263 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.263 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.953 I llm_load_vocab: special tokens cache size = 25
0.00.050.994 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.996 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.997 I llm_load_print_meta: arch             = gptneox
0.00.050.997 I llm_load_print_meta: vocab type       = BPE
0.00.050.997 I llm_load_print_meta: n_vocab          = 50304
0.00.050.997 I llm_load_print_meta: n_merges         = 50009
0.00.050.997 I llm_load_print_meta: vocab_only       = 0
0.00.050.998 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.998 I llm_load_print_meta: n_embd           = 2048
0.00.050.998 I llm_load_print_meta: n_layer          = 24
0.00.051.000 I llm_load_print_meta: n_head           = 16
0.00.051.001 I llm_load_print_meta: n_head_kv        = 16
0.00.051.001 I llm_load_print_meta: n_rot            = 32
0.00.051.001 I llm_load_print_meta: n_swa            = 0
0.00.051.002 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.002 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.003 I llm_load_print_meta: n_gqa            = 1
0.00.051.003 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.004 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.005 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.005 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.005 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.005 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.007 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.008 I llm_load_print_meta: n_ff             = 8192
0.00.051.008 I llm_load_print_meta: n_expert         = 0
0.00.051.008 I llm_load_print_meta: n_expert_used    = 0
0.00.051.008 I llm_load_print_meta: causal attn      = 1
0.00.051.009 I llm_load_print_meta: pooling type     = 0
0.00.051.009 I llm_load_print_meta: rope type        = 2
0.00.051.010 I llm_load_print_meta: rope scaling     = linear
0.00.051.010 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.011 I llm_load_print_meta: freq_scale_train = 1
0.00.051.011 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.011 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.011 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.011 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.012 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.012 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.012 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.012 I llm_load_print_meta: model type       = 1.4B
0.00.051.013 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.014 I llm_load_print_meta: model params     = 1.41 B
0.00.051.015 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.015 I llm_load_print_meta: general.name     = 1.4B
0.00.051.015 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.016 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.016 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.017 I llm_load_print_meta: max token length = 1024
0.00.052.953 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.953 I llm_load_tensors: offloading output layer to GPU
0.00.052.953 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.964 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.965 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.305 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.306 I llama_new_context_with_model: n_ctx         = 128
0.00.053.306 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.306 I llama_new_context_with_model: n_batch       = 128
0.00.053.306 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.306 I llama_new_context_with_model: flash_attn    = 0
0.00.053.307 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.307 I llama_new_context_with_model: freq_scale    = 1
0.00.053.307 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.308 I ggml_metal_init: allocating
0.00.053.315 I ggml_metal_init: found device: Apple M4
0.00.053.317 I ggml_metal_init: picking default device: Apple M4
0.00.053.891 I ggml_metal_init: using embedded metal library
0.00.056.243 I ggml_metal_init: GPU name:   Apple M4
0.00.056.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.246 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.246 I ggml_metal_init: simdgroup reduction   = true
0.00.056.246 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.247 I ggml_metal_init: has bfloat            = true
0.00.056.247 I ggml_metal_init: use bfloat            = true
0.00.056.247 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.248 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.561 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.896 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.898 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.914 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.847 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.848 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.848 I llama_new_context_with_model: graph nodes  = 967
0.00.068.848 I llama_new_context_with_model: graph splits = 2
0.00.068.850 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.850 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.311 I 
0.00.633.363 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.392 I perplexity: tokenizing the input ..
0.00.641.691 I perplexity: tokenization took 8.297 ms
0.00.641.700 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.804 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.784.138 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.784.172 I llama_perf_context_print:        load time =     624.72 ms
0.00.784.173 I llama_perf_context_print: prompt eval time =     140.85 ms /   128 tokens (    1.10 ms per token,   908.79 tokens per second)
0.00.784.174 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.174 I llama_perf_context_print:       total time =     150.87 ms /   129 tokens
0.00.784.674 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.080s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.429 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.285 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.291 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.293 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.294 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.296 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.299 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.302 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.113 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.915 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.916 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.916 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.916 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.917 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.917 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.918 I llama_model_loader: - type  f32:  194 tensors
0.00.025.918 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.521 I llm_load_vocab: special tokens cache size = 25
0.00.052.523 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.526 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.526 I llm_load_print_meta: arch             = gptneox
0.00.052.527 I llm_load_print_meta: vocab type       = BPE
0.00.052.527 I llm_load_print_meta: n_vocab          = 50304
0.00.052.527 I llm_load_print_meta: n_merges         = 50009
0.00.052.527 I llm_load_print_meta: vocab_only       = 0
0.00.052.528 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.528 I llm_load_print_meta: n_embd           = 2048
0.00.052.528 I llm_load_print_meta: n_layer          = 24
0.00.052.531 I llm_load_print_meta: n_head           = 16
0.00.052.531 I llm_load_print_meta: n_head_kv        = 16
0.00.052.532 I llm_load_print_meta: n_rot            = 32
0.00.052.532 I llm_load_print_meta: n_swa            = 0
0.00.052.532 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.532 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.533 I llm_load_print_meta: n_gqa            = 1
0.00.052.534 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.534 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.535 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.535 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.536 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.536 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.536 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.538 I llm_load_print_meta: n_ff             = 8192
0.00.052.539 I llm_load_print_meta: n_expert         = 0
0.00.052.539 I llm_load_print_meta: n_expert_used    = 0
0.00.052.539 I llm_load_print_meta: causal attn      = 1
0.00.052.539 I llm_load_print_meta: pooling type     = 0
0.00.052.539 I llm_load_print_meta: rope type        = 2
0.00.052.539 I llm_load_print_meta: rope scaling     = linear
0.00.052.540 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.540 I llm_load_print_meta: freq_scale_train = 1
0.00.052.540 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.541 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.541 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.541 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.541 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.541 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.541 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.541 I llm_load_print_meta: model type       = 1.4B
0.00.052.542 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.542 I llm_load_print_meta: model params     = 1.41 B
0.00.052.544 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.544 I llm_load_print_meta: general.name     = 1.4B
0.00.052.544 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.545 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.549 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.549 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.549 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.549 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.549 I llm_load_print_meta: max token length = 1024
0.00.054.529 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.529 I llm_load_tensors: offloading output layer to GPU
0.00.054.529 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.540 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.541 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.888 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.889 I llama_new_context_with_model: n_ctx         = 128
0.00.054.889 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.889 I llama_new_context_with_model: n_batch       = 128
0.00.054.889 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.889 I llama_new_context_with_model: flash_attn    = 0
0.00.054.890 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.890 I llama_new_context_with_model: freq_scale    = 1
0.00.054.891 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.891 I ggml_metal_init: allocating
0.00.054.898 I ggml_metal_init: found device: Apple M4
0.00.054.901 I ggml_metal_init: picking default device: Apple M4
0.00.055.470 I ggml_metal_init: using embedded metal library
0.00.057.840 I ggml_metal_init: GPU name:   Apple M4
0.00.057.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.842 I ggml_metal_init: simdgroup reduction   = true
0.00.057.842 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.842 I ggml_metal_init: has bfloat            = true
0.00.057.842 I ggml_metal_init: use bfloat            = true
0.00.057.843 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.844 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.272 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.588 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.594 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.612 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.491 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.493 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.493 I llama_new_context_with_model: graph nodes  = 967
0.00.069.493 I llama_new_context_with_model: graph splits = 2
0.00.069.494 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.495 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.175 I 
0.00.490.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.215 I perplexity: tokenizing the input ..
0.00.498.299 I perplexity: tokenization took 8.083 ms
0.00.498.302 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.638.457 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.639.616 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.639.638 I llama_perf_context_print:        load time =     479.74 ms
0.00.639.639 I llama_perf_context_print: prompt eval time =     139.93 ms /   128 tokens (    1.09 ms per token,   914.75 tokens per second)
0.00.639.640 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.639.641 I llama_perf_context_print:       total time =     149.46 ms /   129 tokens
0.00.639.990 I ggml_metal_free: deallocating

real	0m0.655s
user	0m0.079s
sys	0m0.094s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.231 I build: 4451 (d9feae1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.707 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.953 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.958 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.960 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.961 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.962 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.969 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.969 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.751 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.752 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.752 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.753 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.753 I llama_model_loader: - type  f32:  194 tensors
0.00.055.754 I llama_model_loader: - type  f16:   98 tensors
0.00.082.824 I llm_load_vocab: special tokens cache size = 25
0.00.089.024 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.027 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.027 I llm_load_print_meta: arch             = gptneox
0.00.089.027 I llm_load_print_meta: vocab type       = BPE
0.00.089.027 I llm_load_print_meta: n_vocab          = 50304
0.00.089.028 I llm_load_print_meta: n_merges         = 50009
0.00.089.028 I llm_load_print_meta: vocab_only       = 0
0.00.089.028 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.028 I llm_load_print_meta: n_embd           = 2048
0.00.089.028 I llm_load_print_meta: n_layer          = 24
0.00.089.031 I llm_load_print_meta: n_head           = 16
0.00.089.035 I llm_load_print_meta: n_head_kv        = 16
0.00.089.035 I llm_load_print_meta: n_rot            = 32
0.00.089.035 I llm_load_print_meta: n_swa            = 0
0.00.089.035 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.035 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.036 I llm_load_print_meta: n_gqa            = 1
0.00.089.044 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.052 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.054 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.054 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.054 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.054 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.054 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.055 I llm_load_print_meta: n_ff             = 8192
0.00.089.056 I llm_load_print_meta: n_expert         = 0
0.00.089.056 I llm_load_print_meta: n_expert_used    = 0
0.00.089.056 I llm_load_print_meta: causal attn      = 1
0.00.089.056 I llm_load_print_meta: pooling type     = 0
0.00.089.056 I llm_load_print_meta: rope type        = 2
0.00.089.057 I llm_load_print_meta: rope scaling     = linear
0.00.089.057 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.057 I llm_load_print_meta: freq_scale_train = 1
0.00.089.058 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.058 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.058 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.058 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.058 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.058 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.059 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.059 I llm_load_print_meta: model type       = 1.4B
0.00.089.060 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.060 I llm_load_print_meta: model params     = 1.41 B
0.00.089.061 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.061 I llm_load_print_meta: general.name     = 1.4B
0.00.089.061 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.061 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.061 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.061 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.062 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.063 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.064 I llm_load_print_meta: max token length = 1024
0.00.091.462 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.462 I llm_load_tensors: offloading output layer to GPU
0.00.091.463 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.473 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.474 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.842 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.843 I llama_new_context_with_model: n_ctx         = 128
0.00.091.843 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.843 I llama_new_context_with_model: n_batch       = 128
0.00.091.844 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.844 I llama_new_context_with_model: flash_attn    = 0
0.00.091.844 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.844 I llama_new_context_with_model: freq_scale    = 1
0.00.091.845 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.845 I ggml_metal_init: allocating
0.00.091.849 I ggml_metal_init: found device: Apple M4
0.00.091.851 I ggml_metal_init: picking default device: Apple M4
0.00.092.441 I ggml_metal_init: using embedded metal library
0.00.094.923 I ggml_metal_init: GPU name:   Apple M4
0.00.094.925 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.925 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.926 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.926 I ggml_metal_init: simdgroup reduction   = true
0.00.094.926 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.926 I ggml_metal_init: has bfloat            = true
0.00.094.926 I ggml_metal_init: use bfloat            = true
0.00.094.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.927 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.566 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.937 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.939 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.953 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.803 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.105.804 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.105.804 I llama_new_context_with_model: graph nodes  = 967
0.00.105.805 I llama_new_context_with_model: graph splits = 2
0.00.105.806 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.807 I 
0.00.105.833 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.834 I compute_imatrix: tokenizing the input ..
0.00.112.509 I compute_imatrix: tokenization took 6.675 ms
0.00.112.511 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.556.005 I compute_imatrix: 1.44 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.558.900 I llama_perf_context_print:        load time =    1534.30 ms
0.01.558.901 I llama_perf_context_print: prompt eval time =    1442.88 ms /   128 tokens (   11.27 ms per token,    88.71 tokens per second)
0.01.558.902 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.558.902 I llama_perf_context_print:       total time =    1537.18 ms /   129 tokens
0.01.559.448 I ggml_metal_free: deallocating

real	0m1.752s
user	0m0.173s
sys	0m0.242s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4451 (d9feae1c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e20a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e20ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e20b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e20b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e20bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e20c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e20caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e20d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e20d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e20db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e20e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e20e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e20f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e20f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e210030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e210750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e210e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e211590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e211cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e212480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e212ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e2132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e2139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e214280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e2149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e214c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e215270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e215ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e216420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e2166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e216b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e216e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e2176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e217c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e217ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e218370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e218810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e218cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e219150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e2195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e219a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e219f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e21a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e21a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e21ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e21b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e21b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e21c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e21c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e21cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e21d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e21d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e21dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e21e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e21ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e21f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e21f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e21f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e21fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e2206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e220980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e220e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e2212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e221760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e221c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e2220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e222540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e2229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e222e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e223320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e2237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e223c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e224100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e224650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e224ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e2250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e225640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e225b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e2260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e226630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e226b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e2270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e227620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e227b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e2280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e228610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e228b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e2290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e229600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e229b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e22a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e22a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e22ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e22b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e22b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e22bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e22c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e21bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e22c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e22cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e22d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e22d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e22dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e22e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e22e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e22ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e22f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e22f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e22fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e2301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e230710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e230c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e2311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e231650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e231af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e231f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e232430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e2328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e232d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e233210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e2336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e233b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e233ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e234490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e234930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e234dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e235270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e235710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e235bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e236050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e2364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e236990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e236e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e2372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e237770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e237c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e2380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e238550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e2389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e238e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e239330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e2397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e239c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e23a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e23a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e23aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e23aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e23b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e23b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e23bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e23c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e23c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e23cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e23cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e23d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e23d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e23dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e23e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e23e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e23eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e23efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e23f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e23f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e23fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e240230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e2406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e240b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e241010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e2414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e241950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e241df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e242290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e242730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e242bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e243070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e243510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e2439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e243e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e2442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e244790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e244c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e2450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e245570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e245a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e245eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e246350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e2467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e246c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e247130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e2475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e247a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e247f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e2483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e248900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e248e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e2493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e2498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e249bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e24a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e24a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e24ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e24b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e24ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e24bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e24c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e24c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e24d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e24d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e24da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e24df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e24e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e24ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e24f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e24f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e24fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e250160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e2506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e250c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e251150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e2516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e251bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e252140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e252690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e252be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e253130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e253680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e253bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e254120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e254670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e254bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e255110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e255660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e255bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e256100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e256650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e256ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e2570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e257640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e257b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e2580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e258630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e258b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e2590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e259620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e259b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e25a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e25a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e25ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e25b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e25b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e25bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e25c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e25c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e25cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e25d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e25d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e25db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e25e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e25e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e25eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e25f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e25f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e25fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e260060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e2605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e260b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e261050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e2614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e261990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e261e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e2622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e262770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e262c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e2630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e263550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e2639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e263e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e264330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e2647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e264c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e265110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e2655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e265b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e266220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e266940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e267060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e267780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e267a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e268230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e2684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e268b00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e305810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e305c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e3060f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e306560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e3069d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e306e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e3072b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e307720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e307b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e3080f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e308560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e308be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e309700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e309eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e30a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e30ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e30b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e30bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e30c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e30cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e30d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e30d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e30e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e30e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e30eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e30f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e30f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e30f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e30fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e310180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e3105f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e310b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e310f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e311250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e3116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e311b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e311fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e312410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e312880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e312cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e313160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e3135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e313a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e313eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e314320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e314790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e314c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e315070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e3154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e315950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e315dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e316230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e3166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e316b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e316f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e3173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e317960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e317e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e3182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e318740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e318bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e319020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e319490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e319900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e319d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e31a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e31a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e31aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e31af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e31b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e31b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e31bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e31c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e31c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e31c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e31ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e31d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e31d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e31db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e31e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e31e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e31e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e31ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e31f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e31f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e31faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e31ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e320380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e3207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e320c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e3210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e321540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e3219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e321e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e322290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e322700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e322b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e322fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e323450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e3238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e323d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e3241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e324610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e324a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e324ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e325360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e3257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e325c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e3260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e326520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e326990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e326e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e327270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e3276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e327b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e327fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e328430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e3288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e328d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e329180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e3295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e329a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e329ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e32a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e32a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e32ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e32b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e32b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e32b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e32bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e32c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e32c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e32cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e32cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e32d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e32d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e32dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e32e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e32e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e32ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e32eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e32f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e32f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e32fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e330070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e3304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e330950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e330dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e331230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e3316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e331b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e331f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e3323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e332860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e332cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e333140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e3335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e333a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e333e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e334300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e334770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e334be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e335050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e3354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e335930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e335da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e3369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e336c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e336f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e3373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e337830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e337ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e338110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e338580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e3389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e338e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e3392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e339740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e339bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e33a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e33a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e33a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e33ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e33b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e33b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e33bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e33bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e33c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e33c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e33cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e33d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e33d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e33d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e33de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e33e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e33e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e33eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e33f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e33f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e33f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e33fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e3401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e340720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e340c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e3410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e341510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e341980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e341df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e342310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e342820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e343390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e343650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e343c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e3441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e344790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e344d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e345310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e3458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e345e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e346450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e346a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e346fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e347590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e347b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e348110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e3486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e348c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e349250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e349810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e349dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e34a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e34a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e34af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e34b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e34ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e34c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e34c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e34cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e34d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e34d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e34dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e34e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e34e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e34ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e34f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e34f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e34ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e350550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e350b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e3510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e351690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e351c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e352210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e3527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e352d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e353350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e353910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e353ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e354490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e354a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e355010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e3555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e355b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e356150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e356710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e356cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e357290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e357850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e357d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e358250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e358750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e358c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e359150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e359650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e359b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e35a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e35a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e35aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e35af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e35b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e35b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e35be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e35c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e35cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e35d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e35dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e35e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e35e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e35ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e35f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e35f640 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e108d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e108fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e109430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e1098a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e109d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e10a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e10a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e10aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e10aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e10b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e10b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e10bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e10c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e10d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e10d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e10e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e10e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e10ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e10f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e10fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e110480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e110ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e1112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e1119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e112100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e1123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e112680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e112af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e112f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e1133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e113840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e113d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e1141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e1144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e114910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e114d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e1151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e115660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e115ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e115f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e1163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e116820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e116c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e117100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e117570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e1179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e117e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e1182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e118730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e118ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e119010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e119480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e1198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e119d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e11a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e11a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e11abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e11b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e11b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e11b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e11be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e11c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e11c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e11cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e11cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e11d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e11d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e11dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e11e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e11e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e11ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e11eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e11f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e11f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e11fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e120090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e120500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e120970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e120de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e121250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e1216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e121b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e121fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e122410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e122880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e122cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e123160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e1235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e123a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e123eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e124320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e124790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e124c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e125070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e1254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e125950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e125dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e126230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e1266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e126b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e126f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e1273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e127860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e127e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e128280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e1286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e128b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e128fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e129440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e1298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e129d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e12a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e12a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e12aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e12aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e12b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e12b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e12bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e12c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e12c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e12c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e12cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e12d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e12d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e12db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e12dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e12e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e12e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e12ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e12f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e12f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e12fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e12fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e130330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e1307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e130c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e131080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e1314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e131960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e131dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e132240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e1326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e132b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e132f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e133400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e133870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e133ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e134150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e1345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e134a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e134ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e135310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e135780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e135bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e136060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e1364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e136940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e136db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e137220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e137690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e137b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e137f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e1383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e138850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e138cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e139130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e1395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e139a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e139e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e13a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e13a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e13abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e13b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e13b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e13b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e13bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e13c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e13c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e13cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e13cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e13d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e13d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e13dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e13e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e13e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e13e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e13ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e13f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e13f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e13fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e140020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e140490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e140900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e140d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e1411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e141650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e141ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e141f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e1423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e142810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e142c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e1430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e143560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e1439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e143e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e1442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e144720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e144cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e145120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e145590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e1460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e1463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e146660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e146ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e146f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e1473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e147820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e147c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e148100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e148570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e1489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e148e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e1492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e149730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e149ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e14a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e14a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e14a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e14ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e14b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e14b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e14bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e14bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e14c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e14c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e14cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e14d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e14d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e14d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e14de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e14e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e14e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e14eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e14eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e14f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e14f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e14fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e1501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e150620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e150a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e150f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e151370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e1517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e151c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e1520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e152530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e1529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e152e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e153280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e1536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e153b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e153fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e154440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e1548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e154d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e155190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e155600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e155a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e155ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e156350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e1567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e156c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e1570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e157510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e157980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e157df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e158260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e1586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e158b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e158fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e159420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e159890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e159d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e15a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e15ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e15b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e15bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e15bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e15c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e15ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e15d010 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.839s
user	0m0.297s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4451 (d9feae1c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137f0d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137f0d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137f0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137f0e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137f0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137f0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137f0f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137f0fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137f10190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137f10690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137f10b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137f11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137f11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137f12360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137f12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137f13290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137f139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137f140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137f147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137f14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137f156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137f15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137f16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137f16dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137f174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137f177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137f17db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137f18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137f19220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137f196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137f19980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137f1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137f1a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137f1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137f1aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137f1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137f1b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137f1bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137f1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137f1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137f1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137f1cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137f1d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137f1d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137f1dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137f1e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137f1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137f1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137f1f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137f1fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137f203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137f20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137f21800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137f21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137f22140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137f22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137f22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137f23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137f234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137f23960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137f23e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137f242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137f24740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137f24be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137f25080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137f25520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137f259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137f25e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137f26300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137f267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137f26c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137f27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137f276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137f27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137f28180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137f286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137f28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137f29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137f296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137f29c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137f2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137f2a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137f2ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137f2b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137f2b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137f2bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137f2c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137f2c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137f2cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137f2d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137f2d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137f2dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137f2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137f2e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137f2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137f1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137f2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137f2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137f2fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137f30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137f307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137f30d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137f31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137f317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137f31d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137f32260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137f327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137f32d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137f33250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137f337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137f33cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137f34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137f34630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137f34ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137f34f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137f35410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137f358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137f35d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137f361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137f36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137f36b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137f36fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137f37470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137f37910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137f37db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137f38250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137f386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137f38b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137f39030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137f394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137f39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137f39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137f3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137f3a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137f3abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137f3b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137f3b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137f3b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137f3c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137f3c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137f3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137f3d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137f3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137f3da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137f3ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137f3e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137f3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137f3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137f3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137f3f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137f3fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137f3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137f403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137f40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137f40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137f411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137f41650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137f41af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137f41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137f42430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137f428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137f42d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137f43210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137f436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137f43b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137f43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137f44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137f44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137f44dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137f45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137f45710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137f45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137f46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137f464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137f46990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137f46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137f472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137f47770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137f47c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137f480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137f48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137f489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137f48e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137f49330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137f497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137f49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137f4a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137f4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137f4aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137f4aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137f4b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137f4b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137f4bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137f4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137f4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137f4cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137f4d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137f4d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137f4e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137f4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137f4e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137f4ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137f4f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137f4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137f50120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137f505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137f50a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137f51210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137f51760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137f51cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137f52200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137f52750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137f52ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137f531f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137f53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137f53c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137f541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137f54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137f54c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137f551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137f55720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137f55c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137f561c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137f56710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137f56c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137f571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137f57700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137f57c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137f581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137f586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137f58c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137f59190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137f596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137f59c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137f5a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137f5a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137f5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137f5b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137f5b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137f5bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137f5c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137f5c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137f5cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137f5d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137f5d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137f5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137f5e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137f5e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137f5ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137f5f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137f5f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137f5fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137f60120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137f60670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137f60bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137f61110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137f61660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137f61bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137f62100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137f62650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137f62ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137f630f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137f63640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137f63b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137f64030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137f644d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137f64970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137f64e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137f652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137f65750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137f65bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137f66090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137f66530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137f669d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137f66e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137f67310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137f677b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137f67c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137f680f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137f68640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137f68d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137f69480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137f69ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137f6a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137f6a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137f6ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137f6b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137f6b640 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.105.975 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.979 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127f073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127f07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127f07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127f089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127f091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127f099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127f0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127f0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127f0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127f0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127f0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127f0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127f0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127f0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127f0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127f0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127f0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127f0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127f0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127f0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127f0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127f0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127f0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127f10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127f10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127f109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127f10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127f11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127f11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127f11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127f11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127f12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127f128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127f12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127f131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127f13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127f13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127f13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127f14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127f147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127f14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127f150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127f15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127f15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127f15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127f16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127f166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127f16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127f17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127f175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127f17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127f17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127f18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127f18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127f18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127f19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127f194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127f19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127f19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127f1a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127f1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127f1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127f1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127f1b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127f1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127f1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127f1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127f1c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127f1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127f1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127f1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127f1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127f1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127f1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127f1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127f1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127f1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127f1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127f1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127f1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127f1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127f203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127f20830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127f20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127f21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127f21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127f219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127f21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127f222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127f22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127f22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127f23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127f23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127f23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127f23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127f241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127f24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127f24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127f24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127f253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127f25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127f25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127f260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127f26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127f269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127f26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127f272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127f27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127f27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127f28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127f28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127f288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127f28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127f291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127f29630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127f29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127f29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127f2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127f2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127f2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127f2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127f2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127f2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127f2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127f2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127f2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127f2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127f2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127f2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127f2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127f2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127f2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127f2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127f2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127f2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127f2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127f2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127f2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127f300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127f30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127f30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127f30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127f31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127f316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127f31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127f31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127f32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127f328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127f32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127f33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127f335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127f33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127f33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127f34340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127f347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127f34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127f35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127f35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127f35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127f36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127f366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127f36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127f36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127f37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127f37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127f37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127f38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127f385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127f38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127f38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127f39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127f39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127f39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127f3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127f3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127f3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127f3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127f3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127f3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127f3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127f3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127f3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127f3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127f3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127f3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127f3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127f3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127f3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127f3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127f3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127f3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127f3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127f3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127f3fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127f3ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127f40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127f40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127f40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127f410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127f41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127f41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127f42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127f42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127f42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127f434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127f44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127f45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127f45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127f45d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127f462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127f46880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127f46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127f47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127f479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127f47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127f48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127f48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127f490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127f49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127f4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127f4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127f4ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127f4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f4b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127f4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127f4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127f4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127f4d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127f4d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127f4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127f4e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127f4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127f4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127f4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127f4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127f4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127f503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127f50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127f50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127f51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127f52080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127f531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127f53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127f53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127f54300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127f548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127f54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127f55fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127f56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127f56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127f57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127f57540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127f57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127f57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127f58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127f58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127f58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127f59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127f59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127f59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127f5a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127f5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127f5ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f5b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127f5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127f5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127f5c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127f5ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127f5d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127f5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127f5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127f5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127f5e930 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139805aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139805f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139806380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1398067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139806c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1398070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139807540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1398079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139808290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139808700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139808df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139809910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13980a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13980a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13980aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13980b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13980be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13980c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13980cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13980d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13980dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13980e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13980e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13980f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13980f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13980f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13980fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13980fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1398102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139810c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139811100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1398113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139811830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139811ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139812110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139812580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1398129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139812e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1398132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139813740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139813bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139814020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139814490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139814900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139814d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1398151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139815650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139815ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139815f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1398163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139816810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139816c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1398170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139817560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139817ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139817fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139818440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1398188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139818d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139819190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139819600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139819a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139819ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13981a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13981a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13981ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13981b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13981b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13981b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13981bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13981c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13981c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13981cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13981cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13981d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13981d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13981dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13981e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13981e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13981ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13981eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13981f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13981f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13981fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139820080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1398204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139820960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139820dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139821240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1398216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139821b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139821f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139822400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139822870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139822ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139823150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1398235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139823a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139823ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139824310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139824780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139825010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1398252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139825740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139825bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139826020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139826490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139826900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139826d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1398271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139827650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139827ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139827f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1398283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139828810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139828c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1398290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139829560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1398299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139829e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13982a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13982a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13982ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13982b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13982b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13982b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13982bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13982c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13982c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13982caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13982cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13982d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13982d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13982dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13982e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13982e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13982e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13982ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13982f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13982f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13982fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13982ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139830450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1398308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139830d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1398311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139831610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139831a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139831ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139832360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1398327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139832c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1398330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139833520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139833990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139833e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139834270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1398346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139834b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139834fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139835430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1398358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139835d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139836180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1398365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139836a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139836ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139837340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1398377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139837c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139838090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139838500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139838970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139838de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139839250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1398396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139839b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139839fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13983a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13983a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13983acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13983b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13983b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13983ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13983beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13983c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13983c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13983cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13983d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13983d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13983d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13983ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13983e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13983e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13983eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13983ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13983f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13983f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13983fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139840140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1398405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139840a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139840e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139841300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139841770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139841be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139842050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1398424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139843040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139843300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1398435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139843a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139843ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139844310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139844780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139844bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139845060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1398454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139845940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139845db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139846220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139846690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139846b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139846f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1398473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139847850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139847cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139848130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1398485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139848a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139848e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1398492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139849760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139849bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13984a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13984a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13984a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13984ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13984b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13984b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13984bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13984bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13984c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13984c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13984cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13984d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13984d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13984d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13984de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13984e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13984e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13984ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13984f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13984f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13984f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13984fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1398501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139850650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139850ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139850f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1398513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139851810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139851c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1398520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139852560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1398529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139852e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1398532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139853720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139853b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139854000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139854470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1398548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139854d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1398551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139855630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139855aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139856380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1398567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139856c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1398576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139857df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139858510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139858c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139858ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139859360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139859960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139859f70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.934s
user	0m0.244s
sys	0m0.139s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
