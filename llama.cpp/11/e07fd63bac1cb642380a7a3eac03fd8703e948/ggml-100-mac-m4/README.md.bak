### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.34 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.41 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.40 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.12 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  179.26 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.91 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   25.99 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.33 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 221.55 sec*proc (27 tests)

Total Test time (real) = 221.56 sec

real	3m41.609s
user	7m36.656s
sys	0m6.182s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.20 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.47 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.38 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.04 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.35 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.19 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.24 sec*proc (27 tests)

Total Test time (real) =  51.25 sec

real	0m51.265s
user	1m11.937s
sys	0m5.702s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.065 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.346 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.224 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.017.228 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.230 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.017.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.231 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.017.231 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.017.231 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.017.232 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.017.232 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.017.233 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.017.233 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.017.233 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.017.236 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.017.236 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.017.237 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.017.237 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.017.237 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.017.238 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.017.238 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.019.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.020.024 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.020.025 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.020.026 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.020.026 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.020.026 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.020.026 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.020.027 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.020.027 I llama_model_loader: - type  f32:  124 tensors
0.00.020.028 I llama_model_loader: - type  f16:   73 tensors
0.00.022.523 I llm_load_vocab: special tokens cache size = 5
0.00.023.832 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.023.835 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.023.836 I llm_load_print_meta: arch             = bert
0.00.023.836 I llm_load_print_meta: vocab type       = WPM
0.00.023.836 I llm_load_print_meta: n_vocab          = 30522
0.00.023.836 I llm_load_print_meta: n_merges         = 0
0.00.023.836 I llm_load_print_meta: vocab_only       = 0
0.00.023.837 I llm_load_print_meta: n_ctx_train      = 512
0.00.023.837 I llm_load_print_meta: n_embd           = 384
0.00.023.840 I llm_load_print_meta: n_layer          = 12
0.00.023.862 I llm_load_print_meta: n_head           = 12
0.00.023.863 I llm_load_print_meta: n_head_kv        = 12
0.00.023.863 I llm_load_print_meta: n_rot            = 32
0.00.023.863 I llm_load_print_meta: n_swa            = 0
0.00.023.864 I llm_load_print_meta: n_embd_head_k    = 32
0.00.023.864 I llm_load_print_meta: n_embd_head_v    = 32
0.00.023.864 I llm_load_print_meta: n_gqa            = 1
0.00.023.865 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.023.865 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.023.866 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.023.866 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.023.866 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.023.867 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.023.867 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.023.867 I llm_load_print_meta: n_ff             = 1536
0.00.023.869 I llm_load_print_meta: n_expert         = 0
0.00.023.869 I llm_load_print_meta: n_expert_used    = 0
0.00.023.869 I llm_load_print_meta: causal attn      = 0
0.00.023.869 I llm_load_print_meta: pooling type     = 2
0.00.023.869 I llm_load_print_meta: rope type        = 2
0.00.023.870 I llm_load_print_meta: rope scaling     = linear
0.00.023.870 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.023.870 I llm_load_print_meta: freq_scale_train = 1
0.00.023.870 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.023.871 I llm_load_print_meta: rope_finetuned   = unknown
0.00.023.871 I llm_load_print_meta: ssm_d_conv       = 0
0.00.023.871 I llm_load_print_meta: ssm_d_inner      = 0
0.00.023.871 I llm_load_print_meta: ssm_d_state      = 0
0.00.023.871 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.023.871 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.023.881 I llm_load_print_meta: model type       = 33M
0.00.023.882 I llm_load_print_meta: model ftype      = F16
0.00.023.882 I llm_load_print_meta: model params     = 33.21 M
0.00.023.883 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.023.883 I llm_load_print_meta: general.name     = Bge Small
0.00.023.883 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.023.883 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.023.883 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.023.883 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.023.884 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.023.884 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.023.884 I llm_load_print_meta: max token length = 21
0.00.025.187 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.025.188 I llm_load_tensors: offloading output layer to GPU
0.00.025.189 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.025.209 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.025.210 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.025.598 I llama_new_context_with_model: n_seq_max     = 1
0.00.025.599 I llama_new_context_with_model: n_ctx         = 512
0.00.025.599 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.025.599 I llama_new_context_with_model: n_batch       = 2048
0.00.025.599 I llama_new_context_with_model: n_ubatch      = 2048
0.00.025.599 I llama_new_context_with_model: flash_attn    = 0
0.00.025.600 I llama_new_context_with_model: freq_base     = 10000.0
0.00.025.600 I llama_new_context_with_model: freq_scale    = 1
0.00.025.600 I ggml_metal_init: allocating
0.00.025.608 I ggml_metal_init: found device: Apple M4
0.00.025.612 I ggml_metal_init: picking default device: Apple M4
0.00.026.274 I ggml_metal_init: using embedded metal library
0.00.028.888 I ggml_metal_init: GPU name:   Apple M4
0.00.028.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.028.890 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.028.891 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.028.895 I ggml_metal_init: simdgroup reduction   = true
0.00.028.895 I ggml_metal_init: simdgroup matrix mul. = true
0.00.028.895 I ggml_metal_init: has bfloat            = true
0.00.028.895 I ggml_metal_init: use bfloat            = true
0.00.028.896 I ggml_metal_init: hasUnifiedMemory      = true
0.00.028.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.039.066 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.039.069 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.039.070 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.039.720 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.039.721 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.039.722 I llama_new_context_with_model: graph nodes  = 429
0.00.039.722 I llama_new_context_with_model: graph splits = 2
0.00.039.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.044.691 I 
0.00.044.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.045.282 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.049.360 I llama_perf_context_print:        load time =      29.34 ms
0.00.049.361 I llama_perf_context_print: prompt eval time =       3.95 ms /     9 tokens (    0.44 ms per token,  2279.64 tokens per second)
0.00.049.362 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.049.362 I llama_perf_context_print:       total time =       4.67 ms /    10 tokens
0.00.049.543 I ggml_metal_free: deallocating

real	0m0.220s
user	0m0.033s
sys	0m0.023s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.029 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.686 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.030 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.033 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.034 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.035 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.035 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.035 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.036 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.037 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.037 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.037 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.038 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.038 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.040 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.040 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.041 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.041 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.041 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.042 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.042 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.132 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.696 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.697 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.697 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.697 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.698 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.698 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.698 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.699 I llama_model_loader: - type  f32:  124 tensors
0.00.013.699 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.990 I llm_load_vocab: special tokens cache size = 5
0.00.017.134 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.136 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.136 I llm_load_print_meta: arch             = bert
0.00.017.137 I llm_load_print_meta: vocab type       = WPM
0.00.017.137 I llm_load_print_meta: n_vocab          = 30522
0.00.017.137 I llm_load_print_meta: n_merges         = 0
0.00.017.137 I llm_load_print_meta: vocab_only       = 0
0.00.017.137 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.138 I llm_load_print_meta: n_embd           = 384
0.00.017.138 I llm_load_print_meta: n_layer          = 12
0.00.017.146 I llm_load_print_meta: n_head           = 12
0.00.017.147 I llm_load_print_meta: n_head_kv        = 12
0.00.017.147 I llm_load_print_meta: n_rot            = 32
0.00.017.147 I llm_load_print_meta: n_swa            = 0
0.00.017.147 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.147 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.148 I llm_load_print_meta: n_gqa            = 1
0.00.017.149 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.151 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.152 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.153 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.153 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.153 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.153 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.155 I llm_load_print_meta: n_ff             = 1536
0.00.017.155 I llm_load_print_meta: n_expert         = 0
0.00.017.156 I llm_load_print_meta: n_expert_used    = 0
0.00.017.156 I llm_load_print_meta: causal attn      = 0
0.00.017.156 I llm_load_print_meta: pooling type     = 2
0.00.017.156 I llm_load_print_meta: rope type        = 2
0.00.017.156 I llm_load_print_meta: rope scaling     = linear
0.00.017.157 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.157 I llm_load_print_meta: freq_scale_train = 1
0.00.017.157 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.157 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.158 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.158 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.158 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.158 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.158 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.163 I llm_load_print_meta: model type       = 33M
0.00.017.163 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.164 I llm_load_print_meta: model params     = 33.21 M
0.00.017.164 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.164 I llm_load_print_meta: general.name     = Bge Small
0.00.017.164 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.165 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.165 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.165 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.165 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.165 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.166 I llm_load_print_meta: max token length = 21
0.00.018.324 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.324 I llm_load_tensors: offloading output layer to GPU
0.00.018.324 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.332 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.333 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.018.696 I llama_new_context_with_model: n_seq_max     = 1
0.00.018.697 I llama_new_context_with_model: n_ctx         = 512
0.00.018.697 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.018.698 I llama_new_context_with_model: n_batch       = 2048
0.00.018.698 I llama_new_context_with_model: n_ubatch      = 2048
0.00.018.698 I llama_new_context_with_model: flash_attn    = 0
0.00.018.698 I llama_new_context_with_model: freq_base     = 10000.0
0.00.018.699 I llama_new_context_with_model: freq_scale    = 1
0.00.018.699 I ggml_metal_init: allocating
0.00.018.702 I ggml_metal_init: found device: Apple M4
0.00.018.705 I ggml_metal_init: picking default device: Apple M4
0.00.019.212 I ggml_metal_init: using embedded metal library
0.00.021.574 I ggml_metal_init: GPU name:   Apple M4
0.00.021.576 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.576 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.577 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.577 I ggml_metal_init: simdgroup reduction   = true
0.00.021.577 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.577 I ggml_metal_init: has bfloat            = true
0.00.021.577 I ggml_metal_init: use bfloat            = true
0.00.021.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.579 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.147 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.151 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.153 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.750 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.752 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.752 I llama_new_context_with_model: graph nodes  = 429
0.00.032.752 I llama_new_context_with_model: graph splits = 2
0.00.032.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.824 I 
0.00.037.857 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.409 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.856 I llama_perf_context_print:        load time =      29.13 ms
0.00.042.857 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2080.44 tokens per second)
0.00.042.857 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.858 I llama_perf_context_print:       total time =       5.03 ms /    10 tokens
0.00.043.004 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.028s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.137 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.172 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.938 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.947 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.948 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.949 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.950 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.951 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.952 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.953 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.953 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.954 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.958 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.959 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.960 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.900 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.305 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.305 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.306 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.307 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.307 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.307 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.308 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.308 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.309 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.309 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.310 I llama_model_loader: - type  f32:   41 tensors
0.00.050.311 I llama_model_loader: - type  f16:   29 tensors
0.00.068.934 W llm_load_vocab: empty token at index 5
0.00.073.486 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.742 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.768 I llm_load_vocab: special tokens cache size = 5
0.00.333.917 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.333.927 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.333.927 I llm_load_print_meta: arch             = jina-bert-v2
0.00.333.936 I llm_load_print_meta: vocab type       = BPE
0.00.333.937 I llm_load_print_meta: n_vocab          = 61056
0.00.333.937 I llm_load_print_meta: n_merges         = 39382
0.00.333.937 I llm_load_print_meta: vocab_only       = 0
0.00.333.937 I llm_load_print_meta: n_ctx_train      = 8192
0.00.333.938 I llm_load_print_meta: n_embd           = 384
0.00.333.938 I llm_load_print_meta: n_layer          = 4
0.00.333.975 I llm_load_print_meta: n_head           = 12
0.00.333.978 I llm_load_print_meta: n_head_kv        = 12
0.00.333.978 I llm_load_print_meta: n_rot            = 32
0.00.333.978 I llm_load_print_meta: n_swa            = 0
0.00.333.978 I llm_load_print_meta: n_embd_head_k    = 32
0.00.333.978 I llm_load_print_meta: n_embd_head_v    = 32
0.00.333.979 I llm_load_print_meta: n_gqa            = 1
0.00.333.979 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.333.980 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.333.981 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.333.982 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.333.982 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.333.982 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.333.982 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.333.983 I llm_load_print_meta: n_ff             = 1536
0.00.333.984 I llm_load_print_meta: n_expert         = 0
0.00.333.984 I llm_load_print_meta: n_expert_used    = 0
0.00.333.984 I llm_load_print_meta: causal attn      = 0
0.00.333.984 I llm_load_print_meta: pooling type     = -1
0.00.333.984 I llm_load_print_meta: rope type        = -1
0.00.333.984 I llm_load_print_meta: rope scaling     = linear
0.00.333.985 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.333.986 I llm_load_print_meta: freq_scale_train = 1
0.00.333.986 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.333.989 I llm_load_print_meta: rope_finetuned   = unknown
0.00.333.989 I llm_load_print_meta: ssm_d_conv       = 0
0.00.333.989 I llm_load_print_meta: ssm_d_inner      = 0
0.00.333.989 I llm_load_print_meta: ssm_d_state      = 0
0.00.333.989 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.333.989 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.334.013 I llm_load_print_meta: model type       = 33M
0.00.334.013 I llm_load_print_meta: model ftype      = F16
0.00.334.014 I llm_load_print_meta: model params     = 32.90 M
0.00.334.014 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.334.015 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.334.015 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.334.015 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.334.015 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.334.016 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.334.016 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.334.016 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.334.016 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.334.016 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.334.017 I llm_load_print_meta: max token length = 45
0.00.335.385 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.335.385 I llm_load_tensors: offloading output layer to GPU
0.00.335.385 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.335.414 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.335.415 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.336.294 I llama_new_context_with_model: n_seq_max     = 1
0.00.336.295 I llama_new_context_with_model: n_ctx         = 8192
0.00.336.295 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.336.295 I llama_new_context_with_model: n_batch       = 2048
0.00.336.295 I llama_new_context_with_model: n_ubatch      = 2048
0.00.336.296 I llama_new_context_with_model: flash_attn    = 0
0.00.336.296 I llama_new_context_with_model: freq_base     = 10000.0
0.00.336.296 I llama_new_context_with_model: freq_scale    = 1
0.00.336.297 I ggml_metal_init: allocating
0.00.336.300 I ggml_metal_init: found device: Apple M4
0.00.336.302 I ggml_metal_init: picking default device: Apple M4
0.00.337.118 I ggml_metal_init: using embedded metal library
0.00.340.089 I ggml_metal_init: GPU name:   Apple M4
0.00.340.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.340.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.340.091 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.340.091 I ggml_metal_init: simdgroup reduction   = true
0.00.340.091 I ggml_metal_init: simdgroup matrix mul. = true
0.00.340.091 I ggml_metal_init: has bfloat            = true
0.00.340.092 I ggml_metal_init: use bfloat            = true
0.00.340.092 I ggml_metal_init: hasUnifiedMemory      = true
0.00.340.093 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.352.065 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.352.068 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.352.069 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.352.716 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.352.717 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.352.717 I llama_new_context_with_model: graph nodes  = 154
0.00.352.718 I llama_new_context_with_model: graph splits = 2
0.00.352.736 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.611 I 
0.00.366.654 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.366.807 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.366.808 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.366.811 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.366.811 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.366.815 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.366.816 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.367.368 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.370.884 I llama_perf_context_print:        load time =     343.43 ms
0.00.370.885 I llama_perf_context_print: prompt eval time =       3.51 ms /    62 tokens (    0.06 ms per token, 17683.97 tokens per second)
0.00.370.886 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.370.886 I llama_perf_context_print:       total time =       4.27 ms /    63 tokens
0.00.371.082 I ggml_metal_free: deallocating

real	0m1.059s
user	0m0.340s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.110 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.261 I main: llama backend init
0.00.000.268 I main: load the model and apply lora adapter, if any
0.00.074.549 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.086.180 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.086.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.086.217 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.086.218 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.086.219 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.086.220 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.086.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.086.223 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.086.224 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.086.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.086.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.086.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.086.227 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.086.228 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.086.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.086.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.086.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.093.398 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.095.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.102.844 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.102.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.102.854 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.102.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.102.856 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.102.857 I llama_model_loader: - type  f32:  194 tensors
0.00.102.858 I llama_model_loader: - type  f16:   98 tensors
0.00.143.546 I llm_load_vocab: special tokens cache size = 25
0.00.151.826 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.151.830 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.151.830 I llm_load_print_meta: arch             = gptneox
0.00.151.830 I llm_load_print_meta: vocab type       = BPE
0.00.151.831 I llm_load_print_meta: n_vocab          = 50304
0.00.151.831 I llm_load_print_meta: n_merges         = 50009
0.00.151.831 I llm_load_print_meta: vocab_only       = 0
0.00.151.831 I llm_load_print_meta: n_ctx_train      = 2048
0.00.151.831 I llm_load_print_meta: n_embd           = 2048
0.00.151.832 I llm_load_print_meta: n_layer          = 24
0.00.151.856 I llm_load_print_meta: n_head           = 16
0.00.151.858 I llm_load_print_meta: n_head_kv        = 16
0.00.151.858 I llm_load_print_meta: n_rot            = 32
0.00.151.858 I llm_load_print_meta: n_swa            = 0
0.00.151.859 I llm_load_print_meta: n_embd_head_k    = 128
0.00.151.859 I llm_load_print_meta: n_embd_head_v    = 128
0.00.151.860 I llm_load_print_meta: n_gqa            = 1
0.00.151.860 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.151.861 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.151.862 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.151.862 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.151.862 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.151.862 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.151.862 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.151.863 I llm_load_print_meta: n_ff             = 8192
0.00.151.864 I llm_load_print_meta: n_expert         = 0
0.00.151.866 I llm_load_print_meta: n_expert_used    = 0
0.00.151.866 I llm_load_print_meta: causal attn      = 1
0.00.151.866 I llm_load_print_meta: pooling type     = 0
0.00.151.867 I llm_load_print_meta: rope type        = 2
0.00.151.867 I llm_load_print_meta: rope scaling     = linear
0.00.151.867 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.151.867 I llm_load_print_meta: freq_scale_train = 1
0.00.151.868 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.151.868 I llm_load_print_meta: rope_finetuned   = unknown
0.00.151.868 I llm_load_print_meta: ssm_d_conv       = 0
0.00.151.868 I llm_load_print_meta: ssm_d_inner      = 0
0.00.151.868 I llm_load_print_meta: ssm_d_state      = 0
0.00.151.868 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.151.869 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.151.880 I llm_load_print_meta: model type       = 1.4B
0.00.151.880 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.151.881 I llm_load_print_meta: model params     = 1.41 B
0.00.151.881 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.151.881 I llm_load_print_meta: general.name     = 1.4B
0.00.151.882 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.151.882 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.151.883 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.151.883 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.151.884 I llm_load_print_meta: LF token         = 128 ''
0.00.151.884 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.151.884 I llm_load_print_meta: max token length = 1024
0.00.154.639 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.154.639 I llm_load_tensors: offloading output layer to GPU
0.00.154.639 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.154.659 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.154.660 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.155.735 I llama_new_context_with_model: n_seq_max     = 1
0.00.155.737 I llama_new_context_with_model: n_ctx         = 2048
0.00.155.737 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.155.737 I llama_new_context_with_model: n_batch       = 2048
0.00.155.737 I llama_new_context_with_model: n_ubatch      = 512
0.00.155.738 I llama_new_context_with_model: flash_attn    = 0
0.00.155.738 I llama_new_context_with_model: freq_base     = 10000.0
0.00.155.738 I llama_new_context_with_model: freq_scale    = 1
0.00.155.739 I ggml_metal_init: allocating
0.00.155.749 I ggml_metal_init: found device: Apple M4
0.00.155.752 I ggml_metal_init: picking default device: Apple M4
0.00.156.531 I ggml_metal_init: using embedded metal library
0.00.166.387 I ggml_metal_init: GPU name:   Apple M4
0.00.166.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.166.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.166.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.166.390 I ggml_metal_init: simdgroup reduction   = true
0.00.166.390 I ggml_metal_init: simdgroup matrix mul. = true
0.00.166.390 I ggml_metal_init: has bfloat            = true
0.00.166.390 I ggml_metal_init: use bfloat            = true
0.00.166.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.166.391 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.213.159 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.213.165 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.213.184 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.214.215 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.214.217 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.214.218 I llama_new_context_with_model: graph nodes  = 967
0.00.214.218 I llama_new_context_with_model: graph splits = 2
0.00.214.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.289.448 I main: llama threadpool init, n_threads = 4
0.00.289.482 I 
0.00.289.529 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.289.531 I 
0.00.289.616 I sampler seed: 1234
0.00.289.620 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.289.647 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.289.648 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.289.648 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.129.903 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.02.129.904 I llama_perf_context_print:        load time =     214.88 ms
0.02.129.905 I llama_perf_context_print: prompt eval time =      43.76 ms /     7 tokens (    6.25 ms per token,   159.95 tokens per second)
0.02.129.906 I llama_perf_context_print:        eval time =    1793.55 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.129.906 I llama_perf_context_print:       total time =    1840.46 ms /    70 tokens
0.02.130.096 I ggml_metal_free: deallocating

real	0m2.423s
user	0m0.154s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.488 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.207 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.741 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.753 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.754 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.755 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.755 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.756 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.758 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.758 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.759 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.760 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.761 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.762 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.762 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.767 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.057.902 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.903 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.903 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.904 I llama_model_loader: - type  f32:  194 tensors
0.00.057.905 I llama_model_loader: - type  f16:   98 tensors
0.00.088.168 I llm_load_vocab: special tokens cache size = 25
0.00.094.966 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.969 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.969 I llm_load_print_meta: arch             = gptneox
0.00.094.970 I llm_load_print_meta: vocab type       = BPE
0.00.094.970 I llm_load_print_meta: n_vocab          = 50304
0.00.094.970 I llm_load_print_meta: n_merges         = 50009
0.00.094.970 I llm_load_print_meta: vocab_only       = 0
0.00.094.970 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.971 I llm_load_print_meta: n_embd           = 2048
0.00.094.971 I llm_load_print_meta: n_layer          = 24
0.00.094.985 I llm_load_print_meta: n_head           = 16
0.00.094.986 I llm_load_print_meta: n_head_kv        = 16
0.00.094.986 I llm_load_print_meta: n_rot            = 32
0.00.094.986 I llm_load_print_meta: n_swa            = 0
0.00.094.987 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.987 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.987 I llm_load_print_meta: n_gqa            = 1
0.00.094.988 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.988 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.989 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.992 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.992 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.992 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.992 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.993 I llm_load_print_meta: n_ff             = 8192
0.00.094.993 I llm_load_print_meta: n_expert         = 0
0.00.094.993 I llm_load_print_meta: n_expert_used    = 0
0.00.094.993 I llm_load_print_meta: causal attn      = 1
0.00.094.993 I llm_load_print_meta: pooling type     = 0
0.00.094.993 I llm_load_print_meta: rope type        = 2
0.00.095.002 I llm_load_print_meta: rope scaling     = linear
0.00.095.005 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.006 I llm_load_print_meta: freq_scale_train = 1
0.00.095.006 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.006 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.006 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.007 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.007 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.007 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.007 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.017 I llm_load_print_meta: model type       = 1.4B
0.00.095.018 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.020 I llm_load_print_meta: model params     = 1.41 B
0.00.095.020 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.021 I llm_load_print_meta: general.name     = 1.4B
0.00.095.021 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.021 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.021 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.021 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.022 I llm_load_print_meta: LF token         = 128 ''
0.00.095.026 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.026 I llm_load_print_meta: max token length = 1024
0.00.096.820 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.820 I llm_load_tensors: offloading output layer to GPU
0.00.096.820 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.830 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.831 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.716 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.717 I llama_new_context_with_model: n_ctx         = 128
0.00.097.718 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.097.718 I llama_new_context_with_model: n_batch       = 128
0.00.097.718 I llama_new_context_with_model: n_ubatch      = 128
0.00.097.718 I llama_new_context_with_model: flash_attn    = 0
0.00.097.719 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.719 I llama_new_context_with_model: freq_scale    = 1
0.00.097.719 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.097.720 I ggml_metal_init: allocating
0.00.097.722 I ggml_metal_init: found device: Apple M4
0.00.097.724 I ggml_metal_init: picking default device: Apple M4
0.00.098.330 I ggml_metal_init: using embedded metal library
0.00.100.921 I ggml_metal_init: GPU name:   Apple M4
0.00.100.922 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.923 I ggml_metal_init: simdgroup reduction   = true
0.00.100.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.924 I ggml_metal_init: has bfloat            = true
0.00.100.924 I ggml_metal_init: use bfloat            = true
0.00.100.924 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.821 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.112.825 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.112.839 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.678 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.113.679 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.113.680 I llama_new_context_with_model: graph nodes  = 967
0.00.113.680 I llama_new_context_with_model: graph splits = 2
0.00.113.692 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.970.231 I 
0.00.970.266 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.970.287 I perplexity: tokenizing the input ..
0.00.983.371 I perplexity: tokenization took 13.081 ms
0.00.983.410 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.106.023 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.107.666 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.107.692 I llama_perf_context_print:        load time =     945.01 ms
0.01.107.693 I llama_perf_context_print: prompt eval time =     121.70 ms /   128 tokens (    0.95 ms per token,  1051.75 tokens per second)
0.01.107.695 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.107.695 I llama_perf_context_print:       total time =     137.46 ms /   129 tokens
0.01.108.338 I ggml_metal_free: deallocating

real	0m1.297s
user	0m0.126s
sys	0m0.193s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.724 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.373 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.385 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.386 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.387 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.387 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.387 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.388 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.388 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.391 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.392 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.392 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.382 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.463 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.825 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.827 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.828 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.829 I llama_model_loader: - type  f32:  194 tensors
0.00.036.830 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.913 I llm_load_vocab: special tokens cache size = 25
0.00.068.455 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.459 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.459 I llm_load_print_meta: arch             = gptneox
0.00.068.460 I llm_load_print_meta: vocab type       = BPE
0.00.068.460 I llm_load_print_meta: n_vocab          = 50304
0.00.068.460 I llm_load_print_meta: n_merges         = 50009
0.00.068.460 I llm_load_print_meta: vocab_only       = 0
0.00.068.460 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.462 I llm_load_print_meta: n_embd           = 2048
0.00.068.462 I llm_load_print_meta: n_layer          = 24
0.00.068.479 I llm_load_print_meta: n_head           = 16
0.00.068.481 I llm_load_print_meta: n_head_kv        = 16
0.00.068.481 I llm_load_print_meta: n_rot            = 32
0.00.068.481 I llm_load_print_meta: n_swa            = 0
0.00.068.481 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.481 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.482 I llm_load_print_meta: n_gqa            = 1
0.00.068.483 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.483 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.484 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.484 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.484 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.485 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.485 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.485 I llm_load_print_meta: n_ff             = 8192
0.00.068.488 I llm_load_print_meta: n_expert         = 0
0.00.068.488 I llm_load_print_meta: n_expert_used    = 0
0.00.068.488 I llm_load_print_meta: causal attn      = 1
0.00.068.488 I llm_load_print_meta: pooling type     = 0
0.00.068.488 I llm_load_print_meta: rope type        = 2
0.00.068.488 I llm_load_print_meta: rope scaling     = linear
0.00.068.488 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.489 I llm_load_print_meta: freq_scale_train = 1
0.00.068.489 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.489 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.489 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.489 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.489 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.490 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.490 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.499 I llm_load_print_meta: model type       = 1.4B
0.00.068.500 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.500 I llm_load_print_meta: model params     = 1.41 B
0.00.068.501 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.501 I llm_load_print_meta: general.name     = 1.4B
0.00.068.501 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.501 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.501 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.501 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.502 I llm_load_print_meta: LF token         = 128 ''
0.00.068.502 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.502 I llm_load_print_meta: max token length = 1024
0.00.070.436 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.436 I llm_load_tensors: offloading output layer to GPU
0.00.070.436 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.447 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.448 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.403 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.404 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.404 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.405 I llama_new_context_with_model: n_batch       = 2048
0.00.071.405 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.405 I llama_new_context_with_model: flash_attn    = 0
0.00.071.406 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.406 I llama_new_context_with_model: freq_scale    = 1
0.00.071.406 I ggml_metal_init: allocating
0.00.071.410 I ggml_metal_init: found device: Apple M4
0.00.071.412 I ggml_metal_init: picking default device: Apple M4
0.00.072.174 I ggml_metal_init: using embedded metal library
0.00.075.005 I ggml_metal_init: GPU name:   Apple M4
0.00.075.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.007 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.007 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.008 I ggml_metal_init: simdgroup reduction   = true
0.00.075.008 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.008 I ggml_metal_init: has bfloat            = true
0.00.075.008 I ggml_metal_init: use bfloat            = true
0.00.075.009 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.345 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.362 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.382 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.592 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.114.594 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.114.594 I llama_new_context_with_model: graph nodes  = 967
0.00.114.594 I llama_new_context_with_model: graph splits = 2
0.00.114.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.416.897 I main: llama threadpool init, n_threads = 4
0.01.416.938 I 
0.01.416.968 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.416.969 I 
0.01.417.196 I sampler seed: 1234
0.01.417.201 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.417.240 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.417.243 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.417.244 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.510.962 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.02.510.962 I llama_perf_context_print:        load time =    1407.17 ms
0.02.510.963 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.03 tokens per second)
0.02.510.964 I llama_perf_context_print:        eval time =    1046.99 ms /    63 runs   (   16.62 ms per token,    60.17 tokens per second)
0.02.510.964 I llama_perf_context_print:       total time =    1094.07 ms /    70 tokens
0.02.511.148 I ggml_metal_free: deallocating

real	0m2.530s
user	0m0.119s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.995 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.074 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.080 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.088 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.091 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.092 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.092 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.093 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.093 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.094 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.097 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.098 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.101 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.147 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.609 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.610 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.610 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.611 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.611 I llama_model_loader: - type  f32:  194 tensors
0.00.031.612 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.165 I llm_load_vocab: special tokens cache size = 25
0.00.062.378 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.381 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.381 I llm_load_print_meta: arch             = gptneox
0.00.062.381 I llm_load_print_meta: vocab type       = BPE
0.00.062.381 I llm_load_print_meta: n_vocab          = 50304
0.00.062.382 I llm_load_print_meta: n_merges         = 50009
0.00.062.382 I llm_load_print_meta: vocab_only       = 0
0.00.062.382 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.382 I llm_load_print_meta: n_embd           = 2048
0.00.062.382 I llm_load_print_meta: n_layer          = 24
0.00.062.397 I llm_load_print_meta: n_head           = 16
0.00.062.398 I llm_load_print_meta: n_head_kv        = 16
0.00.062.398 I llm_load_print_meta: n_rot            = 32
0.00.062.398 I llm_load_print_meta: n_swa            = 0
0.00.062.398 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.398 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.399 I llm_load_print_meta: n_gqa            = 1
0.00.062.400 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.400 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.401 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.401 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.402 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.403 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.403 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.403 I llm_load_print_meta: n_ff             = 8192
0.00.062.403 I llm_load_print_meta: n_expert         = 0
0.00.062.404 I llm_load_print_meta: n_expert_used    = 0
0.00.062.404 I llm_load_print_meta: causal attn      = 1
0.00.062.404 I llm_load_print_meta: pooling type     = 0
0.00.062.404 I llm_load_print_meta: rope type        = 2
0.00.062.404 I llm_load_print_meta: rope scaling     = linear
0.00.062.406 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.406 I llm_load_print_meta: freq_scale_train = 1
0.00.062.406 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.407 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.407 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.407 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.407 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.407 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.407 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.416 I llm_load_print_meta: model type       = 1.4B
0.00.062.417 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.417 I llm_load_print_meta: model params     = 1.41 B
0.00.062.418 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.418 I llm_load_print_meta: general.name     = 1.4B
0.00.062.418 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.418 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.418 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.418 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.419 I llm_load_print_meta: LF token         = 128 ''
0.00.062.419 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.419 I llm_load_print_meta: max token length = 1024
0.00.064.388 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.388 I llm_load_tensors: offloading output layer to GPU
0.00.064.388 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.399 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.400 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.277 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.278 I llama_new_context_with_model: n_ctx         = 128
0.00.065.278 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.279 I llama_new_context_with_model: n_batch       = 128
0.00.065.279 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.279 I llama_new_context_with_model: flash_attn    = 0
0.00.065.279 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.280 I llama_new_context_with_model: freq_scale    = 1
0.00.065.280 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.280 I ggml_metal_init: allocating
0.00.065.286 I ggml_metal_init: found device: Apple M4
0.00.065.288 I ggml_metal_init: picking default device: Apple M4
0.00.065.852 I ggml_metal_init: using embedded metal library
0.00.068.219 I ggml_metal_init: GPU name:   Apple M4
0.00.068.221 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.221 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.222 I ggml_metal_init: simdgroup reduction   = true
0.00.068.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.222 I ggml_metal_init: has bfloat            = true
0.00.068.222 I ggml_metal_init: use bfloat            = true
0.00.068.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.739 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.077.741 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.077.756 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.078.618 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.078.619 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.078.620 I llama_new_context_with_model: graph nodes  = 967
0.00.078.620 I llama_new_context_with_model: graph splits = 2
0.00.078.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.936.290 I 
0.00.936.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.936.357 I perplexity: tokenizing the input ..
0.00.944.108 I perplexity: tokenization took 7.749 ms
0.00.944.123 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.067.874 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.069.040 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.069.050 I llama_perf_context_print:        load time =     925.29 ms
0.01.069.055 I llama_perf_context_print: prompt eval time =     123.53 ms /   128 tokens (    0.97 ms per token,  1036.22 tokens per second)
0.01.069.056 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.069.057 I llama_perf_context_print:       total time =     132.76 ms /   129 tokens
0.01.069.330 I ggml_metal_free: deallocating

real	0m1.085s
user	0m0.089s
sys	0m0.158s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.013.629 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.204 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.211 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.220 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.220 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.221 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.221 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.223 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.223 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.223 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.224 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.225 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.226 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.228 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.228 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.228 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.129 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.504 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.543 I llama_model_loader: - type  f32:  194 tensors
0.00.045.543 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.544 I llama_model_loader: - type q6_K:    1 tensors
0.00.079.319 I llm_load_vocab: special tokens cache size = 25
0.00.089.570 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.574 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.575 I llm_load_print_meta: arch             = gptneox
0.00.089.576 I llm_load_print_meta: vocab type       = BPE
0.00.089.576 I llm_load_print_meta: n_vocab          = 50304
0.00.089.576 I llm_load_print_meta: n_merges         = 50009
0.00.089.576 I llm_load_print_meta: vocab_only       = 0
0.00.089.577 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.577 I llm_load_print_meta: n_embd           = 2048
0.00.089.577 I llm_load_print_meta: n_layer          = 24
0.00.089.594 I llm_load_print_meta: n_head           = 16
0.00.089.595 I llm_load_print_meta: n_head_kv        = 16
0.00.089.595 I llm_load_print_meta: n_rot            = 32
0.00.089.595 I llm_load_print_meta: n_swa            = 0
0.00.089.595 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.596 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.597 I llm_load_print_meta: n_gqa            = 1
0.00.089.598 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.599 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.600 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.603 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.603 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.604 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.604 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.605 I llm_load_print_meta: n_ff             = 8192
0.00.089.605 I llm_load_print_meta: n_expert         = 0
0.00.089.605 I llm_load_print_meta: n_expert_used    = 0
0.00.089.606 I llm_load_print_meta: causal attn      = 1
0.00.089.606 I llm_load_print_meta: pooling type     = 0
0.00.089.606 I llm_load_print_meta: rope type        = 2
0.00.089.607 I llm_load_print_meta: rope scaling     = linear
0.00.089.607 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.608 I llm_load_print_meta: freq_scale_train = 1
0.00.089.608 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.608 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.609 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.609 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.611 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.611 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.611 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.622 I llm_load_print_meta: model type       = 1.4B
0.00.089.622 I llm_load_print_meta: model ftype      = Q4_0
0.00.089.623 I llm_load_print_meta: model params     = 1.41 B
0.00.089.624 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.089.626 I llm_load_print_meta: general.name     = 1.4B
0.00.089.626 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.627 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.627 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.627 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.627 I llm_load_print_meta: LF token         = 128 ''
0.00.089.628 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.628 I llm_load_print_meta: max token length = 1024
0.00.092.467 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.468 I llm_load_tensors: offloading output layer to GPU
0.00.092.468 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.480 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.092.482 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.093.938 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.939 I llama_new_context_with_model: n_ctx         = 2048
0.00.093.939 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.093.939 I llama_new_context_with_model: n_batch       = 2048
0.00.093.940 I llama_new_context_with_model: n_ubatch      = 512
0.00.093.940 I llama_new_context_with_model: flash_attn    = 0
0.00.093.941 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.941 I llama_new_context_with_model: freq_scale    = 1
0.00.093.942 I ggml_metal_init: allocating
0.00.093.951 I ggml_metal_init: found device: Apple M4
0.00.093.956 I ggml_metal_init: picking default device: Apple M4
0.00.094.870 I ggml_metal_init: using embedded metal library
0.00.098.497 I ggml_metal_init: GPU name:   Apple M4
0.00.098.500 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.500 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.501 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.501 I ggml_metal_init: simdgroup reduction   = true
0.00.098.501 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.501 I ggml_metal_init: has bfloat            = true
0.00.098.502 I ggml_metal_init: use bfloat            = true
0.00.098.502 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.503 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.134.183 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.134.194 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.134.222 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.135.247 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.135.248 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.135.248 I llama_new_context_with_model: graph nodes  = 967
0.00.135.249 I llama_new_context_with_model: graph splits = 2
0.00.135.260 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.886.799 I main: llama threadpool init, n_threads = 4
0.00.886.904 I 
0.00.886.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.886.999 I 
0.00.887.464 I sampler seed: 1234
0.00.887.471 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.887.535 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.887.540 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.887.540 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.575.908 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.575.911 I llama_perf_context_print:        load time =     873.15 ms
0.01.575.913 I llama_perf_context_print: prompt eval time =      45.41 ms /     7 tokens (    6.49 ms per token,   154.16 tokens per second)
0.01.575.917 I llama_perf_context_print:        eval time =     640.39 ms /    63 runs   (   10.16 ms per token,    98.38 tokens per second)
0.01.575.918 I llama_perf_context_print:       total time =     689.13 ms /    70 tokens
0.01.576.168 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.140s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.929 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.550 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.552 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.552 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.554 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.554 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.554 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.447 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.220 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.221 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.221 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.222 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.222 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.223 I llama_model_loader: - type  f32:  194 tensors
0.00.024.223 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.223 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.326 I llm_load_vocab: special tokens cache size = 25
0.00.050.294 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.297 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.297 I llm_load_print_meta: arch             = gptneox
0.00.050.297 I llm_load_print_meta: vocab type       = BPE
0.00.050.298 I llm_load_print_meta: n_vocab          = 50304
0.00.050.298 I llm_load_print_meta: n_merges         = 50009
0.00.050.298 I llm_load_print_meta: vocab_only       = 0
0.00.050.298 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.298 I llm_load_print_meta: n_embd           = 2048
0.00.050.299 I llm_load_print_meta: n_layer          = 24
0.00.050.313 I llm_load_print_meta: n_head           = 16
0.00.050.314 I llm_load_print_meta: n_head_kv        = 16
0.00.050.314 I llm_load_print_meta: n_rot            = 32
0.00.050.314 I llm_load_print_meta: n_swa            = 0
0.00.050.314 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.314 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.315 I llm_load_print_meta: n_gqa            = 1
0.00.050.316 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.316 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.317 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.317 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.319 I llm_load_print_meta: n_ff             = 8192
0.00.050.319 I llm_load_print_meta: n_expert         = 0
0.00.050.321 I llm_load_print_meta: n_expert_used    = 0
0.00.050.321 I llm_load_print_meta: causal attn      = 1
0.00.050.321 I llm_load_print_meta: pooling type     = 0
0.00.050.321 I llm_load_print_meta: rope type        = 2
0.00.050.321 I llm_load_print_meta: rope scaling     = linear
0.00.050.321 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.322 I llm_load_print_meta: freq_scale_train = 1
0.00.050.322 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.322 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.322 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.322 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.322 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.323 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.323 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.332 I llm_load_print_meta: model type       = 1.4B
0.00.050.332 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.333 I llm_load_print_meta: model params     = 1.41 B
0.00.050.333 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.333 I llm_load_print_meta: general.name     = 1.4B
0.00.050.334 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.334 I llm_load_print_meta: LF token         = 128 ''
0.00.050.335 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.335 I llm_load_print_meta: max token length = 1024
0.00.052.285 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.285 I llm_load_tensors: offloading output layer to GPU
0.00.052.285 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.296 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.297 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.219 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.220 I llama_new_context_with_model: n_ctx         = 128
0.00.053.220 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.220 I llama_new_context_with_model: n_batch       = 128
0.00.053.220 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.220 I llama_new_context_with_model: flash_attn    = 0
0.00.053.221 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.221 I llama_new_context_with_model: freq_scale    = 1
0.00.053.222 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.222 I ggml_metal_init: allocating
0.00.053.228 I ggml_metal_init: found device: Apple M4
0.00.053.230 I ggml_metal_init: picking default device: Apple M4
0.00.053.841 I ggml_metal_init: using embedded metal library
0.00.056.159 I ggml_metal_init: GPU name:   Apple M4
0.00.056.161 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.161 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.162 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.162 I ggml_metal_init: simdgroup reduction   = true
0.00.056.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.162 I ggml_metal_init: has bfloat            = true
0.00.056.162 I ggml_metal_init: use bfloat            = true
0.00.056.163 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.311 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.313 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.337 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.217 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.218 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.218 I llama_new_context_with_model: graph nodes  = 967
0.00.068.218 I llama_new_context_with_model: graph splits = 2
0.00.068.231 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.833 I 
0.00.639.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.907 I perplexity: tokenizing the input ..
0.00.648.203 I perplexity: tokenization took 8.294 ms
0.00.648.220 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.771.049 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.772.214 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.772.230 I llama_perf_context_print:        load time =     629.90 ms
0.00.772.232 I llama_perf_context_print: prompt eval time =     122.60 ms /   128 tokens (    0.96 ms per token,  1044.02 tokens per second)
0.00.772.232 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.772.234 I llama_perf_context_print:       total time =     132.40 ms /   129 tokens
0.00.772.709 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.078s
sys	0m0.120s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.011.750 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.735 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.033.741 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.743 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.743 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.744 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.744 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.748 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.749 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.749 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.751 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.754 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.553 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.554 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.554 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.555 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.555 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.555 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.042.556 I llama_model_loader: - type  f32:  194 tensors
0.00.042.556 I llama_model_loader: - type q4_1:   97 tensors
0.00.042.556 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.874 I llm_load_vocab: special tokens cache size = 25
0.00.074.956 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.959 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.960 I llm_load_print_meta: arch             = gptneox
0.00.074.960 I llm_load_print_meta: vocab type       = BPE
0.00.074.960 I llm_load_print_meta: n_vocab          = 50304
0.00.074.960 I llm_load_print_meta: n_merges         = 50009
0.00.074.962 I llm_load_print_meta: vocab_only       = 0
0.00.074.962 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.962 I llm_load_print_meta: n_embd           = 2048
0.00.074.963 I llm_load_print_meta: n_layer          = 24
0.00.074.977 I llm_load_print_meta: n_head           = 16
0.00.074.978 I llm_load_print_meta: n_head_kv        = 16
0.00.074.978 I llm_load_print_meta: n_rot            = 32
0.00.074.978 I llm_load_print_meta: n_swa            = 0
0.00.074.978 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.978 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.979 I llm_load_print_meta: n_gqa            = 1
0.00.074.980 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.980 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.981 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.981 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.983 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.984 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.984 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.984 I llm_load_print_meta: n_ff             = 8192
0.00.074.985 I llm_load_print_meta: n_expert         = 0
0.00.074.985 I llm_load_print_meta: n_expert_used    = 0
0.00.074.985 I llm_load_print_meta: causal attn      = 1
0.00.074.985 I llm_load_print_meta: pooling type     = 0
0.00.074.985 I llm_load_print_meta: rope type        = 2
0.00.074.985 I llm_load_print_meta: rope scaling     = linear
0.00.074.986 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.986 I llm_load_print_meta: freq_scale_train = 1
0.00.074.986 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.986 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.986 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.986 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.986 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.987 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.987 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.996 I llm_load_print_meta: model type       = 1.4B
0.00.074.996 I llm_load_print_meta: model ftype      = Q4_1
0.00.074.997 I llm_load_print_meta: model params     = 1.41 B
0.00.074.997 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.074.997 I llm_load_print_meta: general.name     = 1.4B
0.00.074.997 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.998 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.998 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.998 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.998 I llm_load_print_meta: LF token         = 128 ''
0.00.074.998 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.998 I llm_load_print_meta: max token length = 1024
0.00.076.554 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.554 I llm_load_tensors: offloading output layer to GPU
0.00.076.554 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.565 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.076.566 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.077.390 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.391 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.391 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.391 I llama_new_context_with_model: n_batch       = 2048
0.00.077.391 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.391 I llama_new_context_with_model: flash_attn    = 0
0.00.077.392 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.392 I llama_new_context_with_model: freq_scale    = 1
0.00.077.393 I ggml_metal_init: allocating
0.00.077.399 I ggml_metal_init: found device: Apple M4
0.00.077.402 I ggml_metal_init: picking default device: Apple M4
0.00.078.065 I ggml_metal_init: using embedded metal library
0.00.080.441 I ggml_metal_init: GPU name:   Apple M4
0.00.080.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.444 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.444 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.444 I ggml_metal_init: simdgroup reduction   = true
0.00.080.445 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.445 I ggml_metal_init: has bfloat            = true
0.00.080.445 I ggml_metal_init: use bfloat            = true
0.00.080.445 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.446 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.722 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.728 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.751 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.738 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.739 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.740 I llama_new_context_with_model: graph nodes  = 967
0.00.113.740 I llama_new_context_with_model: graph splits = 2
0.00.113.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.999.862 I main: llama threadpool init, n_threads = 4
0.00.999.967 I 
0.01.000.053 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.000.055 I 
0.01.000.396 I sampler seed: 1234
0.01.000.403 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.000.434 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.000.436 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.000.436 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.743.913 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55038.76 tokens per second)
0.01.743.913 I llama_perf_context_print:        load time =     988.10 ms
0.01.743.914 I llama_perf_context_print: prompt eval time =      50.52 ms /     7 tokens (    7.22 ms per token,   138.56 tokens per second)
0.01.743.915 I llama_perf_context_print:        eval time =     690.00 ms /    63 runs   (   10.95 ms per token,    91.30 tokens per second)
0.01.743.916 I llama_perf_context_print:       total time =     744.06 ms /    70 tokens
0.01.744.121 I ggml_metal_free: deallocating

real	0m1.794s
user	0m0.127s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.774 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.248 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.252 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.258 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.258 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.258 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.259 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.260 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.260 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.262 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.263 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.263 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.265 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.265 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.265 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.071 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.900 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.901 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.901 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.902 I llama_model_loader: - type  f32:  194 tensors
0.00.022.902 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.902 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.930 I llm_load_vocab: special tokens cache size = 25
0.00.048.848 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.850 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.850 I llm_load_print_meta: arch             = gptneox
0.00.048.851 I llm_load_print_meta: vocab type       = BPE
0.00.048.851 I llm_load_print_meta: n_vocab          = 50304
0.00.048.851 I llm_load_print_meta: n_merges         = 50009
0.00.048.851 I llm_load_print_meta: vocab_only       = 0
0.00.048.852 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.852 I llm_load_print_meta: n_embd           = 2048
0.00.048.852 I llm_load_print_meta: n_layer          = 24
0.00.048.867 I llm_load_print_meta: n_head           = 16
0.00.048.867 I llm_load_print_meta: n_head_kv        = 16
0.00.048.868 I llm_load_print_meta: n_rot            = 32
0.00.048.868 I llm_load_print_meta: n_swa            = 0
0.00.048.868 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.868 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.871 I llm_load_print_meta: n_gqa            = 1
0.00.048.872 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.872 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.873 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.873 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.873 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.873 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.874 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.875 I llm_load_print_meta: n_ff             = 8192
0.00.048.875 I llm_load_print_meta: n_expert         = 0
0.00.048.875 I llm_load_print_meta: n_expert_used    = 0
0.00.048.875 I llm_load_print_meta: causal attn      = 1
0.00.048.876 I llm_load_print_meta: pooling type     = 0
0.00.048.876 I llm_load_print_meta: rope type        = 2
0.00.048.876 I llm_load_print_meta: rope scaling     = linear
0.00.048.876 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.877 I llm_load_print_meta: freq_scale_train = 1
0.00.048.877 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.877 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.877 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.877 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.877 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.877 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.878 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.887 I llm_load_print_meta: model type       = 1.4B
0.00.048.887 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.888 I llm_load_print_meta: model params     = 1.41 B
0.00.048.888 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.888 I llm_load_print_meta: general.name     = 1.4B
0.00.048.889 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.889 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.889 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.889 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.890 I llm_load_print_meta: LF token         = 128 ''
0.00.048.890 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.890 I llm_load_print_meta: max token length = 1024
0.00.050.808 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.808 I llm_load_tensors: offloading output layer to GPU
0.00.050.809 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.819 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.820 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.675 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.676 I llama_new_context_with_model: n_ctx         = 128
0.00.051.676 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.676 I llama_new_context_with_model: n_batch       = 128
0.00.051.676 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.677 I llama_new_context_with_model: flash_attn    = 0
0.00.051.677 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.677 I llama_new_context_with_model: freq_scale    = 1
0.00.051.678 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.678 I ggml_metal_init: allocating
0.00.051.683 I ggml_metal_init: found device: Apple M4
0.00.051.685 I ggml_metal_init: picking default device: Apple M4
0.00.052.255 I ggml_metal_init: using embedded metal library
0.00.054.578 I ggml_metal_init: GPU name:   Apple M4
0.00.054.580 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.580 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.581 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.581 I ggml_metal_init: simdgroup reduction   = true
0.00.054.581 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.581 I ggml_metal_init: has bfloat            = true
0.00.054.581 I ggml_metal_init: use bfloat            = true
0.00.054.582 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.582 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.141 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.143 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.156 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.048 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.049 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.049 I llama_new_context_with_model: graph nodes  = 967
0.00.066.049 I llama_new_context_with_model: graph splits = 2
0.00.066.061 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.089 I 
0.00.658.151 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.172 I perplexity: tokenizing the input ..
0.00.666.464 I perplexity: tokenization took 8.291 ms
0.00.666.479 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.256 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.790.492 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.790.508 I llama_perf_context_print:        load time =     649.31 ms
0.00.790.509 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.53 tokens per second)
0.00.790.510 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.510 I llama_perf_context_print:       total time =     132.42 ms /   129 tokens
0.00.790.928 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.077s
sys	0m0.098s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.420 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.798 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.810 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.810 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.811 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.811 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.812 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.813 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.813 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.814 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.814 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.816 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.817 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.817 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.797 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.911 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.751 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.751 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.752 I llama_model_loader: - type  f32:  194 tensors
0.00.025.752 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.752 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.075 I llm_load_vocab: special tokens cache size = 25
0.00.052.114 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.117 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.117 I llm_load_print_meta: arch             = gptneox
0.00.052.117 I llm_load_print_meta: vocab type       = BPE
0.00.052.118 I llm_load_print_meta: n_vocab          = 50304
0.00.052.118 I llm_load_print_meta: n_merges         = 50009
0.00.052.118 I llm_load_print_meta: vocab_only       = 0
0.00.052.118 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.118 I llm_load_print_meta: n_embd           = 2048
0.00.052.118 I llm_load_print_meta: n_layer          = 24
0.00.052.134 I llm_load_print_meta: n_head           = 16
0.00.052.135 I llm_load_print_meta: n_head_kv        = 16
0.00.052.135 I llm_load_print_meta: n_rot            = 32
0.00.052.135 I llm_load_print_meta: n_swa            = 0
0.00.052.135 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.135 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.136 I llm_load_print_meta: n_gqa            = 1
0.00.052.137 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.138 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.138 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.139 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.139 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.139 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.139 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.140 I llm_load_print_meta: n_ff             = 8192
0.00.052.140 I llm_load_print_meta: n_expert         = 0
0.00.052.140 I llm_load_print_meta: n_expert_used    = 0
0.00.052.140 I llm_load_print_meta: causal attn      = 1
0.00.052.140 I llm_load_print_meta: pooling type     = 0
0.00.052.140 I llm_load_print_meta: rope type        = 2
0.00.052.141 I llm_load_print_meta: rope scaling     = linear
0.00.052.141 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.141 I llm_load_print_meta: freq_scale_train = 1
0.00.052.141 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.141 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.143 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.144 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.144 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.144 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.144 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.154 I llm_load_print_meta: model type       = 1.4B
0.00.052.154 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.155 I llm_load_print_meta: model params     = 1.41 B
0.00.052.156 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.156 I llm_load_print_meta: general.name     = 1.4B
0.00.052.156 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.156 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.157 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.157 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.157 I llm_load_print_meta: LF token         = 128 ''
0.00.052.157 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.157 I llm_load_print_meta: max token length = 1024
0.00.054.269 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.269 I llm_load_tensors: offloading output layer to GPU
0.00.054.269 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.280 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.281 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.214 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.215 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.215 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.215 I llama_new_context_with_model: n_batch       = 2048
0.00.055.216 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.216 I llama_new_context_with_model: flash_attn    = 0
0.00.055.216 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.217 I llama_new_context_with_model: freq_scale    = 1
0.00.055.217 I ggml_metal_init: allocating
0.00.055.224 I ggml_metal_init: found device: Apple M4
0.00.055.227 I ggml_metal_init: picking default device: Apple M4
0.00.055.886 I ggml_metal_init: using embedded metal library
0.00.058.282 I ggml_metal_init: GPU name:   Apple M4
0.00.058.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.284 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.285 I ggml_metal_init: simdgroup reduction   = true
0.00.058.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.285 I ggml_metal_init: has bfloat            = true
0.00.058.285 I ggml_metal_init: use bfloat            = true
0.00.058.285 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.286 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.452 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.462 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.481 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.490 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.492 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.492 I llama_new_context_with_model: graph nodes  = 967
0.00.088.492 I llama_new_context_with_model: graph splits = 2
0.00.088.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.000 I main: llama threadpool init, n_threads = 4
0.00.771.042 I 
0.00.771.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.076 I 
0.00.771.309 I sampler seed: 1234
0.00.771.314 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.342 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.343 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.343 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.562.676 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.562.677 I llama_perf_context_print:        load time =     761.58 ms
0.01.562.678 I llama_perf_context_print: prompt eval time =      47.07 ms /     7 tokens (    6.72 ms per token,   148.72 tokens per second)
0.01.562.679 I llama_perf_context_print:        eval time =     741.22 ms /    63 runs   (   11.77 ms per token,    85.00 tokens per second)
0.01.562.679 I llama_perf_context_print:       total time =     791.68 ms /    70 tokens
0.01.562.876 I ggml_metal_free: deallocating

real	0m1.580s
user	0m0.108s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.208 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.203 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.214 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.216 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.217 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.143 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.920 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.921 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.922 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.923 I llama_model_loader: - type  f32:  194 tensors
0.00.023.924 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.924 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.011 I llm_load_vocab: special tokens cache size = 25
0.00.050.055 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.058 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.058 I llm_load_print_meta: arch             = gptneox
0.00.050.059 I llm_load_print_meta: vocab type       = BPE
0.00.050.059 I llm_load_print_meta: n_vocab          = 50304
0.00.050.059 I llm_load_print_meta: n_merges         = 50009
0.00.050.059 I llm_load_print_meta: vocab_only       = 0
0.00.050.059 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.060 I llm_load_print_meta: n_embd           = 2048
0.00.050.060 I llm_load_print_meta: n_layer          = 24
0.00.050.074 I llm_load_print_meta: n_head           = 16
0.00.050.076 I llm_load_print_meta: n_head_kv        = 16
0.00.050.076 I llm_load_print_meta: n_rot            = 32
0.00.050.076 I llm_load_print_meta: n_swa            = 0
0.00.050.076 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.076 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.077 I llm_load_print_meta: n_gqa            = 1
0.00.050.078 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.078 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.079 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.079 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.080 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.080 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.080 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.080 I llm_load_print_meta: n_ff             = 8192
0.00.050.081 I llm_load_print_meta: n_expert         = 0
0.00.050.081 I llm_load_print_meta: n_expert_used    = 0
0.00.050.081 I llm_load_print_meta: causal attn      = 1
0.00.050.081 I llm_load_print_meta: pooling type     = 0
0.00.050.081 I llm_load_print_meta: rope type        = 2
0.00.050.081 I llm_load_print_meta: rope scaling     = linear
0.00.050.082 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.082 I llm_load_print_meta: freq_scale_train = 1
0.00.050.082 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.082 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.082 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.082 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.082 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.083 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.083 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.092 I llm_load_print_meta: model type       = 1.4B
0.00.050.092 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.092 I llm_load_print_meta: model params     = 1.41 B
0.00.050.093 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.093 I llm_load_print_meta: general.name     = 1.4B
0.00.050.093 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.093 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.094 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.094 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.094 I llm_load_print_meta: LF token         = 128 ''
0.00.050.094 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.094 I llm_load_print_meta: max token length = 1024
0.00.052.021 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.021 I llm_load_tensors: offloading output layer to GPU
0.00.052.021 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.032 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.033 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.973 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.974 I llama_new_context_with_model: n_ctx         = 128
0.00.052.975 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.975 I llama_new_context_with_model: n_batch       = 128
0.00.052.975 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.975 I llama_new_context_with_model: flash_attn    = 0
0.00.052.976 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.976 I llama_new_context_with_model: freq_scale    = 1
0.00.052.976 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.977 I ggml_metal_init: allocating
0.00.052.980 I ggml_metal_init: found device: Apple M4
0.00.052.982 I ggml_metal_init: picking default device: Apple M4
0.00.053.556 I ggml_metal_init: using embedded metal library
0.00.055.948 I ggml_metal_init: GPU name:   Apple M4
0.00.055.949 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.950 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.950 I ggml_metal_init: simdgroup reduction   = true
0.00.055.950 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.950 I ggml_metal_init: has bfloat            = true
0.00.055.951 I ggml_metal_init: use bfloat            = true
0.00.055.951 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.952 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.821 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.827 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.840 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.770 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.771 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.771 I llama_new_context_with_model: graph nodes  = 967
0.00.067.772 I llama_new_context_with_model: graph splits = 2
0.00.067.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.625 I 
0.00.700.670 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.679 I perplexity: tokenizing the input ..
0.00.708.935 I perplexity: tokenization took 8.255 ms
0.00.708.948 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.999 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.845.280 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.845.302 I llama_perf_context_print:        load time =     691.41 ms
0.00.845.303 I llama_perf_context_print: prompt eval time =     134.83 ms /   128 tokens (    1.05 ms per token,   949.37 tokens per second)
0.00.845.304 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.304 I llama_perf_context_print:       total time =     144.68 ms /   129 tokens
0.00.845.759 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.078s
sys	0m0.112s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.157 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.048 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.059 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.059 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.060 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.067 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.934 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.934 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.934 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.935 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.935 I llama_model_loader: - type  f32:  194 tensors
0.00.023.936 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.936 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.148 I llm_load_vocab: special tokens cache size = 25
0.00.050.152 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.154 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.155 I llm_load_print_meta: arch             = gptneox
0.00.050.155 I llm_load_print_meta: vocab type       = BPE
0.00.050.155 I llm_load_print_meta: n_vocab          = 50304
0.00.050.155 I llm_load_print_meta: n_merges         = 50009
0.00.050.156 I llm_load_print_meta: vocab_only       = 0
0.00.050.156 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.156 I llm_load_print_meta: n_embd           = 2048
0.00.050.156 I llm_load_print_meta: n_layer          = 24
0.00.050.170 I llm_load_print_meta: n_head           = 16
0.00.050.172 I llm_load_print_meta: n_head_kv        = 16
0.00.050.172 I llm_load_print_meta: n_rot            = 32
0.00.050.172 I llm_load_print_meta: n_swa            = 0
0.00.050.173 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.173 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.174 I llm_load_print_meta: n_gqa            = 1
0.00.050.174 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.175 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.176 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.176 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.176 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.176 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.176 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.177 I llm_load_print_meta: n_ff             = 8192
0.00.050.177 I llm_load_print_meta: n_expert         = 0
0.00.050.177 I llm_load_print_meta: n_expert_used    = 0
0.00.050.177 I llm_load_print_meta: causal attn      = 1
0.00.050.178 I llm_load_print_meta: pooling type     = 0
0.00.050.178 I llm_load_print_meta: rope type        = 2
0.00.050.179 I llm_load_print_meta: rope scaling     = linear
0.00.050.179 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.179 I llm_load_print_meta: freq_scale_train = 1
0.00.050.179 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.179 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.179 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.180 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.180 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.182 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.182 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.191 I llm_load_print_meta: model type       = 1.4B
0.00.050.191 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.192 I llm_load_print_meta: model params     = 1.41 B
0.00.050.192 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.192 I llm_load_print_meta: general.name     = 1.4B
0.00.050.192 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.193 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.193 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.193 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.193 I llm_load_print_meta: LF token         = 128 ''
0.00.050.193 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.194 I llm_load_print_meta: max token length = 1024
0.00.052.172 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.172 I llm_load_tensors: offloading output layer to GPU
0.00.052.172 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.183 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.184 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.092 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.093 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.093 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.093 I llama_new_context_with_model: n_batch       = 2048
0.00.053.093 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.093 I llama_new_context_with_model: flash_attn    = 0
0.00.053.094 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.094 I llama_new_context_with_model: freq_scale    = 1
0.00.053.095 I ggml_metal_init: allocating
0.00.053.098 I ggml_metal_init: found device: Apple M4
0.00.053.100 I ggml_metal_init: picking default device: Apple M4
0.00.053.689 I ggml_metal_init: using embedded metal library
0.00.055.995 I ggml_metal_init: GPU name:   Apple M4
0.00.055.996 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.997 I ggml_metal_init: simdgroup reduction   = true
0.00.055.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.997 I ggml_metal_init: has bfloat            = true
0.00.055.998 I ggml_metal_init: use bfloat            = true
0.00.055.998 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.999 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.747 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.756 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.774 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.865 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.866 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.867 I llama_new_context_with_model: graph nodes  = 967
0.00.085.867 I llama_new_context_with_model: graph splits = 2
0.00.085.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.822 I main: llama threadpool init, n_threads = 4
0.00.704.859 I 
0.00.704.889 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.890 I 
0.00.705.115 I sampler seed: 1234
0.00.705.119 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.150 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.152 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.152 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.549.334 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61101.55 tokens per second)
0.01.549.334 I llama_perf_context_print:        load time =     695.66 ms
0.01.549.335 I llama_perf_context_print: prompt eval time =      46.27 ms /     7 tokens (    6.61 ms per token,   151.28 tokens per second)
0.01.549.336 I llama_perf_context_print:        eval time =     795.03 ms /    63 runs   (   12.62 ms per token,    79.24 tokens per second)
0.01.549.336 I llama_perf_context_print:       total time =     844.51 ms /    70 tokens
0.01.549.545 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.110s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.840 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.417 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.423 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.428 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.428 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.429 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.429 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.430 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.430 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.431 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.431 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.431 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.432 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.344 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.442 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.444 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.444 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.444 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.445 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.445 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.445 I llama_model_loader: - type  f32:  194 tensors
0.00.023.446 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.446 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.466 I llm_load_vocab: special tokens cache size = 25
0.00.050.462 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.464 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.464 I llm_load_print_meta: arch             = gptneox
0.00.050.465 I llm_load_print_meta: vocab type       = BPE
0.00.050.465 I llm_load_print_meta: n_vocab          = 50304
0.00.050.465 I llm_load_print_meta: n_merges         = 50009
0.00.050.465 I llm_load_print_meta: vocab_only       = 0
0.00.050.466 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.466 I llm_load_print_meta: n_embd           = 2048
0.00.050.466 I llm_load_print_meta: n_layer          = 24
0.00.050.476 I llm_load_print_meta: n_head           = 16
0.00.050.476 I llm_load_print_meta: n_head_kv        = 16
0.00.050.477 I llm_load_print_meta: n_rot            = 32
0.00.050.477 I llm_load_print_meta: n_swa            = 0
0.00.050.477 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.477 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.478 I llm_load_print_meta: n_gqa            = 1
0.00.050.479 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.480 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.481 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.481 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.482 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.482 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.482 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.483 I llm_load_print_meta: n_ff             = 8192
0.00.050.483 I llm_load_print_meta: n_expert         = 0
0.00.050.485 I llm_load_print_meta: n_expert_used    = 0
0.00.050.485 I llm_load_print_meta: causal attn      = 1
0.00.050.485 I llm_load_print_meta: pooling type     = 0
0.00.050.485 I llm_load_print_meta: rope type        = 2
0.00.050.485 I llm_load_print_meta: rope scaling     = linear
0.00.050.487 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.488 I llm_load_print_meta: freq_scale_train = 1
0.00.050.488 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.488 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.489 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.489 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.489 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.489 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.489 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.493 I llm_load_print_meta: model type       = 1.4B
0.00.050.494 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.494 I llm_load_print_meta: model params     = 1.41 B
0.00.050.495 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.495 I llm_load_print_meta: general.name     = 1.4B
0.00.050.495 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.496 I llm_load_print_meta: LF token         = 128 ''
0.00.050.496 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.496 I llm_load_print_meta: max token length = 1024
0.00.052.346 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.346 I llm_load_tensors: offloading output layer to GPU
0.00.052.347 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.352 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.352 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.333 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.334 I llama_new_context_with_model: n_ctx         = 128
0.00.053.334 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.335 I llama_new_context_with_model: n_batch       = 128
0.00.053.335 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.335 I llama_new_context_with_model: flash_attn    = 0
0.00.053.335 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.336 I llama_new_context_with_model: freq_scale    = 1
0.00.053.336 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.337 I ggml_metal_init: allocating
0.00.053.342 I ggml_metal_init: found device: Apple M4
0.00.053.345 I ggml_metal_init: picking default device: Apple M4
0.00.053.943 I ggml_metal_init: using embedded metal library
0.00.056.268 I ggml_metal_init: GPU name:   Apple M4
0.00.056.269 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.269 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.270 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.270 I ggml_metal_init: simdgroup reduction   = true
0.00.056.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.270 I ggml_metal_init: has bfloat            = true
0.00.056.270 I ggml_metal_init: use bfloat            = true
0.00.056.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.922 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.928 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.943 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.787 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.789 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.789 I llama_new_context_with_model: graph nodes  = 967
0.00.067.789 I llama_new_context_with_model: graph splits = 2
0.00.067.802 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.403 I 
0.00.661.435 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.444 I perplexity: tokenizing the input ..
0.00.669.449 I perplexity: tokenization took 8.004 ms
0.00.669.462 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.984 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.805.164 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.805.183 I llama_perf_context_print:        load time =     652.56 ms
0.00.805.184 I llama_perf_context_print: prompt eval time =     134.30 ms /   128 tokens (    1.05 ms per token,   953.10 tokens per second)
0.00.805.185 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.185 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.00.805.632 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.078s
sys	0m0.121s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.144 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.802 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.807 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.808 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.810 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.818 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.790 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.729 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.730 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.731 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.731 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.731 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.732 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.732 I llama_model_loader: - type  f32:  194 tensors
0.00.023.732 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.733 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.733 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.849 I llm_load_vocab: special tokens cache size = 25
0.00.050.958 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.961 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.961 I llm_load_print_meta: arch             = gptneox
0.00.050.962 I llm_load_print_meta: vocab type       = BPE
0.00.050.962 I llm_load_print_meta: n_vocab          = 50304
0.00.050.962 I llm_load_print_meta: n_merges         = 50009
0.00.050.962 I llm_load_print_meta: vocab_only       = 0
0.00.050.962 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.962 I llm_load_print_meta: n_embd           = 2048
0.00.050.963 I llm_load_print_meta: n_layer          = 24
0.00.050.977 I llm_load_print_meta: n_head           = 16
0.00.050.979 I llm_load_print_meta: n_head_kv        = 16
0.00.050.979 I llm_load_print_meta: n_rot            = 32
0.00.050.979 I llm_load_print_meta: n_swa            = 0
0.00.050.979 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.979 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.980 I llm_load_print_meta: n_gqa            = 1
0.00.050.981 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.981 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.982 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.982 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.982 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.983 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.983 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.983 I llm_load_print_meta: n_ff             = 8192
0.00.050.983 I llm_load_print_meta: n_expert         = 0
0.00.050.984 I llm_load_print_meta: n_expert_used    = 0
0.00.050.984 I llm_load_print_meta: causal attn      = 1
0.00.050.985 I llm_load_print_meta: pooling type     = 0
0.00.050.986 I llm_load_print_meta: rope type        = 2
0.00.050.986 I llm_load_print_meta: rope scaling     = linear
0.00.050.986 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.986 I llm_load_print_meta: freq_scale_train = 1
0.00.050.986 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.987 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.987 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.987 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.988 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.988 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.988 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.997 I llm_load_print_meta: model type       = 1.4B
0.00.050.998 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.998 I llm_load_print_meta: model params     = 1.41 B
0.00.050.999 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.999 I llm_load_print_meta: general.name     = 1.4B
0.00.051.000 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.000 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.000 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.000 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.000 I llm_load_print_meta: LF token         = 128 ''
0.00.051.001 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.001 I llm_load_print_meta: max token length = 1024
0.00.052.921 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.921 I llm_load_tensors: offloading output layer to GPU
0.00.052.922 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.932 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.933 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.890 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.891 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.891 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.891 I llama_new_context_with_model: n_batch       = 2048
0.00.053.892 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.892 I llama_new_context_with_model: flash_attn    = 0
0.00.053.892 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.893 I llama_new_context_with_model: freq_scale    = 1
0.00.053.893 I ggml_metal_init: allocating
0.00.053.896 I ggml_metal_init: found device: Apple M4
0.00.053.898 I ggml_metal_init: picking default device: Apple M4
0.00.054.490 I ggml_metal_init: using embedded metal library
0.00.056.853 I ggml_metal_init: GPU name:   Apple M4
0.00.056.854 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.855 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.855 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.855 I ggml_metal_init: simdgroup reduction   = true
0.00.056.855 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.856 I ggml_metal_init: has bfloat            = true
0.00.056.856 I ggml_metal_init: use bfloat            = true
0.00.056.856 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.426 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.431 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.450 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.523 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.525 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.525 I llama_new_context_with_model: graph nodes  = 967
0.00.088.526 I llama_new_context_with_model: graph splits = 2
0.00.088.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.448.285 I main: llama threadpool init, n_threads = 4
0.00.448.323 I 
0.00.448.355 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.448.355 I 
0.00.448.584 I sampler seed: 1234
0.00.448.591 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.448.627 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.448.631 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.448.631 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.125.967 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.125.969 I llama_perf_context_print:        load time =     439.13 ms
0.01.125.969 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.47 tokens per second)
0.01.125.970 I llama_perf_context_print:        eval time =     638.73 ms /    63 runs   (   10.14 ms per token,    98.63 tokens per second)
0.01.125.970 I llama_perf_context_print:       total time =     677.69 ms /    70 tokens
0.01.126.179 I ggml_metal_free: deallocating

real	0m1.145s
user	0m0.111s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.568 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.074 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.078 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.079 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.080 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.080 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.087 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.088 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.993 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.906 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.909 I llama_model_loader: - type  f32:  194 tensors
0.00.024.910 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.910 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.910 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.823 I llm_load_vocab: special tokens cache size = 25
0.00.051.846 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.849 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.849 I llm_load_print_meta: arch             = gptneox
0.00.051.849 I llm_load_print_meta: vocab type       = BPE
0.00.051.850 I llm_load_print_meta: n_vocab          = 50304
0.00.051.850 I llm_load_print_meta: n_merges         = 50009
0.00.051.850 I llm_load_print_meta: vocab_only       = 0
0.00.051.850 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.851 I llm_load_print_meta: n_embd           = 2048
0.00.051.851 I llm_load_print_meta: n_layer          = 24
0.00.051.865 I llm_load_print_meta: n_head           = 16
0.00.051.866 I llm_load_print_meta: n_head_kv        = 16
0.00.051.867 I llm_load_print_meta: n_rot            = 32
0.00.051.867 I llm_load_print_meta: n_swa            = 0
0.00.051.867 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.867 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.868 I llm_load_print_meta: n_gqa            = 1
0.00.051.868 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.869 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.870 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.870 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.870 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.870 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.870 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.871 I llm_load_print_meta: n_ff             = 8192
0.00.051.871 I llm_load_print_meta: n_expert         = 0
0.00.051.871 I llm_load_print_meta: n_expert_used    = 0
0.00.051.872 I llm_load_print_meta: causal attn      = 1
0.00.051.872 I llm_load_print_meta: pooling type     = 0
0.00.051.872 I llm_load_print_meta: rope type        = 2
0.00.051.873 I llm_load_print_meta: rope scaling     = linear
0.00.051.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.876 I llm_load_print_meta: freq_scale_train = 1
0.00.051.876 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.876 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.876 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.877 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.878 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.878 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.878 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.887 I llm_load_print_meta: model type       = 1.4B
0.00.051.888 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.888 I llm_load_print_meta: model params     = 1.41 B
0.00.051.889 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.889 I llm_load_print_meta: general.name     = 1.4B
0.00.051.889 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.889 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.889 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.889 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.890 I llm_load_print_meta: LF token         = 128 ''
0.00.051.890 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.890 I llm_load_print_meta: max token length = 1024
0.00.053.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.826 I llm_load_tensors: offloading output layer to GPU
0.00.053.826 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.836 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.837 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.763 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.764 I llama_new_context_with_model: n_ctx         = 128
0.00.054.765 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.765 I llama_new_context_with_model: n_batch       = 128
0.00.054.765 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.765 I llama_new_context_with_model: flash_attn    = 0
0.00.054.766 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.766 I llama_new_context_with_model: freq_scale    = 1
0.00.054.766 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.767 I ggml_metal_init: allocating
0.00.054.770 I ggml_metal_init: found device: Apple M4
0.00.054.772 I ggml_metal_init: picking default device: Apple M4
0.00.055.371 I ggml_metal_init: using embedded metal library
0.00.057.890 I ggml_metal_init: GPU name:   Apple M4
0.00.057.892 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.892 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.892 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.893 I ggml_metal_init: simdgroup reduction   = true
0.00.057.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.893 I ggml_metal_init: has bfloat            = true
0.00.057.893 I ggml_metal_init: use bfloat            = true
0.00.057.894 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.057 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.060 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.073 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.953 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.954 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.954 I llama_new_context_with_model: graph nodes  = 967
0.00.069.955 I llama_new_context_with_model: graph splits = 2
0.00.069.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.388.018 I 
0.00.388.048 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.388.056 I perplexity: tokenizing the input ..
0.00.396.083 I perplexity: tokenization took 8.025 ms
0.00.396.098 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.193 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.529.616 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.529.637 I llama_perf_context_print:        load time =     377.45 ms
0.00.529.637 I llama_perf_context_print: prompt eval time =     131.79 ms /   128 tokens (    1.03 ms per token,   971.23 tokens per second)
0.00.529.638 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.638 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.529.992 I ggml_metal_free: deallocating

real	0m0.547s
user	0m0.079s
sys	0m0.066s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.644 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.542 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.549 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.551 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.551 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.555 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.555 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.559 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.559 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.592 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.640 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.624 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.626 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.627 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.627 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.628 I llama_model_loader: - type  f32:  194 tensors
0.00.024.628 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.628 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.629 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.629 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.941 I llm_load_vocab: special tokens cache size = 25
0.00.052.921 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.925 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.925 I llm_load_print_meta: arch             = gptneox
0.00.052.926 I llm_load_print_meta: vocab type       = BPE
0.00.052.926 I llm_load_print_meta: n_vocab          = 50304
0.00.052.926 I llm_load_print_meta: n_merges         = 50009
0.00.052.926 I llm_load_print_meta: vocab_only       = 0
0.00.052.926 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.927 I llm_load_print_meta: n_embd           = 2048
0.00.052.927 I llm_load_print_meta: n_layer          = 24
0.00.052.944 I llm_load_print_meta: n_head           = 16
0.00.052.946 I llm_load_print_meta: n_head_kv        = 16
0.00.052.946 I llm_load_print_meta: n_rot            = 32
0.00.052.946 I llm_load_print_meta: n_swa            = 0
0.00.052.946 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.946 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.947 I llm_load_print_meta: n_gqa            = 1
0.00.052.948 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.950 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.950 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.951 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.951 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.951 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.951 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.952 I llm_load_print_meta: n_ff             = 8192
0.00.052.952 I llm_load_print_meta: n_expert         = 0
0.00.052.952 I llm_load_print_meta: n_expert_used    = 0
0.00.052.952 I llm_load_print_meta: causal attn      = 1
0.00.052.952 I llm_load_print_meta: pooling type     = 0
0.00.052.952 I llm_load_print_meta: rope type        = 2
0.00.052.954 I llm_load_print_meta: rope scaling     = linear
0.00.052.954 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.956 I llm_load_print_meta: freq_scale_train = 1
0.00.052.956 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.956 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.956 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.956 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.956 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.956 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.957 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.966 I llm_load_print_meta: model type       = 1.4B
0.00.052.966 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.967 I llm_load_print_meta: model params     = 1.41 B
0.00.052.967 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.968 I llm_load_print_meta: general.name     = 1.4B
0.00.052.969 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: LF token         = 128 ''
0.00.052.970 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.970 I llm_load_print_meta: max token length = 1024
0.00.054.995 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.995 I llm_load_tensors: offloading output layer to GPU
0.00.054.996 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.006 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.008 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.948 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.949 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.949 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.949 I llama_new_context_with_model: n_batch       = 2048
0.00.055.950 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.950 I llama_new_context_with_model: flash_attn    = 0
0.00.055.952 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.952 I llama_new_context_with_model: freq_scale    = 1
0.00.055.953 I ggml_metal_init: allocating
0.00.055.957 I ggml_metal_init: found device: Apple M4
0.00.055.960 I ggml_metal_init: picking default device: Apple M4
0.00.056.572 I ggml_metal_init: using embedded metal library
0.00.058.909 I ggml_metal_init: GPU name:   Apple M4
0.00.058.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.911 I ggml_metal_init: simdgroup reduction   = true
0.00.058.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.912 I ggml_metal_init: has bfloat            = true
0.00.058.912 I ggml_metal_init: use bfloat            = true
0.00.058.912 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.913 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.752 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.763 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.784 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.711 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.713 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.713 I llama_new_context_with_model: graph nodes  = 967
0.00.089.713 I llama_new_context_with_model: graph splits = 2
0.00.089.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.340 I main: llama threadpool init, n_threads = 4
0.00.543.378 I 
0.00.543.429 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.430 I 
0.00.543.677 I sampler seed: 1234
0.00.543.681 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.543.738 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.543.739 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.543.740 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.286.972 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.286.972 I llama_perf_context_print:        load time =     534.69 ms
0.01.286.973 I llama_perf_context_print: prompt eval time =      40.48 ms /     7 tokens (    5.78 ms per token,   172.91 tokens per second)
0.01.286.974 I llama_perf_context_print:        eval time =     699.69 ms /    63 runs   (   11.11 ms per token,    90.04 tokens per second)
0.01.286.974 I llama_perf_context_print:       total time =     743.63 ms /    70 tokens
0.01.287.180 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.112s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.954 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.902 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.907 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.908 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.909 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.909 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.909 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.910 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.910 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.911 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.911 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.912 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.912 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.912 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.914 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.914 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.556 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.461 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.462 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.463 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.463 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.463 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.464 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.464 I llama_model_loader: - type  f32:  194 tensors
0.00.023.464 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.465 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.465 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.465 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.547 I llm_load_vocab: special tokens cache size = 25
0.00.049.665 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.668 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.668 I llm_load_print_meta: arch             = gptneox
0.00.049.668 I llm_load_print_meta: vocab type       = BPE
0.00.049.668 I llm_load_print_meta: n_vocab          = 50304
0.00.049.669 I llm_load_print_meta: n_merges         = 50009
0.00.049.669 I llm_load_print_meta: vocab_only       = 0
0.00.049.669 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.669 I llm_load_print_meta: n_embd           = 2048
0.00.049.669 I llm_load_print_meta: n_layer          = 24
0.00.049.679 I llm_load_print_meta: n_head           = 16
0.00.049.680 I llm_load_print_meta: n_head_kv        = 16
0.00.049.680 I llm_load_print_meta: n_rot            = 32
0.00.049.680 I llm_load_print_meta: n_swa            = 0
0.00.049.680 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.681 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.681 I llm_load_print_meta: n_gqa            = 1
0.00.049.682 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.683 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.683 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.684 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.684 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.684 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.684 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.685 I llm_load_print_meta: n_ff             = 8192
0.00.049.687 I llm_load_print_meta: n_expert         = 0
0.00.049.687 I llm_load_print_meta: n_expert_used    = 0
0.00.049.687 I llm_load_print_meta: causal attn      = 1
0.00.049.687 I llm_load_print_meta: pooling type     = 0
0.00.049.688 I llm_load_print_meta: rope type        = 2
0.00.049.688 I llm_load_print_meta: rope scaling     = linear
0.00.049.688 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.688 I llm_load_print_meta: freq_scale_train = 1
0.00.049.689 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.689 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.689 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.689 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.689 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.689 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.689 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.694 I llm_load_print_meta: model type       = 1.4B
0.00.049.694 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.695 I llm_load_print_meta: model params     = 1.41 B
0.00.049.696 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.696 I llm_load_print_meta: general.name     = 1.4B
0.00.049.696 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.696 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.696 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.697 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.697 I llm_load_print_meta: LF token         = 128 ''
0.00.049.697 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.698 I llm_load_print_meta: max token length = 1024
0.00.051.419 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.419 I llm_load_tensors: offloading output layer to GPU
0.00.051.419 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.424 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.425 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.271 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.272 I llama_new_context_with_model: n_ctx         = 128
0.00.052.272 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.272 I llama_new_context_with_model: n_batch       = 128
0.00.052.272 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.273 I llama_new_context_with_model: flash_attn    = 0
0.00.052.273 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.273 I llama_new_context_with_model: freq_scale    = 1
0.00.052.274 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.274 I ggml_metal_init: allocating
0.00.052.277 I ggml_metal_init: found device: Apple M4
0.00.052.279 I ggml_metal_init: picking default device: Apple M4
0.00.052.845 I ggml_metal_init: using embedded metal library
0.00.055.204 I ggml_metal_init: GPU name:   Apple M4
0.00.055.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.205 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.206 I ggml_metal_init: simdgroup reduction   = true
0.00.055.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.206 I ggml_metal_init: has bfloat            = true
0.00.055.207 I ggml_metal_init: use bfloat            = true
0.00.055.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.461 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.466 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.482 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.395 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.396 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.397 I llama_new_context_with_model: graph nodes  = 967
0.00.067.397 I llama_new_context_with_model: graph splits = 2
0.00.067.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.506 I 
0.00.476.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.476.547 I perplexity: tokenizing the input ..
0.00.483.857 I perplexity: tokenization took 7.308 ms
0.00.483.867 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.616.044 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.617.300 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.617.317 I llama_perf_context_print:        load time =     467.55 ms
0.00.617.317 I llama_perf_context_print: prompt eval time =     131.95 ms /   128 tokens (    1.03 ms per token,   970.06 tokens per second)
0.00.617.318 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.617.319 I llama_perf_context_print:       total time =     140.81 ms /   129 tokens
0.00.617.840 I ggml_metal_free: deallocating

real	0m0.632s
user	0m0.078s
sys	0m0.084s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.011.326 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.514 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.525 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.525 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.526 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.527 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.527 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.528 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.528 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.532 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.500 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.451 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.453 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.453 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.453 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.453 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.454 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.454 I llama_model_loader: - type  f32:  194 tensors
0.00.026.455 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.455 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.455 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.582 I llm_load_vocab: special tokens cache size = 25
0.00.052.599 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.602 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.602 I llm_load_print_meta: arch             = gptneox
0.00.052.603 I llm_load_print_meta: vocab type       = BPE
0.00.052.603 I llm_load_print_meta: n_vocab          = 50304
0.00.052.603 I llm_load_print_meta: n_merges         = 50009
0.00.052.603 I llm_load_print_meta: vocab_only       = 0
0.00.052.603 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.604 I llm_load_print_meta: n_embd           = 2048
0.00.052.604 I llm_load_print_meta: n_layer          = 24
0.00.052.613 I llm_load_print_meta: n_head           = 16
0.00.052.614 I llm_load_print_meta: n_head_kv        = 16
0.00.052.614 I llm_load_print_meta: n_rot            = 32
0.00.052.614 I llm_load_print_meta: n_swa            = 0
0.00.052.616 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.616 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.617 I llm_load_print_meta: n_gqa            = 1
0.00.052.618 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.618 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.619 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.619 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.619 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.620 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.620 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.620 I llm_load_print_meta: n_ff             = 8192
0.00.052.621 I llm_load_print_meta: n_expert         = 0
0.00.052.621 I llm_load_print_meta: n_expert_used    = 0
0.00.052.621 I llm_load_print_meta: causal attn      = 1
0.00.052.621 I llm_load_print_meta: pooling type     = 0
0.00.052.621 I llm_load_print_meta: rope type        = 2
0.00.052.622 I llm_load_print_meta: rope scaling     = linear
0.00.052.624 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.624 I llm_load_print_meta: freq_scale_train = 1
0.00.052.624 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.624 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.625 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.625 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.625 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.626 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.626 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.631 I llm_load_print_meta: model type       = 1.4B
0.00.052.631 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.633 I llm_load_print_meta: model params     = 1.41 B
0.00.052.633 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.633 I llm_load_print_meta: general.name     = 1.4B
0.00.052.634 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.634 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.634 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.634 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.634 I llm_load_print_meta: LF token         = 128 ''
0.00.052.634 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.636 I llm_load_print_meta: max token length = 1024
0.00.054.369 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.369 I llm_load_tensors: offloading output layer to GPU
0.00.054.370 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.375 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.376 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.256 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.257 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.257 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.258 I llama_new_context_with_model: n_batch       = 2048
0.00.055.258 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.258 I llama_new_context_with_model: flash_attn    = 0
0.00.055.259 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.259 I llama_new_context_with_model: freq_scale    = 1
0.00.055.259 I ggml_metal_init: allocating
0.00.055.263 I ggml_metal_init: found device: Apple M4
0.00.055.265 I ggml_metal_init: picking default device: Apple M4
0.00.055.856 I ggml_metal_init: using embedded metal library
0.00.058.238 I ggml_metal_init: GPU name:   Apple M4
0.00.058.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.240 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.240 I ggml_metal_init: simdgroup reduction   = true
0.00.058.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.241 I ggml_metal_init: has bfloat            = true
0.00.058.241 I ggml_metal_init: use bfloat            = true
0.00.058.241 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.477 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.493 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.514 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.606 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.607 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.608 I llama_new_context_with_model: graph nodes  = 967
0.00.088.608 I llama_new_context_with_model: graph splits = 2
0.00.088.623 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.434 I main: llama threadpool init, n_threads = 4
0.00.616.479 I 
0.00.616.508 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.509 I 
0.00.616.681 I sampler seed: 1234
0.00.616.686 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.616.696 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.616.696 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.616.696 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.380.451 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.380.452 I llama_perf_context_print:        load time =     605.10 ms
0.01.380.453 I llama_perf_context_print: prompt eval time =      50.97 ms /     7 tokens (    7.28 ms per token,   137.33 tokens per second)
0.01.380.456 I llama_perf_context_print:        eval time =     709.62 ms /    63 runs   (   11.26 ms per token,    88.78 tokens per second)
0.01.380.456 I llama_perf_context_print:       total time =     764.02 ms /    70 tokens
0.01.380.608 I ggml_metal_free: deallocating

real	0m1.401s
user	0m0.110s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.835 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.598 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.603 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.609 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.610 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.610 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.610 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.611 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.612 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.612 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.612 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.613 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.613 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.613 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.614 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.616 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.289 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.289 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.290 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.290 I llama_model_loader: - type  f32:  194 tensors
0.00.024.291 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.291 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.291 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.412 I llm_load_vocab: special tokens cache size = 25
0.00.050.324 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.327 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.327 I llm_load_print_meta: arch             = gptneox
0.00.050.328 I llm_load_print_meta: vocab type       = BPE
0.00.050.328 I llm_load_print_meta: n_vocab          = 50304
0.00.050.328 I llm_load_print_meta: n_merges         = 50009
0.00.050.328 I llm_load_print_meta: vocab_only       = 0
0.00.050.328 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.329 I llm_load_print_meta: n_embd           = 2048
0.00.050.329 I llm_load_print_meta: n_layer          = 24
0.00.050.343 I llm_load_print_meta: n_head           = 16
0.00.050.344 I llm_load_print_meta: n_head_kv        = 16
0.00.050.345 I llm_load_print_meta: n_rot            = 32
0.00.050.345 I llm_load_print_meta: n_swa            = 0
0.00.050.345 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.345 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.346 I llm_load_print_meta: n_gqa            = 1
0.00.050.347 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.347 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.348 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.348 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.348 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.348 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.349 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.349 I llm_load_print_meta: n_ff             = 8192
0.00.050.349 I llm_load_print_meta: n_expert         = 0
0.00.050.349 I llm_load_print_meta: n_expert_used    = 0
0.00.050.349 I llm_load_print_meta: causal attn      = 1
0.00.050.350 I llm_load_print_meta: pooling type     = 0
0.00.050.350 I llm_load_print_meta: rope type        = 2
0.00.050.350 I llm_load_print_meta: rope scaling     = linear
0.00.050.350 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.350 I llm_load_print_meta: freq_scale_train = 1
0.00.050.350 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.351 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.351 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.352 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.352 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.352 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.353 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.363 I llm_load_print_meta: model type       = 1.4B
0.00.050.364 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.364 I llm_load_print_meta: model params     = 1.41 B
0.00.050.364 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.365 I llm_load_print_meta: general.name     = 1.4B
0.00.050.365 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: LF token         = 128 ''
0.00.050.366 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.366 I llm_load_print_meta: max token length = 1024
0.00.052.322 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.322 I llm_load_tensors: offloading output layer to GPU
0.00.052.322 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.333 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.334 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.273 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.274 I llama_new_context_with_model: n_ctx         = 128
0.00.053.274 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.274 I llama_new_context_with_model: n_batch       = 128
0.00.053.274 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.274 I llama_new_context_with_model: flash_attn    = 0
0.00.053.275 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.275 I llama_new_context_with_model: freq_scale    = 1
0.00.053.275 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.276 I ggml_metal_init: allocating
0.00.053.279 I ggml_metal_init: found device: Apple M4
0.00.053.281 I ggml_metal_init: picking default device: Apple M4
0.00.053.835 I ggml_metal_init: using embedded metal library
0.00.056.188 I ggml_metal_init: GPU name:   Apple M4
0.00.056.189 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.190 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.190 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.190 I ggml_metal_init: simdgroup reduction   = true
0.00.056.191 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.191 I ggml_metal_init: has bfloat            = true
0.00.056.191 I ggml_metal_init: use bfloat            = true
0.00.056.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.192 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.927 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.929 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.943 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.877 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.879 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.879 I llama_new_context_with_model: graph nodes  = 967
0.00.067.879 I llama_new_context_with_model: graph splits = 2
0.00.067.892 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.524 I 
0.00.559.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.559.564 I perplexity: tokenizing the input ..
0.00.567.408 I perplexity: tokenization took 7.842 ms
0.00.567.419 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.702.047 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.703.221 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.703.244 I llama_perf_context_print:        load time =     549.69 ms
0.00.703.245 I llama_perf_context_print: prompt eval time =     134.40 ms /   128 tokens (    1.05 ms per token,   952.39 tokens per second)
0.00.703.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.703.247 I llama_perf_context_print:       total time =     143.72 ms /   129 tokens
0.00.703.726 I ggml_metal_free: deallocating

real	0m0.719s
user	0m0.078s
sys	0m0.108s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.411 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.416 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.425 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.426 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.426 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.426 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.427 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.429 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.429 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.430 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.370 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.393 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.394 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.395 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.396 I llama_model_loader: - type  f32:  194 tensors
0.00.024.396 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.396 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.455 I llm_load_vocab: special tokens cache size = 25
0.00.051.448 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.450 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.451 I llm_load_print_meta: arch             = gptneox
0.00.051.451 I llm_load_print_meta: vocab type       = BPE
0.00.051.451 I llm_load_print_meta: n_vocab          = 50304
0.00.051.452 I llm_load_print_meta: n_merges         = 50009
0.00.051.452 I llm_load_print_meta: vocab_only       = 0
0.00.051.452 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.452 I llm_load_print_meta: n_embd           = 2048
0.00.051.452 I llm_load_print_meta: n_layer          = 24
0.00.051.466 I llm_load_print_meta: n_head           = 16
0.00.051.467 I llm_load_print_meta: n_head_kv        = 16
0.00.051.467 I llm_load_print_meta: n_rot            = 32
0.00.051.467 I llm_load_print_meta: n_swa            = 0
0.00.051.467 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.469 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.470 I llm_load_print_meta: n_gqa            = 1
0.00.051.470 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.472 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.473 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.473 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.474 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.474 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.474 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.474 I llm_load_print_meta: n_ff             = 8192
0.00.051.475 I llm_load_print_meta: n_expert         = 0
0.00.051.475 I llm_load_print_meta: n_expert_used    = 0
0.00.051.475 I llm_load_print_meta: causal attn      = 1
0.00.051.475 I llm_load_print_meta: pooling type     = 0
0.00.051.475 I llm_load_print_meta: rope type        = 2
0.00.051.475 I llm_load_print_meta: rope scaling     = linear
0.00.051.476 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.476 I llm_load_print_meta: freq_scale_train = 1
0.00.051.477 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.478 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.478 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.478 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.478 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.478 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.478 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.488 I llm_load_print_meta: model type       = 1.4B
0.00.051.488 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.489 I llm_load_print_meta: model params     = 1.41 B
0.00.051.489 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.489 I llm_load_print_meta: general.name     = 1.4B
0.00.051.489 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.490 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.490 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.490 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.490 I llm_load_print_meta: LF token         = 128 ''
0.00.051.491 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.491 I llm_load_print_meta: max token length = 1024
0.00.053.569 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.570 I llm_load_tensors: offloading output layer to GPU
0.00.053.570 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.580 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.582 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.587 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.588 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.589 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.589 I llama_new_context_with_model: n_batch       = 2048
0.00.054.589 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.589 I llama_new_context_with_model: flash_attn    = 0
0.00.054.589 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.590 I llama_new_context_with_model: freq_scale    = 1
0.00.054.590 I ggml_metal_init: allocating
0.00.054.593 I ggml_metal_init: found device: Apple M4
0.00.054.595 I ggml_metal_init: picking default device: Apple M4
0.00.055.179 I ggml_metal_init: using embedded metal library
0.00.057.540 I ggml_metal_init: GPU name:   Apple M4
0.00.057.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.542 I ggml_metal_init: simdgroup reduction   = true
0.00.057.542 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.542 I ggml_metal_init: has bfloat            = true
0.00.057.543 I ggml_metal_init: use bfloat            = true
0.00.057.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.995 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.000 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.017 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.062 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.063 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.064 I llama_new_context_with_model: graph nodes  = 967
0.00.088.064 I llama_new_context_with_model: graph splits = 2
0.00.088.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.309 I main: llama threadpool init, n_threads = 4
0.00.695.352 I 
0.00.695.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.389 I 
0.00.695.603 I sampler seed: 1234
0.00.695.607 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.695.619 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.695.620 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.695.620 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.545.165 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.01.545.166 I llama_perf_context_print:        load time =     686.61 ms
0.01.545.167 I llama_perf_context_print: prompt eval time =      51.59 ms /     7 tokens (    7.37 ms per token,   135.69 tokens per second)
0.01.545.167 I llama_perf_context_print:        eval time =     794.98 ms /    63 runs   (   12.62 ms per token,    79.25 tokens per second)
0.01.545.171 I llama_perf_context_print:       total time =     849.86 ms /    70 tokens
0.01.545.357 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.760 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.583 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.596 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.482 I llama_model_loader: - type  f32:  194 tensors
0.00.023.482 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.482 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.296 I llm_load_vocab: special tokens cache size = 25
0.00.050.379 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.382 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.382 I llm_load_print_meta: arch             = gptneox
0.00.050.383 I llm_load_print_meta: vocab type       = BPE
0.00.050.383 I llm_load_print_meta: n_vocab          = 50304
0.00.050.383 I llm_load_print_meta: n_merges         = 50009
0.00.050.383 I llm_load_print_meta: vocab_only       = 0
0.00.050.383 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.383 I llm_load_print_meta: n_embd           = 2048
0.00.050.384 I llm_load_print_meta: n_layer          = 24
0.00.050.397 I llm_load_print_meta: n_head           = 16
0.00.050.398 I llm_load_print_meta: n_head_kv        = 16
0.00.050.398 I llm_load_print_meta: n_rot            = 32
0.00.050.398 I llm_load_print_meta: n_swa            = 0
0.00.050.398 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.398 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.399 I llm_load_print_meta: n_gqa            = 1
0.00.050.400 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.401 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.401 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.402 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.402 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.402 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.402 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.403 I llm_load_print_meta: n_ff             = 8192
0.00.050.405 I llm_load_print_meta: n_expert         = 0
0.00.050.405 I llm_load_print_meta: n_expert_used    = 0
0.00.050.405 I llm_load_print_meta: causal attn      = 1
0.00.050.405 I llm_load_print_meta: pooling type     = 0
0.00.050.405 I llm_load_print_meta: rope type        = 2
0.00.050.406 I llm_load_print_meta: rope scaling     = linear
0.00.050.406 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.406 I llm_load_print_meta: freq_scale_train = 1
0.00.050.407 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.407 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.407 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.407 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.407 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.407 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.407 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.417 I llm_load_print_meta: model type       = 1.4B
0.00.050.417 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.418 I llm_load_print_meta: model params     = 1.41 B
0.00.050.418 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.418 I llm_load_print_meta: general.name     = 1.4B
0.00.050.419 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.419 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.419 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.419 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.419 I llm_load_print_meta: LF token         = 128 ''
0.00.050.420 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.420 I llm_load_print_meta: max token length = 1024
0.00.052.005 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.005 I llm_load_tensors: offloading output layer to GPU
0.00.052.005 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.015 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.016 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.872 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.873 I llama_new_context_with_model: n_ctx         = 128
0.00.052.873 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.873 I llama_new_context_with_model: n_batch       = 128
0.00.052.873 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.873 I llama_new_context_with_model: flash_attn    = 0
0.00.052.874 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.874 I llama_new_context_with_model: freq_scale    = 1
0.00.052.875 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.875 I ggml_metal_init: allocating
0.00.052.881 I ggml_metal_init: found device: Apple M4
0.00.052.885 I ggml_metal_init: picking default device: Apple M4
0.00.053.446 I ggml_metal_init: using embedded metal library
0.00.055.807 I ggml_metal_init: GPU name:   Apple M4
0.00.055.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.809 I ggml_metal_init: simdgroup reduction   = true
0.00.055.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.810 I ggml_metal_init: has bfloat            = true
0.00.055.810 I ggml_metal_init: use bfloat            = true
0.00.055.810 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.551 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.554 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.571 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.454 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.455 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.455 I llama_new_context_with_model: graph nodes  = 967
0.00.067.456 I llama_new_context_with_model: graph splits = 2
0.00.067.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.457 I 
0.00.669.488 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.496 I perplexity: tokenizing the input ..
0.00.677.762 I perplexity: tokenization took 8.265 ms
0.00.677.776 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.710 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.819.876 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.819.889 I llama_perf_context_print:        load time =     660.63 ms
0.00.819.890 I llama_perf_context_print: prompt eval time =     140.71 ms /   128 tokens (    1.10 ms per token,   909.68 tokens per second)
0.00.819.890 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.891 I llama_perf_context_print:       total time =     150.43 ms /   129 tokens
0.00.820.201 I ggml_metal_free: deallocating

real	0m0.833s
user	0m0.079s
sys	0m0.124s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.474 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.250 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.253 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.260 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.261 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.261 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.262 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.262 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.263 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.263 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.263 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.264 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.267 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.268 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.246 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.284 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.129 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.129 I llama_model_loader: - type  f32:  194 tensors
0.00.025.130 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.272 I llm_load_vocab: special tokens cache size = 25
0.00.051.296 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.299 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.299 I llm_load_print_meta: arch             = gptneox
0.00.051.299 I llm_load_print_meta: vocab type       = BPE
0.00.051.300 I llm_load_print_meta: n_vocab          = 50304
0.00.051.300 I llm_load_print_meta: n_merges         = 50009
0.00.051.300 I llm_load_print_meta: vocab_only       = 0
0.00.051.300 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.300 I llm_load_print_meta: n_embd           = 2048
0.00.051.300 I llm_load_print_meta: n_layer          = 24
0.00.051.314 I llm_load_print_meta: n_head           = 16
0.00.051.315 I llm_load_print_meta: n_head_kv        = 16
0.00.051.315 I llm_load_print_meta: n_rot            = 32
0.00.051.315 I llm_load_print_meta: n_swa            = 0
0.00.051.315 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.316 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.318 I llm_load_print_meta: n_gqa            = 1
0.00.051.318 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.319 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.320 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.320 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.320 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.320 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.320 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.321 I llm_load_print_meta: n_ff             = 8192
0.00.051.321 I llm_load_print_meta: n_expert         = 0
0.00.051.321 I llm_load_print_meta: n_expert_used    = 0
0.00.051.321 I llm_load_print_meta: causal attn      = 1
0.00.051.326 I llm_load_print_meta: pooling type     = 0
0.00.051.326 I llm_load_print_meta: rope type        = 2
0.00.051.327 I llm_load_print_meta: rope scaling     = linear
0.00.051.327 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.327 I llm_load_print_meta: freq_scale_train = 1
0.00.051.328 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.328 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.328 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.328 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.328 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.328 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.328 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.338 I llm_load_print_meta: model type       = 1.4B
0.00.051.338 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.339 I llm_load_print_meta: model params     = 1.41 B
0.00.051.339 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.339 I llm_load_print_meta: general.name     = 1.4B
0.00.051.339 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: LF token         = 128 ''
0.00.051.341 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.341 I llm_load_print_meta: max token length = 1024
0.00.052.912 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.912 I llm_load_tensors: offloading output layer to GPU
0.00.052.912 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.922 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.923 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.781 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.782 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.782 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.782 I llama_new_context_with_model: n_batch       = 2048
0.00.053.782 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.782 I llama_new_context_with_model: flash_attn    = 0
0.00.053.783 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.783 I llama_new_context_with_model: freq_scale    = 1
0.00.053.784 I ggml_metal_init: allocating
0.00.053.791 I ggml_metal_init: found device: Apple M4
0.00.053.793 I ggml_metal_init: picking default device: Apple M4
0.00.054.385 I ggml_metal_init: using embedded metal library
0.00.056.742 I ggml_metal_init: GPU name:   Apple M4
0.00.056.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.744 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.745 I ggml_metal_init: simdgroup reduction   = true
0.00.056.745 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.745 I ggml_metal_init: has bfloat            = true
0.00.056.745 I ggml_metal_init: use bfloat            = true
0.00.056.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.084 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.093 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.111 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.325 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.326 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.326 I llama_new_context_with_model: graph nodes  = 967
0.00.086.327 I llama_new_context_with_model: graph splits = 2
0.00.086.340 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.870 I main: llama threadpool init, n_threads = 4
0.00.765.918 I 
0.00.765.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.955 I 
0.00.766.195 I sampler seed: 1234
0.00.766.199 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.211 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.211 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.212 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.644.952 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.644.953 I llama_perf_context_print:        load time =     756.39 ms
0.01.644.954 I llama_perf_context_print: prompt eval time =      54.19 ms /     7 tokens (    7.74 ms per token,   129.17 tokens per second)
0.01.644.955 I llama_perf_context_print:        eval time =     821.97 ms /    63 runs   (   13.05 ms per token,    76.65 tokens per second)
0.01.644.956 I llama_perf_context_print:       total time =     879.09 ms /    70 tokens
0.01.645.172 I ggml_metal_free: deallocating

real	0m1.665s
user	0m0.110s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4323 (11e07fd6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.747 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.583 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.584 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.585 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.591 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.592 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.462 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.463 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.463 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.464 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.464 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.464 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.465 I llama_model_loader: - type  f32:  194 tensors
0.00.024.465 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.297 I llm_load_vocab: special tokens cache size = 25
0.00.051.299 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.302 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.302 I llm_load_print_meta: arch             = gptneox
0.00.051.302 I llm_load_print_meta: vocab type       = BPE
0.00.051.303 I llm_load_print_meta: n_vocab          = 50304
0.00.051.303 I llm_load_print_meta: n_merges         = 50009
0.00.051.303 I llm_load_print_meta: vocab_only       = 0
0.00.051.303 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.303 I llm_load_print_meta: n_embd           = 2048
0.00.051.303 I llm_load_print_meta: n_layer          = 24
0.00.051.318 I llm_load_print_meta: n_head           = 16
0.00.051.320 I llm_load_print_meta: n_head_kv        = 16
0.00.051.320 I llm_load_print_meta: n_rot            = 32
0.00.051.320 I llm_load_print_meta: n_swa            = 0
0.00.051.320 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.320 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.321 I llm_load_print_meta: n_gqa            = 1
0.00.051.322 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.323 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.323 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.324 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.324 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.324 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.324 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.325 I llm_load_print_meta: n_ff             = 8192
0.00.051.325 I llm_load_print_meta: n_expert         = 0
0.00.051.325 I llm_load_print_meta: n_expert_used    = 0
0.00.051.325 I llm_load_print_meta: causal attn      = 1
0.00.051.325 I llm_load_print_meta: pooling type     = 0
0.00.051.325 I llm_load_print_meta: rope type        = 2
0.00.051.326 I llm_load_print_meta: rope scaling     = linear
0.00.051.326 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.326 I llm_load_print_meta: freq_scale_train = 1
0.00.051.326 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.327 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.327 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.327 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.327 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.327 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.328 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.337 I llm_load_print_meta: model type       = 1.4B
0.00.051.338 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.338 I llm_load_print_meta: model params     = 1.41 B
0.00.051.339 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.339 I llm_load_print_meta: general.name     = 1.4B
0.00.051.339 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.339 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.339 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: LF token         = 128 ''
0.00.051.340 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.341 I llm_load_print_meta: max token length = 1024
0.00.053.397 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.398 I llm_load_tensors: offloading output layer to GPU
0.00.053.398 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.408 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.410 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.281 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.282 I llama_new_context_with_model: n_ctx         = 128
0.00.054.282 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.282 I llama_new_context_with_model: n_batch       = 128
0.00.054.283 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.283 I llama_new_context_with_model: flash_attn    = 0
0.00.054.283 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.283 I llama_new_context_with_model: freq_scale    = 1
0.00.054.284 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.284 I ggml_metal_init: allocating
0.00.054.290 I ggml_metal_init: found device: Apple M4
0.00.054.292 I ggml_metal_init: picking default device: Apple M4
0.00.054.832 I ggml_metal_init: using embedded metal library
0.00.057.165 I ggml_metal_init: GPU name:   Apple M4
0.00.057.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.168 I ggml_metal_init: simdgroup reduction   = true
0.00.057.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.168 I ggml_metal_init: has bfloat            = true
0.00.057.168 I ggml_metal_init: use bfloat            = true
0.00.057.168 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.659 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.665 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.680 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.584 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.585 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.586 I llama_new_context_with_model: graph nodes  = 967
0.00.068.586 I llama_new_context_with_model: graph splits = 2
0.00.068.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.241.842 I 
0.00.241.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.241.884 I perplexity: tokenizing the input ..
0.00.249.582 I perplexity: tokenization took 7.696 ms
0.00.249.599 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.390.555 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.391.764 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.391.781 I llama_perf_context_print:        load time =     232.09 ms
0.00.391.782 I llama_perf_context_print: prompt eval time =     140.58 ms /   128 tokens (    1.10 ms per token,   910.50 tokens per second)
0.00.391.783 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.391.784 I llama_perf_context_print:       total time =     149.94 ms /   129 tokens
0.00.392.246 I ggml_metal_free: deallocating

real	0m0.409s
user	0m0.078s
sys	0m0.050s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4323 (11e07fd6)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11fe0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11fe0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11fe0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11fe0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11fe0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11fe0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11fe0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11fe0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11fe0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11fe0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11fe0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11fe0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11fe0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11fe0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11fe0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11fe101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11fe10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11fe11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11fe11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11fe11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11fe12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11fe12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11fe13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11fe13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11fe14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11fe14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11fe14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11fe15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11fe15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11fe16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11fe16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11fe168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11fe17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11fe176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11fe17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11fe17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11fe182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11fe18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11fe18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11fe19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11fe19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11fe199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11fe19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11fe1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11fe1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11fe1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11fe1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11fe1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11fe1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11fe1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11fe1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11fe1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11fe1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11fe1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11fe1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11fe1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11fe1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11fe1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11fe1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11fe20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11fe20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11fe208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11fe20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11fe21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11fe216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11fe21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11fe21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11fe22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11fe22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11fe22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11fe23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11fe23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11fe23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11fe240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11fe24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11fe24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11fe250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11fe25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11fe25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11fe260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11fe26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11fe26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11fe270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11fe27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11fe27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11fe280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11fe28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11fe28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11fe290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11fe295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11fe29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11fe2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11fe2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11fe2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11fe2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11fe2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11fe2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11fe1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11fe2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11fe2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11fe2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11fe2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11fe2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11fe2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11fe2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11fe2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11fe2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11fe2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11fe2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11fe2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11fe301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11fe30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11fe30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11fe310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11fe31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11fe31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11fe31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11fe32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11fe32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11fe32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11fe33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11fe335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11fe33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11fe33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11fe343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11fe34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11fe34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11fe351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11fe35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11fe35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11fe35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11fe36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11fe368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11fe36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11fe37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11fe376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11fe37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11fe37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11fe38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11fe38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11fe38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11fe39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11fe39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11fe39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11fe3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11fe3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11fe3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11fe3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11fe3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11fe3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11fe3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11fe3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11fe3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11fe3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11fe3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11fe3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11fe3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11fe3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11fe3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11fe3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11fe3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11fe3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11fe3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11fe3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11fe3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11fe40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11fe40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11fe40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11fe40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11fe413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11fe41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11fe41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11fe421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11fe42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11fe42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11fe42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11fe43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11fe438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11fe43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11fe44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11fe446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11fe44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11fe45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11fe454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11fe45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11fe45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11fe46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11fe46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11fe46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11fe47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11fe47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11fe479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11fe47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11fe483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11fe488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11fe48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11fe49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11fe49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11fe49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11fe4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11fe4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11fe4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11fe4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11fe4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11fe4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11fe4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11fe4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11fe4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11fe4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11fe4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11fe4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11fe4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11fe4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11fe4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11fe4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11fe4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11fe50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11fe506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11fe50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11fe51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11fe51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11fe51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11fe52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11fe52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11fe52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11fe53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11fe53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11fe53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11fe54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11fe54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11fe54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11fe55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11fe55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11fe55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11fe560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11fe56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11fe56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11fe570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11fe57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11fe57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11fe580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11fe58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11fe58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11fe590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11fe59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11fe59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11fe5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11fe5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11fe5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11fe5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11fe5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11fe5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11fe5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11fe5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11fe5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11fe5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11fe5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11fe5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11fe5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11fe5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11fe5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11fe5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11fe5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11fe5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11fe60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11fe605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11fe60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11fe60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11fe61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11fe618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11fe61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11fe62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11fe626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11fe62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11fe62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11fe63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11fe63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11fe63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11fe64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11fe64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11fe64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11fe65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11fe655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11fe65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11fe663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11fe66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11fe67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11fe674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11fe67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11fe67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11fe685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.130.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11fe0de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11fe0e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11fe0e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11fe0eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11fe0efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11fe0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11fe0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11fe0fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11fe10190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11fe10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11fe10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11fe11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11fe11940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11fe120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11fe128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11fe12f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11fe13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11fe13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11fe14460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11fe14de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11fe154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11fe15bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11fe162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11fe169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11fe17090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11fe17500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11fe17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11fe17de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11fe18250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11fe186c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11fe18b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11fe18fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11fe19410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11fe196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11fe19b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11fe19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11fe1a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11fe1a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11fe1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11fe1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11fe1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11fe1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11fe1bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11fe1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11fe1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11fe1cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11fe1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11fe1d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11fe1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11fe1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11fe1e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11fe1e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11fe1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11fe1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11fe1f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11fe1f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11fe1fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11fe20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11fe205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11fe20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11fe20ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11fe21310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11fe21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11fe21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11fe22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11fe224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11fe22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11fe22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11fe23220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11fe23690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11fe23b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11fe23f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11fe243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11fe24850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11fe24cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11fe25130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11fe255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11fe25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11fe25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11fe262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11fe26760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11fe26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11fe27040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11fe274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11fe27920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11fe27d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11fe28200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11fe28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11fe28ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11fe28f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11fe293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11fe29830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11fe29ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11fe2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11fe2a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11fe2a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11fe2ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11fe2b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11fe2b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11fe2bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11fe2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11fe2c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11fe2c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11fe2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11fe2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11fe2d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11fe2dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11fe2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11fe2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11fe2e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11fe2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11fe2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11fe2f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11fe2f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11fe2fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11fe302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11fe30720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11fe30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11fe31000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11fe31470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11fe318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11fe31d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11fe321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11fe32630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11fe32aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11fe32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11fe33380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11fe337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11fe33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11fe340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11fe34540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11fe349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11fe34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11fe35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11fe35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11fe35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11fe35fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11fe36450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11fe368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11fe36d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11fe371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11fe37610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11fe37a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11fe37ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11fe38360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11fe387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11fe38c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11fe390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11fe39520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11fe39990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11fe39e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11fe3a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11fe3a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11fe3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11fe3afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11fe3b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11fe3b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11fe3bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11fe3c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11fe3c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11fe3ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11fe3ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11fe3d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11fe3d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11fe3dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11fe3e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11fe3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11fe3e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11fe3ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11fe3f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11fe3f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11fe3fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11fe3ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11fe40410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11fe40880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11fe40cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11fe41160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11fe415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11fe41a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11fe41eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11fe42320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11fe42790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11fe42c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11fe43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11fe434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11fe43950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11fe43dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11fe44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11fe446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11fe44b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11fe44f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11fe453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11fe45860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11fe45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11fe46140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11fe465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11fe46a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11fe46e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11fe47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11fe47770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11fe47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11fe48050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11fe484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11fe48930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11fe48da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11fe49210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11fe49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11fe49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11fe49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11fe4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11fe4ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11fe4afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11fe4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11fe4b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11fe4bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11fe4c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11fe4c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11fe4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11fe4ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11fe4d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11fe4d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11fe4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11fe4e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11fe4e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11fe4e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11fe4ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11fe4f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11fe4f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11fe4fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11fe4ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11fe50410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11fe50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11fe50cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11fe51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11fe515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11fe51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11fe51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11fe52320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11fe52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11fe52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11fe53070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11fe534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11fe53950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11fe53dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11fe54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11fe546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11fe54b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11fe54f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11fe553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11fe55860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11fe55cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11fe56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11fe565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11fe56a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11fe56e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11fe57300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11fe57770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11fe57be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11fe58050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11fe584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11fe58930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11fe58da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11fe59210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11fe59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11fe59af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11fe59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11fe5a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11fe5a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11fe5acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11fe5b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11fe5b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11fe5ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11fe5be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11fe5c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11fe5c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11fe5cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11fe5d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11fe5d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11fe5d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11fe5dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11fe5e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11fe5e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11fe5ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11fe5f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11fe5f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11fe5ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11fe60690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11fe60b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11fe60f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11fe613e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11fe61850 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12fe055e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12fe05ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12fe06190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12fe06600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12fe06a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12fe06ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12fe07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12fe077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12fe07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12fe04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12fe046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12fe08190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12fe08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12fe09460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12fe09c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12fe0a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12fe0aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12fe0b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12fe0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12fe0c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12fe0c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12fe0ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12fe0d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12fe0dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12fe0e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12fe0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12fe0e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12fe0edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12fe0f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12fe0f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12fe0fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12fe10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12fe104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12fe10760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12fe10bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12fe11040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12fe114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12fe11920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12fe11d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12fe12200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12fe12670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12fe12ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12fe12f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12fe133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12fe13830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12fe13ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12fe14110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12fe14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12fe149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12fe14e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12fe152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12fe15740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12fe15bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12fe16020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12fe16490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12fe16900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12fe16e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12fe17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12fe177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12fe17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12fe180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12fe18530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12fe189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12fe18e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12fe19280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12fe196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12fe19b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12fe19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12fe1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12fe1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12fe1ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12fe1b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12fe1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12fe1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12fe1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12fe1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12fe1c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12fe1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12fe1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12fe1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12fe1d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12fe1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12fe1e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12fe1e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12fe1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12fe1efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12fe1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12fe1f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12fe1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12fe20170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12fe205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12fe20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12fe20ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12fe21330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12fe217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12fe21c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12fe22080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12fe224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12fe22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12fe22dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12fe23240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12fe236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12fe23b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12fe23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12fe24400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12fe24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12fe24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12fe25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12fe255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12fe25a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12fe25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12fe26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12fe26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12fe26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12fe27060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12fe274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12fe27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12fe27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12fe28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12fe28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12fe28b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12fe28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12fe293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12fe29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12fe29cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12fe2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12fe2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12fe2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12fe2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12fe2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12fe2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12fe2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12fe2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12fe2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12fe2c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12fe2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12fe2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12fe2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12fe2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12fe2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12fe2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12fe2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12fe2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12fe2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12fe2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12fe2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12fe2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12fe302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12fe30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12fe30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12fe31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12fe31490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12fe31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12fe31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12fe321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12fe32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12fe32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12fe32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12fe333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12fe33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12fe33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12fe340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12fe34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12fe349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12fe34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12fe352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12fe35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12fe35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12fe36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12fe36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12fe368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12fe36d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12fe371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12fe37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12fe37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12fe37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12fe38380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12fe387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12fe38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12fe390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12fe39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12fe399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12fe39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12fe3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12fe3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12fe3ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12fe3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12fe3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12fe3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12fe3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12fe3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12fe3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12fe3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12fe3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12fe3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12fe3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12fe3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12fe3e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12fe3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12fe3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12fe3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12fe3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12fe3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12fe3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12fe3ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12fe40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12fe408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12fe40e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12fe412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12fe41710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12fe42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12fe42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12fe427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12fe42c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12fe430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12fe43530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12fe439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12fe43e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12fe44280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12fe446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12fe44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12fe44fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12fe45440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12fe458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12fe45d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12fe46190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12fe46600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12fe46a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12fe46ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12fe47350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12fe477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12fe47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12fe480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12fe48510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12fe48980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12fe48df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12fe49260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12fe496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12fe49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12fe49fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12fe4a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12fe4a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12fe4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12fe4b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12fe4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12fe4bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12fe4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12fe4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12fe4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12fe4cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12fe4d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12fe4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12fe4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12fe4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12fe4e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12fe4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12fe4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12fe4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12fe4f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12fe4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12fe50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12fe504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12fe50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12fe50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12fe511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12fe51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12fe51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12fe51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12fe523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12fe52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12fe52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12fe53100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12fe53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12fe539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12fe53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12fe542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12fe54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12fe54ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12fe55010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12fe55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12fe558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12fe55d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12fe561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12fe56c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12fe57360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12fe57a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12fe581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12fe58460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12fe588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12fe58ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12fe594e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.785s
user	0m0.291s
sys	0m0.303s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4323 (11e07fd6)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a806010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a8066a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a806b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a806f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a8073f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a807860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a807cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a808140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a8085b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a808a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a808e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a809530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a80a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a80a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a80b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a80b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a80be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a80c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a80cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a80d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a80db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a80e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a80e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a80f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a80f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a80fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a80ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a810370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a810a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a810f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a8114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a8119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a811e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a812100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a812570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a8129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a812e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a8132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a813730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a813ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a814010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a814480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a8148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a814d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a8151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a815640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a815ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a815f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a8166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a816b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a816f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a817400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a817870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a817ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a818150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a818870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a818d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a818fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a819440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a819b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a819f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a81a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a81a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a81abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a81b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a81b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a81bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a81bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a81c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a81c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a81ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a81d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a81d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a81ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a81e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a81e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a81eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a81f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a81fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a81fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a8205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a820b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a821100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a8216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a821c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a822210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a8227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a822d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a823320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a8238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a823e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a824430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a8249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a824f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a825540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a825af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a8260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a8161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a826800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a826c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a8270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a827690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a827c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a8281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a8287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a828d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a829300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a8298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a82a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a82a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a82af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a82b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a82bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a82bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a82c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a82c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a82ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a82d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a82d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a82ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a82e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a82e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a82ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a82f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a82f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a82fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a8300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a8305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a830ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a830fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a8314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a8319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a831ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a8323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a8328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a832dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a8332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a8337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a833cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a8341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a8346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a834bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a8350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a8355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a835ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a835fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a8364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a8369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a836ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a8373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a8378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a837dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a8382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a8387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a838cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a8391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a8396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a839bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a83a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a83a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a83aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a83afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a83b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a83b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a83bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a83c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a83c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a83cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a83d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a83d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a83dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a83e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a83e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a83ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a83f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a83f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a83fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a83ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a8404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a8409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a840ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a8413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a8418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a841dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a8422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a8427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a842cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a8431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a8436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a843bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a8440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a8445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a844ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a845080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a845630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a845be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a846190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a8467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a846db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a8473c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a847bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a848050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a848310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a848920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a848f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a849720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a849bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a84a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a84a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a84acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a84b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a84b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a84bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a84c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a84c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a84cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a84d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a84d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a84dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a84e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a84e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a84ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a84f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a84f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a84fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a8501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a850700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a850c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a8511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a8516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a851c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a852190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a8526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a852c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a853180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a8536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a853c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a854170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a8546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a854c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a855160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a8556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a855c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a856150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a8566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a856bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a857140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a857690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a857be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a858130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a858680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a858bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a859120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a859670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a859bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a85a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a85a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a85abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a85b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a85b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a85bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a85c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a85c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a85cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a85d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a85d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a85dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a85df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a85e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a85e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a85ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a85f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a85f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a85fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a85ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a860470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a860910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a860db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a861250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a8616f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a861b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a8620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a862800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a862f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a863640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a863d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a864020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a864810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a864ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a8650e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.091.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a854bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a855020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a855490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a855900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a855d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a8561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a856650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a856ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a856f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a8573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a857810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a857df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a8586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a858e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a859640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a859d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a85a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a85ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a85b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a85bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a85c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a85c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a85d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a85d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a85de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a85e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a85e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a85eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a85eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a85f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a85f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a85fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a8601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a860470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a8608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a860d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a8611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a861630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a861aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a861f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a862380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a8627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a862c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a8630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a863540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a8639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a863e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a864290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a864700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a864b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a864fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a853160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a8535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a853a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a845fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a846450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a8468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a846d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a8471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a847610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a847a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a847ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a848360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a8487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a848c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a8490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a849520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a849990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a849e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a84a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a84a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a84ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a84afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a84b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a84b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a84bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a84c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a84c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a84ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a84ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a84d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a84d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a84dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a84e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a84e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a84e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a84ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a84f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a84f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a84fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a84ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a850410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a850880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a850cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a851160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a8515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a851a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a851eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a852320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a852790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a852c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a845bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a840640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a840ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a840f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a841390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a841800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a841c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a8420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a842550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a8429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a842e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a8432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a843710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a843b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a843ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a844460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a8448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a844d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a8451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a845620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a82bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a82bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a82c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a82c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a82cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a82d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a82d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a82d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a82de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a82e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a82e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a82eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a82f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a82f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a82f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a82fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a8301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a830640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a830ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a830f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a831390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a831800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a831c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a8320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a832550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a8329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a832e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a8332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a833710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a833b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a833ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a834460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a8348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a834d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a8351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a835620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a835a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a835f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a836370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a8367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a836c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a8370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a837530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a8379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a837e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a838280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a8386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a838b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a838fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a839440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a8398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a839d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a83a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a83a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a83aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a83aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a83b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a83b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a83bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a83c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a83c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a83c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a83cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a83d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a83d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a83db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a83dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a83e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a83e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a83ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a83f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a83f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a83fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a83fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a82b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a816850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a816cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a817130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a8175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a817a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a817e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a8182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a818760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a818bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a819040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a8194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a819920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a819d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a81a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a81a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a81adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a81b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a81b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a81bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a81bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a81c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a81c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a81cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a81d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a81d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a81da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a81dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a81e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a81e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a81ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a81f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a81f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a81f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a81fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a820240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a8206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a820b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a820f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a821400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a821870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a821ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a822150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a8225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a822a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a822ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a823310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a823780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a823bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a824060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a8244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a824940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a824db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a825220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a825690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a825b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a825f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a8263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a826850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a826cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a827130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a8275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a827a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a827e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a8282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a828760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a828bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a829040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a8294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a829920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a829d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a82a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a82a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a82aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a82af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a806850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a806cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a807130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a8075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a807a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a807e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a8082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a808760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a808bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a809040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a8094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a809920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a809d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a80a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a80ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a80b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a80b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a80bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a80c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a80c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a80cb10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a854bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a855020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a855490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a855900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a855d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a8561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a856650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a856ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a856f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a8573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a857810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a857df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a8586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a858e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a859640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a859d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a85a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a85ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a85b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a85bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a85c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a85c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a85d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a85d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a85de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a85e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a85e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a85eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a85eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a85f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a85f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a85fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a8601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a860470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a8608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a860d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a8611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a861630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a861aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a861f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a862380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a8627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a862c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a8630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a863540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a8639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a863e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a864290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a864700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a864b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a864fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a8166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a816b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a816f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a8173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a817860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a817cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a818140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a8185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a818a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a818e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a819300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a819770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a819be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a81a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a81a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a81a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a81ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a81b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a81b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a81baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a81bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a81c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a81c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a81ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a81d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a81d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a81da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a81de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a81e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a81e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a81ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a81f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a81f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a81f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a81fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a8201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a820660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a820ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a820f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a8213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a821820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a821c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a822100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a822570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a8229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a822e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a8232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a823730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a823ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a824010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a824480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a8248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a824d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a8251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a825640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a825ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a825f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a826390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a826800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a826c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a8270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a827550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a8279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a827e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a8282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a828710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a828b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a828ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a829460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a8298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a829d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a82a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a82a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a82aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a82af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a82b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a82bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a82bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a82c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a82c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a82cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a82d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a82d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a82d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a82de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a82e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a82e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a82eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a82f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a82f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a82f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a82fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a8301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a830640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a830ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a830f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a831390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a831800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a831c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a8320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a832550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a8329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a832e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a8332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a833710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a833b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a833ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a834460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a8348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a834d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a8351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a835620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a835a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a835f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a836370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a8367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a836c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a8370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a837530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a8379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a837e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a838280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a8386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a838b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a838fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a839440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a8398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a839d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a83a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a83a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a83aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a83aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a83b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a83b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a83bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a83c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a83c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a83c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a83cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a83d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a83d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a83db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a83dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a83e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a83e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a83ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a83f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a83f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a83fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a83fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a840640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a840ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a840f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a841390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a841800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a841c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a8420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a842550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a8429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a843140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a8435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a843a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a843e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a844300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a844770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a844be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a845050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a8454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a845bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a846190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a846600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a846a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a846ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a8477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a847c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a8480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a848510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a848980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a848df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a849260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a8496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a849b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a849fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a84a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a84a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a84ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a84b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a84b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a84ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a84bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a84c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a84c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a84cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a84d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a84d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a84d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a84ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a84e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a84e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a84eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a84ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a84f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a84f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a84fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a850150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a8505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a850a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a850ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a851310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a851780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a851bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a852060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a8524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a852940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a853160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a8535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a853a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a8066a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a806b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a806f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a8073f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a807860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a807cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a808140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a8085b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a808a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a808e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a809300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a809770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a809be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a80a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a80a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a80ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a80b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a80bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a80c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a80c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a80c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a80cdd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.942s
user	0m0.247s
sys	0m0.154s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.54 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.16 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.25 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
