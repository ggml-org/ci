Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_QKK_64=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.762s
user	0m0.418s
sys	0m0.348s
+ make -j
[  2%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Built target ggml
[  5%] Built target build_info
[  5%] Linking C static library libggml_static.a
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[  9%] Built target gguf
[ 11%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 12%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 12%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 13%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 13%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 16%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 18%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 19%] Linking CXX executable ../bin/test-c
[ 20%] Linking CXX executable ../../bin/benchmark
[ 21%] Linking CXX executable ../../bin/quantize
[ 22%] Linking CXX executable ../../bin/quantize-stats
[ 22%] Built target llava
[ 23%] Linking CXX static library libcommon.a
[ 23%] Linking CXX static library libllava_static.a
[ 23%] Built target test-c
[ 23%] Built target common
[ 23%] Built target benchmark
[ 23%] Built target llava_static
[ 23%] Built target quantize
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 26%] Built target quantize-stats
[ 26%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Linking CXX executable ../bin/test-chat-template
[ 40%] Linking CXX executable ../bin/test-quantize-fns
[ 40%] Linking CXX executable ../bin/test-quantize-perf
[ 41%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 42%] Linking CXX executable ../bin/test-sampling
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 44%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 53%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 57%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-llama-grammar
[ 60%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-grad0
[ 63%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../../bin/embedding
[ 64%] Linking CXX executable ../bin/test-rope
[ 65%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 65%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/batched-bench
[ 66%] Linking CXX executable ../../bin/beam-search
[ 67%] Linking CXX executable ../../bin/baby-llama
[ 68%] Linking CXX executable ../../bin/batched
[ 65%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 69%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 70%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 70%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 73%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 74%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 75%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 75%] Built target test-sampling
[ 76%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 76%] Built target test-chat-template
[ 77%] Linking CXX executable ../../bin/infill
[ 78%] Built target test-tokenizer-0-llama
[ 78%] Built target test-quantize-perf
[ 78%] Linking CXX executable ../../bin/simple
[ 78%] Built target test-grammar-parser
[ 78%] Built target test-tokenizer-1-llama
[ 78%] Linking CXX executable ../../bin/finetune
[ 79%] Linking CXX executable ../../bin/perplexity
[ 79%] Built target test-grad0
[ 80%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 81%] Linking CXX executable ../../bin/llava-cli
[ 81%] Linking CXX executable ../../bin/llama-bench
[ 82%] Linking CXX executable ../../bin/parallel
[ 83%] Linking CXX executable ../../bin/main
[ 83%] Built target convert-llama2c-to-ggml
[ 83%] Linking CXX executable ../../bin/save-load-state
[ 83%] Linking CXX executable ../../bin/tokenize
[ 83%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 83%] Built target test-model-load-cancel
[ 83%] Built target test-tokenizer-0-falcon
[ 84%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 84%] Linking CXX executable ../../bin/passkey
[ 85%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 85%] Built target test-backend-ops
[ 86%] Linking CXX executable ../../bin/speculative
[ 86%] Built target baby-llama
[ 86%] Built target test-rope
[ 86%] Built target batched-bench
[ 87%] Linking CXX executable ../../bin/lookahead
[ 87%] Built target test-tokenizer-1-bpe
[ 88%] Linking CXX executable ../../bin/lookup
[ 88%] Built target batched
[ 89%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 89%] Built target embedding
[ 89%] Built target beam-search
[ 89%] Built target test-autorelease
[ 90%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 90%] Built target test-llama-grammar
[ 91%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 92%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 93%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 93%] Built target save-load-state
[ 94%] Linking CXX executable ../../bin/server
[ 95%] Linking CXX executable ../../bin/imatrix
[ 96%] Linking CXX executable ../../bin/train-text-from-scratch
[ 96%] Built target simple
[ 96%] Built target infill
[ 96%] Built target llava-cli
[ 97%] Linking CXX executable ../../bin/q8dot
[ 98%] Linking CXX executable ../../bin/vdot
[ 98%] Built target parallel
[ 98%] Built target llama-bench
[ 98%] Built target finetune
[ 98%] Built target perplexity
[ 98%] Built target lookahead
[ 98%] Built target tokenize
[ 99%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 99%] Built target speculative
[ 99%] Built target main
[ 99%] Built target passkey
[100%] Linking CXX executable ../../bin/export-lora
[100%] Built target lookup
[100%] Built target q8dot
[100%] Built target vdot
[100%] Built target imatrix
[100%] Built target train-text-from-scratch
[100%] Built target export-lora
[100%] Built target server

real	0m1.487s
user	0m5.446s
sys	0m4.477s
Loading model file ../models-mnt/open-llama/3B-v2/pytorch_model.bin
params = Params(n_vocab=32000, n_embd=3200, n_layer=26, n_ctx=2048, n_ff=8640, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../models-mnt/open-llama/3B-v2'))
Found vocab files: {'tokenizer.model': PosixPath('../models-mnt/open-llama/3B-v2/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': None}
Loading vocab file '../models-mnt/open-llama/3B-v2/tokenizer.model', type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 3200]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [3200, 3200]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [3200, 3200]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [3200, 3200]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.0.attn_rot_embd
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [3200]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [3200]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [3200, 3200]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [3200, 3200]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [3200, 3200]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.1.attn_rot_embd
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [3200]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [3200]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [3200, 3200]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [3200, 3200]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [3200, 3200]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.2.attn_rot_embd
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [3200]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [3200]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [3200, 3200]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [3200, 3200]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [3200, 3200]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.3.attn_rot_embd
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [3200]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [3200]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [3200, 3200]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [3200, 3200]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [3200, 3200]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.4.attn_rot_embd
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [3200]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [3200]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [3200, 3200]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [3200, 3200]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [3200, 3200]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.5.attn_rot_embd
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [3200]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [3200]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [3200, 3200]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [3200, 3200]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [3200, 3200]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.6.attn_rot_embd
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [3200]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [3200]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [3200, 3200]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [3200, 3200]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [3200, 3200]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.7.attn_rot_embd
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [3200]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [3200]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [3200, 3200]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [3200, 3200]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [3200, 3200]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.8.attn_rot_embd
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [3200]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [3200]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [3200, 3200]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [3200, 3200]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [3200, 3200]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [3200, 3200]
skipping tensor blk.9.attn_rot_embd
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [8640, 3200]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [3200, 8640]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [8640, 3200]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [3200]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [3200]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [3200, 3200]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [3200, 3200]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [3200, 3200]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.10.attn_rot_embd
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [3200]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [3200]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [3200, 3200]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [3200, 3200]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [3200, 3200]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.11.attn_rot_embd
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [3200]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [3200]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [3200, 3200]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [3200, 3200]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [3200, 3200]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.12.attn_rot_embd
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [3200]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [3200]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [3200, 3200]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [3200, 3200]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [3200, 3200]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.13.attn_rot_embd
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [3200]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [3200]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [3200, 3200]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [3200, 3200]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [3200, 3200]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.14.attn_rot_embd
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [3200]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [3200]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [3200, 3200]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [3200, 3200]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [3200, 3200]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.15.attn_rot_embd
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [3200]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [3200]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [3200, 3200]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [3200, 3200]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [3200, 3200]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.16.attn_rot_embd
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [3200]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [3200]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [3200, 3200]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [3200, 3200]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [3200, 3200]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.17.attn_rot_embd
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [3200]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [3200]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [3200, 3200]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [3200, 3200]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [3200, 3200]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.18.attn_rot_embd
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [3200]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [3200]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [3200, 3200]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [3200, 3200]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [3200, 3200]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.19.attn_rot_embd
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [3200]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [3200]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [3200, 3200]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [3200, 3200]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [3200, 3200]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.20.attn_rot_embd
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [3200]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [3200]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [3200, 3200]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [3200, 3200]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [3200, 3200]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.21.attn_rot_embd
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [3200]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [3200]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [3200, 3200]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [3200, 3200]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [3200, 3200]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.22.attn_rot_embd
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [3200]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [3200]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [3200, 3200]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [3200, 3200]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [3200, 3200]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.23.attn_rot_embd
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [3200]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [3200]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [3200, 3200]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [3200, 3200]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [3200, 3200]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.24.attn_rot_embd
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [3200]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [3200]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [3200, 3200]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [3200, 3200]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [3200, 3200]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [3200, 3200]
skipping tensor blk.25.attn_rot_embd
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [8640, 3200]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [3200, 8640]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [8640, 3200]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [3200]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [3200]
model.norm.weight                                -> output_norm.weight                       | F16    | [3200]
lm_head.weight                                   -> output.weight                            | F16    | [32000, 3200]
Writing ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf, format 1
Ignoring added_tokens.json since model matches vocab size without it.
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting special token type pad to 0
gguf: Setting add_bos_token to True
gguf: Setting add_eos_token to False
[  1/237] Writing tensor token_embd.weight                      | size  32000 x   3200  | type F16  | T+   0
[  2/237] Writing tensor blk.0.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[  3/237] Writing tensor blk.0.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[  4/237] Writing tensor blk.0.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[  5/237] Writing tensor blk.0.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[  6/237] Writing tensor blk.0.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[  7/237] Writing tensor blk.0.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[  8/237] Writing tensor blk.0.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[  9/237] Writing tensor blk.0.attn_norm.weight                 | size   3200           | type F32  | T+   0
[ 10/237] Writing tensor blk.0.ffn_norm.weight                  | size   3200           | type F32  | T+   0
[ 11/237] Writing tensor blk.1.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 12/237] Writing tensor blk.1.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 13/237] Writing tensor blk.1.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 14/237] Writing tensor blk.1.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[ 15/237] Writing tensor blk.1.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[ 16/237] Writing tensor blk.1.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[ 17/237] Writing tensor blk.1.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[ 18/237] Writing tensor blk.1.attn_norm.weight                 | size   3200           | type F32  | T+   0
[ 19/237] Writing tensor blk.1.ffn_norm.weight                  | size   3200           | type F32  | T+   0
[ 20/237] Writing tensor blk.2.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 21/237] Writing tensor blk.2.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 22/237] Writing tensor blk.2.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 23/237] Writing tensor blk.2.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[ 24/237] Writing tensor blk.2.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
[ 25/237] Writing tensor blk.2.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
[ 26/237] Writing tensor blk.2.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
[ 27/237] Writing tensor blk.2.attn_norm.weight                 | size   3200           | type F32  | T+   0
[ 28/237] Writing tensor blk.2.ffn_norm.weight                  | size   3200           | type F32  | T+   0
[ 29/237] Writing tensor blk.3.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 30/237] Writing tensor blk.3.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 31/237] Writing tensor blk.3.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
[ 32/237] Writing tensor blk.3.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
[ 33/237] Writing tensor blk.3.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 34/237] Writing tensor blk.3.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 35/237] Writing tensor blk.3.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 36/237] Writing tensor blk.3.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 37/237] Writing tensor blk.3.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 38/237] Writing tensor blk.4.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 39/237] Writing tensor blk.4.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 40/237] Writing tensor blk.4.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 41/237] Writing tensor blk.4.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 42/237] Writing tensor blk.4.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 43/237] Writing tensor blk.4.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 44/237] Writing tensor blk.4.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 45/237] Writing tensor blk.4.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 46/237] Writing tensor blk.4.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 47/237] Writing tensor blk.5.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 48/237] Writing tensor blk.5.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 49/237] Writing tensor blk.5.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 50/237] Writing tensor blk.5.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 51/237] Writing tensor blk.5.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 52/237] Writing tensor blk.5.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 53/237] Writing tensor blk.5.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 54/237] Writing tensor blk.5.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 55/237] Writing tensor blk.5.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 56/237] Writing tensor blk.6.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 57/237] Writing tensor blk.6.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 58/237] Writing tensor blk.6.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 59/237] Writing tensor blk.6.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 60/237] Writing tensor blk.6.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 61/237] Writing tensor blk.6.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 62/237] Writing tensor blk.6.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 63/237] Writing tensor blk.6.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 64/237] Writing tensor blk.6.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 65/237] Writing tensor blk.7.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 66/237] Writing tensor blk.7.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 67/237] Writing tensor blk.7.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 68/237] Writing tensor blk.7.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 69/237] Writing tensor blk.7.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 70/237] Writing tensor blk.7.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 71/237] Writing tensor blk.7.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 72/237] Writing tensor blk.7.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 73/237] Writing tensor blk.7.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 74/237] Writing tensor blk.8.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 75/237] Writing tensor blk.8.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 76/237] Writing tensor blk.8.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 77/237] Writing tensor blk.8.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 78/237] Writing tensor blk.8.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 79/237] Writing tensor blk.8.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 80/237] Writing tensor blk.8.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
[ 81/237] Writing tensor blk.8.attn_norm.weight                 | size   3200           | type F32  | T+   1
[ 82/237] Writing tensor blk.8.ffn_norm.weight                  | size   3200           | type F32  | T+   1
[ 83/237] Writing tensor blk.9.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 84/237] Writing tensor blk.9.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 85/237] Writing tensor blk.9.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
[ 86/237] Writing tensor blk.9.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
[ 87/237] Writing tensor blk.9.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
[ 88/237] Writing tensor blk.9.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
[ 89/237] Writing tensor blk.9.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   2
[ 90/237] Writing tensor blk.9.attn_norm.weight                 | size   3200           | type F32  | T+   2
[ 91/237] Writing tensor blk.9.ffn_norm.weight                  | size   3200           | type F32  | T+   2
[ 92/237] Writing tensor blk.10.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[ 93/237] Writing tensor blk.10.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[ 94/237] Writing tensor blk.10.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[ 95/237] Writing tensor blk.10.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[ 96/237] Writing tensor blk.10.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[ 97/237] Writing tensor blk.10.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[ 98/237] Writing tensor blk.10.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[ 99/237] Writing tensor blk.10.attn_norm.weight                | size   3200           | type F32  | T+   2
[100/237] Writing tensor blk.10.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[101/237] Writing tensor blk.11.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[102/237] Writing tensor blk.11.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[103/237] Writing tensor blk.11.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[104/237] Writing tensor blk.11.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[105/237] Writing tensor blk.11.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[106/237] Writing tensor blk.11.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[107/237] Writing tensor blk.11.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[108/237] Writing tensor blk.11.attn_norm.weight                | size   3200           | type F32  | T+   2
[109/237] Writing tensor blk.11.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[110/237] Writing tensor blk.12.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[111/237] Writing tensor blk.12.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[112/237] Writing tensor blk.12.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[113/237] Writing tensor blk.12.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[114/237] Writing tensor blk.12.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[115/237] Writing tensor blk.12.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[116/237] Writing tensor blk.12.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[117/237] Writing tensor blk.12.attn_norm.weight                | size   3200           | type F32  | T+   2
[118/237] Writing tensor blk.12.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[119/237] Writing tensor blk.13.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[120/237] Writing tensor blk.13.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[121/237] Writing tensor blk.13.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[122/237] Writing tensor blk.13.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[123/237] Writing tensor blk.13.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[124/237] Writing tensor blk.13.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[125/237] Writing tensor blk.13.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[126/237] Writing tensor blk.13.attn_norm.weight                | size   3200           | type F32  | T+   2
[127/237] Writing tensor blk.13.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[128/237] Writing tensor blk.14.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[129/237] Writing tensor blk.14.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[130/237] Writing tensor blk.14.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[131/237] Writing tensor blk.14.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[132/237] Writing tensor blk.14.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[133/237] Writing tensor blk.14.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[134/237] Writing tensor blk.14.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[135/237] Writing tensor blk.14.attn_norm.weight                | size   3200           | type F32  | T+   2
[136/237] Writing tensor blk.14.ffn_norm.weight                 | size   3200           | type F32  | T+   2
[137/237] Writing tensor blk.15.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
[138/237] Writing tensor blk.15.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
[139/237] Writing tensor blk.15.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
[140/237] Writing tensor blk.15.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
[141/237] Writing tensor blk.15.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
[142/237] Writing tensor blk.15.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
[143/237] Writing tensor blk.15.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
[144/237] Writing tensor blk.15.attn_norm.weight                | size   3200           | type F32  | T+   3
[145/237] Writing tensor blk.15.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[146/237] Writing tensor blk.16.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[147/237] Writing tensor blk.16.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[148/237] Writing tensor blk.16.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[149/237] Writing tensor blk.16.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[150/237] Writing tensor blk.16.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[151/237] Writing tensor blk.16.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[152/237] Writing tensor blk.16.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[153/237] Writing tensor blk.16.attn_norm.weight                | size   3200           | type F32  | T+   3
[154/237] Writing tensor blk.16.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[155/237] Writing tensor blk.17.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[156/237] Writing tensor blk.17.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[157/237] Writing tensor blk.17.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[158/237] Writing tensor blk.17.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[159/237] Writing tensor blk.17.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[160/237] Writing tensor blk.17.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[161/237] Writing tensor blk.17.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[162/237] Writing tensor blk.17.attn_norm.weight                | size   3200           | type F32  | T+   3
[163/237] Writing tensor blk.17.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[164/237] Writing tensor blk.18.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[165/237] Writing tensor blk.18.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[166/237] Writing tensor blk.18.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[167/237] Writing tensor blk.18.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[168/237] Writing tensor blk.18.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[169/237] Writing tensor blk.18.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[170/237] Writing tensor blk.18.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[171/237] Writing tensor blk.18.attn_norm.weight                | size   3200           | type F32  | T+   3
[172/237] Writing tensor blk.18.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[173/237] Writing tensor blk.19.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[174/237] Writing tensor blk.19.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[175/237] Writing tensor blk.19.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[176/237] Writing tensor blk.19.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[177/237] Writing tensor blk.19.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[178/237] Writing tensor blk.19.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[179/237] Writing tensor blk.19.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[180/237] Writing tensor blk.19.attn_norm.weight                | size   3200           | type F32  | T+   3
[181/237] Writing tensor blk.19.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[182/237] Writing tensor blk.20.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[183/237] Writing tensor blk.20.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[184/237] Writing tensor blk.20.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[185/237] Writing tensor blk.20.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[186/237] Writing tensor blk.20.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[187/237] Writing tensor blk.20.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[188/237] Writing tensor blk.20.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[189/237] Writing tensor blk.20.attn_norm.weight                | size   3200           | type F32  | T+   3
[190/237] Writing tensor blk.20.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[191/237] Writing tensor blk.21.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[192/237] Writing tensor blk.21.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[193/237] Writing tensor blk.21.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[194/237] Writing tensor blk.21.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[195/237] Writing tensor blk.21.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[196/237] Writing tensor blk.21.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
[197/237] Writing tensor blk.21.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
[198/237] Writing tensor blk.21.attn_norm.weight                | size   3200           | type F32  | T+   3
[199/237] Writing tensor blk.21.ffn_norm.weight                 | size   3200           | type F32  | T+   3
[200/237] Writing tensor blk.22.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
[201/237] Writing tensor blk.22.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
[202/237] Writing tensor blk.22.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
[203/237] Writing tensor blk.22.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
[204/237] Writing tensor blk.22.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
[205/237] Writing tensor blk.22.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[206/237] Writing tensor blk.22.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[207/237] Writing tensor blk.22.attn_norm.weight                | size   3200           | type F32  | T+   4
[208/237] Writing tensor blk.22.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[209/237] Writing tensor blk.23.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   4
[210/237] Writing tensor blk.23.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
[211/237] Writing tensor blk.23.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
[212/237] Writing tensor blk.23.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
[213/237] Writing tensor blk.23.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
[214/237] Writing tensor blk.23.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[215/237] Writing tensor blk.23.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[216/237] Writing tensor blk.23.attn_norm.weight                | size   3200           | type F32  | T+   4
[217/237] Writing tensor blk.23.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[218/237] Writing tensor blk.24.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   4
[219/237] Writing tensor blk.24.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
[220/237] Writing tensor blk.24.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
[221/237] Writing tensor blk.24.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
[222/237] Writing tensor blk.24.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
[223/237] Writing tensor blk.24.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[224/237] Writing tensor blk.24.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[225/237] Writing tensor blk.24.attn_norm.weight                | size   3200           | type F32  | T+   4
[226/237] Writing tensor blk.24.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[227/237] Writing tensor blk.25.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   4
[228/237] Writing tensor blk.25.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
[229/237] Writing tensor blk.25.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
[230/237] Writing tensor blk.25.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
[231/237] Writing tensor blk.25.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
[232/237] Writing tensor blk.25.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
[233/237] Writing tensor blk.25.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
[234/237] Writing tensor blk.25.attn_norm.weight                | size   3200           | type F32  | T+   4
[235/237] Writing tensor blk.25.ffn_norm.weight                 | size   3200           | type F32  | T+   4
[236/237] Writing tensor output_norm.weight                     | size   3200           | type F32  | T+   4
[237/237] Writing tensor output.weight                          | size  32000 x   3200  | type F16  | T+   4
Wrote ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf

main: quantize time =  9466.91 ms
main:    total time =  9466.91 ms

main: quantize time =  6677.35 ms
main:    total time =  6677.35 ms

main: quantize time =  7275.69 ms
main:    total time =  7275.69 ms

main: quantize time =  7753.30 ms
main:    total time =  7753.30 ms

main: quantize time =  8451.08 ms
main:    total time =  8451.08 ms

main: quantize time = 25642.75 ms
main:    total time = 25642.75 ms

main: quantize time = 23232.83 ms
main:    total time = 23232.83 ms

main: quantize time = 40685.43 ms
main:    total time = 40685.43 ms

main: quantize time = 27602.00 ms
main:    total time = 27602.00 ms

main: quantize time = 26907.91 ms
main:    total time = 26907.91 ms
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love. Love with everything you have and then some more!
Its not easy, but it should be everyones goal in this lifetime because that means your soul has a purpose on Earth or at least deserves an adventure; especially if we are given so much freedom as humans do today (even though many still
llama_print_timings:        load time =    3251.54 ms
llama_print_timings:      sample time =      11.70 ms /    64 runs   (    0.18 ms per token,  5472.42 tokens per second)
llama_print_timings: prompt eval time =    2351.40 ms /     8 tokens (  293.93 ms per token,     3.40 tokens per second)
llama_print_timings:        eval time =   19715.16 ms /    63 runs   (  312.94 ms per token,     3.20 tokens per second)
llama_print_timings:       total time =   22089.95 ms /    71 tokens
Log end

real	0m25.632s
user	1m30.517s
sys	0m3.006s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love. Love unconditionally, without expectations and with a deep sense of gratitude for what you have been given on this day in your life; every single one that God has created so far!
To be able to walk down from childhood years into adulthood feeling complete contentment about yourself as well as the world
llama_print_timings:        load time =    1536.22 ms
llama_print_timings:      sample time =      11.89 ms /    64 runs   (    0.19 ms per token,  5382.67 tokens per second)
llama_print_timings: prompt eval time =     923.11 ms /     8 tokens (  115.39 ms per token,     8.67 tokens per second)
llama_print_timings:        eval time =    8390.98 ms /    63 runs   (  133.19 ms per token,     7.51 tokens per second)
llama_print_timings:       total time =    9337.69 ms /    71 tokens
Log end

real	0m11.154s
user	0m38.003s
sys	0m1.710s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to experience all that this world has in store for us. To take risks, and make mistakes so we can grow as people who have been given a chance on earth with another opportunity at life after death comes knocking your door.
I love living here because there are no boundaries or limitations when it come s to the
llama_print_timings:        load time =    1018.60 ms
llama_print_timings:      sample time =      11.76 ms /    64 runs   (    0.18 ms per token,  5440.79 tokens per second)
llama_print_timings: prompt eval time =     964.92 ms /     8 tokens (  120.61 ms per token,     8.29 tokens per second)
llama_print_timings:        eval time =    7964.41 ms /    63 runs   (  126.42 ms per token,     7.91 tokens per second)
llama_print_timings:       total time =    8952.52 ms /    71 tokens
Log end

real	0m10.163s
user	0m36.463s
sys	0m1.102s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 3
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.02 GiB (5.05 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2064.25 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to make a difference, and that all people are created equal. My goal in this class will be for us to figure out how we can help each other as well as our community improve by doing what you love best: writing stories!
Mission Statement 2016-present day; I am the proud
llama_print_timings:        load time =    1081.71 ms
llama_print_timings:      sample time =      11.60 ms /    64 runs   (    0.18 ms per token,  5519.14 tokens per second)
llama_print_timings: prompt eval time =     919.06 ms /     8 tokens (  114.88 ms per token,     8.70 tokens per second)
llama_print_timings:        eval time =    7706.36 ms /    63 runs   (  122.32 ms per token,     8.18 tokens per second)
llama_print_timings:       total time =    8648.34 ms /    71 tokens
Log end

real	0m9.935s
user	0m35.259s
sys	0m1.138s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 8
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.21 GiB (5.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2262.37 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to find and do your calling.
If you are not sure what that means, or if its a myth about how we all know who we were meant for in this world  then keep reading because once upon an elvish time (I can see my family rolling their eyes at me) I was
llama_print_timings:        load time =    1262.10 ms
llama_print_timings:      sample time =      11.65 ms /    64 runs   (    0.18 ms per token,  5494.03 tokens per second)
llama_print_timings: prompt eval time =    1285.26 ms /     8 tokens (  160.66 ms per token,     6.22 tokens per second)
llama_print_timings:        eval time =   10593.53 ms /    63 runs   (  168.15 ms per token,     5.95 tokens per second)
llama_print_timings:       total time =   11902.57 ms /    71 tokens
Log end

real	0m13.380s
user	0m48.576s
sys	0m1.312s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 9
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.40 GiB (6.02 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2460.49 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to find joy and share it with others.
I am a writer, photographer & graphic designer who loves adventure and getting lost in nature. My main focus for my personal work has been capturing moments through travel photography from around Asia: Singapore, Malaysia, Hong Kong, Thailand (Bangkok), Bali, Vietnam,
llama_print_timings:        load time =    1249.95 ms
llama_print_timings:      sample time =      12.30 ms /    64 runs   (    0.19 ms per token,  5203.68 tokens per second)
llama_print_timings: prompt eval time =    1237.34 ms /     8 tokens (  154.67 ms per token,     6.47 tokens per second)
llama_print_timings:        eval time =   10546.77 ms /    63 runs   (  167.41 ms per token,     5.97 tokens per second)
llama_print_timings:       total time =   11808.25 ms /    71 tokens
Log end

real	0m13.277s
user	0m48.176s
sys	0m1.297s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 10
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q2_K:  105 tensors
llama_model_loader: - type q3_K:   78 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.31 GiB (3.30 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1346.35 MiB
..............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to make it happen and not just for you but also with your children.
I am a 21st century woman, an urbanite who loves going out in nature even if only by walking up my neighbourhood hillock or taking public transport instead of driving around town on weekends! This blog aims at helping women like
llama_print_timings:        load time =     841.38 ms
llama_print_timings:      sample time =      11.94 ms /    64 runs   (    0.19 ms per token,  5359.69 tokens per second)
llama_print_timings: prompt eval time =    1212.67 ms /     8 tokens (  151.58 ms per token,     6.60 tokens per second)
llama_print_timings:        eval time =    9822.67 ms /    63 runs   (  155.92 ms per token,     6.41 tokens per second)
llama_print_timings:       total time =   11059.03 ms /    71 tokens
Log end

real	0m12.058s
user	0m45.031s
sys	0m0.925s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 12
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q3_K:  105 tensors
llama_model_loader: - type q4_K:   75 tensors
llama_model_loader: - type q5_K:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.62 GiB (4.07 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1662.08 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to help others, but its so hard when you have a kid with autism.
I was asked what advice would I give my younger self and this came out  It will get better! This may be cliche in hindsight (and also very vague), because there are many different ways that things could
llama_print_timings:        load time =     940.77 ms
llama_print_timings:      sample time =      11.90 ms /    64 runs   (    0.19 ms per token,  5377.70 tokens per second)
llama_print_timings: prompt eval time =    1143.36 ms /     8 tokens (  142.92 ms per token,     7.00 tokens per second)
llama_print_timings:        eval time =    9370.93 ms /    63 runs   (  148.74 ms per token,     6.72 tokens per second)
llama_print_timings:       total time =   10537.48 ms /    71 tokens
Log end

real	0m11.655s
user	0m42.912s
sys	0m1.038s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.03 GiB (5.10 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2082.62 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love, live and serve.
I am a wife who loves my husband with all her heart! We are best friendswe have been together since we were 14 years old and married for almost ten years now!!! He supports me in every area; hes an awesome cook (that I don
llama_print_timings:        load time =    1133.85 ms
llama_print_timings:      sample time =      11.91 ms /    64 runs   (    0.19 ms per token,  5372.73 tokens per second)
llama_print_timings: prompt eval time =    1015.17 ms /     8 tokens (  126.90 ms per token,     7.88 tokens per second)
llama_print_timings:        eval time =    8384.53 ms /    63 runs   (  133.09 ms per token,     7.51 tokens per second)
llama_print_timings:       total time =    9423.24 ms /    71 tokens
Log end

real	0m10.761s
user	0m38.310s
sys	0m1.303s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 17
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.36 GiB (5.92 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2420.14 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to find and follow your bliss.
Monday, November 25,: #143697) | Contact Support
Please refer to our Terms of Use for further information regarding our privacy policy.
- Type in an item number or a part name (i.e., "A0
llama_print_timings:        load time =    1258.18 ms
llama_print_timings:      sample time =      12.24 ms /    64 runs   (    0.19 ms per token,  5227.90 tokens per second)
llama_print_timings: prompt eval time =    1377.24 ms /     8 tokens (  172.15 ms per token,     5.81 tokens per second)
llama_print_timings:        eval time =   11204.33 ms /    63 runs   (  177.85 ms per token,     5.62 tokens per second)
llama_print_timings:       total time =   12605.10 ms /    71 tokens
Log end

real	0m14.079s
user	0m51.294s
sys	0m1.460s
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 18
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q6_K:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.69 GiB (6.75 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2757.67 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 1


 I believe the meaning of life is to love. Love with everything you have and then some more!
My name's Alexandra, but my friends just call me Lexi or L.E.! :3 My nickname came from one day when in school everyone had trouble saying "Lex-" so they all started calling me that instead of
llama_print_timings:        load time =    1462.39 ms
llama_print_timings:      sample time =      11.85 ms /    64 runs   (    0.19 ms per token,  5402.67 tokens per second)
llama_print_timings: prompt eval time =    1412.94 ms /     8 tokens (  176.62 ms per token,     5.66 tokens per second)
llama_print_timings:        eval time =   11528.30 ms /    63 runs   (  182.99 ms per token,     5.46 tokens per second)
llama_print_timings:       total time =   12964.93 ms /    71 tokens
Log end

real	0m14.666s
user	0m52.864s
sys	0m1.585s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279259
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.91 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 37.57 seconds per pass - ETA 0.62 minutes
[1]4.2442,
llama_print_timings:        load time =    3213.71 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   37565.50 ms /   128 tokens (  293.48 ms per token,     3.41 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   37576.64 ms /   129 tokens

Final estimate: PPL = 4.2442 +/- 1.01574

real	0m41.073s
user	2m32.517s
sys	0m3.049s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279300
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.532 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 14.73 seconds per pass - ETA 0.23 minutes
[1]4.2615,
llama_print_timings:        load time =    1494.04 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   14732.17 ms /   128 tokens (  115.10 ms per token,     8.69 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   14743.36 ms /   129 tokens

Final estimate: PPL = 4.2615 +/- 1.02325

real	0m16.384s
user	0m59.702s
sys	0m1.603s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279316
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.515 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 15.48 seconds per pass - ETA 0.25 minutes
[1]4.1750,
llama_print_timings:        load time =     929.12 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   15484.67 ms /   128 tokens (  120.97 ms per token,     8.27 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   15496.12 ms /   129 tokens

Final estimate: PPL = 4.1750 +/- 0.98064

real	0m16.537s
user	1m2.713s
sys	0m1.000s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279333
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 3
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.02 GiB (5.05 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2064.25 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.823 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 14.76 seconds per pass - ETA 0.23 minutes
[1]4.3872,
llama_print_timings:        load time =    1036.33 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   14756.05 ms /   128 tokens (  115.28 ms per token,     8.67 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   14767.69 ms /   129 tokens

Final estimate: PPL = 4.3872 +/- 1.05084

real	0m15.924s
user	0m59.778s
sys	0m1.108s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279349
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 8
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.21 GiB (5.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2262.37 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.801 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 20.67 seconds per pass - ETA 0.33 minutes
[1]4.3466,
llama_print_timings:        load time =    1204.13 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   20669.88 ms /   128 tokens (  161.48 ms per token,     6.19 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   20681.27 ms /   129 tokens

Final estimate: PPL = 4.3466 +/- 1.04362

real	0m22.009s
user	1m23.775s
sys	0m1.224s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279371
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 9
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.40 GiB (6.02 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2460.49 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.925 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 19.87 seconds per pass - ETA 0.32 minutes
[1]4.2515,
llama_print_timings:        load time =    1288.58 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   19868.35 ms /   128 tokens (  155.22 ms per token,     6.44 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   19879.84 ms /   129 tokens

Final estimate: PPL = 4.2515 +/- 1.01921

real	0m21.294s
user	1m20.543s
sys	0m1.308s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279392
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 10
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q2_K:  105 tensors
llama_model_loader: - type q3_K:   78 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.31 GiB (3.30 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1346.35 MiB
..............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.595 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 19.35 seconds per pass - ETA 0.32 minutes
[1]5.6188,
llama_print_timings:        load time =     809.96 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   19351.04 ms /   128 tokens (  151.18 ms per token,     6.61 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   19362.20 ms /   129 tokens

Final estimate: PPL = 5.6188 +/- 1.37004

real	0m20.270s
user	1m18.117s
sys	0m1.108s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279412
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 12
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q3_K:  105 tensors
llama_model_loader: - type q4_K:   75 tensors
llama_model_loader: - type q5_K:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.62 GiB (4.07 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1662.08 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.872 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 18.37 seconds per pass - ETA 0.30 minutes
[1]4.4258,
llama_print_timings:        load time =     941.74 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   18371.98 ms /   128 tokens (  143.53 ms per token,     6.97 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   18383.68 ms /   129 tokens

Final estimate: PPL = 4.4258 +/- 1.05707

real	0m19.434s
user	1m14.106s
sys	0m1.304s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279432
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.03 GiB (5.10 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2082.62 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.527 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 16.22 seconds per pass - ETA 0.27 minutes
[1]4.1929,
llama_print_timings:        load time =    1022.27 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   16223.08 ms /   128 tokens (  126.74 ms per token,     7.89 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   16234.37 ms /   129 tokens

Final estimate: PPL = 4.1929 +/- 0.99765

real	0m17.373s
user	1m5.475s
sys	0m1.332s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279449
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 17
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.36 GiB (5.92 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2420.14 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.862 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 22.11 seconds per pass - ETA 0.37 minutes
[1]4.2316,
llama_print_timings:        load time =    1278.41 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   22106.62 ms /   128 tokens (  172.71 ms per token,     5.79 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   22118.36 ms /   129 tokens

Final estimate: PPL = 4.2316 +/- 1.00381

real	0m23.519s
user	1m29.351s
sys	0m1.525s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279473
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 18
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q6_K:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.69 GiB (6.75 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  2757.67 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 9.584 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 22.65 seconds per pass - ETA 0.37 minutes
[1]4.2451,
llama_print_timings:        load time =    1350.97 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   22654.79 ms /   128 tokens (  176.99 ms per token,     5.65 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   22666.40 ms /   129 tokens

Final estimate: PPL = 4.2451 +/- 1.02166

real	0m24.144s
user	1m31.619s
sys	0m1.556s
+ ./bin/imatrix --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279497
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
compute_imatrix: tokenizing the input ..
compute_imatrix: tokenization took 9.677 ms
compute_imatrix: computing over 1 chunks with batch_size 128
compute_imatrix: 37.65 seconds per pass - ETA 0.62 minutes
[1]4.2442,
save_imatrix: stored collected data after 1 chunks in imatrix.dat

llama_print_timings:        load time =   40046.90 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   37647.61 ms /   128 tokens (  294.12 ms per token,     3.40 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   40050.91 ms /   129 tokens

Final estimate: PPL = 4.2442 +/- 1.01574

real	0m40.332s
user	2m30.241s
sys	0m2.865s
+ ./bin/save-load-state --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1
main : serialized state into 1804908 out of a maximum of 235995204 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU input buffer size   =     8.26 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph splits (measure): 1
main : deserialized state from 1804908 out of a maximum of 235995204 bytes

main : success

first run: The quick brown fox is on a grassy barren plain and sees a bird in the forest.


second run: The quick brown fox is on a grassy barren plain and sees a bird in the forest.

real	0m6.058s
user	0m19.392s
sys	0m1.164s
  - f16 @ 4.2442 OK
  - q8_0 @ 4.2615 OK
  - q4_0 @ 4.1750 OK
  - q4_1 @ 4.3872 OK
  - q5_0 @ 4.3466 OK
  - q5_1 @ 4.2515 OK
  - q2_k @ 5.6188 OK
  - q3_k @ 4.4258 OK
  - q4_k @ 4.1929 OK
  - q5_k @ 4.2316 OK
  - q6_k @ 4.2451 OK
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraB (3200, 64) float32 0.78MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraA (3200, 64) float32 0.78MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraB (3200, 64) float32 0.78MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraA (3200, 64) float32 0.78MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraB (3200, 64) float32 0.78MB
Converted ../models-mnt/open-llama/3B-v2/lora/adapter_config.json and ../models-mnt/open-llama/3B-v2/lora/adapter_model.bin to ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279546
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 61.601 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 37.56 seconds per pass - ETA 0.62 minutes
[1]9.1017,
llama_print_timings:        load time =    3225.81 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   37562.23 ms /   128 tokens (  293.45 ms per token,     3.41 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   37625.74 ms /   129 tokens

Final estimate: PPL = 9.1017 +/- 2.25284

real	0m41.133s
user	2m32.566s
sys	0m3.068s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279587
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
............. done (2955.86 ms)

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 55.524 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 37.59 seconds per pass - ETA 0.62 minutes
[1]7.0171,
llama_print_timings:        load time =    6253.30 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   37586.42 ms /   128 tokens (  293.64 ms per token,     3.41 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   37643.44 ms /   129 tokens

Final estimate: PPL = 7.0171 +/- 1.75945

real	0m44.272s
user	2m41.413s
sys	0m4.757s
  - f16 shakespeare @ 9.1017 7.0171 OK
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/shakespeare/shakespeare.txt -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279631
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 60.988 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 14.73 seconds per pass - ETA 0.23 minutes
[1]9.1296,
llama_print_timings:        load time =    1491.37 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   14732.98 ms /   128 tokens (  115.10 ms per token,     8.69 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   14795.90 ms /   129 tokens

Final estimate: PPL = 9.1296 +/- 2.27370

real	0m16.434s
user	0m59.696s
sys	0m1.668s
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279648
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
llama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base
............. done (2902.61 ms)

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 58.152 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 14.73 seconds per pass - ETA 0.23 minutes
[1]6.9856,
llama_print_timings:        load time =    4498.15 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   14725.72 ms /   128 tokens (  115.04 ms per token,     8.69 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   14785.76 ms /   129 tokens

Final estimate: PPL = 6.9856 +/- 1.74649

real	0m19.524s
user	1m8.606s
sys	0m3.312s
  - q8_0 shakespeare @ 9.1296 6.9856 OK
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin --lora-base ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -c 128 -b 128 --chunks 1
main: build = 2298 (38d15216)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu
main: seed  = 1709279667
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU input buffer size   =     1.69 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph splits (measure): 1
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/3B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
llama_apply_lora_from_file_internal: loading base model from '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf'
llama_model_loader: loaded meta data with 20 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   4:                          llama.block_count u32              = 26
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
............. done (3430.12 ms)

system_info: n_threads = 4 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 58.803 ms
perplexity: calculating perplexity over 1 chunks, batch_size=128
perplexity: 14.74 seconds per pass - ETA 0.23 minutes
[1]6.9967,
llama_print_timings:        load time =    5078.74 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   14737.24 ms /   128 tokens (  115.13 ms per token,     8.69 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   14797.62 ms /   129 tokens

Final estimate: PPL = 6.9967 +/- 1.74338

real	0m20.119s
user	1m8.848s
sys	0m3.901s
  - q8_0 / f16 base shakespeare @ 9.1296 6.9967 OK
