Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.676s
user	0m0.682s
sys	0m0.993s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Built target sha1
[  7%] Built target build_info
[  7%] Built target xxhash
[  7%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 13%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Built target ggml-cpu
[ 15%] Built target ggml-blas
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 21%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Built target llama-gguf
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 28%] Built target llava
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Linking CXX static library libllava_static.a
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 33%] Linking CXX static library libcommon.a
[ 33%] Built target llama-simple
[ 33%] Built target llama-simple-chat
[ 33%] Built target test-c
[ 33%] Built target llama-quantize-stats
[ 33%] Built target llava_static
[ 33%] Built target llava_shared
[ 33%] Built target common
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 38%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-arg-parser
[ 43%] Linking CXX executable ../bin/test-grammar-integration
[ 44%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-log
[ 47%] Built target test-arg-parser
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-llama-grammar
[ 47%] Built target test-grammar-integration
[ 47%] Built target test-sampling
[ 47%] Built target test-grammar-parser
[ 47%] Built target test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 55%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-barrier
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-rope
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-barrier
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Built target test-quantize-perf
[ 65%] Built target test-quantize-fns
[ 65%] Built target test-autorelease
[ 65%] Linking CXX executable ../../bin/llama-eval-callback
[ 65%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 65%] Built target llama-batched-bench
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Built target test-rope
[ 68%] Built target llama-batched
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-embedding
[ 71%] Built target llama-eval-callback
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Built target llama-gguf-split
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Built target llama-gritlm
[ 75%] Built target llama-infill
[ 75%] Built target llama-imatrix
[ 75%] Built target llama-bench
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-cli
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Built target llama-lookup-stats
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Built target llama-cli
[ 86%] Built target llama-parallel
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Built target llama-perplexity
[ 88%] Built target llama-passkey
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 90%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-run
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-speculative-simple
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-tokenize
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-cvector-generator
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.806s
user	0m5.746s
sys	0m9.091s

main: quantize time =  2864.97 ms
main:    total time =  2864.97 ms

main: quantize time =  1314.12 ms
main:    total time =  1314.12 ms

main: quantize time =  3292.56 ms
main:    total time =  3292.56 ms

main: quantize time =  2659.60 ms
main:    total time =  2659.60 ms

main: quantize time =  4395.90 ms
main:    total time =  4395.90 ms

main: quantize time =  5075.38 ms
main:    total time =  5075.38 ms

main: quantize time =  5881.41 ms
main:    total time =  5881.41 ms

main: quantize time =  6801.61 ms
main:    total time =  6801.61 ms

main: quantize time =  5949.54 ms
main:    total time =  5949.54 ms

main: quantize time =  4637.96 ms
main:    total time =  4637.96 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.108 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.238 I main: llama backend init
0.00.000.244 I main: load the model and apply lora adapter, if any
0.00.029.453 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.677 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.696 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.698 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.699 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.699 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.702 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.702 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.703 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.704 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.705 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.705 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.706 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.712 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.712 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.476 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.803 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.626 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.626 I llama_model_loader: - type  f32:  194 tensors
0.00.062.627 I llama_model_loader: - type  f16:   98 tensors
0.00.093.240 I llm_load_vocab: special tokens cache size = 25
0.00.100.160 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.100.163 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.100.163 I llm_load_print_meta: arch             = gptneox
0.00.100.163 I llm_load_print_meta: vocab type       = BPE
0.00.100.163 I llm_load_print_meta: n_vocab          = 50304
0.00.100.163 I llm_load_print_meta: n_merges         = 50009
0.00.100.164 I llm_load_print_meta: vocab_only       = 0
0.00.100.164 I llm_load_print_meta: n_ctx_train      = 2048
0.00.100.164 I llm_load_print_meta: n_embd           = 2048
0.00.100.164 I llm_load_print_meta: n_layer          = 24
0.00.100.179 I llm_load_print_meta: n_head           = 16
0.00.100.179 I llm_load_print_meta: n_head_kv        = 16
0.00.100.179 I llm_load_print_meta: n_rot            = 32
0.00.100.179 I llm_load_print_meta: n_swa            = 0
0.00.100.180 I llm_load_print_meta: n_embd_head_k    = 128
0.00.100.180 I llm_load_print_meta: n_embd_head_v    = 128
0.00.100.180 I llm_load_print_meta: n_gqa            = 1
0.00.100.181 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.100.182 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.100.182 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.100.182 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.100.183 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.100.183 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.100.183 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.100.184 I llm_load_print_meta: n_ff             = 8192
0.00.100.184 I llm_load_print_meta: n_expert         = 0
0.00.100.184 I llm_load_print_meta: n_expert_used    = 0
0.00.100.184 I llm_load_print_meta: causal attn      = 1
0.00.100.184 I llm_load_print_meta: pooling type     = 0
0.00.100.184 I llm_load_print_meta: rope type        = 2
0.00.100.185 I llm_load_print_meta: rope scaling     = linear
0.00.100.185 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.100.185 I llm_load_print_meta: freq_scale_train = 1
0.00.100.185 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.100.186 I llm_load_print_meta: rope_finetuned   = unknown
0.00.100.186 I llm_load_print_meta: ssm_d_conv       = 0
0.00.100.186 I llm_load_print_meta: ssm_d_inner      = 0
0.00.100.186 I llm_load_print_meta: ssm_d_state      = 0
0.00.100.189 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.100.189 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.100.189 I llm_load_print_meta: model type       = 1.4B
0.00.100.189 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.100.190 I llm_load_print_meta: model params     = 1.41 B
0.00.100.190 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.100.190 I llm_load_print_meta: general.name     = 1.4B
0.00.100.191 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.100.191 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.100.191 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.100.191 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.100.192 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.100.192 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.100.192 I llm_load_print_meta: max token length = 1024
0.00.102.766 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.102.766 I llm_load_tensors: offloading output layer to GPU
0.00.102.767 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.102.785 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.102.786 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.103.742 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.743 I llama_new_context_with_model: n_ctx         = 2048
0.00.103.743 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.103.743 I llama_new_context_with_model: n_batch       = 2048
0.00.103.743 I llama_new_context_with_model: n_ubatch      = 512
0.00.103.743 I llama_new_context_with_model: flash_attn    = 0
0.00.103.744 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.744 I llama_new_context_with_model: freq_scale    = 1
0.00.103.745 I ggml_metal_init: allocating
0.00.103.753 I ggml_metal_init: found device: Apple M4
0.00.103.757 I ggml_metal_init: picking default device: Apple M4
0.00.104.453 I ggml_metal_init: using embedded metal library
0.00.115.045 I ggml_metal_init: GPU name:   Apple M4
0.00.115.047 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.115.047 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.115.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.115.048 I ggml_metal_init: simdgroup reduction   = true
0.00.115.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.115.048 I ggml_metal_init: has bfloat            = true
0.00.115.048 I ggml_metal_init: use bfloat            = true
0.00.115.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.115.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.158.324 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.158.330 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.158.351 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.159.300 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.159.303 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.159.303 I llama_new_context_with_model: graph nodes  = 967
0.00.159.304 I llama_new_context_with_model: graph splits = 2
0.00.159.344 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.159.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.159.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.239.872 I main: llama threadpool init, n_threads = 4
0.00.239.910 I 
0.00.239.950 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.239.951 I 
0.00.240.024 I sampler seed: 1234
0.00.240.029 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.240.063 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.240.065 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.240.065 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.080.422 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.02.080.423 I llama_perf_context_print:        load time =     210.41 ms
0.02.080.425 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.38 tokens per second)
0.02.080.426 I llama_perf_context_print:        eval time =    1793.76 ms /    63 runs   (   28.47 ms per token,    35.12 tokens per second)
0.02.080.428 I llama_perf_context_print:       total time =    1840.55 ms /    70 tokens
0.02.080.624 I ggml_metal_free: deallocating

real	0m2.405s
user	0m0.144s
sys	0m0.101s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.782 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.108 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.114 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.116 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.119 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.121 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.121 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.121 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.122 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.122 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.123 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.126 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.075 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.190 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.210 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.212 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.214 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.214 I llama_model_loader: - type  f32:  194 tensors
0.00.035.214 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.115 I llm_load_vocab: special tokens cache size = 25
0.00.062.228 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.231 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.231 I llm_load_print_meta: arch             = gptneox
0.00.062.232 I llm_load_print_meta: vocab type       = BPE
0.00.062.232 I llm_load_print_meta: n_vocab          = 50304
0.00.062.232 I llm_load_print_meta: n_merges         = 50009
0.00.062.233 I llm_load_print_meta: vocab_only       = 0
0.00.062.233 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.233 I llm_load_print_meta: n_embd           = 2048
0.00.062.233 I llm_load_print_meta: n_layer          = 24
0.00.062.250 I llm_load_print_meta: n_head           = 16
0.00.062.251 I llm_load_print_meta: n_head_kv        = 16
0.00.062.252 I llm_load_print_meta: n_rot            = 32
0.00.062.252 I llm_load_print_meta: n_swa            = 0
0.00.062.252 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.252 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.253 I llm_load_print_meta: n_gqa            = 1
0.00.062.253 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.254 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.254 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.255 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.255 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.257 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.257 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.257 I llm_load_print_meta: n_ff             = 8192
0.00.062.258 I llm_load_print_meta: n_expert         = 0
0.00.062.258 I llm_load_print_meta: n_expert_used    = 0
0.00.062.258 I llm_load_print_meta: causal attn      = 1
0.00.062.258 I llm_load_print_meta: pooling type     = 0
0.00.062.258 I llm_load_print_meta: rope type        = 2
0.00.062.258 I llm_load_print_meta: rope scaling     = linear
0.00.062.261 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.261 I llm_load_print_meta: freq_scale_train = 1
0.00.062.261 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.262 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.262 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.262 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.263 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.263 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.263 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.264 I llm_load_print_meta: model type       = 1.4B
0.00.062.264 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.265 I llm_load_print_meta: model params     = 1.41 B
0.00.062.265 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.265 I llm_load_print_meta: general.name     = 1.4B
0.00.062.265 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.266 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.266 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.266 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.266 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.266 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.266 I llm_load_print_meta: max token length = 1024
0.00.064.010 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.010 I llm_load_tensors: offloading output layer to GPU
0.00.064.010 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.021 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.023 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.914 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.915 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.915 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.915 I llama_new_context_with_model: n_batch       = 2048
0.00.064.915 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.916 I llama_new_context_with_model: flash_attn    = 0
0.00.064.916 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.916 I llama_new_context_with_model: freq_scale    = 1
0.00.064.917 I ggml_metal_init: allocating
0.00.064.924 I ggml_metal_init: found device: Apple M4
0.00.064.927 I ggml_metal_init: picking default device: Apple M4
0.00.065.665 I ggml_metal_init: using embedded metal library
0.00.068.237 I ggml_metal_init: GPU name:   Apple M4
0.00.068.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.240 I ggml_metal_init: simdgroup reduction   = true
0.00.068.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.241 I ggml_metal_init: has bfloat            = true
0.00.068.241 I ggml_metal_init: use bfloat            = true
0.00.068.241 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.606 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.622 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.642 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.740 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.741 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.742 I llama_new_context_with_model: graph nodes  = 967
0.00.105.742 I llama_new_context_with_model: graph splits = 2
0.00.105.771 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.914 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.914 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.345.475 I main: llama threadpool init, n_threads = 4
0.01.345.511 I 
0.01.345.542 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.345.543 I 
0.01.345.792 I sampler seed: 1234
0.01.345.797 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.345.838 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.345.841 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.345.841 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.438.953 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.02.438.953 I llama_perf_context_print:        load time =    1335.69 ms
0.02.438.956 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.39 tokens per second)
0.02.438.957 I llama_perf_context_print:        eval time =    1047.20 ms /    63 runs   (   16.62 ms per token,    60.16 tokens per second)
0.02.438.957 I llama_perf_context_print:       total time =    1093.48 ms /    70 tokens
0.02.439.172 I ggml_metal_free: deallocating

real	0m2.458s
user	0m0.113s
sys	0m0.216s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.850 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.307 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.314 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.314 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.315 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.315 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.316 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.316 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.317 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.317 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.317 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.318 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.318 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.320 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.320 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.320 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.161 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.237 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.021 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.021 I llama_model_loader: - type  f32:  194 tensors
0.00.026.021 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.022 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.726 I llm_load_vocab: special tokens cache size = 25
0.00.052.670 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.673 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.673 I llm_load_print_meta: arch             = gptneox
0.00.052.674 I llm_load_print_meta: vocab type       = BPE
0.00.052.674 I llm_load_print_meta: n_vocab          = 50304
0.00.052.674 I llm_load_print_meta: n_merges         = 50009
0.00.052.675 I llm_load_print_meta: vocab_only       = 0
0.00.052.675 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.675 I llm_load_print_meta: n_embd           = 2048
0.00.052.675 I llm_load_print_meta: n_layer          = 24
0.00.052.692 I llm_load_print_meta: n_head           = 16
0.00.052.693 I llm_load_print_meta: n_head_kv        = 16
0.00.052.693 I llm_load_print_meta: n_rot            = 32
0.00.052.694 I llm_load_print_meta: n_swa            = 0
0.00.052.694 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.694 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.695 I llm_load_print_meta: n_gqa            = 1
0.00.052.695 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.696 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.697 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.698 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.698 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.698 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.698 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.699 I llm_load_print_meta: n_ff             = 8192
0.00.052.699 I llm_load_print_meta: n_expert         = 0
0.00.052.700 I llm_load_print_meta: n_expert_used    = 0
0.00.052.700 I llm_load_print_meta: causal attn      = 1
0.00.052.700 I llm_load_print_meta: pooling type     = 0
0.00.052.702 I llm_load_print_meta: rope type        = 2
0.00.052.702 I llm_load_print_meta: rope scaling     = linear
0.00.052.703 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.703 I llm_load_print_meta: freq_scale_train = 1
0.00.052.703 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.703 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.704 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.704 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.704 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.704 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.704 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.704 I llm_load_print_meta: model type       = 1.4B
0.00.052.705 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.705 I llm_load_print_meta: model params     = 1.41 B
0.00.052.705 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.706 I llm_load_print_meta: general.name     = 1.4B
0.00.052.706 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.706 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.706 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.706 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.706 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.711 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.712 I llm_load_print_meta: max token length = 1024
0.00.054.949 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.949 I llm_load_tensors: offloading output layer to GPU
0.00.054.950 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.962 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.963 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.936 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.937 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.937 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.937 I llama_new_context_with_model: n_batch       = 2048
0.00.055.937 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.938 I llama_new_context_with_model: flash_attn    = 0
0.00.055.938 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.938 I llama_new_context_with_model: freq_scale    = 1
0.00.055.939 I ggml_metal_init: allocating
0.00.055.942 I ggml_metal_init: found device: Apple M4
0.00.055.944 I ggml_metal_init: picking default device: Apple M4
0.00.056.661 I ggml_metal_init: using embedded metal library
0.00.059.217 I ggml_metal_init: GPU name:   Apple M4
0.00.059.218 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.219 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.219 I ggml_metal_init: simdgroup reduction   = true
0.00.059.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.220 I ggml_metal_init: has bfloat            = true
0.00.059.220 I ggml_metal_init: use bfloat            = true
0.00.059.220 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.221 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.002 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.009 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.031 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.275 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.277 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.277 I llama_new_context_with_model: graph nodes  = 967
0.00.094.278 I llama_new_context_with_model: graph splits = 2
0.00.094.305 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.549 I main: llama threadpool init, n_threads = 4
0.00.672.588 I 
0.00.672.618 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.619 I 
0.00.672.847 I sampler seed: 1234
0.00.672.853 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.672.888 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.672.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.672.890 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.352.505 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.352.506 I llama_perf_context_print:        load time =     661.69 ms
0.01.352.506 I llama_perf_context_print: prompt eval time =      39.80 ms /     7 tokens (    5.69 ms per token,   175.86 tokens per second)
0.01.352.507 I llama_perf_context_print:        eval time =     636.81 ms /    63 runs   (   10.11 ms per token,    98.93 tokens per second)
0.01.352.507 I llama_perf_context_print:       total time =     679.96 ms /    70 tokens
0.01.352.701 I ggml_metal_free: deallocating

real	0m1.371s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.825 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.668 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.675 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.676 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.677 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.678 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.678 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.678 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.679 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.681 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.682 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.682 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.529 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.573 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.381 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.383 I llama_model_loader: - type  f32:  194 tensors
0.00.025.384 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.384 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.722 I llm_load_vocab: special tokens cache size = 25
0.00.051.505 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.508 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.508 I llm_load_print_meta: arch             = gptneox
0.00.051.508 I llm_load_print_meta: vocab type       = BPE
0.00.051.508 I llm_load_print_meta: n_vocab          = 50304
0.00.051.509 I llm_load_print_meta: n_merges         = 50009
0.00.051.509 I llm_load_print_meta: vocab_only       = 0
0.00.051.509 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.509 I llm_load_print_meta: n_embd           = 2048
0.00.051.509 I llm_load_print_meta: n_layer          = 24
0.00.051.524 I llm_load_print_meta: n_head           = 16
0.00.051.524 I llm_load_print_meta: n_head_kv        = 16
0.00.051.524 I llm_load_print_meta: n_rot            = 32
0.00.051.525 I llm_load_print_meta: n_swa            = 0
0.00.051.525 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.525 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.526 I llm_load_print_meta: n_gqa            = 1
0.00.051.526 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.527 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.527 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.528 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.528 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.528 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.528 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.529 I llm_load_print_meta: n_ff             = 8192
0.00.051.529 I llm_load_print_meta: n_expert         = 0
0.00.051.530 I llm_load_print_meta: n_expert_used    = 0
0.00.051.531 I llm_load_print_meta: causal attn      = 1
0.00.051.534 I llm_load_print_meta: pooling type     = 0
0.00.051.534 I llm_load_print_meta: rope type        = 2
0.00.051.534 I llm_load_print_meta: rope scaling     = linear
0.00.051.534 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.535 I llm_load_print_meta: freq_scale_train = 1
0.00.051.535 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.535 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.535 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.535 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.535 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.535 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.535 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.536 I llm_load_print_meta: model type       = 1.4B
0.00.051.536 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.538 I llm_load_print_meta: model params     = 1.41 B
0.00.051.538 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.538 I llm_load_print_meta: general.name     = 1.4B
0.00.051.538 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.539 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.539 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.539 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.539 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.539 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.539 I llm_load_print_meta: max token length = 1024
0.00.053.496 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.496 I llm_load_tensors: offloading output layer to GPU
0.00.053.496 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.507 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.508 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.380 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.381 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.381 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.381 I llama_new_context_with_model: n_batch       = 2048
0.00.054.382 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.382 I llama_new_context_with_model: flash_attn    = 0
0.00.054.382 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.383 I llama_new_context_with_model: freq_scale    = 1
0.00.054.383 I ggml_metal_init: allocating
0.00.054.390 I ggml_metal_init: found device: Apple M4
0.00.054.392 I ggml_metal_init: picking default device: Apple M4
0.00.054.979 I ggml_metal_init: using embedded metal library
0.00.057.292 I ggml_metal_init: GPU name:   Apple M4
0.00.057.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.294 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.294 I ggml_metal_init: simdgroup reduction   = true
0.00.057.294 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.294 I ggml_metal_init: has bfloat            = true
0.00.057.296 I ggml_metal_init: use bfloat            = true
0.00.057.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.952 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.958 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.974 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.994 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.996 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.996 I llama_new_context_with_model: graph nodes  = 967
0.00.086.996 I llama_new_context_with_model: graph splits = 2
0.00.087.021 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.175 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.176 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.018 I main: llama threadpool init, n_threads = 4
0.00.717.054 I 
0.00.717.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.086 I 
0.00.717.246 I sampler seed: 1234
0.00.717.251 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.282 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.284 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.284 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.441.970 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61154.18 tokens per second)
0.01.441.970 I llama_perf_context_print:        load time =     707.19 ms
0.01.441.971 I llama_perf_context_print: prompt eval time =      39.58 ms /     7 tokens (    5.65 ms per token,   176.87 tokens per second)
0.01.441.971 I llama_perf_context_print:        eval time =     682.12 ms /    63 runs   (   10.83 ms per token,    92.36 tokens per second)
0.01.441.972 I llama_perf_context_print:       total time =     724.95 ms /    70 tokens
0.01.442.171 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.108s
sys	0m0.147s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.730 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.888 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.892 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.894 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.894 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.895 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.895 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.895 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.896 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.896 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.897 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.897 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.897 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.898 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.901 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.743 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.758 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.570 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.571 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.571 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.572 I llama_model_loader: - type  f32:  194 tensors
0.00.023.572 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.572 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.932 I llm_load_vocab: special tokens cache size = 25
0.00.049.885 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.887 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.888 I llm_load_print_meta: arch             = gptneox
0.00.049.888 I llm_load_print_meta: vocab type       = BPE
0.00.049.888 I llm_load_print_meta: n_vocab          = 50304
0.00.049.889 I llm_load_print_meta: n_merges         = 50009
0.00.049.889 I llm_load_print_meta: vocab_only       = 0
0.00.049.889 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.889 I llm_load_print_meta: n_embd           = 2048
0.00.049.889 I llm_load_print_meta: n_layer          = 24
0.00.049.903 I llm_load_print_meta: n_head           = 16
0.00.049.905 I llm_load_print_meta: n_head_kv        = 16
0.00.049.905 I llm_load_print_meta: n_rot            = 32
0.00.049.905 I llm_load_print_meta: n_swa            = 0
0.00.049.905 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.905 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.906 I llm_load_print_meta: n_gqa            = 1
0.00.049.907 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.907 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.908 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.908 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.908 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.908 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.909 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.909 I llm_load_print_meta: n_ff             = 8192
0.00.049.910 I llm_load_print_meta: n_expert         = 0
0.00.049.911 I llm_load_print_meta: n_expert_used    = 0
0.00.049.913 I llm_load_print_meta: causal attn      = 1
0.00.049.914 I llm_load_print_meta: pooling type     = 0
0.00.049.914 I llm_load_print_meta: rope type        = 2
0.00.049.914 I llm_load_print_meta: rope scaling     = linear
0.00.049.914 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.915 I llm_load_print_meta: freq_scale_train = 1
0.00.049.915 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.915 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.915 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.916 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.916 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.917 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.917 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.917 I llm_load_print_meta: model type       = 1.4B
0.00.049.918 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.918 I llm_load_print_meta: model params     = 1.41 B
0.00.049.919 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.919 I llm_load_print_meta: general.name     = 1.4B
0.00.049.919 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.920 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.920 I llm_load_print_meta: max token length = 1024
0.00.051.928 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.928 I llm_load_tensors: offloading output layer to GPU
0.00.051.928 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.939 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.940 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.831 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.832 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.832 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.832 I llama_new_context_with_model: n_batch       = 2048
0.00.052.832 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.832 I llama_new_context_with_model: flash_attn    = 0
0.00.052.833 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.833 I llama_new_context_with_model: freq_scale    = 1
0.00.052.834 I ggml_metal_init: allocating
0.00.052.840 I ggml_metal_init: found device: Apple M4
0.00.052.842 I ggml_metal_init: picking default device: Apple M4
0.00.053.472 I ggml_metal_init: using embedded metal library
0.00.055.870 I ggml_metal_init: GPU name:   Apple M4
0.00.055.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.873 I ggml_metal_init: simdgroup reduction   = true
0.00.055.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.873 I ggml_metal_init: has bfloat            = true
0.00.055.873 I ggml_metal_init: use bfloat            = true
0.00.055.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.876 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.688 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.694 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.713 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.597 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.598 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.598 I llama_new_context_with_model: graph nodes  = 967
0.00.085.599 I llama_new_context_with_model: graph splits = 2
0.00.085.623 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.744 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.310 I main: llama threadpool init, n_threads = 4
0.00.778.352 I 
0.00.778.398 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.399 I 
0.00.778.643 I sampler seed: 1234
0.00.778.648 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.685 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.686 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.686 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.567.620 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.567.622 I llama_perf_context_print:        load time =     769.58 ms
0.01.567.622 I llama_perf_context_print: prompt eval time =      43.16 ms /     7 tokens (    6.17 ms per token,   162.18 tokens per second)
0.01.567.625 I llama_perf_context_print:        eval time =     742.88 ms /    63 runs   (   11.79 ms per token,    84.80 tokens per second)
0.01.567.625 I llama_perf_context_print:       total time =     789.31 ms /    70 tokens
0.01.567.835 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.466 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.472 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.473 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.476 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.476 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.477 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.477 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.477 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.483 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.414 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.488 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.413 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.415 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.415 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.415 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.415 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.416 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.416 I llama_model_loader: - type  f32:  194 tensors
0.00.025.417 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.417 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.716 I llm_load_vocab: special tokens cache size = 25
0.00.052.739 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.742 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.742 I llm_load_print_meta: arch             = gptneox
0.00.052.743 I llm_load_print_meta: vocab type       = BPE
0.00.052.743 I llm_load_print_meta: n_vocab          = 50304
0.00.052.743 I llm_load_print_meta: n_merges         = 50009
0.00.052.743 I llm_load_print_meta: vocab_only       = 0
0.00.052.744 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.744 I llm_load_print_meta: n_embd           = 2048
0.00.052.744 I llm_load_print_meta: n_layer          = 24
0.00.052.759 I llm_load_print_meta: n_head           = 16
0.00.052.761 I llm_load_print_meta: n_head_kv        = 16
0.00.052.761 I llm_load_print_meta: n_rot            = 32
0.00.052.761 I llm_load_print_meta: n_swa            = 0
0.00.052.761 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.761 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.762 I llm_load_print_meta: n_gqa            = 1
0.00.052.763 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.763 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.764 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.764 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.764 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.764 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.764 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.768 I llm_load_print_meta: n_ff             = 8192
0.00.052.769 I llm_load_print_meta: n_expert         = 0
0.00.052.769 I llm_load_print_meta: n_expert_used    = 0
0.00.052.771 I llm_load_print_meta: causal attn      = 1
0.00.052.772 I llm_load_print_meta: pooling type     = 0
0.00.052.772 I llm_load_print_meta: rope type        = 2
0.00.052.772 I llm_load_print_meta: rope scaling     = linear
0.00.052.773 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.773 I llm_load_print_meta: freq_scale_train = 1
0.00.052.773 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.773 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.776 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.776 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.776 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.776 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.776 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.777 I llm_load_print_meta: model type       = 1.4B
0.00.052.777 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.778 I llm_load_print_meta: model params     = 1.41 B
0.00.052.778 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.778 I llm_load_print_meta: general.name     = 1.4B
0.00.052.778 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.779 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.779 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.779 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.779 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.780 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.780 I llm_load_print_meta: max token length = 1024
0.00.054.841 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.841 I llm_load_tensors: offloading output layer to GPU
0.00.054.841 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.852 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.853 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.780 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.781 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.781 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.781 I llama_new_context_with_model: n_batch       = 2048
0.00.055.782 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.782 I llama_new_context_with_model: flash_attn    = 0
0.00.055.782 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.783 I llama_new_context_with_model: freq_scale    = 1
0.00.055.783 I ggml_metal_init: allocating
0.00.055.788 I ggml_metal_init: found device: Apple M4
0.00.055.791 I ggml_metal_init: picking default device: Apple M4
0.00.056.406 I ggml_metal_init: using embedded metal library
0.00.058.728 I ggml_metal_init: GPU name:   Apple M4
0.00.058.729 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.730 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.730 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.730 I ggml_metal_init: simdgroup reduction   = true
0.00.058.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.732 I ggml_metal_init: has bfloat            = true
0.00.058.732 I ggml_metal_init: use bfloat            = true
0.00.058.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.814 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.822 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.838 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.896 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.897 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.897 I llama_new_context_with_model: graph nodes  = 967
0.00.087.898 I llama_new_context_with_model: graph splits = 2
0.00.087.923 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.063 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.064 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.229 I main: llama threadpool init, n_threads = 4
0.00.738.273 I 
0.00.738.303 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.305 I 
0.00.738.568 I sampler seed: 1234
0.00.738.573 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.614 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.618 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.618 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.585.213 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50070.52 tokens per second)
0.01.585.214 I llama_perf_context_print:        load time =     728.47 ms
0.01.585.215 I llama_perf_context_print: prompt eval time =      48.41 ms /     7 tokens (    6.92 ms per token,   144.59 tokens per second)
0.01.585.216 I llama_perf_context_print:        eval time =     795.30 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.585.217 I llama_perf_context_print:       total time =     846.99 ms /    70 tokens
0.01.585.429 I ggml_metal_free: deallocating

real	0m1.603s
user	0m0.111s
sys	0m0.168s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.267 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.204 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.209 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.215 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.215 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.217 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.217 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.218 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.218 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.219 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.219 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.220 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.220 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.222 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.222 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.222 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.146 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.177 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.025 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.027 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.027 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.028 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.028 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.028 I llama_model_loader: - type  f32:  194 tensors
0.00.024.029 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.029 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.029 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.126 I llm_load_vocab: special tokens cache size = 25
0.00.051.029 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.031 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.032 I llm_load_print_meta: arch             = gptneox
0.00.051.032 I llm_load_print_meta: vocab type       = BPE
0.00.051.032 I llm_load_print_meta: n_vocab          = 50304
0.00.051.033 I llm_load_print_meta: n_merges         = 50009
0.00.051.033 I llm_load_print_meta: vocab_only       = 0
0.00.051.033 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.033 I llm_load_print_meta: n_embd           = 2048
0.00.051.033 I llm_load_print_meta: n_layer          = 24
0.00.051.047 I llm_load_print_meta: n_head           = 16
0.00.051.048 I llm_load_print_meta: n_head_kv        = 16
0.00.051.048 I llm_load_print_meta: n_rot            = 32
0.00.051.048 I llm_load_print_meta: n_swa            = 0
0.00.051.048 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.051 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.051 I llm_load_print_meta: n_gqa            = 1
0.00.051.052 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.054 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.055 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.055 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.055 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.055 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.055 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.056 I llm_load_print_meta: n_ff             = 8192
0.00.051.056 I llm_load_print_meta: n_expert         = 0
0.00.051.056 I llm_load_print_meta: n_expert_used    = 0
0.00.051.057 I llm_load_print_meta: causal attn      = 1
0.00.051.057 I llm_load_print_meta: pooling type     = 0
0.00.051.057 I llm_load_print_meta: rope type        = 2
0.00.051.057 I llm_load_print_meta: rope scaling     = linear
0.00.051.057 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.058 I llm_load_print_meta: freq_scale_train = 1
0.00.051.058 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.058 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.058 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.058 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.059 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.059 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.059 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.059 I llm_load_print_meta: model type       = 1.4B
0.00.051.059 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.060 I llm_load_print_meta: model params     = 1.41 B
0.00.051.060 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.060 I llm_load_print_meta: general.name     = 1.4B
0.00.051.061 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.061 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.061 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.061 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.062 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.062 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.063 I llm_load_print_meta: max token length = 1024
0.00.053.012 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.012 I llm_load_tensors: offloading output layer to GPU
0.00.053.012 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.023 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.024 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.980 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.981 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.981 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.981 I llama_new_context_with_model: n_batch       = 2048
0.00.053.981 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.982 I llama_new_context_with_model: flash_attn    = 0
0.00.053.982 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.982 I llama_new_context_with_model: freq_scale    = 1
0.00.053.983 I ggml_metal_init: allocating
0.00.053.986 I ggml_metal_init: found device: Apple M4
0.00.053.988 I ggml_metal_init: picking default device: Apple M4
0.00.054.610 I ggml_metal_init: using embedded metal library
0.00.056.959 I ggml_metal_init: GPU name:   Apple M4
0.00.056.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.962 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.962 I ggml_metal_init: simdgroup reduction   = true
0.00.056.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.962 I ggml_metal_init: has bfloat            = true
0.00.056.962 I ggml_metal_init: use bfloat            = true
0.00.056.963 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.963 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.761 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.767 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.785 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.848 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.850 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.850 I llama_new_context_with_model: graph nodes  = 967
0.00.087.850 I llama_new_context_with_model: graph splits = 2
0.00.087.865 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.983 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.446.497 I main: llama threadpool init, n_threads = 4
0.00.446.533 I 
0.00.446.563 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.446.564 I 
0.00.446.793 I sampler seed: 1234
0.00.446.799 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.446.814 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.446.816 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.446.816 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.130.516 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61365.60 tokens per second)
0.01.130.516 I llama_perf_context_print:        load time =     437.23 ms
0.01.130.517 I llama_perf_context_print: prompt eval time =      39.29 ms /     7 tokens (    5.61 ms per token,   178.16 tokens per second)
0.01.130.518 I llama_perf_context_print:        eval time =     641.49 ms /    63 runs   (   10.18 ms per token,    98.21 tokens per second)
0.01.130.520 I llama_perf_context_print:       total time =     684.02 ms /    70 tokens
0.01.130.722 I ggml_metal_free: deallocating

real	0m1.148s
user	0m0.111s
sys	0m0.112s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.782 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.181 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.187 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.193 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.193 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.196 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.196 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.197 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.197 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.197 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.201 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.201 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.102 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.143 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.962 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.962 I llama_model_loader: - type  f32:  194 tensors
0.00.024.962 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.963 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.963 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.963 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.327 I llm_load_vocab: special tokens cache size = 25
0.00.051.198 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.201 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.201 I llm_load_print_meta: arch             = gptneox
0.00.051.201 I llm_load_print_meta: vocab type       = BPE
0.00.051.202 I llm_load_print_meta: n_vocab          = 50304
0.00.051.202 I llm_load_print_meta: n_merges         = 50009
0.00.051.202 I llm_load_print_meta: vocab_only       = 0
0.00.051.202 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.202 I llm_load_print_meta: n_embd           = 2048
0.00.051.203 I llm_load_print_meta: n_layer          = 24
0.00.051.217 I llm_load_print_meta: n_head           = 16
0.00.051.218 I llm_load_print_meta: n_head_kv        = 16
0.00.051.218 I llm_load_print_meta: n_rot            = 32
0.00.051.219 I llm_load_print_meta: n_swa            = 0
0.00.051.219 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.219 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.220 I llm_load_print_meta: n_gqa            = 1
0.00.051.220 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.221 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.222 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.222 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.222 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.222 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.222 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.223 I llm_load_print_meta: n_ff             = 8192
0.00.051.225 I llm_load_print_meta: n_expert         = 0
0.00.051.226 I llm_load_print_meta: n_expert_used    = 0
0.00.051.226 I llm_load_print_meta: causal attn      = 1
0.00.051.226 I llm_load_print_meta: pooling type     = 0
0.00.051.227 I llm_load_print_meta: rope type        = 2
0.00.051.227 I llm_load_print_meta: rope scaling     = linear
0.00.051.227 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.227 I llm_load_print_meta: freq_scale_train = 1
0.00.051.228 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.228 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.228 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.228 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.228 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.228 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.229 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.229 I llm_load_print_meta: model type       = 1.4B
0.00.051.229 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.230 I llm_load_print_meta: model params     = 1.41 B
0.00.051.230 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.231 I llm_load_print_meta: general.name     = 1.4B
0.00.051.231 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.231 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.232 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: max token length = 1024
0.00.053.153 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.153 I llm_load_tensors: offloading output layer to GPU
0.00.053.153 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.163 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.164 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.062 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.063 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.063 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.063 I llama_new_context_with_model: n_batch       = 2048
0.00.054.064 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.064 I llama_new_context_with_model: flash_attn    = 0
0.00.054.064 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.065 I llama_new_context_with_model: freq_scale    = 1
0.00.054.065 I ggml_metal_init: allocating
0.00.054.071 I ggml_metal_init: found device: Apple M4
0.00.054.075 I ggml_metal_init: picking default device: Apple M4
0.00.054.676 I ggml_metal_init: using embedded metal library
0.00.057.019 I ggml_metal_init: GPU name:   Apple M4
0.00.057.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.023 I ggml_metal_init: simdgroup reduction   = true
0.00.057.023 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.023 I ggml_metal_init: has bfloat            = true
0.00.057.023 I ggml_metal_init: use bfloat            = true
0.00.057.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.301 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.311 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.334 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.278 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.279 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.279 I llama_new_context_with_model: graph nodes  = 967
0.00.087.279 I llama_new_context_with_model: graph splits = 2
0.00.087.304 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.739 I main: llama threadpool init, n_threads = 4
0.00.557.780 I 
0.00.557.814 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.816 I 
0.00.558.071 I sampler seed: 1234
0.00.558.076 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.558.092 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.558.092 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.558.092 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.300.637 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.300.637 I llama_perf_context_print:        load time =     547.95 ms
0.01.300.638 I llama_perf_context_print: prompt eval time =      40.68 ms /     7 tokens (    5.81 ms per token,   172.09 tokens per second)
0.01.300.639 I llama_perf_context_print:        eval time =     698.66 ms /    63 runs   (   11.09 ms per token,    90.17 tokens per second)
0.01.300.639 I llama_perf_context_print:       total time =     742.90 ms /    70 tokens
0.01.300.827 I ggml_metal_free: deallocating

real	0m1.316s
user	0m0.109s
sys	0m0.130s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.012.241 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.056 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.057 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.058 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.059 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.069 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.241 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.241 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.242 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.242 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.242 I llama_model_loader: - type  f32:  194 tensors
0.00.028.243 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.243 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.243 I llama_model_loader: - type q6_K:   13 tensors
0.00.049.330 I llm_load_vocab: special tokens cache size = 25
0.00.055.084 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.086 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.087 I llm_load_print_meta: arch             = gptneox
0.00.055.087 I llm_load_print_meta: vocab type       = BPE
0.00.055.087 I llm_load_print_meta: n_vocab          = 50304
0.00.055.087 I llm_load_print_meta: n_merges         = 50009
0.00.055.088 I llm_load_print_meta: vocab_only       = 0
0.00.055.088 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.088 I llm_load_print_meta: n_embd           = 2048
0.00.055.088 I llm_load_print_meta: n_layer          = 24
0.00.055.103 I llm_load_print_meta: n_head           = 16
0.00.055.104 I llm_load_print_meta: n_head_kv        = 16
0.00.055.104 I llm_load_print_meta: n_rot            = 32
0.00.055.104 I llm_load_print_meta: n_swa            = 0
0.00.055.105 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.105 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.105 I llm_load_print_meta: n_gqa            = 1
0.00.055.106 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.107 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.109 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.110 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.110 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.110 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.111 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.111 I llm_load_print_meta: n_ff             = 8192
0.00.055.112 I llm_load_print_meta: n_expert         = 0
0.00.055.112 I llm_load_print_meta: n_expert_used    = 0
0.00.055.112 I llm_load_print_meta: causal attn      = 1
0.00.055.112 I llm_load_print_meta: pooling type     = 0
0.00.055.112 I llm_load_print_meta: rope type        = 2
0.00.055.112 I llm_load_print_meta: rope scaling     = linear
0.00.055.113 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.114 I llm_load_print_meta: freq_scale_train = 1
0.00.055.114 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.114 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.114 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.114 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.115 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.115 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.115 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.115 I llm_load_print_meta: model type       = 1.4B
0.00.055.115 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.055.116 I llm_load_print_meta: model params     = 1.41 B
0.00.055.117 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.055.117 I llm_load_print_meta: general.name     = 1.4B
0.00.055.118 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.118 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.118 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.118 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.118 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.119 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.119 I llm_load_print_meta: max token length = 1024
0.00.057.167 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.167 I llm_load_tensors: offloading output layer to GPU
0.00.057.168 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.178 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.057.179 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.058.113 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.114 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.114 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.114 I llama_new_context_with_model: n_batch       = 2048
0.00.058.114 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.114 I llama_new_context_with_model: flash_attn    = 0
0.00.058.115 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.115 I llama_new_context_with_model: freq_scale    = 1
0.00.058.116 I ggml_metal_init: allocating
0.00.058.119 I ggml_metal_init: found device: Apple M4
0.00.058.121 I ggml_metal_init: picking default device: Apple M4
0.00.058.733 I ggml_metal_init: using embedded metal library
0.00.061.047 I ggml_metal_init: GPU name:   Apple M4
0.00.061.048 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.049 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.049 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.049 I ggml_metal_init: simdgroup reduction   = true
0.00.061.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.050 I ggml_metal_init: has bfloat            = true
0.00.061.050 I ggml_metal_init: use bfloat            = true
0.00.061.050 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.051 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.137 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.142 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.159 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.269 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.270 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.271 I llama_new_context_with_model: graph nodes  = 967
0.00.093.271 I llama_new_context_with_model: graph splits = 2
0.00.093.295 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.928 I main: llama threadpool init, n_threads = 4
0.00.620.964 I 
0.00.620.994 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.994 I 
0.00.621.210 I sampler seed: 1234
0.00.621.215 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.621.254 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.621.254 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.621.255 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.381.233 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.381.234 I llama_perf_context_print:        load time =     608.68 ms
0.01.381.235 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.57 tokens per second)
0.01.381.235 I llama_perf_context_print:        eval time =     709.73 ms /    63 runs   (   11.27 ms per token,    88.77 tokens per second)
0.01.381.236 I llama_perf_context_print:       total time =     760.31 ms /    70 tokens
0.01.381.423 I ggml_metal_free: deallocating

real	0m1.400s
user	0m0.112s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.643 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.340 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.342 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.343 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.344 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.344 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.350 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.350 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.350 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.344 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.208 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.209 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.209 I llama_model_loader: - type  f32:  194 tensors
0.00.024.210 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.210 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.588 I llm_load_vocab: special tokens cache size = 25
0.00.050.327 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.329 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.330 I llm_load_print_meta: arch             = gptneox
0.00.050.330 I llm_load_print_meta: vocab type       = BPE
0.00.050.330 I llm_load_print_meta: n_vocab          = 50304
0.00.050.331 I llm_load_print_meta: n_merges         = 50009
0.00.050.331 I llm_load_print_meta: vocab_only       = 0
0.00.050.331 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.331 I llm_load_print_meta: n_embd           = 2048
0.00.050.331 I llm_load_print_meta: n_layer          = 24
0.00.050.346 I llm_load_print_meta: n_head           = 16
0.00.050.346 I llm_load_print_meta: n_head_kv        = 16
0.00.050.347 I llm_load_print_meta: n_rot            = 32
0.00.050.347 I llm_load_print_meta: n_swa            = 0
0.00.050.347 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.348 I llm_load_print_meta: n_gqa            = 1
0.00.050.348 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.349 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.350 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.350 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.350 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.351 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.351 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.351 I llm_load_print_meta: n_ff             = 8192
0.00.050.352 I llm_load_print_meta: n_expert         = 0
0.00.050.352 I llm_load_print_meta: n_expert_used    = 0
0.00.050.353 I llm_load_print_meta: causal attn      = 1
0.00.050.354 I llm_load_print_meta: pooling type     = 0
0.00.050.354 I llm_load_print_meta: rope type        = 2
0.00.050.354 I llm_load_print_meta: rope scaling     = linear
0.00.050.355 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.355 I llm_load_print_meta: freq_scale_train = 1
0.00.050.355 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.355 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.356 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.356 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.357 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.357 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.357 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.357 I llm_load_print_meta: model type       = 1.4B
0.00.050.357 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.358 I llm_load_print_meta: model params     = 1.41 B
0.00.050.358 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.358 I llm_load_print_meta: general.name     = 1.4B
0.00.050.359 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.359 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.359 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.359 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.359 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.359 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.360 I llm_load_print_meta: max token length = 1024
0.00.052.384 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.384 I llm_load_tensors: offloading output layer to GPU
0.00.052.384 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.394 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.396 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.260 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.260 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.261 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.261 I llama_new_context_with_model: n_batch       = 2048
0.00.053.261 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.261 I llama_new_context_with_model: flash_attn    = 0
0.00.053.262 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.262 I llama_new_context_with_model: freq_scale    = 1
0.00.053.263 I ggml_metal_init: allocating
0.00.053.268 I ggml_metal_init: found device: Apple M4
0.00.053.271 I ggml_metal_init: picking default device: Apple M4
0.00.053.870 I ggml_metal_init: using embedded metal library
0.00.056.205 I ggml_metal_init: GPU name:   Apple M4
0.00.056.206 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.209 I ggml_metal_init: simdgroup reduction   = true
0.00.056.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.209 I ggml_metal_init: has bfloat            = true
0.00.056.209 I ggml_metal_init: use bfloat            = true
0.00.056.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.114 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.122 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.151 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.104 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.106 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.106 I llama_new_context_with_model: graph nodes  = 967
0.00.086.107 I llama_new_context_with_model: graph splits = 2
0.00.086.133 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.280 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.982 I main: llama threadpool init, n_threads = 4
0.00.724.032 I 
0.00.724.068 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.068 I 
0.00.724.306 I sampler seed: 1234
0.00.724.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.724.360 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.724.363 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.724.363 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.571.549 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48464.16 tokens per second)
0.01.571.550 I llama_perf_context_print:        load time =     715.33 ms
0.01.571.552 I llama_perf_context_print: prompt eval time =      51.68 ms /     7 tokens (    7.38 ms per token,   135.46 tokens per second)
0.01.571.552 I llama_perf_context_print:        eval time =     792.97 ms /    63 runs   (   12.59 ms per token,    79.45 tokens per second)
0.01.571.554 I llama_perf_context_print:       total time =     847.57 ms /    70 tokens
0.01.571.767 I ggml_metal_free: deallocating

real	0m1.590s
user	0m0.110s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.011.119 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.709 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.714 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.720 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.720 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.721 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.721 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.722 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.722 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.723 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.723 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.725 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.725 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.726 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.726 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.728 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.729 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.730 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.628 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.502 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.503 I llama_model_loader: - type  f32:  194 tensors
0.00.026.504 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.580 I llm_load_vocab: special tokens cache size = 25
0.00.053.539 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.542 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.542 I llm_load_print_meta: arch             = gptneox
0.00.053.543 I llm_load_print_meta: vocab type       = BPE
0.00.053.543 I llm_load_print_meta: n_vocab          = 50304
0.00.053.543 I llm_load_print_meta: n_merges         = 50009
0.00.053.543 I llm_load_print_meta: vocab_only       = 0
0.00.053.543 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.544 I llm_load_print_meta: n_embd           = 2048
0.00.053.544 I llm_load_print_meta: n_layer          = 24
0.00.053.558 I llm_load_print_meta: n_head           = 16
0.00.053.559 I llm_load_print_meta: n_head_kv        = 16
0.00.053.560 I llm_load_print_meta: n_rot            = 32
0.00.053.560 I llm_load_print_meta: n_swa            = 0
0.00.053.560 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.560 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.561 I llm_load_print_meta: n_gqa            = 1
0.00.053.562 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.562 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.563 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.563 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.563 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.564 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.564 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.564 I llm_load_print_meta: n_ff             = 8192
0.00.053.567 I llm_load_print_meta: n_expert         = 0
0.00.053.567 I llm_load_print_meta: n_expert_used    = 0
0.00.053.567 I llm_load_print_meta: causal attn      = 1
0.00.053.568 I llm_load_print_meta: pooling type     = 0
0.00.053.568 I llm_load_print_meta: rope type        = 2
0.00.053.568 I llm_load_print_meta: rope scaling     = linear
0.00.053.568 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.568 I llm_load_print_meta: freq_scale_train = 1
0.00.053.569 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.569 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.569 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.569 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.569 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.569 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.569 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.569 I llm_load_print_meta: model type       = 1.4B
0.00.053.570 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.570 I llm_load_print_meta: model params     = 1.41 B
0.00.053.570 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.571 I llm_load_print_meta: general.name     = 1.4B
0.00.053.571 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.572 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.572 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.572 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.573 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.573 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.573 I llm_load_print_meta: max token length = 1024
0.00.055.647 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.647 I llm_load_tensors: offloading output layer to GPU
0.00.055.648 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.658 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.660 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.611 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.612 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.612 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.613 I llama_new_context_with_model: n_batch       = 2048
0.00.056.613 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.613 I llama_new_context_with_model: flash_attn    = 0
0.00.056.614 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.614 I llama_new_context_with_model: freq_scale    = 1
0.00.056.614 I ggml_metal_init: allocating
0.00.056.618 I ggml_metal_init: found device: Apple M4
0.00.056.620 I ggml_metal_init: picking default device: Apple M4
0.00.057.216 I ggml_metal_init: using embedded metal library
0.00.059.586 I ggml_metal_init: GPU name:   Apple M4
0.00.059.587 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.588 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.588 I ggml_metal_init: simdgroup reduction   = true
0.00.059.588 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.589 I ggml_metal_init: has bfloat            = true
0.00.059.589 I ggml_metal_init: use bfloat            = true
0.00.059.589 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.590 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.107 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.113 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.141 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.193 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.195 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.195 I llama_new_context_with_model: graph nodes  = 967
0.00.091.195 I llama_new_context_with_model: graph splits = 2
0.00.091.221 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.364 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.365 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.952 I main: llama threadpool init, n_threads = 4
0.00.752.987 I 
0.00.753.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.024 I 
0.00.753.251 I sampler seed: 1234
0.00.753.255 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.271 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.271 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.271 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.637.814 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55124.22 tokens per second)
0.01.637.815 I llama_perf_context_print:        load time =     741.83 ms
0.01.637.816 I llama_perf_context_print: prompt eval time =      54.58 ms /     7 tokens (    7.80 ms per token,   128.26 tokens per second)
0.01.637.816 I llama_perf_context_print:        eval time =     826.82 ms /    63 runs   (   13.12 ms per token,    76.20 tokens per second)
0.01.637.821 I llama_perf_context_print:       total time =     884.86 ms /    70 tokens
0.01.638.005 I ggml_metal_free: deallocating

real	0m1.656s
user	0m0.110s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.565 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.317 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.954 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.968 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.968 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.972 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.972 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.973 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.973 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.974 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.974 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.977 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.977 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.961 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.858 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.859 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.859 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.859 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.860 I llama_model_loader: - type  f32:  194 tensors
0.00.053.861 I llama_model_loader: - type  f16:   98 tensors
0.00.083.917 I llm_load_vocab: special tokens cache size = 25
0.00.090.737 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.739 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.740 I llm_load_print_meta: arch             = gptneox
0.00.090.740 I llm_load_print_meta: vocab type       = BPE
0.00.090.740 I llm_load_print_meta: n_vocab          = 50304
0.00.090.740 I llm_load_print_meta: n_merges         = 50009
0.00.090.740 I llm_load_print_meta: vocab_only       = 0
0.00.090.741 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.741 I llm_load_print_meta: n_embd           = 2048
0.00.090.741 I llm_load_print_meta: n_layer          = 24
0.00.090.755 I llm_load_print_meta: n_head           = 16
0.00.090.757 I llm_load_print_meta: n_head_kv        = 16
0.00.090.757 I llm_load_print_meta: n_rot            = 32
0.00.090.757 I llm_load_print_meta: n_swa            = 0
0.00.090.757 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.757 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.759 I llm_load_print_meta: n_gqa            = 1
0.00.090.759 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.760 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.760 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.761 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.761 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.761 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.761 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.762 I llm_load_print_meta: n_ff             = 8192
0.00.090.762 I llm_load_print_meta: n_expert         = 0
0.00.090.762 I llm_load_print_meta: n_expert_used    = 0
0.00.090.762 I llm_load_print_meta: causal attn      = 1
0.00.090.762 I llm_load_print_meta: pooling type     = 0
0.00.090.763 I llm_load_print_meta: rope type        = 2
0.00.090.763 I llm_load_print_meta: rope scaling     = linear
0.00.090.763 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.763 I llm_load_print_meta: freq_scale_train = 1
0.00.090.764 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.764 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.764 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.764 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.764 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.765 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.765 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.765 I llm_load_print_meta: model type       = 1.4B
0.00.090.766 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.766 I llm_load_print_meta: model params     = 1.41 B
0.00.090.767 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.767 I llm_load_print_meta: general.name     = 1.4B
0.00.090.768 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.768 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.768 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.768 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.769 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.769 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.769 I llm_load_print_meta: max token length = 1024
0.00.093.293 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.294 I llm_load_tensors: offloading output layer to GPU
0.00.093.294 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.304 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.305 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.226 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.227 I llama_new_context_with_model: n_ctx         = 128
0.00.094.227 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.227 I llama_new_context_with_model: n_batch       = 128
0.00.094.227 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.227 I llama_new_context_with_model: flash_attn    = 0
0.00.094.228 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.228 I llama_new_context_with_model: freq_scale    = 1
0.00.094.228 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.229 I ggml_metal_init: allocating
0.00.094.236 I ggml_metal_init: found device: Apple M4
0.00.094.239 I ggml_metal_init: picking default device: Apple M4
0.00.094.851 I ggml_metal_init: using embedded metal library
0.00.097.441 I ggml_metal_init: GPU name:   Apple M4
0.00.097.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.443 I ggml_metal_init: simdgroup reduction   = true
0.00.097.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.444 I ggml_metal_init: has bfloat            = true
0.00.097.444 I ggml_metal_init: use bfloat            = true
0.00.097.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.462 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.464 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.476 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.362 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.363 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.363 I llama_new_context_with_model: graph nodes  = 967
0.00.109.363 I llama_new_context_with_model: graph splits = 2
0.00.109.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.960.962 I 
0.00.961.058 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.961.111 I perplexity: tokenizing the input ..
0.00.974.826 I perplexity: tokenization took 13.712 ms
0.00.974.832 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.097.370 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.099.319 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.099.365 I llama_perf_context_print:        load time =     936.63 ms
0.01.099.367 I llama_perf_context_print: prompt eval time =     121.61 ms /   128 tokens (    0.95 ms per token,  1052.55 tokens per second)
0.01.099.368 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.099.372 I llama_perf_context_print:       total time =     138.41 ms /   129 tokens
0.01.100.175 I ggml_metal_free: deallocating

real	0m1.299s
user	0m0.126s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.135 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.549 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.302 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.308 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.311 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.311 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.314 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.318 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.439 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.390 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.391 I llama_model_loader: - type  f32:  194 tensors
0.00.031.391 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.385 I llm_load_vocab: special tokens cache size = 25
0.00.063.557 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.560 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.560 I llm_load_print_meta: arch             = gptneox
0.00.063.560 I llm_load_print_meta: vocab type       = BPE
0.00.063.561 I llm_load_print_meta: n_vocab          = 50304
0.00.063.561 I llm_load_print_meta: n_merges         = 50009
0.00.063.561 I llm_load_print_meta: vocab_only       = 0
0.00.063.561 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.561 I llm_load_print_meta: n_embd           = 2048
0.00.063.561 I llm_load_print_meta: n_layer          = 24
0.00.063.571 I llm_load_print_meta: n_head           = 16
0.00.063.572 I llm_load_print_meta: n_head_kv        = 16
0.00.063.572 I llm_load_print_meta: n_rot            = 32
0.00.063.572 I llm_load_print_meta: n_swa            = 0
0.00.063.573 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.573 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.573 I llm_load_print_meta: n_gqa            = 1
0.00.063.574 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.575 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.575 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.575 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.575 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.576 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.576 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.576 I llm_load_print_meta: n_ff             = 8192
0.00.063.577 I llm_load_print_meta: n_expert         = 0
0.00.063.577 I llm_load_print_meta: n_expert_used    = 0
0.00.063.577 I llm_load_print_meta: causal attn      = 1
0.00.063.577 I llm_load_print_meta: pooling type     = 0
0.00.063.577 I llm_load_print_meta: rope type        = 2
0.00.063.577 I llm_load_print_meta: rope scaling     = linear
0.00.063.578 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.578 I llm_load_print_meta: freq_scale_train = 1
0.00.063.578 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.578 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.578 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.579 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.579 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.579 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.579 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.579 I llm_load_print_meta: model type       = 1.4B
0.00.063.580 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.580 I llm_load_print_meta: model params     = 1.41 B
0.00.063.580 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.580 I llm_load_print_meta: general.name     = 1.4B
0.00.063.581 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.581 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.581 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.581 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.584 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.584 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.584 I llm_load_print_meta: max token length = 1024
0.00.065.515 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.515 I llm_load_tensors: offloading output layer to GPU
0.00.065.515 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.520 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.521 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.473 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.473 I llama_new_context_with_model: n_ctx         = 128
0.00.066.474 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.474 I llama_new_context_with_model: n_batch       = 128
0.00.066.474 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.474 I llama_new_context_with_model: flash_attn    = 0
0.00.066.475 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.475 I llama_new_context_with_model: freq_scale    = 1
0.00.066.475 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.476 I ggml_metal_init: allocating
0.00.066.479 I ggml_metal_init: found device: Apple M4
0.00.066.481 I ggml_metal_init: picking default device: Apple M4
0.00.067.063 I ggml_metal_init: using embedded metal library
0.00.069.427 I ggml_metal_init: GPU name:   Apple M4
0.00.069.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.428 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.429 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.429 I ggml_metal_init: simdgroup reduction   = true
0.00.069.429 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.430 I ggml_metal_init: has bfloat            = true
0.00.069.430 I ggml_metal_init: use bfloat            = true
0.00.069.430 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.132 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.139 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.154 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.075 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.082.076 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.082.077 I llama_new_context_with_model: graph nodes  = 967
0.00.082.077 I llama_new_context_with_model: graph splits = 2
0.00.082.085 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.936.552 I 
0.00.936.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.936.614 I perplexity: tokenizing the input ..
0.00.944.814 I perplexity: tokenization took 8.199 ms
0.00.944.818 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.069.620 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.070.881 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.070.892 I llama_perf_context_print:        load time =     925.00 ms
0.01.070.893 I llama_perf_context_print: prompt eval time =     124.58 ms /   128 tokens (    0.97 ms per token,  1027.49 tokens per second)
0.01.070.894 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.070.894 I llama_perf_context_print:       total time =     134.34 ms /   129 tokens
0.01.071.217 I ggml_metal_free: deallocating

real	0m1.088s
user	0m0.092s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.714 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.655 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.659 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.661 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.662 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.662 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.662 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.662 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.663 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.664 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.664 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.665 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.666 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.668 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.668 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.669 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.670 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.670 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.569 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.639 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.640 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.641 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.641 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.641 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.642 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.642 I llama_model_loader: - type  f32:  194 tensors
0.00.024.643 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.643 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.538 I llm_load_vocab: special tokens cache size = 25
0.00.051.408 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.411 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.411 I llm_load_print_meta: arch             = gptneox
0.00.051.412 I llm_load_print_meta: vocab type       = BPE
0.00.051.412 I llm_load_print_meta: n_vocab          = 50304
0.00.051.412 I llm_load_print_meta: n_merges         = 50009
0.00.051.412 I llm_load_print_meta: vocab_only       = 0
0.00.051.413 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.413 I llm_load_print_meta: n_embd           = 2048
0.00.051.413 I llm_load_print_meta: n_layer          = 24
0.00.051.427 I llm_load_print_meta: n_head           = 16
0.00.051.429 I llm_load_print_meta: n_head_kv        = 16
0.00.051.429 I llm_load_print_meta: n_rot            = 32
0.00.051.429 I llm_load_print_meta: n_swa            = 0
0.00.051.429 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.429 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.430 I llm_load_print_meta: n_gqa            = 1
0.00.051.431 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.431 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.432 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.432 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.433 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.433 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.433 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.434 I llm_load_print_meta: n_ff             = 8192
0.00.051.434 I llm_load_print_meta: n_expert         = 0
0.00.051.434 I llm_load_print_meta: n_expert_used    = 0
0.00.051.434 I llm_load_print_meta: causal attn      = 1
0.00.051.434 I llm_load_print_meta: pooling type     = 0
0.00.051.434 I llm_load_print_meta: rope type        = 2
0.00.051.435 I llm_load_print_meta: rope scaling     = linear
0.00.051.435 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.435 I llm_load_print_meta: freq_scale_train = 1
0.00.051.435 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.435 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.437 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.437 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.437 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.437 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.437 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.437 I llm_load_print_meta: model type       = 1.4B
0.00.051.438 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.438 I llm_load_print_meta: model params     = 1.41 B
0.00.051.440 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.440 I llm_load_print_meta: general.name     = 1.4B
0.00.051.440 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.440 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.440 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.440 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.441 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.441 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.441 I llm_load_print_meta: max token length = 1024
0.00.053.477 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.477 I llm_load_tensors: offloading output layer to GPU
0.00.053.477 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.488 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.489 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.405 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.405 I llama_new_context_with_model: n_ctx         = 128
0.00.054.406 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.406 I llama_new_context_with_model: n_batch       = 128
0.00.054.406 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.406 I llama_new_context_with_model: flash_attn    = 0
0.00.054.407 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.407 I llama_new_context_with_model: freq_scale    = 1
0.00.054.407 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.408 I ggml_metal_init: allocating
0.00.054.414 I ggml_metal_init: found device: Apple M4
0.00.054.416 I ggml_metal_init: picking default device: Apple M4
0.00.055.017 I ggml_metal_init: using embedded metal library
0.00.057.389 I ggml_metal_init: GPU name:   Apple M4
0.00.057.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.391 I ggml_metal_init: simdgroup reduction   = true
0.00.057.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.392 I ggml_metal_init: has bfloat            = true
0.00.057.392 I ggml_metal_init: use bfloat            = true
0.00.057.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.122 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.127 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.140 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.026 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.027 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.028 I llama_new_context_with_model: graph nodes  = 967
0.00.069.028 I llama_new_context_with_model: graph splits = 2
0.00.069.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.606 I 
0.00.618.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.661 I perplexity: tokenizing the input ..
0.00.626.587 I perplexity: tokenization took 7.924 ms
0.00.626.590 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.749.550 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.750.726 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.750.747 I llama_perf_context_print:        load time =     608.89 ms
0.00.750.749 I llama_perf_context_print: prompt eval time =     122.73 ms /   128 tokens (    0.96 ms per token,  1042.90 tokens per second)
0.00.750.750 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.750 I llama_perf_context_print:       total time =     132.14 ms /   129 tokens
0.00.751.309 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.079s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.601 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.433 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.439 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.440 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.440 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.443 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.443 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.444 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.444 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.444 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.447 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.448 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.386 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.184 I llama_model_loader: - type  f32:  194 tensors
0.00.023.184 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.185 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.346 I llm_load_vocab: special tokens cache size = 25
0.00.049.219 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.222 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.222 I llm_load_print_meta: arch             = gptneox
0.00.049.222 I llm_load_print_meta: vocab type       = BPE
0.00.049.223 I llm_load_print_meta: n_vocab          = 50304
0.00.049.223 I llm_load_print_meta: n_merges         = 50009
0.00.049.223 I llm_load_print_meta: vocab_only       = 0
0.00.049.223 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.223 I llm_load_print_meta: n_embd           = 2048
0.00.049.224 I llm_load_print_meta: n_layer          = 24
0.00.049.238 I llm_load_print_meta: n_head           = 16
0.00.049.239 I llm_load_print_meta: n_head_kv        = 16
0.00.049.240 I llm_load_print_meta: n_rot            = 32
0.00.049.240 I llm_load_print_meta: n_swa            = 0
0.00.049.240 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.240 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.241 I llm_load_print_meta: n_gqa            = 1
0.00.049.242 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.242 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.243 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.243 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.243 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.244 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.244 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.244 I llm_load_print_meta: n_ff             = 8192
0.00.049.244 I llm_load_print_meta: n_expert         = 0
0.00.049.245 I llm_load_print_meta: n_expert_used    = 0
0.00.049.245 I llm_load_print_meta: causal attn      = 1
0.00.049.247 I llm_load_print_meta: pooling type     = 0
0.00.049.247 I llm_load_print_meta: rope type        = 2
0.00.049.247 I llm_load_print_meta: rope scaling     = linear
0.00.049.247 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.248 I llm_load_print_meta: freq_scale_train = 1
0.00.049.248 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.248 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.248 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.248 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.248 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.248 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.248 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.250 I llm_load_print_meta: model type       = 1.4B
0.00.049.250 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.250 I llm_load_print_meta: model params     = 1.41 B
0.00.049.251 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.251 I llm_load_print_meta: general.name     = 1.4B
0.00.049.255 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.255 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.255 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.255 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.255 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.256 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.256 I llm_load_print_meta: max token length = 1024
0.00.051.221 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.221 I llm_load_tensors: offloading output layer to GPU
0.00.051.222 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.232 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.233 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.132 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.133 I llama_new_context_with_model: n_ctx         = 128
0.00.052.133 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.134 I llama_new_context_with_model: n_batch       = 128
0.00.052.134 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.134 I llama_new_context_with_model: flash_attn    = 0
0.00.052.134 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.135 I llama_new_context_with_model: freq_scale    = 1
0.00.052.135 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.135 I ggml_metal_init: allocating
0.00.052.142 I ggml_metal_init: found device: Apple M4
0.00.052.144 I ggml_metal_init: picking default device: Apple M4
0.00.052.727 I ggml_metal_init: using embedded metal library
0.00.055.056 I ggml_metal_init: GPU name:   Apple M4
0.00.055.058 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.058 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.059 I ggml_metal_init: simdgroup reduction   = true
0.00.055.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.059 I ggml_metal_init: has bfloat            = true
0.00.055.059 I ggml_metal_init: use bfloat            = true
0.00.055.060 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.964 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.966 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.987 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.908 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.909 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.910 I llama_new_context_with_model: graph nodes  = 967
0.00.066.910 I llama_new_context_with_model: graph splits = 2
0.00.066.923 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.923 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.406 I 
0.00.691.449 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.469 I perplexity: tokenizing the input ..
0.00.700.093 I perplexity: tokenization took 8.622 ms
0.00.700.099 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.030 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.823.374 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.823.389 I llama_perf_context_print:        load time =     682.80 ms
0.00.823.390 I llama_perf_context_print: prompt eval time =     121.69 ms /   128 tokens (    0.95 ms per token,  1051.84 tokens per second)
0.00.823.390 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.391 I llama_perf_context_print:       total time =     131.99 ms /   129 tokens
0.00.823.759 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.088 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.092 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.096 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.096 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.096 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.097 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.099 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.021 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.094 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.036 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.036 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.037 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.037 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.038 I llama_model_loader: - type  f32:  194 tensors
0.00.028.038 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.039 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.466 I llm_load_vocab: special tokens cache size = 25
0.00.055.410 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.415 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.415 I llm_load_print_meta: arch             = gptneox
0.00.055.416 I llm_load_print_meta: vocab type       = BPE
0.00.055.416 I llm_load_print_meta: n_vocab          = 50304
0.00.055.416 I llm_load_print_meta: n_merges         = 50009
0.00.055.416 I llm_load_print_meta: vocab_only       = 0
0.00.055.416 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.417 I llm_load_print_meta: n_embd           = 2048
0.00.055.417 I llm_load_print_meta: n_layer          = 24
0.00.055.434 I llm_load_print_meta: n_head           = 16
0.00.055.435 I llm_load_print_meta: n_head_kv        = 16
0.00.055.435 I llm_load_print_meta: n_rot            = 32
0.00.055.436 I llm_load_print_meta: n_swa            = 0
0.00.055.436 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.436 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.437 I llm_load_print_meta: n_gqa            = 1
0.00.055.437 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.438 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.438 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.438 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.439 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.439 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.439 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.443 I llm_load_print_meta: n_ff             = 8192
0.00.055.443 I llm_load_print_meta: n_expert         = 0
0.00.055.443 I llm_load_print_meta: n_expert_used    = 0
0.00.055.443 I llm_load_print_meta: causal attn      = 1
0.00.055.443 I llm_load_print_meta: pooling type     = 0
0.00.055.444 I llm_load_print_meta: rope type        = 2
0.00.055.444 I llm_load_print_meta: rope scaling     = linear
0.00.055.444 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.445 I llm_load_print_meta: freq_scale_train = 1
0.00.055.445 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.445 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.445 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.445 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.446 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.446 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.446 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.446 I llm_load_print_meta: model type       = 1.4B
0.00.055.447 I llm_load_print_meta: model ftype      = Q5_0
0.00.055.447 I llm_load_print_meta: model params     = 1.41 B
0.00.055.447 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.055.448 I llm_load_print_meta: general.name     = 1.4B
0.00.055.448 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.448 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.448 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.448 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.449 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.449 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.449 I llm_load_print_meta: max token length = 1024
0.00.057.403 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.403 I llm_load_tensors: offloading output layer to GPU
0.00.057.404 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.415 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.057.416 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.058.348 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.349 I llama_new_context_with_model: n_ctx         = 128
0.00.058.349 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.058.349 I llama_new_context_with_model: n_batch       = 128
0.00.058.350 I llama_new_context_with_model: n_ubatch      = 128
0.00.058.350 I llama_new_context_with_model: flash_attn    = 0
0.00.058.350 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.351 I llama_new_context_with_model: freq_scale    = 1
0.00.058.351 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.352 I ggml_metal_init: allocating
0.00.058.356 I ggml_metal_init: found device: Apple M4
0.00.058.358 I ggml_metal_init: picking default device: Apple M4
0.00.058.969 I ggml_metal_init: using embedded metal library
0.00.061.384 I ggml_metal_init: GPU name:   Apple M4
0.00.061.386 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.387 I ggml_metal_init: simdgroup reduction   = true
0.00.061.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.388 I ggml_metal_init: has bfloat            = true
0.00.061.388 I ggml_metal_init: use bfloat            = true
0.00.061.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.628 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.638 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.666 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.526 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.527 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.527 I llama_new_context_with_model: graph nodes  = 967
0.00.073.527 I llama_new_context_with_model: graph splits = 2
0.00.073.541 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.833 I 
0.00.735.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.886 I perplexity: tokenizing the input ..
0.00.743.877 I perplexity: tokenization took 7.99 ms
0.00.743.881 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.878.329 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.879.524 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.879.532 I llama_perf_context_print:        load time =     725.13 ms
0.00.879.533 I llama_perf_context_print: prompt eval time =     134.21 ms /   128 tokens (    1.05 ms per token,   953.76 tokens per second)
0.00.879.534 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.879.534 I llama_perf_context_print:       total time =     143.70 ms /   129 tokens
0.00.879.925 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.083s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.759 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.760 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.760 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.761 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.762 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.762 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.762 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.763 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.763 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.764 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.764 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.765 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.766 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.619 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.628 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.451 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.453 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.453 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.453 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.454 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.454 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.454 I llama_model_loader: - type  f32:  194 tensors
0.00.023.455 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.455 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.426 I llm_load_vocab: special tokens cache size = 25
0.00.050.305 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.308 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.308 I llm_load_print_meta: arch             = gptneox
0.00.050.309 I llm_load_print_meta: vocab type       = BPE
0.00.050.309 I llm_load_print_meta: n_vocab          = 50304
0.00.050.309 I llm_load_print_meta: n_merges         = 50009
0.00.050.309 I llm_load_print_meta: vocab_only       = 0
0.00.050.309 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.310 I llm_load_print_meta: n_embd           = 2048
0.00.050.310 I llm_load_print_meta: n_layer          = 24
0.00.050.324 I llm_load_print_meta: n_head           = 16
0.00.050.326 I llm_load_print_meta: n_head_kv        = 16
0.00.050.326 I llm_load_print_meta: n_rot            = 32
0.00.050.326 I llm_load_print_meta: n_swa            = 0
0.00.050.326 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.327 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.327 I llm_load_print_meta: n_gqa            = 1
0.00.050.328 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.329 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.329 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.330 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.330 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.330 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.330 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.331 I llm_load_print_meta: n_ff             = 8192
0.00.050.331 I llm_load_print_meta: n_expert         = 0
0.00.050.331 I llm_load_print_meta: n_expert_used    = 0
0.00.050.331 I llm_load_print_meta: causal attn      = 1
0.00.050.331 I llm_load_print_meta: pooling type     = 0
0.00.050.331 I llm_load_print_meta: rope type        = 2
0.00.050.332 I llm_load_print_meta: rope scaling     = linear
0.00.050.332 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.332 I llm_load_print_meta: freq_scale_train = 1
0.00.050.333 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.333 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.333 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.333 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.333 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.333 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.333 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.334 I llm_load_print_meta: model type       = 1.4B
0.00.050.334 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.337 I llm_load_print_meta: model params     = 1.41 B
0.00.050.337 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.337 I llm_load_print_meta: general.name     = 1.4B
0.00.050.338 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.338 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.338 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.338 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.339 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.339 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.339 I llm_load_print_meta: max token length = 1024
0.00.052.431 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.431 I llm_load_tensors: offloading output layer to GPU
0.00.052.431 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.442 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.443 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.372 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.373 I llama_new_context_with_model: n_ctx         = 128
0.00.053.373 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.373 I llama_new_context_with_model: n_batch       = 128
0.00.053.373 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.373 I llama_new_context_with_model: flash_attn    = 0
0.00.053.374 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.374 I llama_new_context_with_model: freq_scale    = 1
0.00.053.374 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.375 I ggml_metal_init: allocating
0.00.053.382 I ggml_metal_init: found device: Apple M4
0.00.053.384 I ggml_metal_init: picking default device: Apple M4
0.00.053.954 I ggml_metal_init: using embedded metal library
0.00.056.269 I ggml_metal_init: GPU name:   Apple M4
0.00.056.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.271 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.271 I ggml_metal_init: simdgroup reduction   = true
0.00.056.271 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.271 I ggml_metal_init: has bfloat            = true
0.00.056.272 I ggml_metal_init: use bfloat            = true
0.00.056.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.743 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.747 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.761 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.668 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.670 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.670 I llama_new_context_with_model: graph nodes  = 967
0.00.067.670 I llama_new_context_with_model: graph splits = 2
0.00.067.682 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.683 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.823 I 
0.00.666.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.903 I perplexity: tokenizing the input ..
0.00.674.835 I perplexity: tokenization took 7.931 ms
0.00.674.843 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.187 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.811.435 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.811.451 I llama_perf_context_print:        load time =     658.05 ms
0.00.811.451 I llama_perf_context_print: prompt eval time =     135.12 ms /   128 tokens (    1.06 ms per token,   947.31 tokens per second)
0.00.811.452 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.453 I llama_perf_context_print:       total time =     144.64 ms /   129 tokens
0.00.811.859 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.078s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.810 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.246 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.249 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.250 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.250 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.251 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.252 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.252 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.252 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.253 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.253 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.269 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.056 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.968 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.969 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.969 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.970 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.970 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.971 I llama_model_loader: - type  f32:  194 tensors
0.00.026.971 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.971 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.971 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.128 I llm_load_vocab: special tokens cache size = 25
0.00.053.111 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.114 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.114 I llm_load_print_meta: arch             = gptneox
0.00.053.114 I llm_load_print_meta: vocab type       = BPE
0.00.053.115 I llm_load_print_meta: n_vocab          = 50304
0.00.053.115 I llm_load_print_meta: n_merges         = 50009
0.00.053.115 I llm_load_print_meta: vocab_only       = 0
0.00.053.115 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.115 I llm_load_print_meta: n_embd           = 2048
0.00.053.115 I llm_load_print_meta: n_layer          = 24
0.00.053.130 I llm_load_print_meta: n_head           = 16
0.00.053.131 I llm_load_print_meta: n_head_kv        = 16
0.00.053.131 I llm_load_print_meta: n_rot            = 32
0.00.053.131 I llm_load_print_meta: n_swa            = 0
0.00.053.132 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.132 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.132 I llm_load_print_meta: n_gqa            = 1
0.00.053.133 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.134 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.134 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.135 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.135 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.135 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.135 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.136 I llm_load_print_meta: n_ff             = 8192
0.00.053.136 I llm_load_print_meta: n_expert         = 0
0.00.053.136 I llm_load_print_meta: n_expert_used    = 0
0.00.053.136 I llm_load_print_meta: causal attn      = 1
0.00.053.137 I llm_load_print_meta: pooling type     = 0
0.00.053.137 I llm_load_print_meta: rope type        = 2
0.00.053.137 I llm_load_print_meta: rope scaling     = linear
0.00.053.138 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.138 I llm_load_print_meta: freq_scale_train = 1
0.00.053.138 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.138 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.140 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.140 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.140 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.140 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.140 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.140 I llm_load_print_meta: model type       = 1.4B
0.00.053.141 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.141 I llm_load_print_meta: model params     = 1.41 B
0.00.053.142 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.142 I llm_load_print_meta: general.name     = 1.4B
0.00.053.142 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.142 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.143 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.143 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.143 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.143 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.144 I llm_load_print_meta: max token length = 1024
0.00.055.016 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.016 I llm_load_tensors: offloading output layer to GPU
0.00.055.016 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.027 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.028 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.966 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.967 I llama_new_context_with_model: n_ctx         = 128
0.00.055.967 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.967 I llama_new_context_with_model: n_batch       = 128
0.00.055.967 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.967 I llama_new_context_with_model: flash_attn    = 0
0.00.055.968 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.968 I llama_new_context_with_model: freq_scale    = 1
0.00.055.968 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.969 I ggml_metal_init: allocating
0.00.055.974 I ggml_metal_init: found device: Apple M4
0.00.055.976 I ggml_metal_init: picking default device: Apple M4
0.00.056.582 I ggml_metal_init: using embedded metal library
0.00.058.924 I ggml_metal_init: GPU name:   Apple M4
0.00.058.925 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.926 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.926 I ggml_metal_init: simdgroup reduction   = true
0.00.058.926 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.927 I ggml_metal_init: has bfloat            = true
0.00.058.927 I ggml_metal_init: use bfloat            = true
0.00.058.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.773 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.775 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.790 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.691 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.692 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.693 I llama_new_context_with_model: graph nodes  = 967
0.00.070.693 I llama_new_context_with_model: graph splits = 2
0.00.070.706 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.707 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.397.919 I 
0.00.397.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.397.969 I perplexity: tokenizing the input ..
0.00.406.150 I perplexity: tokenization took 8.18 ms
0.00.406.158 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.538.549 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.539.700 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.539.716 I llama_perf_context_print:        load time =     387.10 ms
0.00.539.717 I llama_perf_context_print: prompt eval time =     132.17 ms /   128 tokens (    1.03 ms per token,   968.48 tokens per second)
0.00.539.718 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.539.718 I llama_perf_context_print:       total time =     141.80 ms /   129 tokens
0.00.540.183 I ggml_metal_free: deallocating

real	0m0.555s
user	0m0.078s
sys	0m0.076s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.241 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.065 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.071 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.072 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.073 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.073 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.074 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.074 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.076 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.077 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.077 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.077 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.078 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.078 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.080 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.080 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.947 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.992 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.865 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.865 I llama_model_loader: - type  f32:  194 tensors
0.00.023.866 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.866 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.866 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.866 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.703 I llm_load_vocab: special tokens cache size = 25
0.00.050.764 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.766 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.766 I llm_load_print_meta: arch             = gptneox
0.00.050.767 I llm_load_print_meta: vocab type       = BPE
0.00.050.767 I llm_load_print_meta: n_vocab          = 50304
0.00.050.767 I llm_load_print_meta: n_merges         = 50009
0.00.050.767 I llm_load_print_meta: vocab_only       = 0
0.00.050.767 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.768 I llm_load_print_meta: n_embd           = 2048
0.00.050.768 I llm_load_print_meta: n_layer          = 24
0.00.050.783 I llm_load_print_meta: n_head           = 16
0.00.050.783 I llm_load_print_meta: n_head_kv        = 16
0.00.050.784 I llm_load_print_meta: n_rot            = 32
0.00.050.784 I llm_load_print_meta: n_swa            = 0
0.00.050.784 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.784 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.785 I llm_load_print_meta: n_gqa            = 1
0.00.050.786 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.786 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.787 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.787 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.788 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.788 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.788 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.788 I llm_load_print_meta: n_ff             = 8192
0.00.050.789 I llm_load_print_meta: n_expert         = 0
0.00.050.789 I llm_load_print_meta: n_expert_used    = 0
0.00.050.789 I llm_load_print_meta: causal attn      = 1
0.00.050.789 I llm_load_print_meta: pooling type     = 0
0.00.050.789 I llm_load_print_meta: rope type        = 2
0.00.050.789 I llm_load_print_meta: rope scaling     = linear
0.00.050.789 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.790 I llm_load_print_meta: freq_scale_train = 1
0.00.050.790 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.790 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.790 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.790 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.792 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.792 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.792 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.793 I llm_load_print_meta: model type       = 1.4B
0.00.050.793 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.794 I llm_load_print_meta: model params     = 1.41 B
0.00.050.794 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.795 I llm_load_print_meta: general.name     = 1.4B
0.00.050.796 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.796 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.796 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.796 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.797 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.797 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.797 I llm_load_print_meta: max token length = 1024
0.00.052.791 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.791 I llm_load_tensors: offloading output layer to GPU
0.00.052.791 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.801 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.803 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.690 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.691 I llama_new_context_with_model: n_ctx         = 128
0.00.053.691 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.691 I llama_new_context_with_model: n_batch       = 128
0.00.053.691 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.692 I llama_new_context_with_model: flash_attn    = 0
0.00.053.692 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.692 I llama_new_context_with_model: freq_scale    = 1
0.00.053.693 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.693 I ggml_metal_init: allocating
0.00.053.699 I ggml_metal_init: found device: Apple M4
0.00.053.701 I ggml_metal_init: picking default device: Apple M4
0.00.054.253 I ggml_metal_init: using embedded metal library
0.00.056.599 I ggml_metal_init: GPU name:   Apple M4
0.00.056.601 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.601 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.602 I ggml_metal_init: simdgroup reduction   = true
0.00.056.602 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.602 I ggml_metal_init: has bfloat            = true
0.00.056.602 I ggml_metal_init: use bfloat            = true
0.00.056.603 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.603 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.138 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.141 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.155 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.015 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.017 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.017 I llama_new_context_with_model: graph nodes  = 967
0.00.068.017 I llama_new_context_with_model: graph splits = 2
0.00.068.030 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.031 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.486.013 I 
0.00.486.065 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.486.078 I perplexity: tokenizing the input ..
0.00.494.188 I perplexity: tokenization took 8.109 ms
0.00.494.191 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.626.399 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.627.577 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.627.605 I llama_perf_context_print:        load time =     476.76 ms
0.00.627.606 I llama_perf_context_print: prompt eval time =     131.98 ms /   128 tokens (    1.03 ms per token,   969.85 tokens per second)
0.00.627.607 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.627.610 I llama_perf_context_print:       total time =     141.59 ms /   129 tokens
0.00.628.037 I ggml_metal_free: deallocating

real	0m0.642s
user	0m0.079s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.908 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.824 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.828 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.830 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.832 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.835 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.836 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.837 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.837 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.837 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.721 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.564 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.565 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.566 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.566 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.567 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.567 I llama_model_loader: - type  f32:  194 tensors
0.00.024.568 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.568 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.568 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.746 I llm_load_vocab: special tokens cache size = 25
0.00.050.687 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.690 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.690 I llm_load_print_meta: arch             = gptneox
0.00.050.691 I llm_load_print_meta: vocab type       = BPE
0.00.050.691 I llm_load_print_meta: n_vocab          = 50304
0.00.050.691 I llm_load_print_meta: n_merges         = 50009
0.00.050.691 I llm_load_print_meta: vocab_only       = 0
0.00.050.691 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.692 I llm_load_print_meta: n_embd           = 2048
0.00.050.692 I llm_load_print_meta: n_layer          = 24
0.00.050.706 I llm_load_print_meta: n_head           = 16
0.00.050.707 I llm_load_print_meta: n_head_kv        = 16
0.00.050.709 I llm_load_print_meta: n_rot            = 32
0.00.050.709 I llm_load_print_meta: n_swa            = 0
0.00.050.709 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.710 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.710 I llm_load_print_meta: n_gqa            = 1
0.00.050.711 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.712 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.712 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.713 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.713 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.713 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.713 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.715 I llm_load_print_meta: n_ff             = 8192
0.00.050.715 I llm_load_print_meta: n_expert         = 0
0.00.050.715 I llm_load_print_meta: n_expert_used    = 0
0.00.050.715 I llm_load_print_meta: causal attn      = 1
0.00.050.715 I llm_load_print_meta: pooling type     = 0
0.00.050.715 I llm_load_print_meta: rope type        = 2
0.00.050.715 I llm_load_print_meta: rope scaling     = linear
0.00.050.716 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.716 I llm_load_print_meta: freq_scale_train = 1
0.00.050.716 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.716 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.717 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.717 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.717 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.717 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.718 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.718 I llm_load_print_meta: model type       = 1.4B
0.00.050.718 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.718 I llm_load_print_meta: model params     = 1.41 B
0.00.050.719 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.719 I llm_load_print_meta: general.name     = 1.4B
0.00.050.719 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.720 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.720 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.720 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.720 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.721 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: max token length = 1024
0.00.052.664 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.665 I llm_load_tensors: offloading output layer to GPU
0.00.052.665 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.675 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.676 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.567 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.568 I llama_new_context_with_model: n_ctx         = 128
0.00.053.568 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.569 I llama_new_context_with_model: n_batch       = 128
0.00.053.569 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.569 I llama_new_context_with_model: flash_attn    = 0
0.00.053.569 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.570 I llama_new_context_with_model: freq_scale    = 1
0.00.053.570 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.570 I ggml_metal_init: allocating
0.00.053.573 I ggml_metal_init: found device: Apple M4
0.00.053.575 I ggml_metal_init: picking default device: Apple M4
0.00.054.130 I ggml_metal_init: using embedded metal library
0.00.056.432 I ggml_metal_init: GPU name:   Apple M4
0.00.056.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.434 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.434 I ggml_metal_init: simdgroup reduction   = true
0.00.056.434 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.434 I ggml_metal_init: has bfloat            = true
0.00.056.435 I ggml_metal_init: use bfloat            = true
0.00.056.435 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.436 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.216 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.218 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.231 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.185 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.186 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.186 I llama_new_context_with_model: graph nodes  = 967
0.00.068.186 I llama_new_context_with_model: graph splits = 2
0.00.068.198 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.199 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.706 I 
0.00.558.763 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.558.782 I perplexity: tokenizing the input ..
0.00.566.779 I perplexity: tokenization took 7.994 ms
0.00.566.783 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.700.930 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.702.178 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.702.198 I llama_perf_context_print:        load time =     548.79 ms
0.00.702.199 I llama_perf_context_print: prompt eval time =     133.92 ms /   128 tokens (    1.05 ms per token,   955.79 tokens per second)
0.00.702.200 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.702.202 I llama_perf_context_print:       total time =     143.49 ms /   129 tokens
0.00.702.726 I ggml_metal_free: deallocating

real	0m0.718s
user	0m0.078s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.853 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.652 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.656 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.658 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.659 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.659 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.660 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.660 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.661 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.661 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.661 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.662 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.662 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.664 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.664 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.464 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.465 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.465 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.466 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.466 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.466 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.467 I llama_model_loader: - type  f32:  194 tensors
0.00.023.467 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.468 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.630 I llm_load_vocab: special tokens cache size = 25
0.00.049.678 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.682 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.682 I llm_load_print_meta: arch             = gptneox
0.00.049.683 I llm_load_print_meta: vocab type       = BPE
0.00.049.683 I llm_load_print_meta: n_vocab          = 50304
0.00.049.683 I llm_load_print_meta: n_merges         = 50009
0.00.049.683 I llm_load_print_meta: vocab_only       = 0
0.00.049.686 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.686 I llm_load_print_meta: n_embd           = 2048
0.00.049.686 I llm_load_print_meta: n_layer          = 24
0.00.049.696 I llm_load_print_meta: n_head           = 16
0.00.049.697 I llm_load_print_meta: n_head_kv        = 16
0.00.049.697 I llm_load_print_meta: n_rot            = 32
0.00.049.697 I llm_load_print_meta: n_swa            = 0
0.00.049.697 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.697 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.698 I llm_load_print_meta: n_gqa            = 1
0.00.049.699 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.700 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.700 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.701 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.702 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.702 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.702 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.703 I llm_load_print_meta: n_ff             = 8192
0.00.049.703 I llm_load_print_meta: n_expert         = 0
0.00.049.703 I llm_load_print_meta: n_expert_used    = 0
0.00.049.703 I llm_load_print_meta: causal attn      = 1
0.00.049.703 I llm_load_print_meta: pooling type     = 0
0.00.049.703 I llm_load_print_meta: rope type        = 2
0.00.049.704 I llm_load_print_meta: rope scaling     = linear
0.00.049.704 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.704 I llm_load_print_meta: freq_scale_train = 1
0.00.049.704 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.705 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.705 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.708 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.708 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.709 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.709 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.709 I llm_load_print_meta: model type       = 1.4B
0.00.049.709 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.710 I llm_load_print_meta: model params     = 1.41 B
0.00.049.710 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.710 I llm_load_print_meta: general.name     = 1.4B
0.00.049.711 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.712 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.712 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.712 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.712 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.713 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.713 I llm_load_print_meta: max token length = 1024
0.00.051.486 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.487 I llm_load_tensors: offloading output layer to GPU
0.00.051.487 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.492 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.493 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.556 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.557 I llama_new_context_with_model: n_ctx         = 128
0.00.052.557 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.557 I llama_new_context_with_model: n_batch       = 128
0.00.052.557 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.557 I llama_new_context_with_model: flash_attn    = 0
0.00.052.558 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.558 I llama_new_context_with_model: freq_scale    = 1
0.00.052.558 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.559 I ggml_metal_init: allocating
0.00.052.566 I ggml_metal_init: found device: Apple M4
0.00.052.568 I ggml_metal_init: picking default device: Apple M4
0.00.053.137 I ggml_metal_init: using embedded metal library
0.00.055.448 I ggml_metal_init: GPU name:   Apple M4
0.00.055.449 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.450 I ggml_metal_init: simdgroup reduction   = true
0.00.055.450 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.451 I ggml_metal_init: has bfloat            = true
0.00.055.451 I ggml_metal_init: use bfloat            = true
0.00.055.451 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.452 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.176 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.185 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.200 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.086 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.088 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.088 I llama_new_context_with_model: graph nodes  = 967
0.00.067.088 I llama_new_context_with_model: graph splits = 2
0.00.067.101 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.102 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.084 I 
0.00.638.115 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.127 I perplexity: tokenizing the input ..
0.00.646.279 I perplexity: tokenization took 8.15 ms
0.00.646.286 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.175 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.788.360 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.788.373 I llama_perf_context_print:        load time =     629.23 ms
0.00.788.374 I llama_perf_context_print: prompt eval time =     140.66 ms /   128 tokens (    1.10 ms per token,   909.98 tokens per second)
0.00.788.375 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.375 I llama_perf_context_print:       total time =     150.29 ms /   129 tokens
0.00.788.826 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.078s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.666 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.584 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.591 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.594 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.595 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.597 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.437 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.438 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.439 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.440 I llama_model_loader: - type  f32:  194 tensors
0.00.025.440 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.325 I llm_load_vocab: special tokens cache size = 25
0.00.052.218 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.221 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.221 I llm_load_print_meta: arch             = gptneox
0.00.052.221 I llm_load_print_meta: vocab type       = BPE
0.00.052.222 I llm_load_print_meta: n_vocab          = 50304
0.00.052.222 I llm_load_print_meta: n_merges         = 50009
0.00.052.222 I llm_load_print_meta: vocab_only       = 0
0.00.052.222 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.222 I llm_load_print_meta: n_embd           = 2048
0.00.052.222 I llm_load_print_meta: n_layer          = 24
0.00.052.236 I llm_load_print_meta: n_head           = 16
0.00.052.237 I llm_load_print_meta: n_head_kv        = 16
0.00.052.237 I llm_load_print_meta: n_rot            = 32
0.00.052.237 I llm_load_print_meta: n_swa            = 0
0.00.052.238 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.238 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.238 I llm_load_print_meta: n_gqa            = 1
0.00.052.239 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.240 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.240 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.243 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.244 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.244 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.244 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.244 I llm_load_print_meta: n_ff             = 8192
0.00.052.245 I llm_load_print_meta: n_expert         = 0
0.00.052.245 I llm_load_print_meta: n_expert_used    = 0
0.00.052.245 I llm_load_print_meta: causal attn      = 1
0.00.052.245 I llm_load_print_meta: pooling type     = 0
0.00.052.245 I llm_load_print_meta: rope type        = 2
0.00.052.245 I llm_load_print_meta: rope scaling     = linear
0.00.052.246 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.246 I llm_load_print_meta: freq_scale_train = 1
0.00.052.246 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.246 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.246 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.247 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.247 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.247 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.247 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.247 I llm_load_print_meta: model type       = 1.4B
0.00.052.248 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.249 I llm_load_print_meta: model params     = 1.41 B
0.00.052.249 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.250 I llm_load_print_meta: general.name     = 1.4B
0.00.052.250 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.250 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.251 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.251 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.251 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.251 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.251 I llm_load_print_meta: max token length = 1024
0.00.054.265 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.265 I llm_load_tensors: offloading output layer to GPU
0.00.054.265 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.276 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.277 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.188 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.189 I llama_new_context_with_model: n_ctx         = 128
0.00.055.189 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.189 I llama_new_context_with_model: n_batch       = 128
0.00.055.189 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.190 I llama_new_context_with_model: flash_attn    = 0
0.00.055.190 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.190 I llama_new_context_with_model: freq_scale    = 1
0.00.055.191 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.191 I ggml_metal_init: allocating
0.00.055.194 I ggml_metal_init: found device: Apple M4
0.00.055.197 I ggml_metal_init: picking default device: Apple M4
0.00.055.760 I ggml_metal_init: using embedded metal library
0.00.058.093 I ggml_metal_init: GPU name:   Apple M4
0.00.058.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.095 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.095 I ggml_metal_init: simdgroup reduction   = true
0.00.058.095 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.095 I ggml_metal_init: has bfloat            = true
0.00.058.095 I ggml_metal_init: use bfloat            = true
0.00.058.096 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.096 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.105 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.108 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.122 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.087 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.088 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.088 I llama_new_context_with_model: graph nodes  = 967
0.00.070.089 I llama_new_context_with_model: graph splits = 2
0.00.070.101 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.102 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.234.226 I 
0.00.234.259 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.234.272 I perplexity: tokenizing the input ..
0.00.241.763 I perplexity: tokenization took 7.488 ms
0.00.241.766 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.382.662 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.383.890 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.383.914 I llama_perf_context_print:        load time =     223.56 ms
0.00.383.916 I llama_perf_context_print: prompt eval time =     140.59 ms /   128 tokens (    1.10 ms per token,   910.46 tokens per second)
0.00.383.917 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.383.917 I llama_perf_context_print:       total time =     149.69 ms /   129 tokens
0.00.384.252 I ggml_metal_free: deallocating

real	0m0.399s
user	0m0.078s
sys	0m0.051s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.245 I build: 4341 (382bc7f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.486 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.989 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.995 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.997 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.997 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.000 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.000 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.001 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.002 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.002 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.003 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.004 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.004 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.004 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.005 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.008 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.012 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.012 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.461 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.714 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.717 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.718 I llama_model_loader: - type  f32:  194 tensors
0.00.053.719 I llama_model_loader: - type  f16:   98 tensors
0.00.082.315 I llm_load_vocab: special tokens cache size = 25
0.00.088.817 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.819 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.820 I llm_load_print_meta: arch             = gptneox
0.00.088.820 I llm_load_print_meta: vocab type       = BPE
0.00.088.820 I llm_load_print_meta: n_vocab          = 50304
0.00.088.820 I llm_load_print_meta: n_merges         = 50009
0.00.088.821 I llm_load_print_meta: vocab_only       = 0
0.00.088.821 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.821 I llm_load_print_meta: n_embd           = 2048
0.00.088.821 I llm_load_print_meta: n_layer          = 24
0.00.088.834 I llm_load_print_meta: n_head           = 16
0.00.088.835 I llm_load_print_meta: n_head_kv        = 16
0.00.088.836 I llm_load_print_meta: n_rot            = 32
0.00.088.836 I llm_load_print_meta: n_swa            = 0
0.00.088.836 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.836 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.837 I llm_load_print_meta: n_gqa            = 1
0.00.088.837 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.838 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.838 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.839 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.839 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.839 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.839 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.840 I llm_load_print_meta: n_ff             = 8192
0.00.088.840 I llm_load_print_meta: n_expert         = 0
0.00.088.840 I llm_load_print_meta: n_expert_used    = 0
0.00.088.840 I llm_load_print_meta: causal attn      = 1
0.00.088.840 I llm_load_print_meta: pooling type     = 0
0.00.088.840 I llm_load_print_meta: rope type        = 2
0.00.088.840 I llm_load_print_meta: rope scaling     = linear
0.00.088.841 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.841 I llm_load_print_meta: freq_scale_train = 1
0.00.088.841 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.841 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.842 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.842 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.842 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.842 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.842 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.842 I llm_load_print_meta: model type       = 1.4B
0.00.088.845 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.845 I llm_load_print_meta: model params     = 1.41 B
0.00.088.846 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.846 I llm_load_print_meta: general.name     = 1.4B
0.00.088.846 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.846 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.846 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.847 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.847 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.848 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.848 I llm_load_print_meta: max token length = 1024
0.00.091.385 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.385 I llm_load_tensors: offloading output layer to GPU
0.00.091.385 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.396 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.397 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.303 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.304 I llama_new_context_with_model: n_ctx         = 128
0.00.092.304 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.304 I llama_new_context_with_model: n_batch       = 128
0.00.092.305 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.305 I llama_new_context_with_model: flash_attn    = 0
0.00.092.305 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.305 I llama_new_context_with_model: freq_scale    = 1
0.00.092.306 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.306 I ggml_metal_init: allocating
0.00.092.309 I ggml_metal_init: found device: Apple M4
0.00.092.310 I ggml_metal_init: picking default device: Apple M4
0.00.092.896 I ggml_metal_init: using embedded metal library
0.00.095.371 I ggml_metal_init: GPU name:   Apple M4
0.00.095.373 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.373 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.374 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.374 I ggml_metal_init: simdgroup reduction   = true
0.00.095.374 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.374 I ggml_metal_init: has bfloat            = true
0.00.095.374 I ggml_metal_init: use bfloat            = true
0.00.095.375 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.375 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.498 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.501 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.525 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.453 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.454 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.454 I llama_new_context_with_model: graph nodes  = 967
0.00.106.454 I llama_new_context_with_model: graph splits = 2
0.00.106.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.468 I 
0.00.106.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.508 I compute_imatrix: tokenizing the input ..
0.00.113.325 I compute_imatrix: tokenization took 6.816 ms
0.00.113.326 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.641.140 I compute_imatrix: 1.53 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.643.828 I llama_perf_context_print:        load time =    1618.65 ms
0.01.643.829 I llama_perf_context_print: prompt eval time =    1527.18 ms /   128 tokens (   11.93 ms per token,    83.81 tokens per second)
0.01.643.830 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.643.831 I llama_perf_context_print:       total time =    1621.34 ms /   129 tokens
0.01.644.366 I ggml_metal_free: deallocating

real	0m1.863s
user	0m0.152s
sys	0m0.249s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4341 (382bc7f2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105904280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105904a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105904e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1059052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105905750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105905bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105906030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1059064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105906910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105906d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1059071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105907890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1059083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105908b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105909370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105909a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10590a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10590a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10590aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10590b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10590bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10590c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10590cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10590d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10590dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10590dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10590e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10590e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10590ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10590f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10590f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10590fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105910060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105910320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105910790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105911040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105911300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105911770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105911be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105912050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1059124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105912930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105912da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105913210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105913680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105913af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105913f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105914990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105914c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1059150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105915530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1059159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105915e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105916280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1059166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105916da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105917240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105917500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105917970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105918040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105918440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105918700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105918c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105919100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105919600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105919b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10591a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10591a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10591aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10591af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10591b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10591b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10591be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10591c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10591c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10591ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10591d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10591d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10591df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10591e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10591ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10591f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10591f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10591fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105920190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105920740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105920cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1059212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105921850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105921e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1059223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105922960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105922f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1059234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105923a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105924020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1059245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105914580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105924d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1059251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105925610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105925bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105926170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105926720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105926cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105927280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105927830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105927de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105928390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105928940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105928ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1059294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105929a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10592a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10592a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10592aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10592af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10592b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10592b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10592be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10592c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10592c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10592cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10592d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10592d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10592dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10592e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10592e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10592eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10592f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10592f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10592fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10592ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105930400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105930900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105930e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105931300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105931800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105931d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105932200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105932700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105932c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105933100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105933600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105933b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105934000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105934500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105934a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105934f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105935400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105935900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105935e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105936300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105936800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105936d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105937200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105937700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105937c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105938100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105938600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105938b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105939000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105939500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105939a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105939f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10593a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10593a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10593ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10593b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10593b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10593bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10593c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10593c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10593cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10593d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10593d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10593db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10593e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10593e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10593ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10593ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10593f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10593f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10593fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105940300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105940800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105940d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105941200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105941700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105941c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105942100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105942600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105942b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105943000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1059435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105943b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105944110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1059446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105944cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1059452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1059458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1059460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105946580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105946840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105946e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105947460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105947c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1059480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105948590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105948a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1059491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105949730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105949c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10594a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10594a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10594ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10594b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10594b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10594bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10594c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10594c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10594cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10594d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10594d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10594dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10594e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10594e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10594ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10594f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10594f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10594fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105950170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1059506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105950c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105951160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1059516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105951c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105952150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1059526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105952bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105953140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105953690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105953be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105954130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105954680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105954bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105955120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105955670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105955bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105956110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105956660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105956bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105957100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105957650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105957ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1059580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105958640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105958b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1059590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105959630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105959b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10595a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10595a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10595ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10595b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10595b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10595bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10595c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10595c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10595c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10595cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10595d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10595d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10595dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10595e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10595e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10595e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10595ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10595f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139c04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139c044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139c04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139c04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139c05890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139c05fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139c066d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139c06df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139c070b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139c07370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139c077e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139c07c50 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.116.694 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.697 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105944f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1059453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105945850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105945cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105946130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1059465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105946a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105946e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1059472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105947760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105947bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1059481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105948aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105949220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105949a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10594a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10594a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10594aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10594b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10594bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10594c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10594cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10594d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10594db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10594e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10594e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10594ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10594ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10594f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10594f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10594fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105950100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105950570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105950830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105950ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105951110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105951580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1059519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105951e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1059522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105952740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105952bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105953020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105953490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105953900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105953d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1059541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105954650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105954ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105954f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1059553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105955810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105955c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1059560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105956560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1059569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105956e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1059572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105957720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105957b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105958000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105958470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1059588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105958d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1059591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105959630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105959aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105959f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10595a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10595a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10595ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10595b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10595b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10595b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10595be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10595c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10595c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10595cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10595cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10595d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10595d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10595dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10595e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10595e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10595ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10595eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105943f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105929e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10592a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10592a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10592a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10592ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10592b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10592b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10592bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10592c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10592c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10592c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10592cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10592d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10592d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10592dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10592df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10592e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10592e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10592ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10592f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10592f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10592f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10592fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1059302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105930720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105930b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105931000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105931470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1059318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105931d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1059321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105932630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105932aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105932f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105933380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1059337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105933c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1059340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105934540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1059349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105934e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105935290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105935700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105935b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105935fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105936450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1059368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105936d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1059371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105937610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105937a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105937ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105938360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1059387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105938c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1059390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105939520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105939990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105939e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10593a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10593a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10593ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10593afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10593b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10593b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10593bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10593c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10593c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10593ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10593ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10593d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10593d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10593dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10593e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10593e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10593e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10593ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10593f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10593f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10593fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10593ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105940410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105940880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105940cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105941160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1059415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105941a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105941eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105942320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105942790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105942c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105943070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1059434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105943950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105929890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105904a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105904e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1059052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105905750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105905bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105906030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1059064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105906910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105906d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1059071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105907660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105907ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105907f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1059083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105908820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105908c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105909100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105909570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1059099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105909e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10590a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10590a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10590aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10590b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10590b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10590b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10590bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10590c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10590c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10590cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10590d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10590d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10590db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10590df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10590e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10590e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10590ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10590f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10590f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10590fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10590fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105910300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105910770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105910be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105911050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1059114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105911930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105911da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105912210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105912680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105912af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105912f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1059133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105913840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105913cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105914120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105914590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105914a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105914e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1059152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105915750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105915bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105916030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1059164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105916910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105916d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1059171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105917660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105917ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105917f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1059183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105918820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105918c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105919100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105919570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1059199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105919e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10591a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10591a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10591aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10591b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10591b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10591b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10591bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10591c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10591c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10591cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10591cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10591d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10591d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10591dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10591e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10591e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10591e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10591ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10591f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10591f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10591fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10591fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105920460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1059208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105920fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1059216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105921da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105922490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105922900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105922d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1059231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105923650 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139c056f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139c05b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139c05fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139c06440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139c068b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139c06d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139c07190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139c07600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139c07a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139c04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139c04340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139c04920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139c04d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139c08f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139c09750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139c09e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139c0a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139c0acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139c0b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139c0bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139c0c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139c0c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139c0d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139c0d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139c0dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139c0e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139c0e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139c0ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139c0eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139c0f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139c0f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139c0fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139c101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139c10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139c10900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139c10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139c112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139c117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139c11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139c12150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139c12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139c12af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139c12fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139c13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139c13960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139c13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139c14240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139c146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139c14b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139c14f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139c15400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139c15870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139c15ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139c16150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139c167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139c16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139c17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139c173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139c17830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139c17ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139c181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139c18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139c18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139c19120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139c19630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139c19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139c1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139c1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139c1aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139c1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139c1b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139c1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139c1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139c1c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139c1c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139c1cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139c1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139c1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139c1e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139c1e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139c1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139c1f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139c1f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139c1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139c202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139c208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139c20e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139c21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139c219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139c21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139c22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139c22b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139c230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139c236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139c23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139c24220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139c247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139c24da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139c25360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139c25920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139c25ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139c264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139c26a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139c27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139c275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139c27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139c28160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139c28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139c28ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139c292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139c29860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139c29e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139c2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139c2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139c2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139c2b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139c2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139c2bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139c2c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139c2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139c2cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139c2d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139c2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139c2dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139c2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139c2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139c2eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139c2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139c2f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139c2faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139c2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139c304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139c309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139c30ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139c313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139c31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139c31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139c32320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139c32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139c32d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139c33250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139c33760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139c33c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139c34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139c34690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139c34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139c350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139c355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139c35ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139c35fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139c364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139c369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139c36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139c37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139c37910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139c37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139c38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139c38840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139c38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139c39260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139c39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139c39c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139c3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139c3a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139c3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139c3b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139c3b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139c3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139c3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139c3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139c3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139c3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139c3d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139c3d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139c3de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139c3e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139c3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139c3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139c3f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139c3f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139c3fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139c401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139c406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139c40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139c410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139c41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139c41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139c42020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139c42530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139c42a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139c42f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139c43460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139c43970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139c43e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139c44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139c449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139c44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139c45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139c45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139c46160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139c46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139c46f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139c47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139c476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139c47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139c482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139c48ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139c48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139c49410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139c498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139c4a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139c4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139c4ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139c4b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139c4b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139c4baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139c4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139c4c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139c4cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139c4d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139c4d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139c4dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139c4e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139c4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139c4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139c4f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139c4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139c4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139c50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139c50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139c50aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139c50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139c51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139c51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139c51fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139c52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139c52a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139c52fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139c53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139c53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139c53fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139c54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139c54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139c54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139c55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139c55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139c55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139c564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139c56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139c56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139c574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139c57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139c57f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139c584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139c58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139c58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139c594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139c59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139c59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139c5a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139c5aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139c5af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139c5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139c5b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139c5bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139c5c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139c5c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139c5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139c5d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139c5d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139c5dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139c5e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139c5e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139c5ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139c5eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139c5f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139c5f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139c5fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139c60160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139c60600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139c60aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139c60f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139c61490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139c61bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139c622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139c629f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139c63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139c633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139c63bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139c63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139c64490 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.754s
user	0m0.272s
sys	0m0.286s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4341 (382bc7f2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158f0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158f0d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158f0df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158f0e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158f0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158f0f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158f0f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158f0fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158f10130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158f10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158f10b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158f11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158f11b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158f12300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158f12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158f13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158f13950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158f14070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158f14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158f14f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158f15680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158f15da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158f164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158f16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158f17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158f17740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158f17d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158f189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158f18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158f191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158f19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158f19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158f1a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158f1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158f1a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158f1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158f1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158f1b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158f1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158f1c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158f1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158f1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158f1ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158f1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158f1d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158f1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158f1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158f1eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158f1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158f1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158f1fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158f20390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158f209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158f20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158f217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158f21c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158f220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158f223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158f229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158f231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158f23460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158f23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158f23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158f24240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158f246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158f24b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158f25020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158f254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158f25960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158f25e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158f262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158f26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158f26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158f27130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158f27680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158f27bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158f28120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158f28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158f28bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158f29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158f29660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158f29bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158f2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158f2a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158f2aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158f2b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158f2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158f2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158f2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158f2c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158f2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158f2d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158f2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158f2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158f2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158f2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158f2eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158f1e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158f2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158f2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158f2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158f30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158f30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158f30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158f31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158f31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158f31cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158f32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158f32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158f32ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158f331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158f33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158f33c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158f34130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158f345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158f34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158f34f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158f353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158f35850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158f35cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158f36190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158f36630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158f36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158f36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158f37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158f378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158f37d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158f381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158f38690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158f38b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158f38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158f39470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158f39910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158f39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158f3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158f3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158f3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158f3b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158f3b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158f3b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158f3be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158f3c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158f3cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158f3d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158f3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158f3d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158f3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158f3e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158f3e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158f3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158f3f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158f3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158f3fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158f3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158f40370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158f40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158f40cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158f41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158f415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158f41a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158f41f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158f423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158f42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158f42d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158f431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158f43650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158f43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158f43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158f44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158f448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158f44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158f45210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158f456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158f45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158f45ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158f46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158f46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158f46dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158f47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158f47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158f47bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158f48050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158f484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158f48990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158f48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158f492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158f49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158f49c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158f4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158f4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158f4a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158f4ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158f4b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158f4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158f4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158f4c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158f4c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158f4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158f4d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158f4d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158f4e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158f4e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158f4e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158f4ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158f4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158f4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158f500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158f50560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158f50a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158f511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158f51700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158f51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158f521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158f526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158f52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158f53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158f536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158f53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158f54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158f546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158f54c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158f55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158f556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158f55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158f56160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158f566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158f56c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158f57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158f576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158f57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158f58140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158f58690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158f58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158f59130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158f59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158f59bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158f5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158f5a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158f5abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158f5b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158f5b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158f5bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158f5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158f5c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158f5cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158f5d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158f5d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158f5db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158f5e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158f5e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158f5eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158f5f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158f5f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158f5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158f600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158f60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158f60b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158f610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158f61600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158f61b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158f620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158f625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158f62b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158f63090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158f635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158f63b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158f63fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158f64470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158f64910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158f64db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158f65250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158f656f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158f65b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158f66030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158f664d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158f66970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158f66e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158f672b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158f67750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158f67bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158f68090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158f685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158f68d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158f69420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158f69b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158f6a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158f6a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158f6ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158f6afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158f6b5e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.659 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158e08cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158e09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158e095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158e09a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158e09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158e0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158e0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158e0abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158e0b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158e0b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158e0b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158e0c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158e0cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158e0d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158e0db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158e0e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158e0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158e0f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158e0f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158e0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158e106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158e10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158e11500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158e11c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158e12340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158e12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158e128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158e12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158e131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158e13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158e13b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158e14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158e14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158e14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158e14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158e15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158e15590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158e15a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158e15f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158e16490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158e16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158e16e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158e17390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158e17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158e17d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158e18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158e18670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158e18ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158e18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158e193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158e19830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158e19ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158e1a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158e1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158e1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158e1b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158e1b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158e1b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158e1bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158e1c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158e1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158e1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158e1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158e1d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158e1de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158e1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158e1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158e1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158e1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158e1f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158e1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158e1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158e20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158e20890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158e20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158e21330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158e21880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158e21dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158e22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158e22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158e22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158e23310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158e23860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158e23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158e24300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158e24850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158e24da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158e252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158e25840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158e25d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158e262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158e26830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158e26d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158e272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158e27820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158e27d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158e282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158e28810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158e28d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158e292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158e29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158e29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158e2a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158e2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158e2ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158e2b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158e2b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158e2bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158e2c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158e2c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158e2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158e2d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158e2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158e2dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158e2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158e2e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158e2ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158e2eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158e2f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158e2f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158e2fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158e30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158e30600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158e30aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158e30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158e313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158e31880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158e31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158e321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158e32660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158e32b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158e32fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158e33440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158e338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158e33d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158e34220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158e346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158e34b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158e35000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158e354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158e35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158e35de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158e36280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158e36720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158e36bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158e37060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158e37500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158e379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158e37e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158e382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158e38780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158e38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158e390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158e39560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158e39a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158e39ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158e3a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158e3a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158e3ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158e3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158e3b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158e3ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158e3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158e3c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158e3c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158e3cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158e3d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158e3d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158e3dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158e3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158e3e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158e3e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158e3ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158e3f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158e3f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158e3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158e3ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158e40460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158e40900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158e40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158e41240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158e416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158e41b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158e42020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158e424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158e42960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158e42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158e432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158e43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158e43be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158e44080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158e44520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158e449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158e44f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158e45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158e459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158e45f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158e461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158e467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158e46de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158e473f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158e47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158e48080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158e48340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158e48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158e48f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158e49750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158e49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158e4a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158e4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158e4ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158e4b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158e4b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158e4bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158e4c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158e4c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158e4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158e4d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158e4d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158e4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158e4e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158e4e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158e4eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158e4f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158e4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158e4fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158e501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158e50730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158e50c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158e511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158e51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158e51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158e521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158e52710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158e52c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158e531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158e53700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158e53c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158e541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158e546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158e54c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158e55190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158e556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158e55c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158e56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158e566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158e56c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158e57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158e576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158e57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158e58160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158e586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158e58c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158e59150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158e596a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158e59bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158e5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158e5a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158e5abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158e5b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158e5b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158e5bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158e5c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158e5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158e5cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158e5d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158e5d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158e5db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158e5dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158e5e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158e5e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158e5ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158e5f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158e5f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158e5fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158e60000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158e604a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158e60940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158e60de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158e61280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158e61720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158e61bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158e62110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158e62830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158e62f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158e63670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158e63d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158e64050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158e64840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158e64b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158e65110 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158f0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158f0df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158f0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158f0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158f0f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158f0f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158f0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158f100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158f0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158f27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158f27e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158f28270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158f28b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158f292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158f29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158f2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158f2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158f2af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158f2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158f2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158f2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158f2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158f2d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158f2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158f2e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158f2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158f2eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158f2f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158f2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158f2f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158f2fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158f301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158f30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158f308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158f30d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158f311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158f31640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158f31ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158f31f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158f32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158f32800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158f32c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158f330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158f33550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158f339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158f33e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158f342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158f34710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158f34b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158f34ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158f35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158f358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158f35d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158f361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158f36620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158f36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158f36f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158f37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158f377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158f37c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158f380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158f38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158f389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158f38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158f39280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158f396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158f39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158f39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158f3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158f3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158f3ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158f3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158f3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158f3ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158f3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158f3c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158f3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158f3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158f3d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158f3d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158f3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158f3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158f3e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158f3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158f3eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158f3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158f3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158f3f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158f3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158f40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158f405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158f40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158f40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158f41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158f417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158f41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158f42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158f424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158f42960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158f42dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158f43240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158f436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158f43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158f43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158f44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158f44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158f44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158f45150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158f455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158f45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158f45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158f46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158f46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158f47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158f474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158f47940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158f47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158f48220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158f48690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158f48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158f48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158f493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158f49850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158f49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158f4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158f4a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158f4aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158f4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158f4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158f4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158f4bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158f4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158f4c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158f4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158f4cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158f4d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158f4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158f4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158f4df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158f4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158f4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158f4eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158f4f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158f4f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158f4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158f4fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158f502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158f50740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158f50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158f51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158f51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158f51900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158f51d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158f521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158f52650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158f52ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158f52f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158f533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158f53810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158f53c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158f540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158f54560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158f549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158f54e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158f552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158f55720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158f55b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158f56000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158f56470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158f568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158f56d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158f571c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158f57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158f57aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158f57f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158f58380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158f587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158f58c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158f590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158f59540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158f599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158f59e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158f5a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158f5a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158f5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158f5afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158f5b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158f5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158f5bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158f5c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158f5c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158f5ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158f5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158f5d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158f5d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158f5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158f5e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158f5e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158f5e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158f5ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158f5f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158f5f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158f5fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158f5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158f60430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158f608a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158f60d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158f61180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158f615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158f61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158f621e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158f62650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158f62ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158f62f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158f633a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158f63810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158f63c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158f640f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158f64560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158f649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158f64e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158f652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158f65720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158f65b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158f66000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158f66470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158f668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158f66d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158f671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158f67630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158f67aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158f67f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158f68380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158f687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158f68c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158f690d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158f69540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158f699b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158f69e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158f6a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158f6a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158f6ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158f6afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158f6b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158f1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158f1ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158f1afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158f1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158f1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158f1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158f1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158f1c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158f1ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158f1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158f1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158f1d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158f1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158f1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158f1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158f1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158f1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158f1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158f1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158f1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158f1ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158f20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158f20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158f20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158f21160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158f215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158f21a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158f21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158f22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158f22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158f22c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158f23070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158f234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158f23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158f23dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158f24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158f246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158f24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158f25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158f258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158f25fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158f266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158f26b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158f26fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158f27420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158f18ef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.933s
user	0m0.242s
sys	0m0.148s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
