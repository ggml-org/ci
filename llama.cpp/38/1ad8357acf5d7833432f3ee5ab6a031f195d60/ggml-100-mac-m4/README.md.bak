### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.21 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.28 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.46 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  177.68 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.20 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 220.72 sec*proc (28 tests)

Total Test time (real) = 220.73 sec

real	3m40.737s
user	7m33.455s
sys	0m6.459s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.95 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.40 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.34 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.39 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.11 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.58 sec*proc (28 tests)

Total Test time (real) =  51.59 sec

real	0m51.599s
user	1m11.905s
sys	0m5.560s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.073 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.878 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.468 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.018.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.476 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.018.479 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.479 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.018.480 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.018.480 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.018.482 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.018.482 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.018.483 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.018.483 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.018.484 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.018.487 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.018.487 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.018.488 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.018.488 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.018.491 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.018.492 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.018.492 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.022.836 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.183 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.187 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.188 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.188 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.189 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.024.189 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.190 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.024.190 I llama_model_loader: - type  f32:  124 tensors
0.00.024.191 I llama_model_loader: - type  f16:   73 tensors
0.00.028.622 I llm_load_vocab: special tokens cache size = 5
0.00.030.887 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.030.893 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.030.893 I llm_load_print_meta: arch             = bert
0.00.030.899 I llm_load_print_meta: vocab type       = WPM
0.00.030.899 I llm_load_print_meta: n_vocab          = 30522
0.00.030.901 I llm_load_print_meta: n_merges         = 0
0.00.030.902 I llm_load_print_meta: vocab_only       = 0
0.00.030.902 I llm_load_print_meta: n_ctx_train      = 512
0.00.030.902 I llm_load_print_meta: n_embd           = 384
0.00.030.902 I llm_load_print_meta: n_layer          = 12
0.00.030.909 I llm_load_print_meta: n_head           = 12
0.00.030.910 I llm_load_print_meta: n_head_kv        = 12
0.00.030.913 I llm_load_print_meta: n_rot            = 32
0.00.030.913 I llm_load_print_meta: n_swa            = 0
0.00.030.913 I llm_load_print_meta: n_embd_head_k    = 32
0.00.030.913 I llm_load_print_meta: n_embd_head_v    = 32
0.00.030.914 I llm_load_print_meta: n_gqa            = 1
0.00.030.915 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.030.916 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.030.917 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.030.918 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.030.919 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.030.923 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.030.923 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.030.924 I llm_load_print_meta: n_ff             = 1536
0.00.030.938 I llm_load_print_meta: n_expert         = 0
0.00.030.938 I llm_load_print_meta: n_expert_used    = 0
0.00.030.940 I llm_load_print_meta: causal attn      = 0
0.00.030.940 I llm_load_print_meta: pooling type     = 2
0.00.030.940 I llm_load_print_meta: rope type        = 2
0.00.030.940 I llm_load_print_meta: rope scaling     = linear
0.00.030.941 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.030.941 I llm_load_print_meta: freq_scale_train = 1
0.00.030.942 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.030.942 I llm_load_print_meta: rope_finetuned   = unknown
0.00.030.942 I llm_load_print_meta: ssm_d_conv       = 0
0.00.030.943 I llm_load_print_meta: ssm_d_inner      = 0
0.00.030.943 I llm_load_print_meta: ssm_d_state      = 0
0.00.030.943 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.030.943 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.030.966 I llm_load_print_meta: model type       = 33M
0.00.030.966 I llm_load_print_meta: model ftype      = F16
0.00.030.967 I llm_load_print_meta: model params     = 33.21 M
0.00.030.969 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.030.969 I llm_load_print_meta: general.name     = Bge Small
0.00.030.969 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.030.970 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.030.970 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.030.970 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.030.971 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.030.971 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.030.972 I llm_load_print_meta: max token length = 21
0.00.032.933 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.032.937 I llm_load_tensors: offloading output layer to GPU
0.00.032.938 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.032.963 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.032.964 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.033.505 I llama_new_context_with_model: n_seq_max     = 1
0.00.033.507 I llama_new_context_with_model: n_ctx         = 512
0.00.033.507 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.033.507 I llama_new_context_with_model: n_batch       = 2048
0.00.033.507 I llama_new_context_with_model: n_ubatch      = 2048
0.00.033.508 I llama_new_context_with_model: flash_attn    = 0
0.00.033.508 I llama_new_context_with_model: freq_base     = 10000.0
0.00.033.509 I llama_new_context_with_model: freq_scale    = 1
0.00.033.510 I ggml_metal_init: allocating
0.00.033.514 I ggml_metal_init: found device: Apple M4
0.00.033.517 I ggml_metal_init: picking default device: Apple M4
0.00.034.335 I ggml_metal_init: using embedded metal library
0.00.038.374 I ggml_metal_init: GPU name:   Apple M4
0.00.038.376 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.038.377 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.038.377 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.038.378 I ggml_metal_init: simdgroup reduction   = true
0.00.038.378 I ggml_metal_init: simdgroup matrix mul. = true
0.00.038.378 I ggml_metal_init: has bfloat            = true
0.00.038.378 I ggml_metal_init: use bfloat            = true
0.00.038.379 I ggml_metal_init: hasUnifiedMemory      = true
0.00.038.380 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.050.455 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.025 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.027 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.049 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.051.784 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.051.786 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.051.786 I llama_new_context_with_model: graph nodes  = 429
0.00.051.786 I llama_new_context_with_model: graph splits = 2
0.00.051.787 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.051.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.058.112 I 
0.00.058.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.058.782 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.063.570 I llama_perf_context_print:        load time =      43.23 ms
0.00.063.571 I llama_perf_context_print: prompt eval time =       4.64 ms /     9 tokens (    0.52 ms per token,  1939.24 tokens per second)
0.00.063.572 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.063.572 I llama_perf_context_print:       total time =       5.46 ms /    10 tokens
0.00.063.706 I ggml_metal_free: deallocating

real	0m0.240s
user	0m0.047s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.971 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.937 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.942 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.942 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.943 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.943 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.943 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.944 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.944 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.945 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.946 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.946 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.948 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.949 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.010.949 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.010.949 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.010.950 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.950 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.010.950 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.960 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.961 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.962 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.962 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.962 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.963 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.963 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.963 I llama_model_loader: - type  f32:  124 tensors
0.00.013.964 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.479 I llm_load_vocab: special tokens cache size = 5
0.00.017.771 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.774 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.774 I llm_load_print_meta: arch             = bert
0.00.017.775 I llm_load_print_meta: vocab type       = WPM
0.00.017.775 I llm_load_print_meta: n_vocab          = 30522
0.00.017.775 I llm_load_print_meta: n_merges         = 0
0.00.017.775 I llm_load_print_meta: vocab_only       = 0
0.00.017.775 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.776 I llm_load_print_meta: n_embd           = 384
0.00.017.776 I llm_load_print_meta: n_layer          = 12
0.00.017.778 I llm_load_print_meta: n_head           = 12
0.00.017.779 I llm_load_print_meta: n_head_kv        = 12
0.00.017.781 I llm_load_print_meta: n_rot            = 32
0.00.017.781 I llm_load_print_meta: n_swa            = 0
0.00.017.782 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.782 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.782 I llm_load_print_meta: n_gqa            = 1
0.00.017.783 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.784 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.784 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.785 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.785 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.785 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.785 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.786 I llm_load_print_meta: n_ff             = 1536
0.00.017.792 I llm_load_print_meta: n_expert         = 0
0.00.017.793 I llm_load_print_meta: n_expert_used    = 0
0.00.017.793 I llm_load_print_meta: causal attn      = 0
0.00.017.793 I llm_load_print_meta: pooling type     = 2
0.00.017.793 I llm_load_print_meta: rope type        = 2
0.00.017.793 I llm_load_print_meta: rope scaling     = linear
0.00.017.794 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.794 I llm_load_print_meta: freq_scale_train = 1
0.00.017.794 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.794 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.794 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.796 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.796 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.796 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.796 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.801 I llm_load_print_meta: model type       = 33M
0.00.017.802 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.802 I llm_load_print_meta: model params     = 33.21 M
0.00.017.802 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.803 I llm_load_print_meta: general.name     = Bge Small
0.00.017.803 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.803 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.803 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.804 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.804 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.804 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.804 I llm_load_print_meta: max token length = 21
0.00.019.080 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.081 I llm_load_tensors: offloading output layer to GPU
0.00.019.081 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.089 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.090 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.434 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.435 I llama_new_context_with_model: n_ctx         = 512
0.00.019.435 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.435 I llama_new_context_with_model: n_batch       = 2048
0.00.019.435 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.436 I llama_new_context_with_model: flash_attn    = 0
0.00.019.436 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.436 I llama_new_context_with_model: freq_scale    = 1
0.00.019.437 I ggml_metal_init: allocating
0.00.019.440 I ggml_metal_init: found device: Apple M4
0.00.019.442 I ggml_metal_init: picking default device: Apple M4
0.00.020.037 I ggml_metal_init: using embedded metal library
0.00.022.589 I ggml_metal_init: GPU name:   Apple M4
0.00.022.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.591 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.592 I ggml_metal_init: simdgroup reduction   = true
0.00.022.592 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.592 I ggml_metal_init: has bfloat            = true
0.00.022.592 I ggml_metal_init: use bfloat            = true
0.00.022.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.593 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.810 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.299 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.300 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.314 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.936 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.937 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.937 I llama_new_context_with_model: graph nodes  = 429
0.00.033.938 I llama_new_context_with_model: graph splits = 2
0.00.033.939 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.939 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.218 I 
0.00.039.243 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.794 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.261 I llama_perf_context_print:        load time =      30.24 ms
0.00.044.262 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2077.08 tokens per second)
0.00.044.263 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.263 I llama_perf_context_print:       total time =       5.04 ms /    10 tokens
0.00.044.470 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.235 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.550 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.881 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.887 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.889 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.891 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.892 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.893 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.894 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.895 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.896 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.896 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.897 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.900 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.901 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.902 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.902 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.903 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.461 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.256 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.257 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.257 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.258 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.258 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.258 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.259 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.259 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.259 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.260 I llama_model_loader: - type  f32:   40 tensors
0.00.048.261 I llama_model_loader: - type  f16:   30 tensors
0.00.066.522 W llm_load_vocab: empty token at index 5
0.00.071.358 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.713 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.743 I llm_load_vocab: special tokens cache size = 5
0.00.332.168 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.332.175 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.332.175 I llm_load_print_meta: arch             = jina-bert-v2
0.00.332.175 I llm_load_print_meta: vocab type       = BPE
0.00.332.175 I llm_load_print_meta: n_vocab          = 61056
0.00.332.176 I llm_load_print_meta: n_merges         = 39382
0.00.332.176 I llm_load_print_meta: vocab_only       = 0
0.00.332.176 I llm_load_print_meta: n_ctx_train      = 8192
0.00.332.179 I llm_load_print_meta: n_embd           = 384
0.00.332.179 I llm_load_print_meta: n_layer          = 4
0.00.332.186 I llm_load_print_meta: n_head           = 12
0.00.332.187 I llm_load_print_meta: n_head_kv        = 12
0.00.332.187 I llm_load_print_meta: n_rot            = 32
0.00.332.187 I llm_load_print_meta: n_swa            = 0
0.00.332.187 I llm_load_print_meta: n_embd_head_k    = 32
0.00.332.187 I llm_load_print_meta: n_embd_head_v    = 32
0.00.332.188 I llm_load_print_meta: n_gqa            = 1
0.00.332.191 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.332.192 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.332.193 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.332.194 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.332.194 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.332.194 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.332.195 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.332.195 I llm_load_print_meta: n_ff             = 1536
0.00.332.220 I llm_load_print_meta: n_expert         = 0
0.00.332.221 I llm_load_print_meta: n_expert_used    = 0
0.00.332.221 I llm_load_print_meta: causal attn      = 0
0.00.332.222 I llm_load_print_meta: pooling type     = -1
0.00.332.222 I llm_load_print_meta: rope type        = -1
0.00.332.222 I llm_load_print_meta: rope scaling     = linear
0.00.332.223 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.332.223 I llm_load_print_meta: freq_scale_train = 1
0.00.332.223 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.332.223 I llm_load_print_meta: rope_finetuned   = unknown
0.00.332.224 I llm_load_print_meta: ssm_d_conv       = 0
0.00.332.224 I llm_load_print_meta: ssm_d_inner      = 0
0.00.332.224 I llm_load_print_meta: ssm_d_state      = 0
0.00.332.224 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.332.224 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.332.225 I llm_load_print_meta: model type       = 33M
0.00.332.226 I llm_load_print_meta: model ftype      = F16
0.00.332.226 I llm_load_print_meta: model params     = 32.90 M
0.00.332.227 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.332.227 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.332.227 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.332.229 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.332.229 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.332.230 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.332.230 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.332.230 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.332.230 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.332.230 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.332.231 I llm_load_print_meta: max token length = 45
0.00.333.514 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.333.514 I llm_load_tensors: offloading output layer to GPU
0.00.333.514 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.333.540 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.541 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.334.439 I llama_new_context_with_model: n_seq_max     = 1
0.00.334.440 I llama_new_context_with_model: n_ctx         = 8192
0.00.334.440 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.334.440 I llama_new_context_with_model: n_batch       = 2048
0.00.334.441 I llama_new_context_with_model: n_ubatch      = 2048
0.00.334.441 I llama_new_context_with_model: flash_attn    = 0
0.00.334.441 I llama_new_context_with_model: freq_base     = 10000.0
0.00.334.442 I llama_new_context_with_model: freq_scale    = 1
0.00.334.442 I ggml_metal_init: allocating
0.00.334.446 I ggml_metal_init: found device: Apple M4
0.00.334.448 I ggml_metal_init: picking default device: Apple M4
0.00.335.230 I ggml_metal_init: using embedded metal library
0.00.338.147 I ggml_metal_init: GPU name:   Apple M4
0.00.338.149 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.338.149 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.338.150 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.338.150 I ggml_metal_init: simdgroup reduction   = true
0.00.338.150 I ggml_metal_init: simdgroup matrix mul. = true
0.00.338.150 I ggml_metal_init: has bfloat            = true
0.00.338.151 I ggml_metal_init: use bfloat            = true
0.00.338.151 I ggml_metal_init: hasUnifiedMemory      = true
0.00.338.152 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.748 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.350.417 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.350.420 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.350.443 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.351.041 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.351.042 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.351.042 I llama_new_context_with_model: graph nodes  = 154
0.00.351.042 I llama_new_context_with_model: graph splits = 2
0.00.351.043 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.351.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.364.187 I 
0.00.364.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.364.447 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.364.448 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.364.453 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.364.453 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.364.461 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.364.461 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.364.973 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.368.761 I llama_perf_context_print:        load time =     340.63 ms
0.00.368.762 I llama_perf_context_print: prompt eval time =       3.78 ms /    62 tokens (    0.06 ms per token, 16410.80 tokens per second)
0.00.368.763 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.368.766 I llama_perf_context_print:       total time =       4.57 ms /    63 tokens
0.00.369.023 I ggml_metal_free: deallocating

real	0m1.109s
user	0m0.339s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.108 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.208 I main: llama backend init
0.00.000.214 I main: load the model and apply lora adapter, if any
0.00.040.203 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.050.835 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.050.850 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.050.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.050.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.050.854 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.050.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.050.855 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.050.856 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.050.857 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.050.857 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.050.858 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.050.858 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.050.859 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.050.859 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.050.862 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.050.863 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.050.863 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.057.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.060.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.067.432 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.067.446 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.067.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.067.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.067.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.067.449 I llama_model_loader: - type  f32:  194 tensors
0.00.067.450 I llama_model_loader: - type  f16:   98 tensors
0.00.102.607 I llm_load_vocab: special tokens cache size = 25
0.00.110.081 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.110.085 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.110.085 I llm_load_print_meta: arch             = gptneox
0.00.110.085 I llm_load_print_meta: vocab type       = BPE
0.00.110.085 I llm_load_print_meta: n_vocab          = 50304
0.00.110.086 I llm_load_print_meta: n_merges         = 50009
0.00.110.086 I llm_load_print_meta: vocab_only       = 0
0.00.110.086 I llm_load_print_meta: n_ctx_train      = 2048
0.00.110.086 I llm_load_print_meta: n_embd           = 2048
0.00.110.088 I llm_load_print_meta: n_layer          = 24
0.00.110.092 I llm_load_print_meta: n_head           = 16
0.00.110.093 I llm_load_print_meta: n_head_kv        = 16
0.00.110.093 I llm_load_print_meta: n_rot            = 32
0.00.110.094 I llm_load_print_meta: n_swa            = 0
0.00.110.094 I llm_load_print_meta: n_embd_head_k    = 128
0.00.110.094 I llm_load_print_meta: n_embd_head_v    = 128
0.00.110.096 I llm_load_print_meta: n_gqa            = 1
0.00.110.096 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.110.097 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.110.098 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.110.098 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.110.098 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.110.099 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.110.099 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.110.100 I llm_load_print_meta: n_ff             = 8192
0.00.110.112 I llm_load_print_meta: n_expert         = 0
0.00.110.112 I llm_load_print_meta: n_expert_used    = 0
0.00.110.112 I llm_load_print_meta: causal attn      = 1
0.00.110.112 I llm_load_print_meta: pooling type     = 0
0.00.110.112 I llm_load_print_meta: rope type        = 2
0.00.110.113 I llm_load_print_meta: rope scaling     = linear
0.00.110.113 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.110.113 I llm_load_print_meta: freq_scale_train = 1
0.00.110.114 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.110.114 I llm_load_print_meta: rope_finetuned   = unknown
0.00.110.114 I llm_load_print_meta: ssm_d_conv       = 0
0.00.110.114 I llm_load_print_meta: ssm_d_inner      = 0
0.00.110.114 I llm_load_print_meta: ssm_d_state      = 0
0.00.110.114 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.110.115 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.110.115 I llm_load_print_meta: model type       = 1.4B
0.00.110.116 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.110.116 I llm_load_print_meta: model params     = 1.41 B
0.00.110.117 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.110.117 I llm_load_print_meta: general.name     = 1.4B
0.00.110.117 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.110.118 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.110.118 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.110.118 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.110.118 I llm_load_print_meta: LF token         = 128 ''
0.00.110.119 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.110.119 I llm_load_print_meta: max token length = 1024
0.00.112.292 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.112.292 I llm_load_tensors: offloading output layer to GPU
0.00.112.293 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.112.312 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.112.314 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.113.383 I llama_new_context_with_model: n_seq_max     = 1
0.00.113.384 I llama_new_context_with_model: n_ctx         = 2048
0.00.113.385 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.113.385 I llama_new_context_with_model: n_batch       = 2048
0.00.113.385 I llama_new_context_with_model: n_ubatch      = 512
0.00.113.385 I llama_new_context_with_model: flash_attn    = 0
0.00.113.386 I llama_new_context_with_model: freq_base     = 10000.0
0.00.113.386 I llama_new_context_with_model: freq_scale    = 1
0.00.113.387 I ggml_metal_init: allocating
0.00.113.397 I ggml_metal_init: found device: Apple M4
0.00.113.400 I ggml_metal_init: picking default device: Apple M4
0.00.114.201 I ggml_metal_init: using embedded metal library
0.00.148.647 I ggml_metal_init: GPU name:   Apple M4
0.00.148.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.148.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.148.654 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.148.654 I ggml_metal_init: simdgroup reduction   = true
0.00.148.654 I ggml_metal_init: simdgroup matrix mul. = true
0.00.148.654 I ggml_metal_init: has bfloat            = true
0.00.148.655 I ggml_metal_init: use bfloat            = true
0.00.148.655 I ggml_metal_init: hasUnifiedMemory      = true
0.00.148.657 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.233.112 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.255.875 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.255.885 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.255.943 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.256.963 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.256.966 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.256.967 I llama_new_context_with_model: graph nodes  = 967
0.00.256.967 I llama_new_context_with_model: graph splits = 2
0.00.256.971 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.257.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.257.113 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.348.716 I main: llama threadpool init, n_threads = 4
0.00.348.763 I 
0.00.348.782 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.348.782 I 
0.00.348.860 I sampler seed: 1234
0.00.348.865 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.348.888 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.348.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.348.889 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.274.491 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.02.274.492 I llama_perf_context_print:        load time =     308.51 ms
0.02.274.493 I llama_perf_context_print: prompt eval time =      43.95 ms /     7 tokens (    6.28 ms per token,   159.26 tokens per second)
0.02.274.493 I llama_perf_context_print:        eval time =    1878.68 ms /    63 runs   (   29.82 ms per token,    33.53 tokens per second)
0.02.274.494 I llama_perf_context_print:       total time =    1925.78 ms /    70 tokens
0.02.274.698 I ggml_metal_free: deallocating

real	0m2.714s
user	0m0.147s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.855 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.489 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.167 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.183 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.184 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.185 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.187 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.187 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.188 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.191 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.167 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.386 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.387 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.387 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.388 I llama_model_loader: - type  f32:  194 tensors
0.00.055.389 I llama_model_loader: - type  f16:   98 tensors
0.00.085.024 I llm_load_vocab: special tokens cache size = 25
0.00.091.749 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.752 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.752 I llm_load_print_meta: arch             = gptneox
0.00.091.753 I llm_load_print_meta: vocab type       = BPE
0.00.091.753 I llm_load_print_meta: n_vocab          = 50304
0.00.091.753 I llm_load_print_meta: n_merges         = 50009
0.00.091.753 I llm_load_print_meta: vocab_only       = 0
0.00.091.753 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.753 I llm_load_print_meta: n_embd           = 2048
0.00.091.754 I llm_load_print_meta: n_layer          = 24
0.00.091.756 I llm_load_print_meta: n_head           = 16
0.00.091.757 I llm_load_print_meta: n_head_kv        = 16
0.00.091.757 I llm_load_print_meta: n_rot            = 32
0.00.091.758 I llm_load_print_meta: n_swa            = 0
0.00.091.758 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.758 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.759 I llm_load_print_meta: n_gqa            = 1
0.00.091.759 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.760 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.760 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.761 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.761 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.761 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.761 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.762 I llm_load_print_meta: n_ff             = 8192
0.00.091.769 I llm_load_print_meta: n_expert         = 0
0.00.091.769 I llm_load_print_meta: n_expert_used    = 0
0.00.091.769 I llm_load_print_meta: causal attn      = 1
0.00.091.769 I llm_load_print_meta: pooling type     = 0
0.00.091.769 I llm_load_print_meta: rope type        = 2
0.00.091.769 I llm_load_print_meta: rope scaling     = linear
0.00.091.770 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.770 I llm_load_print_meta: freq_scale_train = 1
0.00.091.770 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.771 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.773 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.773 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.773 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.773 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.774 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.774 I llm_load_print_meta: model type       = 1.4B
0.00.091.775 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.775 I llm_load_print_meta: model params     = 1.41 B
0.00.091.775 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.776 I llm_load_print_meta: general.name     = 1.4B
0.00.091.776 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.776 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.776 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.776 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.777 I llm_load_print_meta: LF token         = 128 ''
0.00.091.777 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.777 I llm_load_print_meta: max token length = 1024
0.00.093.794 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.794 I llm_load_tensors: offloading output layer to GPU
0.00.093.794 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.800 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.801 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.800 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.801 I llama_new_context_with_model: n_ctx         = 128
0.00.094.801 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.801 I llama_new_context_with_model: n_batch       = 128
0.00.094.801 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.802 I llama_new_context_with_model: flash_attn    = 0
0.00.094.802 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.802 I llama_new_context_with_model: freq_scale    = 1
0.00.094.803 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.803 I ggml_metal_init: allocating
0.00.094.812 I ggml_metal_init: found device: Apple M4
0.00.094.815 I ggml_metal_init: picking default device: Apple M4
0.00.095.460 I ggml_metal_init: using embedded metal library
0.00.098.016 I ggml_metal_init: GPU name:   Apple M4
0.00.098.018 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.019 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.019 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.019 I ggml_metal_init: simdgroup reduction   = true
0.00.098.020 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.020 I ggml_metal_init: has bfloat            = true
0.00.098.020 I ggml_metal_init: use bfloat            = true
0.00.098.020 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.667 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.907 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.910 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.933 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.789 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.790 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.790 I llama_new_context_with_model: graph nodes  = 967
0.00.109.790 I llama_new_context_with_model: graph splits = 2
0.00.109.792 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.792 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.328.132 I 
0.01.328.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.328.222 I perplexity: tokenizing the input ..
0.01.341.064 I perplexity: tokenization took 12.838 ms
0.01.341.069 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.463.231 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.465.195 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.465.226 I llama_perf_context_print:        load time =    1302.63 ms
0.01.465.228 I llama_perf_context_print: prompt eval time =     121.62 ms /   128 tokens (    0.95 ms per token,  1052.46 tokens per second)
0.01.465.229 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.465.230 I llama_perf_context_print:       total time =     137.10 ms /   129 tokens
0.01.466.053 I ggml_metal_free: deallocating

real	0m1.659s
user	0m0.126s
sys	0m0.222s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.600 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.032 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.033.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.049 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.245 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.952 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.955 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.956 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.044.956 I llama_model_loader: - type  f32:  194 tensors
0.00.044.957 I llama_model_loader: - type q8_0:   98 tensors
0.00.078.321 I llm_load_vocab: special tokens cache size = 25
0.00.087.597 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.601 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.601 I llm_load_print_meta: arch             = gptneox
0.00.087.602 I llm_load_print_meta: vocab type       = BPE
0.00.087.602 I llm_load_print_meta: n_vocab          = 50304
0.00.087.602 I llm_load_print_meta: n_merges         = 50009
0.00.087.603 I llm_load_print_meta: vocab_only       = 0
0.00.087.603 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.603 I llm_load_print_meta: n_embd           = 2048
0.00.087.603 I llm_load_print_meta: n_layer          = 24
0.00.087.608 I llm_load_print_meta: n_head           = 16
0.00.087.609 I llm_load_print_meta: n_head_kv        = 16
0.00.087.609 I llm_load_print_meta: n_rot            = 32
0.00.087.610 I llm_load_print_meta: n_swa            = 0
0.00.087.610 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.610 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.611 I llm_load_print_meta: n_gqa            = 1
0.00.087.612 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.613 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.613 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.614 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.614 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.614 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.615 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.615 I llm_load_print_meta: n_ff             = 8192
0.00.087.628 I llm_load_print_meta: n_expert         = 0
0.00.087.628 I llm_load_print_meta: n_expert_used    = 0
0.00.087.628 I llm_load_print_meta: causal attn      = 1
0.00.087.629 I llm_load_print_meta: pooling type     = 0
0.00.087.629 I llm_load_print_meta: rope type        = 2
0.00.087.629 I llm_load_print_meta: rope scaling     = linear
0.00.087.630 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.630 I llm_load_print_meta: freq_scale_train = 1
0.00.087.630 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.631 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.631 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.631 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.633 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.633 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.633 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.634 I llm_load_print_meta: model type       = 1.4B
0.00.087.635 I llm_load_print_meta: model ftype      = Q8_0
0.00.087.635 I llm_load_print_meta: model params     = 1.41 B
0.00.087.636 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.087.636 I llm_load_print_meta: general.name     = 1.4B
0.00.087.636 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.637 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.637 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.637 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.638 I llm_load_print_meta: LF token         = 128 ''
0.00.087.638 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.638 I llm_load_print_meta: max token length = 1024
0.00.090.646 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.646 I llm_load_tensors: offloading output layer to GPU
0.00.090.646 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.658 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.090.660 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.091.983 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.984 I llama_new_context_with_model: n_ctx         = 2048
0.00.091.985 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.091.985 I llama_new_context_with_model: n_batch       = 2048
0.00.091.985 I llama_new_context_with_model: n_ubatch      = 512
0.00.091.985 I llama_new_context_with_model: flash_attn    = 0
0.00.091.986 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.986 I llama_new_context_with_model: freq_scale    = 1
0.00.091.987 I ggml_metal_init: allocating
0.00.091.996 I ggml_metal_init: found device: Apple M4
0.00.091.999 I ggml_metal_init: picking default device: Apple M4
0.00.092.911 I ggml_metal_init: using embedded metal library
0.00.096.380 I ggml_metal_init: GPU name:   Apple M4
0.00.096.382 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.382 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.383 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.383 I ggml_metal_init: simdgroup reduction   = true
0.00.096.383 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.383 I ggml_metal_init: has bfloat            = true
0.00.096.383 I ggml_metal_init: use bfloat            = true
0.00.096.384 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.385 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.327 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.132.340 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.132.351 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.132.393 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.133.440 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.133.442 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.133.443 I llama_new_context_with_model: graph nodes  = 967
0.00.133.443 I llama_new_context_with_model: graph splits = 2
0.00.133.447 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.133.590 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.133.590 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.418.669 I main: llama threadpool init, n_threads = 4
0.01.418.746 I 
0.01.418.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.418.801 I 
0.01.419.283 I sampler seed: 1234
0.01.419.289 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.419.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.419.345 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.419.345 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.517.517 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.02.517.517 I llama_perf_context_print:        load time =    1409.06 ms
0.02.517.518 I llama_perf_context_print: prompt eval time =      49.61 ms /     7 tokens (    7.09 ms per token,   141.11 tokens per second)
0.02.517.519 I llama_perf_context_print:        eval time =    1045.79 ms /    63 runs   (   16.60 ms per token,    60.24 tokens per second)
0.02.517.519 I llama_perf_context_print:       total time =    1098.86 ms /    70 tokens
0.02.517.781 I ggml_metal_free: deallocating

real	0m2.554s
user	0m0.144s
sys	0m0.280s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.332 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.843 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.741 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.748 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.760 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.969 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.469 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.470 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.471 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.471 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.471 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.472 I llama_model_loader: - type  f32:  194 tensors
0.00.038.472 I llama_model_loader: - type q8_0:   98 tensors
0.00.066.030 I llm_load_vocab: special tokens cache size = 25
0.00.072.132 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.135 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.135 I llm_load_print_meta: arch             = gptneox
0.00.072.136 I llm_load_print_meta: vocab type       = BPE
0.00.072.136 I llm_load_print_meta: n_vocab          = 50304
0.00.072.136 I llm_load_print_meta: n_merges         = 50009
0.00.072.136 I llm_load_print_meta: vocab_only       = 0
0.00.072.136 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.136 I llm_load_print_meta: n_embd           = 2048
0.00.072.137 I llm_load_print_meta: n_layer          = 24
0.00.072.140 I llm_load_print_meta: n_head           = 16
0.00.072.141 I llm_load_print_meta: n_head_kv        = 16
0.00.072.143 I llm_load_print_meta: n_rot            = 32
0.00.072.143 I llm_load_print_meta: n_swa            = 0
0.00.072.144 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.144 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.144 I llm_load_print_meta: n_gqa            = 1
0.00.072.145 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.145 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.146 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.146 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.146 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.147 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.147 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.147 I llm_load_print_meta: n_ff             = 8192
0.00.072.160 I llm_load_print_meta: n_expert         = 0
0.00.072.160 I llm_load_print_meta: n_expert_used    = 0
0.00.072.160 I llm_load_print_meta: causal attn      = 1
0.00.072.160 I llm_load_print_meta: pooling type     = 0
0.00.072.160 I llm_load_print_meta: rope type        = 2
0.00.072.161 I llm_load_print_meta: rope scaling     = linear
0.00.072.161 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.162 I llm_load_print_meta: freq_scale_train = 1
0.00.072.162 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.162 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.162 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.162 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.162 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.163 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.163 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.163 I llm_load_print_meta: model type       = 1.4B
0.00.072.164 I llm_load_print_meta: model ftype      = Q8_0
0.00.072.164 I llm_load_print_meta: model params     = 1.41 B
0.00.072.164 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.072.164 I llm_load_print_meta: general.name     = 1.4B
0.00.072.165 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.165 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.165 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.165 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.165 I llm_load_print_meta: LF token         = 128 ''
0.00.072.166 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.166 I llm_load_print_meta: max token length = 1024
0.00.074.527 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.527 I llm_load_tensors: offloading output layer to GPU
0.00.074.527 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.539 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.074.540 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.075.490 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.491 I llama_new_context_with_model: n_ctx         = 128
0.00.075.491 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.075.491 I llama_new_context_with_model: n_batch       = 128
0.00.075.491 I llama_new_context_with_model: n_ubatch      = 128
0.00.075.492 I llama_new_context_with_model: flash_attn    = 0
0.00.075.492 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.492 I llama_new_context_with_model: freq_scale    = 1
0.00.075.493 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.075.493 I ggml_metal_init: allocating
0.00.075.499 I ggml_metal_init: found device: Apple M4
0.00.075.501 I ggml_metal_init: picking default device: Apple M4
0.00.076.105 I ggml_metal_init: using embedded metal library
0.00.078.777 I ggml_metal_init: GPU name:   Apple M4
0.00.078.779 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.779 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.780 I ggml_metal_init: simdgroup reduction   = true
0.00.078.780 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.780 I ggml_metal_init: has bfloat            = true
0.00.078.781 I ggml_metal_init: use bfloat            = true
0.00.078.781 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.782 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.970 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.344 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.089.347 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.089.376 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.386 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.090.387 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.090.388 I llama_new_context_with_model: graph nodes  = 967
0.00.090.388 I llama_new_context_with_model: graph splits = 2
0.00.090.389 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.090.390 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.032.385 I 
0.01.032.448 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.032.460 I perplexity: tokenizing the input ..
0.01.041.353 I perplexity: tokenization took 8.891 ms
0.01.041.357 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.166.587 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.167.752 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.167.768 I llama_perf_context_print:        load time =    1018.53 ms
0.01.167.769 I llama_perf_context_print: prompt eval time =     125.01 ms /   128 tokens (    0.98 ms per token,  1023.94 tokens per second)
0.01.167.770 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.167.770 I llama_perf_context_print:       total time =     135.38 ms /   129 tokens
0.01.168.229 I ggml_metal_free: deallocating

real	0m1.190s
user	0m0.100s
sys	0m0.169s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.019.063 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.240 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.027.244 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.246 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.247 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.247 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.248 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.248 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.249 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.249 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.249 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.250 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.250 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.250 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.252 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.253 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.253 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.036.673 I llama_model_loader: - type  f32:  194 tensors
0.00.036.673 I llama_model_loader: - type q4_0:   97 tensors
0.00.036.674 I llama_model_loader: - type q6_K:    1 tensors
0.00.063.087 I llm_load_vocab: special tokens cache size = 25
0.00.069.612 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.614 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.615 I llm_load_print_meta: arch             = gptneox
0.00.069.615 I llm_load_print_meta: vocab type       = BPE
0.00.069.615 I llm_load_print_meta: n_vocab          = 50304
0.00.069.615 I llm_load_print_meta: n_merges         = 50009
0.00.069.616 I llm_load_print_meta: vocab_only       = 0
0.00.069.616 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.616 I llm_load_print_meta: n_embd           = 2048
0.00.069.616 I llm_load_print_meta: n_layer          = 24
0.00.069.620 I llm_load_print_meta: n_head           = 16
0.00.069.620 I llm_load_print_meta: n_head_kv        = 16
0.00.069.621 I llm_load_print_meta: n_rot            = 32
0.00.069.621 I llm_load_print_meta: n_swa            = 0
0.00.069.621 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.624 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.625 I llm_load_print_meta: n_gqa            = 1
0.00.069.626 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.626 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.627 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.628 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.628 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.628 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.628 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.629 I llm_load_print_meta: n_ff             = 8192
0.00.069.642 I llm_load_print_meta: n_expert         = 0
0.00.069.642 I llm_load_print_meta: n_expert_used    = 0
0.00.069.642 I llm_load_print_meta: causal attn      = 1
0.00.069.643 I llm_load_print_meta: pooling type     = 0
0.00.069.643 I llm_load_print_meta: rope type        = 2
0.00.069.643 I llm_load_print_meta: rope scaling     = linear
0.00.069.644 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.645 I llm_load_print_meta: freq_scale_train = 1
0.00.069.645 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.645 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.645 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.645 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.646 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.646 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.646 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.647 I llm_load_print_meta: model type       = 1.4B
0.00.069.647 I llm_load_print_meta: model ftype      = Q4_0
0.00.069.647 I llm_load_print_meta: model params     = 1.41 B
0.00.069.648 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.069.648 I llm_load_print_meta: general.name     = 1.4B
0.00.069.648 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.648 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.648 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.649 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.649 I llm_load_print_meta: LF token         = 128 ''
0.00.069.649 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.649 I llm_load_print_meta: max token length = 1024
0.00.072.165 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.165 I llm_load_tensors: offloading output layer to GPU
0.00.072.166 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.178 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.072.179 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.073.262 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.263 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.263 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.263 I llama_new_context_with_model: n_batch       = 2048
0.00.073.264 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.264 I llama_new_context_with_model: flash_attn    = 0
0.00.073.264 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.265 I llama_new_context_with_model: freq_scale    = 1
0.00.073.265 I ggml_metal_init: allocating
0.00.073.272 I ggml_metal_init: found device: Apple M4
0.00.073.274 I ggml_metal_init: picking default device: Apple M4
0.00.074.020 I ggml_metal_init: using embedded metal library
0.00.077.019 I ggml_metal_init: GPU name:   Apple M4
0.00.077.021 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.022 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.022 I ggml_metal_init: simdgroup reduction   = true
0.00.077.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.022 I ggml_metal_init: has bfloat            = true
0.00.077.022 I ggml_metal_init: use bfloat            = true
0.00.077.023 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.455 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.116.053 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.061 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.097 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.205 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.207 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.207 I llama_new_context_with_model: graph nodes  = 967
0.00.117.208 I llama_new_context_with_model: graph splits = 2
0.00.117.211 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.117.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.117.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.994 I main: llama threadpool init, n_threads = 4
0.00.749.051 I 
0.00.749.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.090 I 
0.00.749.314 I sampler seed: 1234
0.00.749.321 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.361 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.365 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.365 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.433.728 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.01.433.729 I llama_perf_context_print:        load time =     729.92 ms
0.01.433.729 I llama_perf_context_print: prompt eval time =      42.42 ms /     7 tokens (    6.06 ms per token,   165.01 tokens per second)
0.01.433.730 I llama_perf_context_print:        eval time =     639.00 ms /    63 runs   (   10.14 ms per token,    98.59 tokens per second)
0.01.433.731 I llama_perf_context_print:       total time =     684.74 ms /    70 tokens
0.01.433.974 I ggml_metal_free: deallocating

real	0m1.452s
user	0m0.119s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.256 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.065 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.544 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.548 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.552 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.552 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.553 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.553 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.554 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.554 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.555 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.555 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.558 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.559 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.559 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.560 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.353 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.144 I llama_model_loader: - type  f32:  194 tensors
0.00.024.144 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.145 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.533 I llm_load_vocab: special tokens cache size = 25
0.00.050.413 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.416 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.416 I llm_load_print_meta: arch             = gptneox
0.00.050.417 I llm_load_print_meta: vocab type       = BPE
0.00.050.417 I llm_load_print_meta: n_vocab          = 50304
0.00.050.417 I llm_load_print_meta: n_merges         = 50009
0.00.050.417 I llm_load_print_meta: vocab_only       = 0
0.00.050.417 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.417 I llm_load_print_meta: n_embd           = 2048
0.00.050.418 I llm_load_print_meta: n_layer          = 24
0.00.050.420 I llm_load_print_meta: n_head           = 16
0.00.050.421 I llm_load_print_meta: n_head_kv        = 16
0.00.050.421 I llm_load_print_meta: n_rot            = 32
0.00.050.421 I llm_load_print_meta: n_swa            = 0
0.00.050.421 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.422 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.422 I llm_load_print_meta: n_gqa            = 1
0.00.050.423 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.425 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.425 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.426 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.426 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.426 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.426 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.427 I llm_load_print_meta: n_ff             = 8192
0.00.050.439 I llm_load_print_meta: n_expert         = 0
0.00.050.439 I llm_load_print_meta: n_expert_used    = 0
0.00.050.439 I llm_load_print_meta: causal attn      = 1
0.00.050.439 I llm_load_print_meta: pooling type     = 0
0.00.050.440 I llm_load_print_meta: rope type        = 2
0.00.050.440 I llm_load_print_meta: rope scaling     = linear
0.00.050.440 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.440 I llm_load_print_meta: freq_scale_train = 1
0.00.050.441 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.441 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.441 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.441 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.441 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.443 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.443 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.444 I llm_load_print_meta: model type       = 1.4B
0.00.050.444 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.444 I llm_load_print_meta: model params     = 1.41 B
0.00.050.445 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.445 I llm_load_print_meta: general.name     = 1.4B
0.00.050.445 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.446 I llm_load_print_meta: LF token         = 128 ''
0.00.050.446 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.446 I llm_load_print_meta: max token length = 1024
0.00.052.350 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.351 I llm_load_tensors: offloading output layer to GPU
0.00.052.351 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.361 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.362 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.305 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.306 I llama_new_context_with_model: n_ctx         = 128
0.00.053.306 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.306 I llama_new_context_with_model: n_batch       = 128
0.00.053.306 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.306 I llama_new_context_with_model: flash_attn    = 0
0.00.053.307 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.307 I llama_new_context_with_model: freq_scale    = 1
0.00.053.307 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.308 I ggml_metal_init: allocating
0.00.053.311 I ggml_metal_init: found device: Apple M4
0.00.053.313 I ggml_metal_init: picking default device: Apple M4
0.00.053.866 I ggml_metal_init: using embedded metal library
0.00.056.170 I ggml_metal_init: GPU name:   Apple M4
0.00.056.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.172 I ggml_metal_init: simdgroup reduction   = true
0.00.056.172 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.172 I ggml_metal_init: has bfloat            = true
0.00.056.172 I ggml_metal_init: use bfloat            = true
0.00.056.173 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.173 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.948 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.237 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.240 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.266 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.203 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.204 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.204 I llama_new_context_with_model: graph nodes  = 967
0.00.068.205 I llama_new_context_with_model: graph splits = 2
0.00.068.206 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.392 I 
0.00.619.425 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.440 I perplexity: tokenizing the input ..
0.00.627.545 I perplexity: tokenization took 8.103 ms
0.00.627.548 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.750.182 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.751.355 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.751.373 I llama_perf_context_print:        load time =     609.32 ms
0.00.751.374 I llama_perf_context_print: prompt eval time =     122.41 ms /   128 tokens (    0.96 ms per token,  1045.69 tokens per second)
0.00.751.375 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.751.375 I llama_perf_context_print:       total time =     131.98 ms /   129 tokens
0.00.751.871 I ggml_metal_free: deallocating

real	0m0.767s
user	0m0.078s
sys	0m0.099s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.922 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.672 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.676 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.685 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.687 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.688 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.688 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.689 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.689 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.691 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.692 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.501 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.370 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.372 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.372 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.372 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.372 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.373 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.373 I llama_model_loader: - type  f32:  194 tensors
0.00.032.374 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.374 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.592 I llm_load_vocab: special tokens cache size = 25
0.00.059.431 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.433 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.434 I llm_load_print_meta: arch             = gptneox
0.00.059.434 I llm_load_print_meta: vocab type       = BPE
0.00.059.434 I llm_load_print_meta: n_vocab          = 50304
0.00.059.434 I llm_load_print_meta: n_merges         = 50009
0.00.059.434 I llm_load_print_meta: vocab_only       = 0
0.00.059.435 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.435 I llm_load_print_meta: n_embd           = 2048
0.00.059.435 I llm_load_print_meta: n_layer          = 24
0.00.059.437 I llm_load_print_meta: n_head           = 16
0.00.059.438 I llm_load_print_meta: n_head_kv        = 16
0.00.059.440 I llm_load_print_meta: n_rot            = 32
0.00.059.441 I llm_load_print_meta: n_swa            = 0
0.00.059.441 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.441 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.442 I llm_load_print_meta: n_gqa            = 1
0.00.059.442 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.443 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.444 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.444 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.444 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.445 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.445 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.445 I llm_load_print_meta: n_ff             = 8192
0.00.059.457 I llm_load_print_meta: n_expert         = 0
0.00.059.457 I llm_load_print_meta: n_expert_used    = 0
0.00.059.458 I llm_load_print_meta: causal attn      = 1
0.00.059.458 I llm_load_print_meta: pooling type     = 0
0.00.059.458 I llm_load_print_meta: rope type        = 2
0.00.059.458 I llm_load_print_meta: rope scaling     = linear
0.00.059.458 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.459 I llm_load_print_meta: freq_scale_train = 1
0.00.059.459 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.459 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.459 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.459 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.460 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.460 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.460 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.461 I llm_load_print_meta: model type       = 1.4B
0.00.059.461 I llm_load_print_meta: model ftype      = Q4_1
0.00.059.462 I llm_load_print_meta: model params     = 1.41 B
0.00.059.463 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.059.464 I llm_load_print_meta: general.name     = 1.4B
0.00.059.464 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.464 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.464 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.464 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.465 I llm_load_print_meta: LF token         = 128 ''
0.00.059.465 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.465 I llm_load_print_meta: max token length = 1024
0.00.061.474 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.474 I llm_load_tensors: offloading output layer to GPU
0.00.061.475 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.485 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.061.486 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.062.381 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.382 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.382 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.382 I llama_new_context_with_model: n_batch       = 2048
0.00.062.382 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.382 I llama_new_context_with_model: flash_attn    = 0
0.00.062.383 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.383 I llama_new_context_with_model: freq_scale    = 1
0.00.062.384 I ggml_metal_init: allocating
0.00.062.387 I ggml_metal_init: found device: Apple M4
0.00.062.389 I ggml_metal_init: picking default device: Apple M4
0.00.063.022 I ggml_metal_init: using embedded metal library
0.00.065.404 I ggml_metal_init: GPU name:   Apple M4
0.00.065.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.406 I ggml_metal_init: simdgroup reduction   = true
0.00.065.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.407 I ggml_metal_init: has bfloat            = true
0.00.065.407 I ggml_metal_init: use bfloat            = true
0.00.065.407 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.360 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.651 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.659 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.694 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.820 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.824 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.825 I llama_new_context_with_model: graph nodes  = 967
0.00.098.825 I llama_new_context_with_model: graph splits = 2
0.00.098.828 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.962 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.963 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.859.502 I main: llama threadpool init, n_threads = 4
0.00.859.543 I 
0.00.859.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.859.562 I 
0.00.859.793 I sampler seed: 1234
0.00.859.797 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.859.809 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.859.809 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.859.809 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.588.185 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63223.51 tokens per second)
0.01.588.186 I llama_perf_context_print:        load time =     850.58 ms
0.01.588.186 I llama_perf_context_print: prompt eval time =      39.75 ms /     7 tokens (    5.68 ms per token,   176.09 tokens per second)
0.01.588.187 I llama_perf_context_print:        eval time =     685.77 ms /    63 runs   (   10.89 ms per token,    91.87 tokens per second)
0.01.588.188 I llama_perf_context_print:       total time =     728.68 ms /    70 tokens
0.01.588.388 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.111s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.850 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.466 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.472 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.473 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.473 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.474 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.474 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.475 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.475 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.475 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.476 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.476 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.478 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.478 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.479 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.474 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.376 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.379 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.379 I llama_model_loader: - type  f32:  194 tensors
0.00.023.380 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.380 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.509 I llm_load_vocab: special tokens cache size = 25
0.00.050.398 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.401 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.401 I llm_load_print_meta: arch             = gptneox
0.00.050.401 I llm_load_print_meta: vocab type       = BPE
0.00.050.401 I llm_load_print_meta: n_vocab          = 50304
0.00.050.402 I llm_load_print_meta: n_merges         = 50009
0.00.050.402 I llm_load_print_meta: vocab_only       = 0
0.00.050.402 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.402 I llm_load_print_meta: n_embd           = 2048
0.00.050.402 I llm_load_print_meta: n_layer          = 24
0.00.050.405 I llm_load_print_meta: n_head           = 16
0.00.050.406 I llm_load_print_meta: n_head_kv        = 16
0.00.050.406 I llm_load_print_meta: n_rot            = 32
0.00.050.406 I llm_load_print_meta: n_swa            = 0
0.00.050.406 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.408 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.409 I llm_load_print_meta: n_gqa            = 1
0.00.050.409 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.412 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.412 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.413 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.414 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.414 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.415 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.415 I llm_load_print_meta: n_ff             = 8192
0.00.050.427 I llm_load_print_meta: n_expert         = 0
0.00.050.427 I llm_load_print_meta: n_expert_used    = 0
0.00.050.427 I llm_load_print_meta: causal attn      = 1
0.00.050.428 I llm_load_print_meta: pooling type     = 0
0.00.050.428 I llm_load_print_meta: rope type        = 2
0.00.050.428 I llm_load_print_meta: rope scaling     = linear
0.00.050.428 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.429 I llm_load_print_meta: freq_scale_train = 1
0.00.050.429 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.430 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.430 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.430 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.430 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.430 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.430 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.431 I llm_load_print_meta: model type       = 1.4B
0.00.050.431 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.431 I llm_load_print_meta: model params     = 1.41 B
0.00.050.432 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.432 I llm_load_print_meta: general.name     = 1.4B
0.00.050.432 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.432 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.432 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.433 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.434 I llm_load_print_meta: LF token         = 128 ''
0.00.050.434 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.434 I llm_load_print_meta: max token length = 1024
0.00.052.462 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.463 I llm_load_tensors: offloading output layer to GPU
0.00.052.463 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.474 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.475 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.435 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.436 I llama_new_context_with_model: n_ctx         = 128
0.00.053.436 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.436 I llama_new_context_with_model: n_batch       = 128
0.00.053.436 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.436 I llama_new_context_with_model: flash_attn    = 0
0.00.053.437 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.437 I llama_new_context_with_model: freq_scale    = 1
0.00.053.438 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.438 I ggml_metal_init: allocating
0.00.053.446 I ggml_metal_init: found device: Apple M4
0.00.053.448 I ggml_metal_init: picking default device: Apple M4
0.00.054.029 I ggml_metal_init: using embedded metal library
0.00.056.348 I ggml_metal_init: GPU name:   Apple M4
0.00.056.350 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.350 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.350 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.351 I ggml_metal_init: simdgroup reduction   = true
0.00.056.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.351 I ggml_metal_init: has bfloat            = true
0.00.056.351 I ggml_metal_init: use bfloat            = true
0.00.056.351 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.352 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.075 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.418 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.423 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.449 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.303 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.304 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.305 I llama_new_context_with_model: graph nodes  = 967
0.00.068.305 I llama_new_context_with_model: graph splits = 2
0.00.068.306 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.503 I 
0.00.665.535 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.550 I perplexity: tokenizing the input ..
0.00.673.655 I perplexity: tokenization took 8.102 ms
0.00.673.658 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.219 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.797.401 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.797.412 I llama_perf_context_print:        load time =     656.64 ms
0.00.797.413 I llama_perf_context_print: prompt eval time =     122.33 ms /   128 tokens (    0.96 ms per token,  1046.32 tokens per second)
0.00.797.416 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.416 I llama_perf_context_print:       total time =     131.91 ms /   129 tokens
0.00.797.925 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.079s
sys	0m0.094s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.105 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.439 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.445 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.445 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.446 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.450 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.450 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.452 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.452 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.453 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.453 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.453 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.454 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.458 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.458 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.459 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.198 I llama_model_loader: - type  f32:  194 tensors
0.00.025.199 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.199 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.426 I llm_load_vocab: special tokens cache size = 25
0.00.052.379 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.382 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.382 I llm_load_print_meta: arch             = gptneox
0.00.052.383 I llm_load_print_meta: vocab type       = BPE
0.00.052.383 I llm_load_print_meta: n_vocab          = 50304
0.00.052.383 I llm_load_print_meta: n_merges         = 50009
0.00.052.383 I llm_load_print_meta: vocab_only       = 0
0.00.052.383 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.384 I llm_load_print_meta: n_embd           = 2048
0.00.052.384 I llm_load_print_meta: n_layer          = 24
0.00.052.387 I llm_load_print_meta: n_head           = 16
0.00.052.387 I llm_load_print_meta: n_head_kv        = 16
0.00.052.388 I llm_load_print_meta: n_rot            = 32
0.00.052.388 I llm_load_print_meta: n_swa            = 0
0.00.052.388 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.388 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.389 I llm_load_print_meta: n_gqa            = 1
0.00.052.390 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.391 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.391 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.392 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.392 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.392 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.392 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.393 I llm_load_print_meta: n_ff             = 8192
0.00.052.405 I llm_load_print_meta: n_expert         = 0
0.00.052.405 I llm_load_print_meta: n_expert_used    = 0
0.00.052.407 I llm_load_print_meta: causal attn      = 1
0.00.052.409 I llm_load_print_meta: pooling type     = 0
0.00.052.409 I llm_load_print_meta: rope type        = 2
0.00.052.409 I llm_load_print_meta: rope scaling     = linear
0.00.052.409 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.410 I llm_load_print_meta: freq_scale_train = 1
0.00.052.410 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.411 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.411 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.411 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.411 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.411 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.411 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.412 I llm_load_print_meta: model type       = 1.4B
0.00.052.412 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.412 I llm_load_print_meta: model params     = 1.41 B
0.00.052.413 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.413 I llm_load_print_meta: general.name     = 1.4B
0.00.052.413 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.413 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.414 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.414 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.414 I llm_load_print_meta: LF token         = 128 ''
0.00.052.414 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.414 I llm_load_print_meta: max token length = 1024
0.00.054.467 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.467 I llm_load_tensors: offloading output layer to GPU
0.00.054.468 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.478 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.479 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.391 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.392 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.392 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.392 I llama_new_context_with_model: n_batch       = 2048
0.00.055.392 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.392 I llama_new_context_with_model: flash_attn    = 0
0.00.055.393 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.393 I llama_new_context_with_model: freq_scale    = 1
0.00.055.394 I ggml_metal_init: allocating
0.00.055.397 I ggml_metal_init: found device: Apple M4
0.00.055.399 I ggml_metal_init: picking default device: Apple M4
0.00.055.999 I ggml_metal_init: using embedded metal library
0.00.058.396 I ggml_metal_init: GPU name:   Apple M4
0.00.058.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.398 I ggml_metal_init: simdgroup reduction   = true
0.00.058.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.399 I ggml_metal_init: has bfloat            = true
0.00.058.399 I ggml_metal_init: use bfloat            = true
0.00.058.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.360 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.016 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.025 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.056 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.055 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.056 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.056 I llama_new_context_with_model: graph nodes  = 967
0.00.088.056 I llama_new_context_with_model: graph splits = 2
0.00.088.059 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.212 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.613 I main: llama threadpool init, n_threads = 4
0.00.779.653 I 
0.00.779.676 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.676 I 
0.00.779.912 I sampler seed: 1234
0.00.779.917 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.945 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.946 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.946 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.566.801 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.566.802 I llama_perf_context_print:        load time =     769.50 ms
0.01.566.803 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.36 tokens per second)
0.01.566.804 I llama_perf_context_print:        eval time =     740.69 ms /    63 runs   (   11.76 ms per token,    85.06 tokens per second)
0.01.566.804 I llama_perf_context_print:       total time =     787.19 ms /    70 tokens
0.01.566.997 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.194 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.204 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.204 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.205 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.205 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.207 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.209 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.211 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.211 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.211 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.107 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.987 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.988 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.989 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.989 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.989 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.989 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.990 I llama_model_loader: - type  f32:  194 tensors
0.00.023.991 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.991 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.222 I llm_load_vocab: special tokens cache size = 25
0.00.051.220 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.223 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.223 I llm_load_print_meta: arch             = gptneox
0.00.051.223 I llm_load_print_meta: vocab type       = BPE
0.00.051.223 I llm_load_print_meta: n_vocab          = 50304
0.00.051.224 I llm_load_print_meta: n_merges         = 50009
0.00.051.224 I llm_load_print_meta: vocab_only       = 0
0.00.051.224 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.224 I llm_load_print_meta: n_embd           = 2048
0.00.051.224 I llm_load_print_meta: n_layer          = 24
0.00.051.226 I llm_load_print_meta: n_head           = 16
0.00.051.227 I llm_load_print_meta: n_head_kv        = 16
0.00.051.227 I llm_load_print_meta: n_rot            = 32
0.00.051.228 I llm_load_print_meta: n_swa            = 0
0.00.051.228 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.230 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.231 I llm_load_print_meta: n_gqa            = 1
0.00.051.231 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.232 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.234 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.235 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.235 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.235 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.235 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.236 I llm_load_print_meta: n_ff             = 8192
0.00.051.248 I llm_load_print_meta: n_expert         = 0
0.00.051.248 I llm_load_print_meta: n_expert_used    = 0
0.00.051.248 I llm_load_print_meta: causal attn      = 1
0.00.051.248 I llm_load_print_meta: pooling type     = 0
0.00.051.248 I llm_load_print_meta: rope type        = 2
0.00.051.248 I llm_load_print_meta: rope scaling     = linear
0.00.051.249 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.249 I llm_load_print_meta: freq_scale_train = 1
0.00.051.250 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.250 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.250 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.250 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.250 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.251 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.251 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.251 I llm_load_print_meta: model type       = 1.4B
0.00.051.252 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.252 I llm_load_print_meta: model params     = 1.41 B
0.00.051.252 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.252 I llm_load_print_meta: general.name     = 1.4B
0.00.051.253 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.253 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.253 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.253 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.253 I llm_load_print_meta: LF token         = 128 ''
0.00.051.254 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.254 I llm_load_print_meta: max token length = 1024
0.00.053.332 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.332 I llm_load_tensors: offloading output layer to GPU
0.00.053.332 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.342 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.344 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.299 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.299 I llama_new_context_with_model: n_ctx         = 128
0.00.054.300 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.300 I llama_new_context_with_model: n_batch       = 128
0.00.054.300 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.300 I llama_new_context_with_model: flash_attn    = 0
0.00.054.301 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.301 I llama_new_context_with_model: freq_scale    = 1
0.00.054.301 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.302 I ggml_metal_init: allocating
0.00.054.308 I ggml_metal_init: found device: Apple M4
0.00.054.311 I ggml_metal_init: picking default device: Apple M4
0.00.054.896 I ggml_metal_init: using embedded metal library
0.00.057.236 I ggml_metal_init: GPU name:   Apple M4
0.00.057.237 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.238 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.238 I ggml_metal_init: simdgroup reduction   = true
0.00.057.238 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.239 I ggml_metal_init: has bfloat            = true
0.00.057.239 I ggml_metal_init: use bfloat            = true
0.00.057.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.240 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.608 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.829 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.831 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.857 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.759 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.760 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.760 I llama_new_context_with_model: graph nodes  = 967
0.00.068.760 I llama_new_context_with_model: graph splits = 2
0.00.068.762 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.762 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.834 I 
0.00.714.918 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.714.947 I perplexity: tokenizing the input ..
0.00.723.177 I perplexity: tokenization took 8.229 ms
0.00.723.180 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.858.249 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.859.419 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.859.432 I llama_perf_context_print:        load time =     705.10 ms
0.00.859.433 I llama_perf_context_print: prompt eval time =     134.84 ms /   128 tokens (    1.05 ms per token,   949.26 tokens per second)
0.00.859.433 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.434 I llama_perf_context_print:       total time =     144.60 ms /   129 tokens
0.00.859.801 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.079s
sys	0m0.110s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.836 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.135 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.139 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.152 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.152 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.152 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.076 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.868 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.869 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.869 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.870 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.870 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.870 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.871 I llama_model_loader: - type  f32:  194 tensors
0.00.023.871 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.872 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.028 I llm_load_vocab: special tokens cache size = 25
0.00.050.965 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.968 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.968 I llm_load_print_meta: arch             = gptneox
0.00.050.968 I llm_load_print_meta: vocab type       = BPE
0.00.050.968 I llm_load_print_meta: n_vocab          = 50304
0.00.050.969 I llm_load_print_meta: n_merges         = 50009
0.00.050.969 I llm_load_print_meta: vocab_only       = 0
0.00.050.969 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.969 I llm_load_print_meta: n_embd           = 2048
0.00.050.969 I llm_load_print_meta: n_layer          = 24
0.00.050.972 I llm_load_print_meta: n_head           = 16
0.00.050.973 I llm_load_print_meta: n_head_kv        = 16
0.00.050.974 I llm_load_print_meta: n_rot            = 32
0.00.050.974 I llm_load_print_meta: n_swa            = 0
0.00.050.974 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.974 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.975 I llm_load_print_meta: n_gqa            = 1
0.00.050.976 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.976 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.977 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.977 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.977 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.977 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.978 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.978 I llm_load_print_meta: n_ff             = 8192
0.00.050.990 I llm_load_print_meta: n_expert         = 0
0.00.050.990 I llm_load_print_meta: n_expert_used    = 0
0.00.050.990 I llm_load_print_meta: causal attn      = 1
0.00.050.991 I llm_load_print_meta: pooling type     = 0
0.00.050.991 I llm_load_print_meta: rope type        = 2
0.00.050.991 I llm_load_print_meta: rope scaling     = linear
0.00.050.991 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.991 I llm_load_print_meta: freq_scale_train = 1
0.00.050.992 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.992 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.992 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.992 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.992 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.992 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.992 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.993 I llm_load_print_meta: model type       = 1.4B
0.00.050.993 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.994 I llm_load_print_meta: model params     = 1.41 B
0.00.050.994 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.994 I llm_load_print_meta: general.name     = 1.4B
0.00.050.995 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.995 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.995 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.995 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.996 I llm_load_print_meta: LF token         = 128 ''
0.00.050.996 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.996 I llm_load_print_meta: max token length = 1024
0.00.053.092 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.092 I llm_load_tensors: offloading output layer to GPU
0.00.053.092 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.103 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.104 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.071 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.072 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.072 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.072 I llama_new_context_with_model: n_batch       = 2048
0.00.054.072 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.072 I llama_new_context_with_model: flash_attn    = 0
0.00.054.073 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.073 I llama_new_context_with_model: freq_scale    = 1
0.00.054.073 I ggml_metal_init: allocating
0.00.054.079 I ggml_metal_init: found device: Apple M4
0.00.054.081 I ggml_metal_init: picking default device: Apple M4
0.00.054.674 I ggml_metal_init: using embedded metal library
0.00.057.023 I ggml_metal_init: GPU name:   Apple M4
0.00.057.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.025 I ggml_metal_init: simdgroup reduction   = true
0.00.057.025 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.025 I ggml_metal_init: has bfloat            = true
0.00.057.026 I ggml_metal_init: use bfloat            = true
0.00.057.026 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.027 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.525 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.587 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.592 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.619 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.668 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.669 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.670 I llama_new_context_with_model: graph nodes  = 967
0.00.086.670 I llama_new_context_with_model: graph splits = 2
0.00.086.673 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.807 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.808 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.562 I main: llama threadpool init, n_threads = 4
0.00.705.647 I 
0.00.705.670 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.670 I 
0.00.705.903 I sampler seed: 1234
0.00.705.908 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.949 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.950 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.950 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.546.717 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.546.718 I llama_perf_context_print:        load time =     696.72 ms
0.01.546.719 I llama_perf_context_print: prompt eval time =      42.31 ms /     7 tokens (    6.04 ms per token,   165.44 tokens per second)
0.01.546.719 I llama_perf_context_print:        eval time =     795.52 ms /    63 runs   (   12.63 ms per token,    79.19 tokens per second)
0.01.546.720 I llama_perf_context_print:       total time =     841.16 ms /    70 tokens
0.01.546.960 I ggml_metal_free: deallocating

real	0m1.564s
user	0m0.111s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.952 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.703 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.707 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.709 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.709 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.711 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.711 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.712 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.712 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.714 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.539 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.254 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.255 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.255 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.256 I llama_model_loader: - type  f32:  194 tensors
0.00.023.256 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.256 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.337 I llm_load_vocab: special tokens cache size = 25
0.00.050.106 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.108 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.108 I llm_load_print_meta: arch             = gptneox
0.00.050.109 I llm_load_print_meta: vocab type       = BPE
0.00.050.109 I llm_load_print_meta: n_vocab          = 50304
0.00.050.109 I llm_load_print_meta: n_merges         = 50009
0.00.050.109 I llm_load_print_meta: vocab_only       = 0
0.00.050.110 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.110 I llm_load_print_meta: n_embd           = 2048
0.00.050.110 I llm_load_print_meta: n_layer          = 24
0.00.050.113 I llm_load_print_meta: n_head           = 16
0.00.050.113 I llm_load_print_meta: n_head_kv        = 16
0.00.050.114 I llm_load_print_meta: n_rot            = 32
0.00.050.114 I llm_load_print_meta: n_swa            = 0
0.00.050.114 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.114 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.115 I llm_load_print_meta: n_gqa            = 1
0.00.050.116 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.116 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.117 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.117 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.117 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.117 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.117 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.118 I llm_load_print_meta: n_ff             = 8192
0.00.050.130 I llm_load_print_meta: n_expert         = 0
0.00.050.130 I llm_load_print_meta: n_expert_used    = 0
0.00.050.130 I llm_load_print_meta: causal attn      = 1
0.00.050.131 I llm_load_print_meta: pooling type     = 0
0.00.050.131 I llm_load_print_meta: rope type        = 2
0.00.050.131 I llm_load_print_meta: rope scaling     = linear
0.00.050.131 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.132 I llm_load_print_meta: freq_scale_train = 1
0.00.050.132 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.132 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.132 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.132 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.132 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.133 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.133 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.135 I llm_load_print_meta: model type       = 1.4B
0.00.050.135 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.136 I llm_load_print_meta: model params     = 1.41 B
0.00.050.136 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.136 I llm_load_print_meta: general.name     = 1.4B
0.00.050.137 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.137 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.137 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.137 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.137 I llm_load_print_meta: LF token         = 128 ''
0.00.050.137 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.138 I llm_load_print_meta: max token length = 1024
0.00.052.198 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.198 I llm_load_tensors: offloading output layer to GPU
0.00.052.199 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.210 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.211 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.106 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.107 I llama_new_context_with_model: n_ctx         = 128
0.00.053.107 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.107 I llama_new_context_with_model: n_batch       = 128
0.00.053.107 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.107 I llama_new_context_with_model: flash_attn    = 0
0.00.053.108 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.108 I llama_new_context_with_model: freq_scale    = 1
0.00.053.109 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.109 I ggml_metal_init: allocating
0.00.053.115 I ggml_metal_init: found device: Apple M4
0.00.053.117 I ggml_metal_init: picking default device: Apple M4
0.00.053.649 I ggml_metal_init: using embedded metal library
0.00.055.989 I ggml_metal_init: GPU name:   Apple M4
0.00.055.991 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.991 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.992 I ggml_metal_init: simdgroup reduction   = true
0.00.055.992 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.992 I ggml_metal_init: has bfloat            = true
0.00.055.992 I ggml_metal_init: use bfloat            = true
0.00.055.992 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.994 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.339 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.604 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.607 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.635 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.524 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.525 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.525 I llama_new_context_with_model: graph nodes  = 967
0.00.067.525 I llama_new_context_with_model: graph splits = 2
0.00.067.527 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.028 I 
0.00.642.072 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.107 I perplexity: tokenizing the input ..
0.00.650.050 I perplexity: tokenization took 7.941 ms
0.00.650.054 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.415 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.786.659 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.786.673 I llama_perf_context_print:        load time =     633.07 ms
0.00.786.674 I llama_perf_context_print: prompt eval time =     135.12 ms /   128 tokens (    1.06 ms per token,   947.29 tokens per second)
0.00.786.674 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.675 I llama_perf_context_print:       total time =     144.65 ms /   129 tokens
0.00.787.162 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.078s
sys	0m0.109s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.799 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.291 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.297 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.297 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.298 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.298 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.298 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.299 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.300 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.300 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.301 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.177 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.218 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.011 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.012 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.013 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.013 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.013 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.014 I llama_model_loader: - type  f32:  194 tensors
0.00.024.014 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.014 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.014 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.528 I llm_load_vocab: special tokens cache size = 25
0.00.050.483 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.486 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.487 I llm_load_print_meta: arch             = gptneox
0.00.050.487 I llm_load_print_meta: vocab type       = BPE
0.00.050.487 I llm_load_print_meta: n_vocab          = 50304
0.00.050.487 I llm_load_print_meta: n_merges         = 50009
0.00.050.487 I llm_load_print_meta: vocab_only       = 0
0.00.050.488 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.488 I llm_load_print_meta: n_embd           = 2048
0.00.050.488 I llm_load_print_meta: n_layer          = 24
0.00.050.491 I llm_load_print_meta: n_head           = 16
0.00.050.494 I llm_load_print_meta: n_head_kv        = 16
0.00.050.494 I llm_load_print_meta: n_rot            = 32
0.00.050.494 I llm_load_print_meta: n_swa            = 0
0.00.050.494 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.495 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.495 I llm_load_print_meta: n_gqa            = 1
0.00.050.501 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.502 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.502 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.503 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.503 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.503 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.505 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.505 I llm_load_print_meta: n_ff             = 8192
0.00.050.517 I llm_load_print_meta: n_expert         = 0
0.00.050.517 I llm_load_print_meta: n_expert_used    = 0
0.00.050.518 I llm_load_print_meta: causal attn      = 1
0.00.050.518 I llm_load_print_meta: pooling type     = 0
0.00.050.519 I llm_load_print_meta: rope type        = 2
0.00.050.519 I llm_load_print_meta: rope scaling     = linear
0.00.050.519 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.519 I llm_load_print_meta: freq_scale_train = 1
0.00.050.520 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.520 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.520 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.520 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.520 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.520 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.520 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.521 I llm_load_print_meta: model type       = 1.4B
0.00.050.521 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.522 I llm_load_print_meta: model params     = 1.41 B
0.00.050.522 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.522 I llm_load_print_meta: general.name     = 1.4B
0.00.050.522 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.523 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.523 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.523 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.523 I llm_load_print_meta: LF token         = 128 ''
0.00.050.523 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.523 I llm_load_print_meta: max token length = 1024
0.00.052.417 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.417 I llm_load_tensors: offloading output layer to GPU
0.00.052.417 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.428 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.429 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.309 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.310 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.310 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.310 I llama_new_context_with_model: n_batch       = 2048
0.00.053.310 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.311 I llama_new_context_with_model: flash_attn    = 0
0.00.053.311 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.311 I llama_new_context_with_model: freq_scale    = 1
0.00.053.312 I ggml_metal_init: allocating
0.00.053.315 I ggml_metal_init: found device: Apple M4
0.00.053.317 I ggml_metal_init: picking default device: Apple M4
0.00.053.892 I ggml_metal_init: using embedded metal library
0.00.056.263 I ggml_metal_init: GPU name:   Apple M4
0.00.056.264 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.265 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.265 I ggml_metal_init: simdgroup reduction   = true
0.00.056.265 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.266 I ggml_metal_init: has bfloat            = true
0.00.056.266 I ggml_metal_init: use bfloat            = true
0.00.056.266 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.267 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.965 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.762 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.768 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.798 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.849 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.850 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.851 I llama_new_context_with_model: graph nodes  = 967
0.00.086.851 I llama_new_context_with_model: graph splits = 2
0.00.086.854 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.971 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.972 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.495.417 I main: llama threadpool init, n_threads = 4
0.00.495.467 I 
0.00.495.496 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.495.499 I 
0.00.495.742 I sampler seed: 1234
0.00.495.751 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.495.786 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.495.803 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.495.803 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.176.047 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64486.83 tokens per second)
0.01.176.047 I llama_perf_context_print:        load time =     485.61 ms
0.01.176.049 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.72 tokens per second)
0.01.176.049 I llama_perf_context_print:        eval time =     641.62 ms /    63 runs   (   10.18 ms per token,    98.19 tokens per second)
0.01.176.050 I llama_perf_context_print:       total time =     680.64 ms /    70 tokens
0.01.176.308 I ggml_metal_free: deallocating

real	0m1.194s
user	0m0.110s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.532 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.080 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.084 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.090 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.091 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.091 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.092 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.092 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.093 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.093 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.094 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.094 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.094 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.096 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.096 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.096 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.994 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.912 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.913 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.914 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.914 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.915 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.915 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.915 I llama_model_loader: - type  f32:  194 tensors
0.00.023.916 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.916 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.916 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.956 I llm_load_vocab: special tokens cache size = 25
0.00.050.836 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.839 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.839 I llm_load_print_meta: arch             = gptneox
0.00.050.840 I llm_load_print_meta: vocab type       = BPE
0.00.050.840 I llm_load_print_meta: n_vocab          = 50304
0.00.050.840 I llm_load_print_meta: n_merges         = 50009
0.00.050.840 I llm_load_print_meta: vocab_only       = 0
0.00.050.841 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.841 I llm_load_print_meta: n_embd           = 2048
0.00.050.841 I llm_load_print_meta: n_layer          = 24
0.00.050.843 I llm_load_print_meta: n_head           = 16
0.00.050.844 I llm_load_print_meta: n_head_kv        = 16
0.00.050.844 I llm_load_print_meta: n_rot            = 32
0.00.050.844 I llm_load_print_meta: n_swa            = 0
0.00.050.845 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.845 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.846 I llm_load_print_meta: n_gqa            = 1
0.00.050.846 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.847 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.847 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.848 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.848 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.848 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.848 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.849 I llm_load_print_meta: n_ff             = 8192
0.00.050.861 I llm_load_print_meta: n_expert         = 0
0.00.050.861 I llm_load_print_meta: n_expert_used    = 0
0.00.050.861 I llm_load_print_meta: causal attn      = 1
0.00.050.862 I llm_load_print_meta: pooling type     = 0
0.00.050.862 I llm_load_print_meta: rope type        = 2
0.00.050.862 I llm_load_print_meta: rope scaling     = linear
0.00.050.862 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.862 I llm_load_print_meta: freq_scale_train = 1
0.00.050.863 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.863 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.863 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.863 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.863 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.863 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.863 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.864 I llm_load_print_meta: model type       = 1.4B
0.00.050.864 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.865 I llm_load_print_meta: model params     = 1.41 B
0.00.050.865 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.867 I llm_load_print_meta: general.name     = 1.4B
0.00.050.867 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.867 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.868 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.868 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.868 I llm_load_print_meta: LF token         = 128 ''
0.00.050.869 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.869 I llm_load_print_meta: max token length = 1024
0.00.052.804 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.804 I llm_load_tensors: offloading output layer to GPU
0.00.052.805 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.815 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.817 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.763 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.764 I llama_new_context_with_model: n_ctx         = 128
0.00.053.764 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.764 I llama_new_context_with_model: n_batch       = 128
0.00.053.764 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.765 I llama_new_context_with_model: flash_attn    = 0
0.00.053.765 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.765 I llama_new_context_with_model: freq_scale    = 1
0.00.053.766 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.766 I ggml_metal_init: allocating
0.00.053.772 I ggml_metal_init: found device: Apple M4
0.00.053.774 I ggml_metal_init: picking default device: Apple M4
0.00.054.311 I ggml_metal_init: using embedded metal library
0.00.056.615 I ggml_metal_init: GPU name:   Apple M4
0.00.056.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.616 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.617 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.617 I ggml_metal_init: simdgroup reduction   = true
0.00.056.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.617 I ggml_metal_init: has bfloat            = true
0.00.056.617 I ggml_metal_init: use bfloat            = true
0.00.056.618 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.103 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.468 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.472 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.494 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.364 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.365 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.366 I llama_new_context_with_model: graph nodes  = 967
0.00.068.366 I llama_new_context_with_model: graph splits = 2
0.00.068.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.422.906 I 
0.00.422.929 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.422.944 I perplexity: tokenizing the input ..
0.00.431.416 I perplexity: tokenization took 8.469 ms
0.00.431.419 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.564.027 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.565.196 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.565.217 I llama_perf_context_print:        load time =     413.37 ms
0.00.565.218 I llama_perf_context_print: prompt eval time =     132.36 ms /   128 tokens (    1.03 ms per token,   967.04 tokens per second)
0.00.565.223 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.565.223 I llama_perf_context_print:       total time =     142.31 ms /   129 tokens
0.00.565.712 I ggml_metal_free: deallocating

real	0m0.581s
user	0m0.079s
sys	0m0.070s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.078 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.714 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.719 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.720 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.721 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.721 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.724 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.725 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.729 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.729 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.730 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.730 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.730 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.731 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.734 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.734 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.734 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.620 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.497 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.498 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.498 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.498 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.499 I llama_model_loader: - type  f32:  194 tensors
0.00.025.499 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.499 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.500 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.500 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.744 I llm_load_vocab: special tokens cache size = 25
0.00.052.792 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.795 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.795 I llm_load_print_meta: arch             = gptneox
0.00.052.795 I llm_load_print_meta: vocab type       = BPE
0.00.052.796 I llm_load_print_meta: n_vocab          = 50304
0.00.052.796 I llm_load_print_meta: n_merges         = 50009
0.00.052.796 I llm_load_print_meta: vocab_only       = 0
0.00.052.796 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.796 I llm_load_print_meta: n_embd           = 2048
0.00.052.796 I llm_load_print_meta: n_layer          = 24
0.00.052.799 I llm_load_print_meta: n_head           = 16
0.00.052.800 I llm_load_print_meta: n_head_kv        = 16
0.00.052.800 I llm_load_print_meta: n_rot            = 32
0.00.052.800 I llm_load_print_meta: n_swa            = 0
0.00.052.801 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.801 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.802 I llm_load_print_meta: n_gqa            = 1
0.00.052.803 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.804 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.804 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.805 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.806 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.807 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.807 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.807 I llm_load_print_meta: n_ff             = 8192
0.00.052.820 I llm_load_print_meta: n_expert         = 0
0.00.052.821 I llm_load_print_meta: n_expert_used    = 0
0.00.052.821 I llm_load_print_meta: causal attn      = 1
0.00.052.822 I llm_load_print_meta: pooling type     = 0
0.00.052.822 I llm_load_print_meta: rope type        = 2
0.00.052.822 I llm_load_print_meta: rope scaling     = linear
0.00.052.822 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.822 I llm_load_print_meta: freq_scale_train = 1
0.00.052.823 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.824 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.824 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.824 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.824 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.824 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.824 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.825 I llm_load_print_meta: model type       = 1.4B
0.00.052.825 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.825 I llm_load_print_meta: model params     = 1.41 B
0.00.052.829 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.829 I llm_load_print_meta: general.name     = 1.4B
0.00.052.830 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.830 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.831 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.831 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.832 I llm_load_print_meta: LF token         = 128 ''
0.00.052.832 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.832 I llm_load_print_meta: max token length = 1024
0.00.054.836 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.837 I llm_load_tensors: offloading output layer to GPU
0.00.054.837 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.847 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.849 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.764 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.765 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.765 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.765 I llama_new_context_with_model: n_batch       = 2048
0.00.055.765 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.766 I llama_new_context_with_model: flash_attn    = 0
0.00.055.766 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.766 I llama_new_context_with_model: freq_scale    = 1
0.00.055.767 I ggml_metal_init: allocating
0.00.055.775 I ggml_metal_init: found device: Apple M4
0.00.055.778 I ggml_metal_init: picking default device: Apple M4
0.00.056.362 I ggml_metal_init: using embedded metal library
0.00.058.975 I ggml_metal_init: GPU name:   Apple M4
0.00.058.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.979 I ggml_metal_init: simdgroup reduction   = true
0.00.058.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.979 I ggml_metal_init: has bfloat            = true
0.00.058.979 I ggml_metal_init: use bfloat            = true
0.00.058.979 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.980 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.833 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.187 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.195 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.227 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.238 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.239 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.240 I llama_new_context_with_model: graph nodes  = 967
0.00.089.240 I llama_new_context_with_model: graph splits = 2
0.00.089.242 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.390 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.391 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.568 I main: llama threadpool init, n_threads = 4
0.00.575.606 I 
0.00.575.634 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.575.635 I 
0.00.575.869 I sampler seed: 1234
0.00.575.873 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.575.884 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.575.884 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.575.884 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.318.941 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.318.941 I llama_perf_context_print:        load time =     565.49 ms
0.01.318.942 I llama_perf_context_print: prompt eval time =      43.84 ms /     7 tokens (    6.26 ms per token,   159.68 tokens per second)
0.01.318.943 I llama_perf_context_print:        eval time =     696.16 ms /    63 runs   (   11.05 ms per token,    90.50 tokens per second)
0.01.318.944 I llama_perf_context_print:       total time =     743.38 ms /    70 tokens
0.01.319.162 I ggml_metal_free: deallocating

real	0m1.337s
user	0m0.111s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.774 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.685 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.686 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.687 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.688 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.688 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.690 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.691 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.557 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.496 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.497 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.498 I llama_model_loader: - type  f32:  194 tensors
0.00.023.498 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.498 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.498 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.499 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.676 I llm_load_vocab: special tokens cache size = 25
0.00.050.563 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.566 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.566 I llm_load_print_meta: arch             = gptneox
0.00.050.567 I llm_load_print_meta: vocab type       = BPE
0.00.050.567 I llm_load_print_meta: n_vocab          = 50304
0.00.050.567 I llm_load_print_meta: n_merges         = 50009
0.00.050.567 I llm_load_print_meta: vocab_only       = 0
0.00.050.567 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.568 I llm_load_print_meta: n_embd           = 2048
0.00.050.568 I llm_load_print_meta: n_layer          = 24
0.00.050.570 I llm_load_print_meta: n_head           = 16
0.00.050.571 I llm_load_print_meta: n_head_kv        = 16
0.00.050.571 I llm_load_print_meta: n_rot            = 32
0.00.050.572 I llm_load_print_meta: n_swa            = 0
0.00.050.572 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.574 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.575 I llm_load_print_meta: n_gqa            = 1
0.00.050.576 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.577 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.578 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.578 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.578 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.578 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.579 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.579 I llm_load_print_meta: n_ff             = 8192
0.00.050.591 I llm_load_print_meta: n_expert         = 0
0.00.050.591 I llm_load_print_meta: n_expert_used    = 0
0.00.050.591 I llm_load_print_meta: causal attn      = 1
0.00.050.591 I llm_load_print_meta: pooling type     = 0
0.00.050.591 I llm_load_print_meta: rope type        = 2
0.00.050.591 I llm_load_print_meta: rope scaling     = linear
0.00.050.592 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.592 I llm_load_print_meta: freq_scale_train = 1
0.00.050.592 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.592 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.592 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.592 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.594 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.594 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.594 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.594 I llm_load_print_meta: model type       = 1.4B
0.00.050.595 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.595 I llm_load_print_meta: model params     = 1.41 B
0.00.050.595 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.595 I llm_load_print_meta: general.name     = 1.4B
0.00.050.596 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.597 I llm_load_print_meta: LF token         = 128 ''
0.00.050.597 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.597 I llm_load_print_meta: max token length = 1024
0.00.052.563 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.563 I llm_load_tensors: offloading output layer to GPU
0.00.052.563 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.574 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.575 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.485 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.486 I llama_new_context_with_model: n_ctx         = 128
0.00.053.486 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.486 I llama_new_context_with_model: n_batch       = 128
0.00.053.486 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.486 I llama_new_context_with_model: flash_attn    = 0
0.00.053.487 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.487 I llama_new_context_with_model: freq_scale    = 1
0.00.053.488 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.488 I ggml_metal_init: allocating
0.00.053.494 I ggml_metal_init: found device: Apple M4
0.00.053.497 I ggml_metal_init: picking default device: Apple M4
0.00.054.026 I ggml_metal_init: using embedded metal library
0.00.056.332 I ggml_metal_init: GPU name:   Apple M4
0.00.056.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.334 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.334 I ggml_metal_init: simdgroup reduction   = true
0.00.056.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.335 I ggml_metal_init: has bfloat            = true
0.00.056.335 I ggml_metal_init: use bfloat            = true
0.00.056.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.697 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.939 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.944 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.978 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.838 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.839 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.839 I llama_new_context_with_model: graph nodes  = 967
0.00.067.839 I llama_new_context_with_model: graph splits = 2
0.00.067.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.262 I 
0.00.493.289 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.493.303 I perplexity: tokenizing the input ..
0.00.500.902 I perplexity: tokenization took 7.599 ms
0.00.500.906 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.633.510 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.634.704 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.634.729 I llama_perf_context_print:        load time =     484.48 ms
0.00.634.731 I llama_perf_context_print: prompt eval time =     132.37 ms /   128 tokens (    1.03 ms per token,   967.02 tokens per second)
0.00.634.731 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.634.732 I llama_perf_context_print:       total time =     141.47 ms /   129 tokens
0.00.635.119 I ggml_metal_free: deallocating

real	0m0.648s
user	0m0.079s
sys	0m0.089s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.627 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.120 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.126 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.132 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.946 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.744 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.745 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.746 I llama_model_loader: - type  f32:  194 tensors
0.00.023.746 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.747 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.747 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.934 I llm_load_vocab: special tokens cache size = 25
0.00.050.939 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.941 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.942 I llm_load_print_meta: arch             = gptneox
0.00.050.942 I llm_load_print_meta: vocab type       = BPE
0.00.050.942 I llm_load_print_meta: n_vocab          = 50304
0.00.050.943 I llm_load_print_meta: n_merges         = 50009
0.00.050.943 I llm_load_print_meta: vocab_only       = 0
0.00.050.943 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.943 I llm_load_print_meta: n_embd           = 2048
0.00.050.943 I llm_load_print_meta: n_layer          = 24
0.00.050.946 I llm_load_print_meta: n_head           = 16
0.00.050.947 I llm_load_print_meta: n_head_kv        = 16
0.00.050.947 I llm_load_print_meta: n_rot            = 32
0.00.050.947 I llm_load_print_meta: n_swa            = 0
0.00.050.947 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.948 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.948 I llm_load_print_meta: n_gqa            = 1
0.00.050.949 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.950 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.950 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.950 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.951 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.951 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.951 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.952 I llm_load_print_meta: n_ff             = 8192
0.00.050.963 I llm_load_print_meta: n_expert         = 0
0.00.050.965 I llm_load_print_meta: n_expert_used    = 0
0.00.050.967 I llm_load_print_meta: causal attn      = 1
0.00.050.967 I llm_load_print_meta: pooling type     = 0
0.00.050.967 I llm_load_print_meta: rope type        = 2
0.00.050.967 I llm_load_print_meta: rope scaling     = linear
0.00.050.967 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.968 I llm_load_print_meta: freq_scale_train = 1
0.00.050.968 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.968 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.968 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.968 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.969 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.969 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.969 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.970 I llm_load_print_meta: model type       = 1.4B
0.00.050.970 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.971 I llm_load_print_meta: model params     = 1.41 B
0.00.050.971 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.972 I llm_load_print_meta: general.name     = 1.4B
0.00.050.972 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.972 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.972 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.972 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.972 I llm_load_print_meta: LF token         = 128 ''
0.00.050.973 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.973 I llm_load_print_meta: max token length = 1024
0.00.053.030 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.030 I llm_load_tensors: offloading output layer to GPU
0.00.053.030 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.041 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.042 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.951 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.952 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.952 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.952 I llama_new_context_with_model: n_batch       = 2048
0.00.053.952 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.953 I llama_new_context_with_model: flash_attn    = 0
0.00.053.953 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.953 I llama_new_context_with_model: freq_scale    = 1
0.00.053.954 I ggml_metal_init: allocating
0.00.053.957 I ggml_metal_init: found device: Apple M4
0.00.053.959 I ggml_metal_init: picking default device: Apple M4
0.00.054.564 I ggml_metal_init: using embedded metal library
0.00.056.972 I ggml_metal_init: GPU name:   Apple M4
0.00.056.974 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.975 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.977 I ggml_metal_init: simdgroup reduction   = true
0.00.056.977 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.977 I ggml_metal_init: has bfloat            = true
0.00.056.977 I ggml_metal_init: use bfloat            = true
0.00.056.978 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.978 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.017 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.916 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.921 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.951 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.002 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.003 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.003 I llama_new_context_with_model: graph nodes  = 967
0.00.088.004 I llama_new_context_with_model: graph splits = 2
0.00.088.006 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.420 I main: llama threadpool init, n_threads = 4
0.00.629.459 I 
0.00.629.486 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.486 I 
0.00.629.723 I sampler seed: 1234
0.00.629.727 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.768 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.769 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.769 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.386.080 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.01.386.081 I llama_perf_context_print:        load time =     620.79 ms
0.01.386.081 I llama_perf_context_print: prompt eval time =      47.18 ms /     7 tokens (    6.74 ms per token,   148.36 tokens per second)
0.01.386.082 I llama_perf_context_print:        eval time =     706.05 ms /    63 runs   (   11.21 ms per token,    89.23 tokens per second)
0.01.386.082 I llama_perf_context_print:       total time =     756.66 ms /    70 tokens
0.01.386.305 I ggml_metal_free: deallocating

real	0m1.402s
user	0m0.110s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.573 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.615 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.623 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.624 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.624 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.626 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.626 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.628 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.144 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.144 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.144 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.145 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.145 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.146 I llama_model_loader: - type  f32:  194 tensors
0.00.023.146 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.146 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.146 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.315 I llm_load_vocab: special tokens cache size = 25
0.00.050.259 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.261 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.262 I llm_load_print_meta: arch             = gptneox
0.00.050.262 I llm_load_print_meta: vocab type       = BPE
0.00.050.262 I llm_load_print_meta: n_vocab          = 50304
0.00.050.262 I llm_load_print_meta: n_merges         = 50009
0.00.050.263 I llm_load_print_meta: vocab_only       = 0
0.00.050.263 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.263 I llm_load_print_meta: n_embd           = 2048
0.00.050.263 I llm_load_print_meta: n_layer          = 24
0.00.050.266 I llm_load_print_meta: n_head           = 16
0.00.050.267 I llm_load_print_meta: n_head_kv        = 16
0.00.050.267 I llm_load_print_meta: n_rot            = 32
0.00.050.267 I llm_load_print_meta: n_swa            = 0
0.00.050.267 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.268 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.268 I llm_load_print_meta: n_gqa            = 1
0.00.050.269 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.270 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.270 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.271 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.272 I llm_load_print_meta: n_ff             = 8192
0.00.050.283 I llm_load_print_meta: n_expert         = 0
0.00.050.283 I llm_load_print_meta: n_expert_used    = 0
0.00.050.284 I llm_load_print_meta: causal attn      = 1
0.00.050.284 I llm_load_print_meta: pooling type     = 0
0.00.050.284 I llm_load_print_meta: rope type        = 2
0.00.050.284 I llm_load_print_meta: rope scaling     = linear
0.00.050.285 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.285 I llm_load_print_meta: freq_scale_train = 1
0.00.050.285 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.285 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.285 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.285 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.285 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.286 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.286 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.286 I llm_load_print_meta: model type       = 1.4B
0.00.050.287 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.287 I llm_load_print_meta: model params     = 1.41 B
0.00.050.287 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.289 I llm_load_print_meta: general.name     = 1.4B
0.00.050.289 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.290 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.290 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.290 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.290 I llm_load_print_meta: LF token         = 128 ''
0.00.050.292 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.292 I llm_load_print_meta: max token length = 1024
0.00.052.353 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.353 I llm_load_tensors: offloading output layer to GPU
0.00.052.353 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.364 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.365 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.279 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.279 I llama_new_context_with_model: n_ctx         = 128
0.00.053.280 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.280 I llama_new_context_with_model: n_batch       = 128
0.00.053.280 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.280 I llama_new_context_with_model: flash_attn    = 0
0.00.053.281 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.281 I llama_new_context_with_model: freq_scale    = 1
0.00.053.281 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.282 I ggml_metal_init: allocating
0.00.053.288 I ggml_metal_init: found device: Apple M4
0.00.053.291 I ggml_metal_init: picking default device: Apple M4
0.00.053.865 I ggml_metal_init: using embedded metal library
0.00.056.194 I ggml_metal_init: GPU name:   Apple M4
0.00.056.195 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.196 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.196 I ggml_metal_init: simdgroup reduction   = true
0.00.056.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.197 I ggml_metal_init: has bfloat            = true
0.00.056.197 I ggml_metal_init: use bfloat            = true
0.00.056.197 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.569 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.815 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.819 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.846 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.742 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.743 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.743 I llama_new_context_with_model: graph nodes  = 967
0.00.067.743 I llama_new_context_with_model: graph splits = 2
0.00.067.744 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.320 I 
0.00.627.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.383 I perplexity: tokenizing the input ..
0.00.635.605 I perplexity: tokenization took 8.22 ms
0.00.635.608 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.770.008 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.771.175 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.771.188 I llama_perf_context_print:        load time =     618.74 ms
0.00.771.188 I llama_perf_context_print: prompt eval time =     134.17 ms /   128 tokens (    1.05 ms per token,   953.99 tokens per second)
0.00.771.190 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.190 I llama_perf_context_print:       total time =     143.87 ms /   129 tokens
0.00.771.642 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.079s
sys	0m0.107s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.770 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.432 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.433 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.433 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.436 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.436 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.437 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.437 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.438 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.307 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.085 I llama_model_loader: - type  f32:  194 tensors
0.00.026.085 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.086 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.403 I llm_load_vocab: special tokens cache size = 25
0.00.053.358 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.360 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.361 I llm_load_print_meta: arch             = gptneox
0.00.053.361 I llm_load_print_meta: vocab type       = BPE
0.00.053.361 I llm_load_print_meta: n_vocab          = 50304
0.00.053.362 I llm_load_print_meta: n_merges         = 50009
0.00.053.362 I llm_load_print_meta: vocab_only       = 0
0.00.053.362 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.362 I llm_load_print_meta: n_embd           = 2048
0.00.053.362 I llm_load_print_meta: n_layer          = 24
0.00.053.365 I llm_load_print_meta: n_head           = 16
0.00.053.365 I llm_load_print_meta: n_head_kv        = 16
0.00.053.366 I llm_load_print_meta: n_rot            = 32
0.00.053.366 I llm_load_print_meta: n_swa            = 0
0.00.053.366 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.366 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.367 I llm_load_print_meta: n_gqa            = 1
0.00.053.368 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.368 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.369 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.369 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.369 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.370 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.370 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.371 I llm_load_print_meta: n_ff             = 8192
0.00.053.382 I llm_load_print_meta: n_expert         = 0
0.00.053.383 I llm_load_print_meta: n_expert_used    = 0
0.00.053.384 I llm_load_print_meta: causal attn      = 1
0.00.053.385 I llm_load_print_meta: pooling type     = 0
0.00.053.385 I llm_load_print_meta: rope type        = 2
0.00.053.386 I llm_load_print_meta: rope scaling     = linear
0.00.053.386 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.386 I llm_load_print_meta: freq_scale_train = 1
0.00.053.387 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.387 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.389 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.389 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.389 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.389 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.389 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.390 I llm_load_print_meta: model type       = 1.4B
0.00.053.391 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.391 I llm_load_print_meta: model params     = 1.41 B
0.00.053.392 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.392 I llm_load_print_meta: general.name     = 1.4B
0.00.053.392 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.393 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.394 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.394 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.395 I llm_load_print_meta: LF token         = 128 ''
0.00.053.395 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.395 I llm_load_print_meta: max token length = 1024
0.00.055.531 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.531 I llm_load_tensors: offloading output layer to GPU
0.00.055.531 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.542 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.543 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.502 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.503 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.503 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.503 I llama_new_context_with_model: n_batch       = 2048
0.00.056.504 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.504 I llama_new_context_with_model: flash_attn    = 0
0.00.056.504 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.505 I llama_new_context_with_model: freq_scale    = 1
0.00.056.505 I ggml_metal_init: allocating
0.00.056.511 I ggml_metal_init: found device: Apple M4
0.00.056.513 I ggml_metal_init: picking default device: Apple M4
0.00.057.131 I ggml_metal_init: using embedded metal library
0.00.059.471 I ggml_metal_init: GPU name:   Apple M4
0.00.059.472 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.474 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.474 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.475 I ggml_metal_init: simdgroup reduction   = true
0.00.059.475 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.475 I ggml_metal_init: has bfloat            = true
0.00.059.475 I ggml_metal_init: use bfloat            = true
0.00.059.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.476 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.109 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.183 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.194 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.225 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.152 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.154 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.154 I llama_new_context_with_model: graph nodes  = 967
0.00.090.154 I llama_new_context_with_model: graph splits = 2
0.00.090.157 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.305 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.665 I main: llama threadpool init, n_threads = 4
0.00.697.706 I 
0.00.697.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.730 I 
0.00.697.968 I sampler seed: 1234
0.00.697.973 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.984 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.984 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.984 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.547.497 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.547.498 I llama_perf_context_print:        load time =     686.89 ms
0.01.547.498 I llama_perf_context_print: prompt eval time =      51.71 ms /     7 tokens (    7.39 ms per token,   135.38 tokens per second)
0.01.547.499 I llama_perf_context_print:        eval time =     794.79 ms /    63 runs   (   12.62 ms per token,    79.27 tokens per second)
0.01.547.499 I llama_perf_context_print:       total time =     849.84 ms /    70 tokens
0.01.547.726 I ggml_metal_free: deallocating

real	0m1.568s
user	0m0.111s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.075 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.746 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.757 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.757 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.758 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.758 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.758 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.759 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.760 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.760 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.760 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.761 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.761 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.761 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.763 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.763 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.763 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.526 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.299 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.299 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.300 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.300 I llama_model_loader: - type  f32:  194 tensors
0.00.024.301 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.301 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.522 I llm_load_vocab: special tokens cache size = 25
0.00.051.303 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.306 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.307 I llm_load_print_meta: arch             = gptneox
0.00.051.307 I llm_load_print_meta: vocab type       = BPE
0.00.051.307 I llm_load_print_meta: n_vocab          = 50304
0.00.051.307 I llm_load_print_meta: n_merges         = 50009
0.00.051.307 I llm_load_print_meta: vocab_only       = 0
0.00.051.308 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.308 I llm_load_print_meta: n_embd           = 2048
0.00.051.308 I llm_load_print_meta: n_layer          = 24
0.00.051.311 I llm_load_print_meta: n_head           = 16
0.00.051.311 I llm_load_print_meta: n_head_kv        = 16
0.00.051.312 I llm_load_print_meta: n_rot            = 32
0.00.051.312 I llm_load_print_meta: n_swa            = 0
0.00.051.312 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.312 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.313 I llm_load_print_meta: n_gqa            = 1
0.00.051.314 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.314 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.315 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.315 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.315 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.315 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.316 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.318 I llm_load_print_meta: n_ff             = 8192
0.00.051.329 I llm_load_print_meta: n_expert         = 0
0.00.051.330 I llm_load_print_meta: n_expert_used    = 0
0.00.051.330 I llm_load_print_meta: causal attn      = 1
0.00.051.330 I llm_load_print_meta: pooling type     = 0
0.00.051.330 I llm_load_print_meta: rope type        = 2
0.00.051.330 I llm_load_print_meta: rope scaling     = linear
0.00.051.331 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.332 I llm_load_print_meta: freq_scale_train = 1
0.00.051.333 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.333 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.333 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.333 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.333 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.333 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.333 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.334 I llm_load_print_meta: model type       = 1.4B
0.00.051.334 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.334 I llm_load_print_meta: model params     = 1.41 B
0.00.051.335 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.335 I llm_load_print_meta: general.name     = 1.4B
0.00.051.335 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.335 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.336 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.336 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.336 I llm_load_print_meta: LF token         = 128 ''
0.00.051.336 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.337 I llm_load_print_meta: max token length = 1024
0.00.052.931 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.931 I llm_load_tensors: offloading output layer to GPU
0.00.052.931 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.941 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.942 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.821 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.822 I llama_new_context_with_model: n_ctx         = 128
0.00.053.822 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.822 I llama_new_context_with_model: n_batch       = 128
0.00.053.822 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.822 I llama_new_context_with_model: flash_attn    = 0
0.00.053.823 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.823 I llama_new_context_with_model: freq_scale    = 1
0.00.053.823 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.824 I ggml_metal_init: allocating
0.00.053.830 I ggml_metal_init: found device: Apple M4
0.00.053.833 I ggml_metal_init: picking default device: Apple M4
0.00.054.364 I ggml_metal_init: using embedded metal library
0.00.056.681 I ggml_metal_init: GPU name:   Apple M4
0.00.056.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.683 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.683 I ggml_metal_init: simdgroup reduction   = true
0.00.056.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.683 I ggml_metal_init: has bfloat            = true
0.00.056.683 I ggml_metal_init: use bfloat            = true
0.00.056.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.052 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.325 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.328 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.354 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.198 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.199 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.199 I llama_new_context_with_model: graph nodes  = 967
0.00.068.199 I llama_new_context_with_model: graph splits = 2
0.00.068.201 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.612 I 
0.00.658.647 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.661 I perplexity: tokenizing the input ..
0.00.666.831 I perplexity: tokenization took 8.169 ms
0.00.666.835 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.671 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.808.825 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.808.846 I llama_perf_context_print:        load time =     648.53 ms
0.00.808.847 I llama_perf_context_print: prompt eval time =     140.61 ms /   128 tokens (    1.10 ms per token,   910.32 tokens per second)
0.00.808.848 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.848 I llama_perf_context_print:       total time =     150.24 ms /   129 tokens
0.00.809.318 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.079s
sys	0m0.135s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.754 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.275 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.281 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.281 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.282 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.286 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.287 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.287 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.288 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.289 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.189 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.200 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.038 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.039 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.040 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.041 I llama_model_loader: - type  f32:  194 tensors
0.00.025.041 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.220 I llm_load_vocab: special tokens cache size = 25
0.00.052.242 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.245 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.245 I llm_load_print_meta: arch             = gptneox
0.00.052.245 I llm_load_print_meta: vocab type       = BPE
0.00.052.246 I llm_load_print_meta: n_vocab          = 50304
0.00.052.246 I llm_load_print_meta: n_merges         = 50009
0.00.052.246 I llm_load_print_meta: vocab_only       = 0
0.00.052.246 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.247 I llm_load_print_meta: n_embd           = 2048
0.00.052.247 I llm_load_print_meta: n_layer          = 24
0.00.052.250 I llm_load_print_meta: n_head           = 16
0.00.052.250 I llm_load_print_meta: n_head_kv        = 16
0.00.052.251 I llm_load_print_meta: n_rot            = 32
0.00.052.251 I llm_load_print_meta: n_swa            = 0
0.00.052.251 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.251 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.252 I llm_load_print_meta: n_gqa            = 1
0.00.052.253 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.253 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.255 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.255 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.255 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.255 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.256 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.256 I llm_load_print_meta: n_ff             = 8192
0.00.052.268 I llm_load_print_meta: n_expert         = 0
0.00.052.268 I llm_load_print_meta: n_expert_used    = 0
0.00.052.268 I llm_load_print_meta: causal attn      = 1
0.00.052.270 I llm_load_print_meta: pooling type     = 0
0.00.052.271 I llm_load_print_meta: rope type        = 2
0.00.052.271 I llm_load_print_meta: rope scaling     = linear
0.00.052.272 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.272 I llm_load_print_meta: freq_scale_train = 1
0.00.052.272 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.272 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.273 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.273 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.273 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.273 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.273 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.274 I llm_load_print_meta: model type       = 1.4B
0.00.052.275 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.275 I llm_load_print_meta: model params     = 1.41 B
0.00.052.275 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.276 I llm_load_print_meta: general.name     = 1.4B
0.00.052.276 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.276 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.276 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.276 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.276 I llm_load_print_meta: LF token         = 128 ''
0.00.052.277 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.277 I llm_load_print_meta: max token length = 1024
0.00.054.405 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.405 I llm_load_tensors: offloading output layer to GPU
0.00.054.406 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.416 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.418 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.331 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.332 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.332 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.332 I llama_new_context_with_model: n_batch       = 2048
0.00.055.332 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.332 I llama_new_context_with_model: flash_attn    = 0
0.00.055.333 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.333 I llama_new_context_with_model: freq_scale    = 1
0.00.055.333 I ggml_metal_init: allocating
0.00.055.337 I ggml_metal_init: found device: Apple M4
0.00.055.339 I ggml_metal_init: picking default device: Apple M4
0.00.055.944 I ggml_metal_init: using embedded metal library
0.00.058.326 I ggml_metal_init: GPU name:   Apple M4
0.00.058.327 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.328 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.328 I ggml_metal_init: simdgroup reduction   = true
0.00.058.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.330 I ggml_metal_init: has bfloat            = true
0.00.058.330 I ggml_metal_init: use bfloat            = true
0.00.058.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.312 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.334 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.340 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.369 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.390 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.392 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.392 I llama_new_context_with_model: graph nodes  = 967
0.00.089.392 I llama_new_context_with_model: graph splits = 2
0.00.089.394 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.428 I main: llama threadpool init, n_threads = 4
0.00.765.465 I 
0.00.765.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.487 I 
0.00.765.740 I sampler seed: 1234
0.00.765.745 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.765.758 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.765.758 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.765.758 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.645.546 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48365.12 tokens per second)
0.01.645.547 I llama_perf_context_print:        load time =     756.67 ms
0.01.645.547 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.67 tokens per second)
0.01.645.548 I llama_perf_context_print:        eval time =     822.86 ms /    63 runs   (   13.06 ms per token,    76.56 tokens per second)
0.01.645.548 I llama_perf_context_print:       total time =     880.12 ms /    70 tokens
0.01.645.797 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.112s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4423 (381ad835) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.781 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.533 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.533 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.534 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.536 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.536 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.537 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.537 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.539 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.539 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.318 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.074 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.075 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.076 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.077 I llama_model_loader: - type  f32:  194 tensors
0.00.023.077 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.383 I llm_load_vocab: special tokens cache size = 25
0.00.049.293 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.295 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.296 I llm_load_print_meta: arch             = gptneox
0.00.049.296 I llm_load_print_meta: vocab type       = BPE
0.00.049.296 I llm_load_print_meta: n_vocab          = 50304
0.00.049.296 I llm_load_print_meta: n_merges         = 50009
0.00.049.296 I llm_load_print_meta: vocab_only       = 0
0.00.049.297 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.297 I llm_load_print_meta: n_embd           = 2048
0.00.049.297 I llm_load_print_meta: n_layer          = 24
0.00.049.300 I llm_load_print_meta: n_head           = 16
0.00.049.300 I llm_load_print_meta: n_head_kv        = 16
0.00.049.301 I llm_load_print_meta: n_rot            = 32
0.00.049.301 I llm_load_print_meta: n_swa            = 0
0.00.049.301 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.301 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.302 I llm_load_print_meta: n_gqa            = 1
0.00.049.303 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.303 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.304 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.304 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.304 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.304 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.305 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.305 I llm_load_print_meta: n_ff             = 8192
0.00.049.317 I llm_load_print_meta: n_expert         = 0
0.00.049.317 I llm_load_print_meta: n_expert_used    = 0
0.00.049.318 I llm_load_print_meta: causal attn      = 1
0.00.049.318 I llm_load_print_meta: pooling type     = 0
0.00.049.318 I llm_load_print_meta: rope type        = 2
0.00.049.318 I llm_load_print_meta: rope scaling     = linear
0.00.049.318 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.319 I llm_load_print_meta: freq_scale_train = 1
0.00.049.319 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.319 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.319 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.320 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.320 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.320 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.320 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.321 I llm_load_print_meta: model type       = 1.4B
0.00.049.321 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.321 I llm_load_print_meta: model params     = 1.41 B
0.00.049.323 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.323 I llm_load_print_meta: general.name     = 1.4B
0.00.049.323 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.323 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.323 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.323 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.324 I llm_load_print_meta: LF token         = 128 ''
0.00.049.324 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.324 I llm_load_print_meta: max token length = 1024
0.00.051.355 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.355 I llm_load_tensors: offloading output layer to GPU
0.00.051.355 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.366 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.367 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.251 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.252 I llama_new_context_with_model: n_ctx         = 128
0.00.052.252 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.252 I llama_new_context_with_model: n_batch       = 128
0.00.052.252 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.253 I llama_new_context_with_model: flash_attn    = 0
0.00.052.253 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.253 I llama_new_context_with_model: freq_scale    = 1
0.00.052.254 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.254 I ggml_metal_init: allocating
0.00.052.259 I ggml_metal_init: found device: Apple M4
0.00.052.262 I ggml_metal_init: picking default device: Apple M4
0.00.052.817 I ggml_metal_init: using embedded metal library
0.00.055.212 I ggml_metal_init: GPU name:   Apple M4
0.00.055.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.214 I ggml_metal_init: simdgroup reduction   = true
0.00.055.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.214 I ggml_metal_init: has bfloat            = true
0.00.055.214 I ggml_metal_init: use bfloat            = true
0.00.055.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.761 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.009 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.035 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.859 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.860 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.860 I llama_new_context_with_model: graph nodes  = 967
0.00.066.860 I llama_new_context_with_model: graph splits = 2
0.00.066.861 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.862 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.529.182 I 
0.00.529.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.529.224 I perplexity: tokenizing the input ..
0.00.537.426 I perplexity: tokenization took 8.2 ms
0.00.537.431 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.677.436 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.678.702 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.678.723 I llama_perf_context_print:        load time =     520.40 ms
0.00.678.724 I llama_perf_context_print: prompt eval time =     139.78 ms /   128 tokens (    1.09 ms per token,   915.73 tokens per second)
0.00.678.725 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.678.725 I llama_perf_context_print:       total time =     149.54 ms /   129 tokens
0.00.679.169 I ggml_metal_free: deallocating

real	0m0.693s
user	0m0.077s
sys	0m0.107s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4423 (381ad835)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149a075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149a07d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149a082b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149a08860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149a08e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149a093c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149a09970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149a09f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149a0a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149a0a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149a0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149a0b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149a0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149a0c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149a0ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149a0d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149a0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149a0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149a0eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149a0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149a0fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149a10140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149a10860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149a11100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149a11820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149a11ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149a120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149a12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149a132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149a13560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149a13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149a13cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149a14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149a14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149a14d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149a151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149a15690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149a15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149a15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149a16470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149a16910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149a16db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149a17250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149a176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149a179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149a17fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149a185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149a18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149a19500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149a19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149a1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149a1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149a1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149a1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149a1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149a1bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149a1c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149a1c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149a1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149a1d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149a1d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149a1dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149a1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149a1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149a1ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149a1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149a1f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149a1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149a1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149a201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149a20640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149a20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149a20f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149a214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149a21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149a21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149a224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149a22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149a22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149a234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149a23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149a23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149a244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149a249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149a24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149a25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149a259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149a25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149a26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149a269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149a26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149a27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149a279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149a27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149a28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149a289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149a28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149a18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149a29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149a29b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149a2a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149a2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149a2ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149a2b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149a2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149a2bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149a2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149a2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149a2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149a2d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149a2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149a2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149a2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149a2e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149a2e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149a2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149a2f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149a2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149a2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149a30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149a30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149a309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149a30e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149a31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149a317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149a31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149a320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149a32590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149a32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149a32ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149a33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149a33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149a33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149a34150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149a345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149a34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149a34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149a353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149a35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149a35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149a361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149a36650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149a36af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149a36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149a37430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149a378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149a37d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149a38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149a386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149a38b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149a38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149a39490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149a39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149a39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149a3a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149a3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149a3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149a3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149a3b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149a3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149a3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149a3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149a3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149a3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149a3d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149a3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149a3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149a3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149a3e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149a3e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149a3ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149a3f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149a3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149a3fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149a3fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149a40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149a40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149a40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149a41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149a41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149a41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149a41f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149a423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149a42890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149a42d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149a431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149a43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149a43b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149a43fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149a44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149a448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149a44d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149a45230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149a45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149a45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149a46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149a46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149a46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149a47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149a47650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149a47c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149a48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149a488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149a48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149a491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149a497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149a49fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149a4a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149a4a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149a4ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149a4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149a4baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149a4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149a4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149a4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149a4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149a4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149a4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149a4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149a4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149a4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149a4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149a4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149a4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149a4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149a50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149a50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149a50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149a514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149a51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149a51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149a524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149a52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149a52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149a534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149a53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149a53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149a544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149a54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149a54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149a554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149a55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149a55f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149a564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149a569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149a56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149a57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149a579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149a57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149a58480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149a589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149a58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149a59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149a599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149a59f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149a5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149a5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149a5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149a5b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149a5b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149a5bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149a5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149a5c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149a5cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149a5d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149a5d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149a5ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149a5e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149a5e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149a5ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149a5f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149a5f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149a5fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149a5ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149a603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149a60870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149a60d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149a611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149a61650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149a61af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149a61f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149a62430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149a62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149a630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149a637c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149a63ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149a64600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149a648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149a650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149a65370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149a65980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.138.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.113 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149b0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149b0bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149b0c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149b0cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149b0d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149b0d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149b0dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149b0e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149b0e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149b0e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149b0ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149b0f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149b0fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149b101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149b109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149b11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149b11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149b11f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149b12670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149b13020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149b13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149b13e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149b14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149b14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149b153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149b15680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149b15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149b162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149b168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149b170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149b17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149b17800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149b18090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149b185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149b18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149b18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149b191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149b19670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149b19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149b19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149b1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149b1a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149b1ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149b1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149b1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149b1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149b1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149b1c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149b1cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149b1d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149b1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149b1df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149b1e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149b1eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149b1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149b1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149b1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149b1ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149b20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149b20d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149b21210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149b216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149b21b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149b21ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149b22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149b22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149b22dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149b23270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149b23710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149b23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149b24050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149b244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149b24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149b24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149b25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149b25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149b25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149b26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149b26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149b26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149b27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149b27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149b27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149b28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149b28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149b28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149b293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149b29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149b29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149b2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149b2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149b2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149b2b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149b2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149b2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149b2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149b2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149b2ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149b2d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149b2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149b2de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149b2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149b2e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149b2ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149b2f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149b2f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149b2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149b30380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149b308d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149b30e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149b31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149b318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149b31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149b322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149b32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149b32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149b33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149b33530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149b339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149b33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149b34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149b347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149b34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149b350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149b35590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149b35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149b35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149b36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149b36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149b36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149b37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149b375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149b37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149b37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149b383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149b38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149b38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149b391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149b39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149b39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149b39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149b3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149b3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149b3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149b3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149b3b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149b3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149b3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149b3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149b3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149b3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149b3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149b3d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149b3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149b3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149b3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149b3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149b3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149b3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149b3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149b3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149b400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149b40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149b409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149b40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149b41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149b417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149b41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149b42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149b425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149b42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149b42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149b43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149b43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149b43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149b44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149b44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149b44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149b44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149b453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149b45890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149b45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149b461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149b46670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149b46b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149b46fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149b47450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149b478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149b47d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149b48230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149b486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149b48b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149b49010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149b49560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149b49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149b4a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149b4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149b4a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149b4ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149b4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149b4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149b4c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149b4c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149b4c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149b4cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149b4d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149b4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149b4e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149b4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149b4eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149b4f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149b4f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149b4fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149b50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149b50870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149b50dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149b51310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149b51860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149b51db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149b52300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149b52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149b52da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149b532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149b53840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149b53d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149b542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149b54830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149b54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149b552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149b55820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149b55d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149b562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149b56810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149b56d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149b572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149b57800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149b57d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149b582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149b587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149b58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149b59290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149b597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149b59d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149b5a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149b5a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149b5ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149b5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149b5b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149b5bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149b5c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149b5c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149b5cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149b5d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149b5d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149b5dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149b5e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149b5e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149b5ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149b5f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149b5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149b5fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149b60220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149b60770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149b60cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149b61210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149b61760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149b61cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149b62150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149b625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149b62a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149b62f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149b633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149b63870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149b63d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149b641b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149b64650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149b64af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149b64f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149b65430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149b658d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149b65d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149b66210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149b66760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149b66e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149b675a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149b67cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149b683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149b686a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149b68e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149b69150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149b69760 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149b4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149b15f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149b69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149b4aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149b4b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149b1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149b1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149b20230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149b4d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149b15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149b1c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149b1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149b1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149b1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149b1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149b1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149b1ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149b20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149b68960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149b17ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149b17d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149b16b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149b4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149b4bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149b0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149b0c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149b69bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149b69e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149b6a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149b6a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149b6a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149b6a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149b6ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149b6af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149b6b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149b6b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149b6b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149b6ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149b6bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149b6bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149b6c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149b6c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149b6c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149b6ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149b6cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149b6d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149b6d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149b6d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149b6d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149b6db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149b6ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149b6e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149b6e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149b6e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149b6e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149b6eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149b6ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149b6f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149b6f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149b6f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149b6f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149b6fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149b6fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149b70180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149b70440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149b70700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149b709c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149b70c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149b70f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149b71200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149b714c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149b71780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149b71a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149b71d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149b71fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149b72280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149b72540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149b72800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149b72ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149b72d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149b73040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149b73300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149b735c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149b73880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149b73b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149b73e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149b740c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149b74380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149b74640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149b74900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149b74bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149b74e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149b75140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149b75400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149b756c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149b75980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149b75c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149b75f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149b761c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149b76480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149b76740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149b76a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149b76cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149b76f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149b77240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149b77500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149b777c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149b77a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149b77d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149b78000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149b782c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149b78580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149b78840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149b78b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149b78dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149b79080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149b79340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149b79600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149b798c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149b79b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149b79e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149b7a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149b7a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149b7a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149b7a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149b7ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149b7aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149b7b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149b7b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149b7b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149b7b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149b7bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149b7bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149b7c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149b7c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149b7c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149b7ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149b7cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149b7cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149b7d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149b7d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149b7d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149b7dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149b7dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149b7e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149b7e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149b7e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149b7e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149b7eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149b7ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149b7f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149b7f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149b7f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149b7f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149b7fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149b7fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149b80140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149b80400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149b806c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149b80980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149b80c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149b80f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149b811c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149b81480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149b81740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149b81a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149b81cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149b81f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149b82240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149b82500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149b827c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149b82a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149b82d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149b83000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149b832c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149b83580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149b83840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149b83b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149b83dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149b84080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149b84340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149b84600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149b848c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149b84b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149b84e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149b85100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149b853c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149b85680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149b85940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149b85c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149b85ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149b86180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149b86440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149b86700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149b869c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149b86c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149b86f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149b87200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149b874c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149b87780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149b87a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149b87d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149b87fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149b88280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149b88540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149b88800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149b88ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149b88d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149b89040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149b89300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149b898d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149b89b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149b89e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149b8a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149b8a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149b8a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149b8a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149b8ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149b8aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149b8b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149b8b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149b8b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149b8b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149b8bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149b8bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149b8c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149b8c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149b8c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149b8ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149b8cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149b8cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149b8d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149b8d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149b8d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149b8dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149b8dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149b8e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149b8e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149b8e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149b8e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149b8eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149b8ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149b8f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149b8f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149b8f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149b8fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149b900f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149b90640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149b90b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149b910e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149b91630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149b91b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149b920d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149b92620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149b92b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149b930c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149b93610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149b93b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149b940b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149b94600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149b94b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149b950a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149b955f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149b95b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149b96090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149b965e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149b96b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149b96df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149b970b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149b97370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149b977e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149b97c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149b980c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149b98530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149b989a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149b98e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149b99280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149b996f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149b99b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149b99fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149b9a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149b9a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149b9ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149b9b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149b9be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149b9c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149b9ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149b9cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149b9d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149b9d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149b9e000 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.791s
user	0m0.292s
sys	0m0.311s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4423 (381ad835)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13bf0d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13bf0d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13bf0def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13bf0e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13bf0ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13bf0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13bf0f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13bf0fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13bf10110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13bf10610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13bf10b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13bf11010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13bf11b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13bf122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13bf12af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13bf13210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13bf13930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13bf14050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13bf14770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13bf14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13bf15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13bf15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13bf164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13bf16d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13bf17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13bf17720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13bf17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13bf189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13bf18ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13bf191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13bf19640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13bf19900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13bf1a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13bf1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13bf1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13bf1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13bf1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13bf1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13bf1bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13bf1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13bf1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13bf1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13bf1ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13bf1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13bf1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13bf1dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13bf1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13bf1eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13bf1f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13bf1f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13bf1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13bf20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13bf20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13bf20f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13bf21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13bf21c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13bf220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13bf22380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13bf22990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13bf23180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13bf23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13bf238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13bf23d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13bf24220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13bf246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13bf24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13bf25000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13bf254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13bf25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13bf25de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13bf26280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13bf26720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13bf26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13bf27110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13bf27660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13bf27bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13bf28100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13bf28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13bf28ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13bf290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13bf29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13bf29b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13bf2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13bf2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13bf2ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13bf2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13bf2b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13bf2bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13bf2c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13bf2c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13bf2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13bf2d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13bf2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13bf2db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13bf2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13bf2e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13bf2eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13bf1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13bf2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13bf2f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13bf2fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13bf30200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13bf30750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13bf30ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13bf311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13bf31740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13bf31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13bf321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13bf32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13bf32c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13bf331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13bf33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13bf33c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13bf34110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13bf345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13bf34a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13bf34ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13bf35390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13bf35830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13bf35cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13bf36170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13bf36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13bf36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13bf36f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13bf373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13bf37890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13bf37d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13bf381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13bf38670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13bf38b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13bf38fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13bf39450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13bf398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13bf39d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13bf3a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13bf3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13bf3ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13bf3b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13bf3b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13bf3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13bf3bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13bf3c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13bf3c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13bf3cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13bf3d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13bf3d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13bf3d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13bf3de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13bf3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13bf3e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13bf3ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13bf3f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13bf3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13bf3fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13bf3feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13bf40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13bf407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13bf40c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13bf41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13bf415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13bf41a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13bf41f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13bf423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13bf42850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13bf42cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13bf43190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13bf43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13bf43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13bf43f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13bf44410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13bf448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13bf44d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13bf451f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13bf45690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13bf45b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13bf45fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13bf46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13bf46910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13bf46db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13bf47250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13bf476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13bf47b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13bf48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13bf484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13bf48970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13bf48e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13bf492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13bf49750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13bf49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13bf4a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13bf4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13bf4a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13bf4ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13bf4b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13bf4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13bf4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13bf4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13bf4c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13bf4cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13bf4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13bf4d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13bf4e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13bf4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13bf4e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13bf4ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13bf4f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13bf4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13bf500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13bf50540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13bf509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13bf51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13bf516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13bf51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13bf52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13bf526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13bf52c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13bf53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13bf536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13bf53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13bf54160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13bf546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13bf54c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13bf55150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13bf556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13bf55bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13bf56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13bf56690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13bf56be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13bf57130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13bf57680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13bf57bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13bf58120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13bf58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13bf58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13bf59110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13bf59660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13bf59bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13bf5a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13bf5a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13bf5aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13bf5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13bf5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13bf5bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13bf5c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13bf5c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13bf5cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13bf5d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13bf5d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13bf5db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13bf5e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13bf5e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13bf5eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13bf5f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13bf5f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13bf5fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13bf600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13bf605f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13bf60b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13bf61090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13bf615e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13bf61b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13bf62080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13bf625d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13bf62b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13bf63070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13bf635c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13bf63b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13bf63fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13bf64450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13bf648f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13bf64d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13bf65230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13bf656d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13bf65b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13bf66010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13bf664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13bf66950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13bf66df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13bf67290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13bf67730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13bf67bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13bf68070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13bf685c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13bf68ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13bf69400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13bf69b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13bf6a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13bf6a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13bf6acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13bf6afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13bf6b5c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13be08d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13be091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13be09650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13be09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13be09f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13be0a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13be0a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13be0ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13be0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13be0b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13be0bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13be0c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13be0cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13be0d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13be0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13be0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13be0ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13be0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13be0f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13be10090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13be107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13be10ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13be115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13be11d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13be12430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13be126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13be129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13be12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13be13290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13be13700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13be13c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13be14110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13be14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13be14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13be14cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13be15120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13be15680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13be15b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13be16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13be16580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13be16a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13be16f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13be17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13be17980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13be17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13be182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13be18760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13be18bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13be19040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13be194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13be19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13be19d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13be1a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13be1a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13be1aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13be1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13be1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13be1ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13be1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13be1c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13be1ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13be1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13be1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13be1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13be1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13be1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13be1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13be1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13be1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13be1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13be1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13be1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13be20430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13be20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13be20ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13be21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13be21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13be21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13be22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13be22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13be22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13be23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13be23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13be23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13be243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13be24940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13be24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13be253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13be25930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13be25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13be263d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13be26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13be26e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13be273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13be27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13be27e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13be283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13be28900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13be28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13be293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13be298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13be29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13be2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13be2a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13be2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13be2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13be2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13be2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13be2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13be2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13be2ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13be2d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13be2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13be2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13be2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13be2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13be2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13be2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13be2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13be2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13be2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13be30250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13be306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13be30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13be31030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13be314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13be31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13be31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13be322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13be32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13be32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13be33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13be33530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13be339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13be33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13be34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13be347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13be34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13be350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13be35590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13be35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13be35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13be36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13be36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13be36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13be37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13be375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13be37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13be37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13be383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13be38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13be38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13be391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13be39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13be39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13be39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13be3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13be3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13be3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13be3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13be3b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13be3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13be3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13be3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13be3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13be3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13be3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13be3d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13be3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13be3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13be3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13be3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13be3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13be3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13be3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13be3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13be400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13be40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13be409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13be40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13be41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13be417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13be41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13be42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13be425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13be42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13be42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13be43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13be43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13be43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13be44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13be44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13be44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13be45000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13be45550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13be45aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13be45ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13be462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13be468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13be46ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13be474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13be47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13be48170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13be48430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13be48a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13be49050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13be49840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13be49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13be4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13be4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13be4add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13be4b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13be4b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13be4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13be4c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13be4c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13be4cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13be4d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13be4d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13be4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13be4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13be4e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13be4ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13be4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13be4f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13be4fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13be502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13be50820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13be50d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13be512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13be51810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13be51d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13be522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13be52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13be52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13be532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13be537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13be53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13be54290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13be547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13be54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13be55280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13be557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13be55d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13be56270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13be567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13be56d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13be57260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13be577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13be57d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13be58250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13be587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13be58cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13be59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13be59790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13be59ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13be5a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13be5a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13be5acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13be5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13be5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13be5bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13be5c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13be5c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13be5ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13be5d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13be5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13be5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13be5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13be5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13be5e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13be5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13be5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13be5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13be5fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13be600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13be60590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13be60a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13be60ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13be61370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13be61810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13be61cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13be62200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13be62920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13be63040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13be63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13be63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13be64140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13be64930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13be64bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13be65200 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d0044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d0056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d0063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d007810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d008330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d008ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d0092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d009a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d00a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d00a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d00af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d00b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d00be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d00c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d00cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d00d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d00dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d00dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d00e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d00e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d00e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d00edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d00f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d00f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d00fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d00fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d0102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d010bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d0114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d012200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d0133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d013830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d013ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d014580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d0149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d014e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d0152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d015740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d015bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d016590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d016f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d017370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d0177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d0180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d0189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d0196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d01a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d01a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d01ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d01b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d01b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d01ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d01bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d01c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d01c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d01cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d01d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d01d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d01d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d01ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d01e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d01e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d01eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d01efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d01f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d01f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d01fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d0205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d020a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d020ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d0217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d022080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d0224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d022960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d023240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d023ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d023d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d024200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d024670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d024ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d024f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d0253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d025830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d025ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d026110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d026580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d0269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d026e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d0272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d027740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d027bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d028020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d028490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d028900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d028d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d0291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d029650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d029ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d029f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d02a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d02a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d02ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d02b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d02b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d02b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d02be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d02c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d02c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d02cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d02d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d02d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d02d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d02dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d02e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d02e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d02eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d02ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d02f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d02f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d02fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d0300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d030540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d0309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d031290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d031700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d031b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d032450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d0328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d0331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d033610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d033a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d034360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d0347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d0350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d035520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d035990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d035e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d036270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d0366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d036b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d036fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d037430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d0378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d037d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d038180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d0385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d038ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d0397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d039c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d03a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d03a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d03a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d03ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d03b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d03b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d03bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d03bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d03c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d03c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d03ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d03d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d03d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d03da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d03deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d03e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d03e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d03ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d03f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d03f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d03f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d03fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d040230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d0406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d040b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d040f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d041b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d041dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d042080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d0424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d042960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d042dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d043240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d0436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d043b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d044400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d044870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d044ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d045150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d0455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d045a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d045ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d046310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d046780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d046bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d047060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d0474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d047940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d047db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d048220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d048b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d0493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d049850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d049cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d04a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d04a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d04aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d04ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d04b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d04b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d04bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d04c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d04c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d04c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d04cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d04d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d04d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d04dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d04df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d04e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d04e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d04eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d04f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d04f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d04f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d04fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d0502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d050740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d051020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d051490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d051900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d051d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d0521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d052650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d052ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d052f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d0533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d053810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d053c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d0540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d054560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d0549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d054e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d0552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d055720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d056190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d0568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d056fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d0576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d0579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d057e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d058420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d058a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.916s
user	0m0.243s
sys	0m0.134s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.26 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.15 user         0.04 sys
```
