### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.83 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.41 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.34 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.61 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.29 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.16 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.19 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  173.86 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.89 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.95 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.25 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.21 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 217.12 sec*proc (27 tests)

Total Test time (real) = 217.13 sec

real	3m37.162s
user	7m25.626s
sys	0m6.148s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.90 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.16 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   28.31 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.28 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.01 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.22 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.14 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  49.83 sec*proc (27 tests)

Total Test time (real) =  49.84 sec

real	0m49.845s
user	1m9.977s
sys	0m4.921s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.148 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.541 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.574 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.585 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.586 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.587 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.588 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.590 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.590 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.591 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.592 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.593 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.597 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.598 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.599 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.599 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.600 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.601 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.601 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.012 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.401 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.403 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.404 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.404 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.405 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.033.405 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.406 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.033.406 I llama_model_loader: - type  f32:  124 tensors
0.00.033.407 I llama_model_loader: - type  f16:   73 tensors
0.00.037.839 I llm_load_vocab: special tokens cache size = 5
0.00.040.264 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.040.268 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.040.269 I llm_load_print_meta: arch             = bert
0.00.040.270 I llm_load_print_meta: vocab type       = WPM
0.00.040.270 I llm_load_print_meta: n_vocab          = 30522
0.00.040.270 I llm_load_print_meta: n_merges         = 0
0.00.040.271 I llm_load_print_meta: vocab_only       = 0
0.00.040.271 I llm_load_print_meta: n_ctx_train      = 512
0.00.040.274 I llm_load_print_meta: n_embd           = 384
0.00.040.275 I llm_load_print_meta: n_layer          = 12
0.00.040.278 I llm_load_print_meta: n_head           = 12
0.00.040.279 I llm_load_print_meta: n_head_kv        = 12
0.00.040.279 I llm_load_print_meta: n_rot            = 32
0.00.040.280 I llm_load_print_meta: n_swa            = 0
0.00.040.280 I llm_load_print_meta: n_embd_head_k    = 32
0.00.040.280 I llm_load_print_meta: n_embd_head_v    = 32
0.00.040.281 I llm_load_print_meta: n_gqa            = 1
0.00.040.282 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.040.283 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.040.284 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.040.285 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.040.285 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.040.286 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.040.292 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.040.293 I llm_load_print_meta: n_ff             = 1536
0.00.040.293 I llm_load_print_meta: n_expert         = 0
0.00.040.294 I llm_load_print_meta: n_expert_used    = 0
0.00.040.294 I llm_load_print_meta: causal attn      = 0
0.00.040.294 I llm_load_print_meta: pooling type     = 2
0.00.040.294 I llm_load_print_meta: rope type        = 2
0.00.040.295 I llm_load_print_meta: rope scaling     = linear
0.00.040.295 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.040.296 I llm_load_print_meta: freq_scale_train = 1
0.00.040.296 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.040.296 I llm_load_print_meta: rope_finetuned   = unknown
0.00.040.297 I llm_load_print_meta: ssm_d_conv       = 0
0.00.040.297 I llm_load_print_meta: ssm_d_inner      = 0
0.00.040.297 I llm_load_print_meta: ssm_d_state      = 0
0.00.040.299 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.040.300 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.040.309 I llm_load_print_meta: model type       = 33M
0.00.040.310 I llm_load_print_meta: model ftype      = F16
0.00.040.310 I llm_load_print_meta: model params     = 33.21 M
0.00.040.311 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.040.311 I llm_load_print_meta: general.name     = Bge Small
0.00.040.313 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.040.314 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.040.314 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.040.314 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.040.315 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.040.315 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.040.316 I llm_load_print_meta: max token length = 21
0.00.042.488 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.042.489 I llm_load_tensors: offloading output layer to GPU
0.00.042.490 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.042.510 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.042.512 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.043.098 I llama_new_context_with_model: n_seq_max     = 1
0.00.043.099 I llama_new_context_with_model: n_ctx         = 512
0.00.043.100 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.043.100 I llama_new_context_with_model: n_batch       = 2048
0.00.043.100 I llama_new_context_with_model: n_ubatch      = 2048
0.00.043.100 I llama_new_context_with_model: flash_attn    = 0
0.00.043.101 I llama_new_context_with_model: freq_base     = 10000.0
0.00.043.101 I llama_new_context_with_model: freq_scale    = 1
0.00.043.102 I ggml_metal_init: allocating
0.00.043.109 I ggml_metal_init: found device: Apple M4
0.00.043.112 I ggml_metal_init: picking default device: Apple M4
0.00.043.949 I ggml_metal_init: using embedded metal library
0.00.047.650 I ggml_metal_init: GPU name:   Apple M4
0.00.047.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.047.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.047.654 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.047.654 I ggml_metal_init: simdgroup reduction   = true
0.00.047.654 I ggml_metal_init: simdgroup matrix mul. = true
0.00.047.654 I ggml_metal_init: has bfloat            = true
0.00.047.655 I ggml_metal_init: use bfloat            = true
0.00.047.655 I ggml_metal_init: hasUnifiedMemory      = true
0.00.047.656 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.059.682 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.684 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.686 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.060.652 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.060.654 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.060.654 I llama_new_context_with_model: graph nodes  = 429
0.00.060.655 I llama_new_context_with_model: graph splits = 2
0.00.060.679 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.068.304 I 
0.00.068.323 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.069.145 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.074.560 I llama_perf_context_print:        load time =      46.76 ms
0.00.074.561 I llama_perf_context_print: prompt eval time =       5.26 ms /     9 tokens (    0.58 ms per token,  1711.68 tokens per second)
0.00.074.562 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.074.562 I llama_perf_context_print:       total time =       6.26 ms /    10 tokens
0.00.074.709 I ggml_metal_free: deallocating

real	0m0.289s
user	0m0.053s
sys	0m0.036s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.030 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.034 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.037 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.039 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.039 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.039 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.040 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.040 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.041 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.041 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.041 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.042 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.042 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.045 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.045 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.046 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.046 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.046 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.046 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.047 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.504 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.172 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.173 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.174 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.174 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.174 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.175 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.175 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.175 I llama_model_loader: - type  f32:  124 tensors
0.00.014.175 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.620 I llm_load_vocab: special tokens cache size = 5
0.00.017.904 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.906 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.907 I llm_load_print_meta: arch             = bert
0.00.017.907 I llm_load_print_meta: vocab type       = WPM
0.00.017.907 I llm_load_print_meta: n_vocab          = 30522
0.00.017.907 I llm_load_print_meta: n_merges         = 0
0.00.017.908 I llm_load_print_meta: vocab_only       = 0
0.00.017.908 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.908 I llm_load_print_meta: n_embd           = 384
0.00.017.908 I llm_load_print_meta: n_layer          = 12
0.00.017.910 I llm_load_print_meta: n_head           = 12
0.00.017.910 I llm_load_print_meta: n_head_kv        = 12
0.00.017.910 I llm_load_print_meta: n_rot            = 32
0.00.017.911 I llm_load_print_meta: n_swa            = 0
0.00.017.911 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.911 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.912 I llm_load_print_meta: n_gqa            = 1
0.00.017.912 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.913 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.914 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.914 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.914 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.914 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.916 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.917 I llm_load_print_meta: n_ff             = 1536
0.00.017.917 I llm_load_print_meta: n_expert         = 0
0.00.017.917 I llm_load_print_meta: n_expert_used    = 0
0.00.017.917 I llm_load_print_meta: causal attn      = 0
0.00.017.917 I llm_load_print_meta: pooling type     = 2
0.00.017.917 I llm_load_print_meta: rope type        = 2
0.00.017.918 I llm_load_print_meta: rope scaling     = linear
0.00.017.918 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.919 I llm_load_print_meta: freq_scale_train = 1
0.00.017.919 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.919 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.919 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.920 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.920 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.920 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.926 I llm_load_print_meta: model type       = 33M
0.00.017.926 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.926 I llm_load_print_meta: model params     = 33.21 M
0.00.017.927 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.927 I llm_load_print_meta: general.name     = Bge Small
0.00.017.927 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.928 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.929 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.929 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.929 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.929 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.929 I llm_load_print_meta: max token length = 21
0.00.019.071 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.071 I llm_load_tensors: offloading output layer to GPU
0.00.019.072 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.079 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.080 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.420 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.421 I llama_new_context_with_model: n_ctx         = 512
0.00.019.421 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.422 I llama_new_context_with_model: n_batch       = 2048
0.00.019.422 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.422 I llama_new_context_with_model: flash_attn    = 0
0.00.019.422 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.423 I llama_new_context_with_model: freq_scale    = 1
0.00.019.423 I ggml_metal_init: allocating
0.00.019.426 I ggml_metal_init: found device: Apple M4
0.00.019.428 I ggml_metal_init: picking default device: Apple M4
0.00.019.927 I ggml_metal_init: using embedded metal library
0.00.022.001 I ggml_metal_init: GPU name:   Apple M4
0.00.022.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.004 I ggml_metal_init: simdgroup reduction   = true
0.00.022.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.004 I ggml_metal_init: has bfloat            = true
0.00.022.005 I ggml_metal_init: use bfloat            = true
0.00.022.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.006 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.011 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.031.013 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.031.015 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.031.622 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.031.623 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.031.623 I llama_new_context_with_model: graph nodes  = 429
0.00.031.623 I llama_new_context_with_model: graph splits = 2
0.00.031.636 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.036.052 I 
0.00.036.069 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.036.572 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.040.966 I llama_perf_context_print:        load time =      27.02 ms
0.00.040.967 I llama_perf_context_print: prompt eval time =       4.25 ms /     9 tokens (    0.47 ms per token,  2118.64 tokens per second)
0.00.040.968 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.040.968 I llama_perf_context_print:       total time =       4.91 ms /    10 tokens
0.00.041.136 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.191 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.383 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.892 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.899 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.030.900 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.901 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.030.902 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.030.902 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.030.903 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.030.904 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.030.905 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.030.906 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.030.906 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.030.909 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.910 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.911 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.030.911 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.912 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.039.087 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.041.528 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.435 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.046.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.437 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.046.438 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.046.438 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.046.438 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.046.439 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.046.439 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.046.439 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.046.440 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.046.440 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.046.440 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.046.441 I llama_model_loader: - type  f32:   41 tensors
0.00.046.441 I llama_model_loader: - type  f16:   29 tensors
0.00.064.980 W llm_load_vocab: empty token at index 5
0.00.069.615 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.070.947 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.972 I llm_load_vocab: special tokens cache size = 5
0.00.309.819 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.309.827 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.309.827 I llm_load_print_meta: arch             = jina-bert-v2
0.00.309.827 I llm_load_print_meta: vocab type       = BPE
0.00.309.827 I llm_load_print_meta: n_vocab          = 61056
0.00.309.827 I llm_load_print_meta: n_merges         = 39382
0.00.309.828 I llm_load_print_meta: vocab_only       = 0
0.00.309.828 I llm_load_print_meta: n_ctx_train      = 8192
0.00.309.828 I llm_load_print_meta: n_embd           = 384
0.00.309.828 I llm_load_print_meta: n_layer          = 4
0.00.309.837 I llm_load_print_meta: n_head           = 12
0.00.309.838 I llm_load_print_meta: n_head_kv        = 12
0.00.309.838 I llm_load_print_meta: n_rot            = 32
0.00.309.838 I llm_load_print_meta: n_swa            = 0
0.00.309.838 I llm_load_print_meta: n_embd_head_k    = 32
0.00.309.838 I llm_load_print_meta: n_embd_head_v    = 32
0.00.309.839 I llm_load_print_meta: n_gqa            = 1
0.00.309.840 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.309.840 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.309.842 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.309.842 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.309.842 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.309.846 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.309.846 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.309.847 I llm_load_print_meta: n_ff             = 1536
0.00.309.847 I llm_load_print_meta: n_expert         = 0
0.00.309.847 I llm_load_print_meta: n_expert_used    = 0
0.00.309.847 I llm_load_print_meta: causal attn      = 0
0.00.309.848 I llm_load_print_meta: pooling type     = -1
0.00.309.848 I llm_load_print_meta: rope type        = -1
0.00.309.848 I llm_load_print_meta: rope scaling     = linear
0.00.309.848 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.309.848 I llm_load_print_meta: freq_scale_train = 1
0.00.309.849 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.309.849 I llm_load_print_meta: rope_finetuned   = unknown
0.00.309.849 I llm_load_print_meta: ssm_d_conv       = 0
0.00.309.849 I llm_load_print_meta: ssm_d_inner      = 0
0.00.309.849 I llm_load_print_meta: ssm_d_state      = 0
0.00.309.849 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.309.849 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.309.879 I llm_load_print_meta: model type       = 33M
0.00.309.880 I llm_load_print_meta: model ftype      = F16
0.00.309.880 I llm_load_print_meta: model params     = 32.90 M
0.00.309.881 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.309.881 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.309.881 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.309.882 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.309.882 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.309.882 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.309.883 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.309.883 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.309.883 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.309.883 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.309.883 I llm_load_print_meta: max token length = 45
0.00.311.065 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.311.065 I llm_load_tensors: offloading output layer to GPU
0.00.311.065 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.311.088 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.311.089 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.312.105 I llama_new_context_with_model: n_seq_max     = 1
0.00.312.106 I llama_new_context_with_model: n_ctx         = 8192
0.00.312.107 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.312.107 I llama_new_context_with_model: n_batch       = 2048
0.00.312.107 I llama_new_context_with_model: n_ubatch      = 2048
0.00.312.107 I llama_new_context_with_model: flash_attn    = 0
0.00.312.107 I llama_new_context_with_model: freq_base     = 10000.0
0.00.312.108 I llama_new_context_with_model: freq_scale    = 1
0.00.312.108 I ggml_metal_init: allocating
0.00.312.111 I ggml_metal_init: found device: Apple M4
0.00.312.113 I ggml_metal_init: picking default device: Apple M4
0.00.313.108 I ggml_metal_init: using embedded metal library
0.00.315.454 I ggml_metal_init: GPU name:   Apple M4
0.00.315.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.315.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.315.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.315.457 I ggml_metal_init: simdgroup reduction   = true
0.00.315.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.315.457 I ggml_metal_init: has bfloat            = true
0.00.315.457 I ggml_metal_init: use bfloat            = true
0.00.315.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.315.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.325.857 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.325.859 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.325.860 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.326.474 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.326.475 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.326.476 I llama_new_context_with_model: graph nodes  = 154
0.00.326.476 I llama_new_context_with_model: graph splits = 2
0.00.326.494 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.337.522 I 
0.00.337.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.337.692 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.337.693 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.337.696 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.337.696 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.337.700 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.337.700 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.338.262 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.342.018 I llama_perf_context_print:        load time =     317.13 ms
0.00.342.019 I llama_perf_context_print: prompt eval time =       3.75 ms /    62 tokens (    0.06 ms per token, 16537.74 tokens per second)
0.00.342.020 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.342.020 I llama_perf_context_print:       total time =       4.50 ms /    63 tokens
0.00.342.271 I ggml_metal_free: deallocating

real	0m1.025s
user	0m0.319s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.156 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.274 I main: llama backend init
0.00.000.293 I main: load the model and apply lora adapter, if any
0.00.055.255 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.066.652 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.066.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.066.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.066.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.066.679 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.066.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.066.680 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.066.682 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.066.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.066.683 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.066.684 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.066.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.066.685 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.066.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.066.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.066.692 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.066.692 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.073.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.076.613 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.085.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.554 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.555 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.556 I llama_model_loader: - type  f32:  194 tensors
0.00.085.557 I llama_model_loader: - type  f16:   98 tensors
0.00.118.495 I llm_load_vocab: special tokens cache size = 25
0.00.125.995 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.125.998 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.125.999 I llm_load_print_meta: arch             = gptneox
0.00.125.999 I llm_load_print_meta: vocab type       = BPE
0.00.125.999 I llm_load_print_meta: n_vocab          = 50304
0.00.125.999 I llm_load_print_meta: n_merges         = 50009
0.00.126.000 I llm_load_print_meta: vocab_only       = 0
0.00.126.000 I llm_load_print_meta: n_ctx_train      = 2048
0.00.126.000 I llm_load_print_meta: n_embd           = 2048
0.00.126.000 I llm_load_print_meta: n_layer          = 24
0.00.126.003 I llm_load_print_meta: n_head           = 16
0.00.126.004 I llm_load_print_meta: n_head_kv        = 16
0.00.126.004 I llm_load_print_meta: n_rot            = 32
0.00.126.004 I llm_load_print_meta: n_swa            = 0
0.00.126.004 I llm_load_print_meta: n_embd_head_k    = 128
0.00.126.006 I llm_load_print_meta: n_embd_head_v    = 128
0.00.126.007 I llm_load_print_meta: n_gqa            = 1
0.00.126.008 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.126.008 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.126.009 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.126.010 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.126.010 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.126.010 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.126.010 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.126.011 I llm_load_print_meta: n_ff             = 8192
0.00.126.011 I llm_load_print_meta: n_expert         = 0
0.00.126.011 I llm_load_print_meta: n_expert_used    = 0
0.00.126.011 I llm_load_print_meta: causal attn      = 1
0.00.126.012 I llm_load_print_meta: pooling type     = 0
0.00.126.012 I llm_load_print_meta: rope type        = 2
0.00.126.012 I llm_load_print_meta: rope scaling     = linear
0.00.126.012 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.126.013 I llm_load_print_meta: freq_scale_train = 1
0.00.126.013 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.126.013 I llm_load_print_meta: rope_finetuned   = unknown
0.00.126.013 I llm_load_print_meta: ssm_d_conv       = 0
0.00.126.013 I llm_load_print_meta: ssm_d_inner      = 0
0.00.126.013 I llm_load_print_meta: ssm_d_state      = 0
0.00.126.014 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.126.014 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.126.026 I llm_load_print_meta: model type       = 1.4B
0.00.126.026 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.126.027 I llm_load_print_meta: model params     = 1.41 B
0.00.126.028 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.126.028 I llm_load_print_meta: general.name     = 1.4B
0.00.126.029 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.126.029 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.126.029 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.126.029 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.126.029 I llm_load_print_meta: LF token         = 128 ''
0.00.126.030 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.126.030 I llm_load_print_meta: max token length = 1024
0.00.128.599 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.128.599 I llm_load_tensors: offloading output layer to GPU
0.00.128.599 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.128.616 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.128.617 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.129.611 I llama_new_context_with_model: n_seq_max     = 1
0.00.129.612 I llama_new_context_with_model: n_ctx         = 2048
0.00.129.612 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.129.612 I llama_new_context_with_model: n_batch       = 2048
0.00.129.613 I llama_new_context_with_model: n_ubatch      = 512
0.00.129.613 I llama_new_context_with_model: flash_attn    = 0
0.00.129.613 I llama_new_context_with_model: freq_base     = 10000.0
0.00.129.614 I llama_new_context_with_model: freq_scale    = 1
0.00.129.614 I ggml_metal_init: allocating
0.00.129.617 I ggml_metal_init: found device: Apple M4
0.00.129.619 I ggml_metal_init: picking default device: Apple M4
0.00.130.252 I ggml_metal_init: using embedded metal library
0.00.137.688 I ggml_metal_init: GPU name:   Apple M4
0.00.137.690 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.137.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.137.691 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.137.691 I ggml_metal_init: simdgroup reduction   = true
0.00.137.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.137.692 I ggml_metal_init: has bfloat            = true
0.00.137.692 I ggml_metal_init: use bfloat            = true
0.00.137.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.137.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.174.078 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.174.083 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.174.100 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.174.997 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.174.998 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.174.999 I llama_new_context_with_model: graph nodes  = 967
0.00.174.999 I llama_new_context_with_model: graph splits = 2
0.00.175.035 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.254.238 I main: llama threadpool init, n_threads = 4
0.00.254.270 I 
0.00.254.294 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.254.294 I 
0.00.254.367 I sampler seed: 1234
0.00.254.372 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.254.396 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.254.398 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.254.398 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.116.784 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.02.116.785 I llama_perf_context_print:        load time =     198.97 ms
0.02.116.786 I llama_perf_context_print: prompt eval time =      39.16 ms /     7 tokens (    5.59 ms per token,   178.74 tokens per second)
0.02.116.787 I llama_perf_context_print:        eval time =    1820.20 ms /    63 runs   (   28.89 ms per token,    34.61 tokens per second)
0.02.116.787 I llama_perf_context_print:       total time =    1862.55 ms /    70 tokens
0.02.116.961 I ggml_metal_free: deallocating

real	0m2.452s
user	0m0.147s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.803 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.805 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.697 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.711 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.716 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.403 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.656 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.690 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.692 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.693 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.693 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.694 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.695 I llama_model_loader: - type  f32:  194 tensors
0.00.054.695 I llama_model_loader: - type  f16:   98 tensors
0.00.084.483 I llm_load_vocab: special tokens cache size = 25
0.00.091.051 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.053 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.054 I llm_load_print_meta: arch             = gptneox
0.00.091.054 I llm_load_print_meta: vocab type       = BPE
0.00.091.054 I llm_load_print_meta: n_vocab          = 50304
0.00.091.054 I llm_load_print_meta: n_merges         = 50009
0.00.091.055 I llm_load_print_meta: vocab_only       = 0
0.00.091.055 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.055 I llm_load_print_meta: n_embd           = 2048
0.00.091.055 I llm_load_print_meta: n_layer          = 24
0.00.091.058 I llm_load_print_meta: n_head           = 16
0.00.091.058 I llm_load_print_meta: n_head_kv        = 16
0.00.091.059 I llm_load_print_meta: n_rot            = 32
0.00.091.059 I llm_load_print_meta: n_swa            = 0
0.00.091.059 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.059 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.060 I llm_load_print_meta: n_gqa            = 1
0.00.091.060 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.061 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.061 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.062 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.062 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.062 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.062 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.063 I llm_load_print_meta: n_ff             = 8192
0.00.091.063 I llm_load_print_meta: n_expert         = 0
0.00.091.063 I llm_load_print_meta: n_expert_used    = 0
0.00.091.063 I llm_load_print_meta: causal attn      = 1
0.00.091.063 I llm_load_print_meta: pooling type     = 0
0.00.091.063 I llm_load_print_meta: rope type        = 2
0.00.091.064 I llm_load_print_meta: rope scaling     = linear
0.00.091.064 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.064 I llm_load_print_meta: freq_scale_train = 1
0.00.091.064 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.065 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.065 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.065 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.065 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.065 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.065 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.077 I llm_load_print_meta: model type       = 1.4B
0.00.091.077 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.078 I llm_load_print_meta: model params     = 1.41 B
0.00.091.078 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.078 I llm_load_print_meta: general.name     = 1.4B
0.00.091.078 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.079 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.079 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.079 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.079 I llm_load_print_meta: LF token         = 128 ''
0.00.091.080 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.080 I llm_load_print_meta: max token length = 1024
0.00.093.722 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.722 I llm_load_tensors: offloading output layer to GPU
0.00.093.723 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.733 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.734 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.714 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.715 I llama_new_context_with_model: n_ctx         = 128
0.00.094.715 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.715 I llama_new_context_with_model: n_batch       = 128
0.00.094.715 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.715 I llama_new_context_with_model: flash_attn    = 0
0.00.094.716 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.716 I llama_new_context_with_model: freq_scale    = 1
0.00.094.717 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.717 I ggml_metal_init: allocating
0.00.094.724 I ggml_metal_init: found device: Apple M4
0.00.094.727 I ggml_metal_init: picking default device: Apple M4
0.00.095.286 I ggml_metal_init: using embedded metal library
0.00.097.365 I ggml_metal_init: GPU name:   Apple M4
0.00.097.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.367 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.367 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.368 I ggml_metal_init: simdgroup reduction   = true
0.00.097.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.368 I ggml_metal_init: has bfloat            = true
0.00.097.368 I ggml_metal_init: use bfloat            = true
0.00.097.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.575 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.591 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.506 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.507 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.507 I llama_new_context_with_model: graph nodes  = 967
0.00.107.508 I llama_new_context_with_model: graph splits = 2
0.00.107.520 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.911.900 I 
0.00.911.924 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.911.928 I perplexity: tokenizing the input ..
0.00.924.736 I perplexity: tokenization took 12.805 ms
0.00.924.775 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.045.702 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.047.667 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.047.685 I llama_perf_context_print:        load time =     887.08 ms
0.01.047.687 I llama_perf_context_print: prompt eval time =     120.03 ms /   128 tokens (    0.94 ms per token,  1066.39 tokens per second)
0.01.047.692 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.047.693 I llama_perf_context_print:       total time =     135.78 ms /   129 tokens
0.01.048.284 I ggml_metal_free: deallocating

real	0m1.247s
user	0m0.125s
sys	0m0.199s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.870 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.464 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.476 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.479 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.481 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.272 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.372 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.373 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.373 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.374 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.374 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.374 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.375 I llama_model_loader: - type  f32:  194 tensors
0.00.033.375 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.509 I llm_load_vocab: special tokens cache size = 25
0.00.060.645 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.649 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.650 I llm_load_print_meta: arch             = gptneox
0.00.060.650 I llm_load_print_meta: vocab type       = BPE
0.00.060.652 I llm_load_print_meta: n_vocab          = 50304
0.00.060.652 I llm_load_print_meta: n_merges         = 50009
0.00.060.652 I llm_load_print_meta: vocab_only       = 0
0.00.060.653 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.653 I llm_load_print_meta: n_embd           = 2048
0.00.060.653 I llm_load_print_meta: n_layer          = 24
0.00.060.659 I llm_load_print_meta: n_head           = 16
0.00.060.660 I llm_load_print_meta: n_head_kv        = 16
0.00.060.660 I llm_load_print_meta: n_rot            = 32
0.00.060.660 I llm_load_print_meta: n_swa            = 0
0.00.060.660 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.660 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.661 I llm_load_print_meta: n_gqa            = 1
0.00.060.664 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.665 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.666 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.666 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.667 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.667 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.667 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.668 I llm_load_print_meta: n_ff             = 8192
0.00.060.668 I llm_load_print_meta: n_expert         = 0
0.00.060.668 I llm_load_print_meta: n_expert_used    = 0
0.00.060.668 I llm_load_print_meta: causal attn      = 1
0.00.060.668 I llm_load_print_meta: pooling type     = 0
0.00.060.669 I llm_load_print_meta: rope type        = 2
0.00.060.669 I llm_load_print_meta: rope scaling     = linear
0.00.060.669 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.670 I llm_load_print_meta: freq_scale_train = 1
0.00.060.670 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.670 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.672 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.672 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.672 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.672 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.672 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.681 I llm_load_print_meta: model type       = 1.4B
0.00.060.682 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.682 I llm_load_print_meta: model params     = 1.41 B
0.00.060.683 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.683 I llm_load_print_meta: general.name     = 1.4B
0.00.060.683 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.683 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.684 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.684 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.684 I llm_load_print_meta: LF token         = 128 ''
0.00.060.684 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.684 I llm_load_print_meta: max token length = 1024
0.00.062.736 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.737 I llm_load_tensors: offloading output layer to GPU
0.00.062.737 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.742 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.743 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.718 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.719 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.719 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.719 I llama_new_context_with_model: n_batch       = 2048
0.00.063.720 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.720 I llama_new_context_with_model: flash_attn    = 0
0.00.063.720 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.720 I llama_new_context_with_model: freq_scale    = 1
0.00.063.721 I ggml_metal_init: allocating
0.00.063.724 I ggml_metal_init: found device: Apple M4
0.00.063.728 I ggml_metal_init: picking default device: Apple M4
0.00.064.399 I ggml_metal_init: using embedded metal library
0.00.066.505 I ggml_metal_init: GPU name:   Apple M4
0.00.066.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.508 I ggml_metal_init: simdgroup reduction   = true
0.00.066.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.508 I ggml_metal_init: has bfloat            = true
0.00.066.508 I ggml_metal_init: use bfloat            = true
0.00.066.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.509 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.712 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.719 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.740 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.811 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.100.812 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.100.813 I llama_new_context_with_model: graph nodes  = 967
0.00.100.813 I llama_new_context_with_model: graph splits = 2
0.00.100.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.386.811 I main: llama threadpool init, n_threads = 4
0.01.386.840 I 
0.01.386.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.01.386.861 I 
0.01.386.994 I sampler seed: 1234
0.01.386.999 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.387.035 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.387.036 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.387.036 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.481.520 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.02.481.521 I llama_perf_context_print:        load time =    1376.94 ms
0.02.481.522 I llama_perf_context_print: prompt eval time =      37.20 ms /     7 tokens (    5.31 ms per token,   188.15 tokens per second)
0.02.481.523 I llama_perf_context_print:        eval time =    1054.32 ms /    63 runs   (   16.74 ms per token,    59.75 tokens per second)
0.02.481.523 I llama_perf_context_print:       total time =    1094.71 ms /    70 tokens
0.02.481.709 I ggml_metal_free: deallocating

real	0m2.501s
user	0m0.112s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.138 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.497 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.231 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.236 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.238 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.243 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.244 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.244 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.245 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.248 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.249 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.792 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.792 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.793 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.793 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.793 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.794 I llama_model_loader: - type  f32:  194 tensors
0.00.029.794 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.864 I llm_load_vocab: special tokens cache size = 25
0.00.059.935 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.938 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.938 I llm_load_print_meta: arch             = gptneox
0.00.059.939 I llm_load_print_meta: vocab type       = BPE
0.00.059.939 I llm_load_print_meta: n_vocab          = 50304
0.00.059.939 I llm_load_print_meta: n_merges         = 50009
0.00.059.939 I llm_load_print_meta: vocab_only       = 0
0.00.059.939 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.939 I llm_load_print_meta: n_embd           = 2048
0.00.059.940 I llm_load_print_meta: n_layer          = 24
0.00.059.942 I llm_load_print_meta: n_head           = 16
0.00.059.942 I llm_load_print_meta: n_head_kv        = 16
0.00.059.943 I llm_load_print_meta: n_rot            = 32
0.00.059.943 I llm_load_print_meta: n_swa            = 0
0.00.059.943 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.943 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.947 I llm_load_print_meta: n_gqa            = 1
0.00.059.947 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.948 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.949 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.949 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.949 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.949 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.950 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.950 I llm_load_print_meta: n_ff             = 8192
0.00.059.950 I llm_load_print_meta: n_expert         = 0
0.00.059.951 I llm_load_print_meta: n_expert_used    = 0
0.00.059.951 I llm_load_print_meta: causal attn      = 1
0.00.059.952 I llm_load_print_meta: pooling type     = 0
0.00.059.952 I llm_load_print_meta: rope type        = 2
0.00.059.952 I llm_load_print_meta: rope scaling     = linear
0.00.059.952 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.953 I llm_load_print_meta: freq_scale_train = 1
0.00.059.953 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.953 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.953 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.953 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.954 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.954 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.954 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.966 I llm_load_print_meta: model type       = 1.4B
0.00.059.966 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.967 I llm_load_print_meta: model params     = 1.41 B
0.00.059.967 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.967 I llm_load_print_meta: general.name     = 1.4B
0.00.059.967 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.968 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.968 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.968 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.968 I llm_load_print_meta: LF token         = 128 ''
0.00.059.969 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.969 I llm_load_print_meta: max token length = 1024
0.00.062.157 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.158 I llm_load_tensors: offloading output layer to GPU
0.00.062.158 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.168 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.169 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.106 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.107 I llama_new_context_with_model: n_ctx         = 128
0.00.063.107 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.107 I llama_new_context_with_model: n_batch       = 128
0.00.063.107 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.107 I llama_new_context_with_model: flash_attn    = 0
0.00.063.108 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.108 I llama_new_context_with_model: freq_scale    = 1
0.00.063.108 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.109 I ggml_metal_init: allocating
0.00.063.117 I ggml_metal_init: found device: Apple M4
0.00.063.119 I ggml_metal_init: picking default device: Apple M4
0.00.063.692 I ggml_metal_init: using embedded metal library
0.00.066.581 I ggml_metal_init: GPU name:   Apple M4
0.00.066.583 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.583 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.583 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.584 I ggml_metal_init: simdgroup reduction   = true
0.00.066.584 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.584 I ggml_metal_init: has bfloat            = true
0.00.066.584 I ggml_metal_init: use bfloat            = true
0.00.066.585 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.585 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.538 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.544 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.562 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.474 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.475 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.475 I llama_new_context_with_model: graph nodes  = 967
0.00.076.475 I llama_new_context_with_model: graph splits = 2
0.00.076.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.957.252 I 
0.00.957.272 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.957.275 I perplexity: tokenizing the input ..
0.00.965.402 I perplexity: tokenization took 8.125 ms
0.00.965.416 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.087.430 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.088.673 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.088.688 I llama_perf_context_print:        load time =     946.75 ms
0.01.088.689 I llama_perf_context_print: prompt eval time =     121.79 ms /   128 tokens (    0.95 ms per token,  1050.99 tokens per second)
0.01.088.690 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.088.690 I llama_perf_context_print:       total time =     131.44 ms /   129 tokens
0.01.089.011 I ggml_metal_free: deallocating

real	0m1.106s
user	0m0.087s
sys	0m0.163s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.836 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.381 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.385 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.389 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.390 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.391 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.391 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.392 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.392 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.393 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.292 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.420 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.432 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.434 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.434 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.434 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.435 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.435 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.436 I llama_model_loader: - type  f32:  194 tensors
0.00.026.436 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.436 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.408 I llm_load_vocab: special tokens cache size = 25
0.00.053.563 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.566 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.566 I llm_load_print_meta: arch             = gptneox
0.00.053.567 I llm_load_print_meta: vocab type       = BPE
0.00.053.567 I llm_load_print_meta: n_vocab          = 50304
0.00.053.567 I llm_load_print_meta: n_merges         = 50009
0.00.053.568 I llm_load_print_meta: vocab_only       = 0
0.00.053.568 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.568 I llm_load_print_meta: n_embd           = 2048
0.00.053.568 I llm_load_print_meta: n_layer          = 24
0.00.053.573 I llm_load_print_meta: n_head           = 16
0.00.053.576 I llm_load_print_meta: n_head_kv        = 16
0.00.053.577 I llm_load_print_meta: n_rot            = 32
0.00.053.577 I llm_load_print_meta: n_swa            = 0
0.00.053.577 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.577 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.578 I llm_load_print_meta: n_gqa            = 1
0.00.053.579 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.580 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.581 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.581 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.581 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.583 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.583 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.584 I llm_load_print_meta: n_ff             = 8192
0.00.053.584 I llm_load_print_meta: n_expert         = 0
0.00.053.584 I llm_load_print_meta: n_expert_used    = 0
0.00.053.584 I llm_load_print_meta: causal attn      = 1
0.00.053.584 I llm_load_print_meta: pooling type     = 0
0.00.053.584 I llm_load_print_meta: rope type        = 2
0.00.053.585 I llm_load_print_meta: rope scaling     = linear
0.00.053.585 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.585 I llm_load_print_meta: freq_scale_train = 1
0.00.053.586 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.590 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.591 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.591 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.591 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.591 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.591 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.603 I llm_load_print_meta: model type       = 1.4B
0.00.053.604 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.604 I llm_load_print_meta: model params     = 1.41 B
0.00.053.604 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.605 I llm_load_print_meta: general.name     = 1.4B
0.00.053.605 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.605 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.605 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.605 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.606 I llm_load_print_meta: LF token         = 128 ''
0.00.053.606 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.606 I llm_load_print_meta: max token length = 1024
0.00.055.418 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.418 I llm_load_tensors: offloading output layer to GPU
0.00.055.418 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.428 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.429 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.313 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.314 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.314 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.314 I llama_new_context_with_model: n_batch       = 2048
0.00.056.314 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.314 I llama_new_context_with_model: flash_attn    = 0
0.00.056.315 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.315 I llama_new_context_with_model: freq_scale    = 1
0.00.056.316 I ggml_metal_init: allocating
0.00.056.322 I ggml_metal_init: found device: Apple M4
0.00.056.325 I ggml_metal_init: picking default device: Apple M4
0.00.056.987 I ggml_metal_init: using embedded metal library
0.00.059.279 I ggml_metal_init: GPU name:   Apple M4
0.00.059.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.281 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.282 I ggml_metal_init: simdgroup reduction   = true
0.00.059.282 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.282 I ggml_metal_init: has bfloat            = true
0.00.059.282 I ggml_metal_init: use bfloat            = true
0.00.059.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.284 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.851 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.860 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.884 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.084 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.100.086 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.100.086 I llama_new_context_with_model: graph nodes  = 967
0.00.100.086 I llama_new_context_with_model: graph splits = 2
0.00.100.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.852 I main: llama threadpool init, n_threads = 4
0.00.689.892 I 
0.00.689.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.689.932 I 
0.00.690.163 I sampler seed: 1234
0.00.690.169 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.215 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.217 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.217 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.367.062 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.367.063 I llama_perf_context_print:        load time =     679.01 ms
0.01.367.064 I llama_perf_context_print: prompt eval time =      32.76 ms /     7 tokens (    4.68 ms per token,   213.66 tokens per second)
0.01.367.064 I llama_perf_context_print:        eval time =     641.03 ms /    63 runs   (   10.18 ms per token,    98.28 tokens per second)
0.01.367.065 I llama_perf_context_print:       total time =     677.22 ms /    70 tokens
0.01.367.231 I ggml_metal_free: deallocating

real	0m1.387s
user	0m0.112s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.190 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.123 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.124 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.127 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.127 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.127 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.128 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.128 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.128 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.129 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.132 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.132 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.133 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.929 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.034 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.854 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.856 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.856 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.856 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.856 I llama_model_loader: - type  f32:  194 tensors
0.00.025.857 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.857 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.816 I llm_load_vocab: special tokens cache size = 25
0.00.052.959 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.962 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.963 I llm_load_print_meta: arch             = gptneox
0.00.052.963 I llm_load_print_meta: vocab type       = BPE
0.00.052.963 I llm_load_print_meta: n_vocab          = 50304
0.00.052.963 I llm_load_print_meta: n_merges         = 50009
0.00.052.964 I llm_load_print_meta: vocab_only       = 0
0.00.052.964 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.964 I llm_load_print_meta: n_embd           = 2048
0.00.052.964 I llm_load_print_meta: n_layer          = 24
0.00.052.967 I llm_load_print_meta: n_head           = 16
0.00.052.968 I llm_load_print_meta: n_head_kv        = 16
0.00.052.968 I llm_load_print_meta: n_rot            = 32
0.00.052.968 I llm_load_print_meta: n_swa            = 0
0.00.052.968 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.968 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.969 I llm_load_print_meta: n_gqa            = 1
0.00.052.970 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.970 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.971 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.971 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.972 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.972 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.972 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.973 I llm_load_print_meta: n_ff             = 8192
0.00.052.973 I llm_load_print_meta: n_expert         = 0
0.00.052.973 I llm_load_print_meta: n_expert_used    = 0
0.00.052.973 I llm_load_print_meta: causal attn      = 1
0.00.052.973 I llm_load_print_meta: pooling type     = 0
0.00.052.973 I llm_load_print_meta: rope type        = 2
0.00.052.974 I llm_load_print_meta: rope scaling     = linear
0.00.052.974 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.974 I llm_load_print_meta: freq_scale_train = 1
0.00.052.975 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.975 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.975 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.975 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.977 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.977 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.977 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.984 I llm_load_print_meta: model type       = 1.4B
0.00.052.985 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.985 I llm_load_print_meta: model params     = 1.41 B
0.00.052.986 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.986 I llm_load_print_meta: general.name     = 1.4B
0.00.052.986 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.987 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.987 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.987 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.987 I llm_load_print_meta: LF token         = 128 ''
0.00.052.987 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.988 I llm_load_print_meta: max token length = 1024
0.00.054.714 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.714 I llm_load_tensors: offloading output layer to GPU
0.00.054.715 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.720 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.720 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.758 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.758 I llama_new_context_with_model: n_ctx         = 128
0.00.055.758 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.759 I llama_new_context_with_model: n_batch       = 128
0.00.055.759 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.759 I llama_new_context_with_model: flash_attn    = 0
0.00.055.759 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.760 I llama_new_context_with_model: freq_scale    = 1
0.00.055.760 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.760 I ggml_metal_init: allocating
0.00.055.764 I ggml_metal_init: found device: Apple M4
0.00.055.766 I ggml_metal_init: picking default device: Apple M4
0.00.056.296 I ggml_metal_init: using embedded metal library
0.00.058.237 I ggml_metal_init: GPU name:   Apple M4
0.00.058.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.239 I ggml_metal_init: simdgroup reduction   = true
0.00.058.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.240 I ggml_metal_init: has bfloat            = true
0.00.058.240 I ggml_metal_init: use bfloat            = true
0.00.058.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.241 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.317 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.321 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.343 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.199 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.201 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.201 I llama_new_context_with_model: graph nodes  = 967
0.00.068.201 I llama_new_context_with_model: graph splits = 2
0.00.068.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.621 I 
0.00.614.664 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.614.686 I perplexity: tokenizing the input ..
0.00.622.734 I perplexity: tokenization took 8.047 ms
0.00.622.747 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.745.492 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.746.743 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.746.759 I llama_perf_context_print:        load time =     603.42 ms
0.00.746.760 I llama_perf_context_print: prompt eval time =     122.48 ms /   128 tokens (    0.96 ms per token,  1045.04 tokens per second)
0.00.746.761 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.746.761 I llama_perf_context_print:       total time =     132.14 ms /   129 tokens
0.00.747.239 I ggml_metal_free: deallocating

real	0m0.764s
user	0m0.077s
sys	0m0.100s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.419 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.621 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.629 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.630 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.630 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.630 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.631 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.632 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.632 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.633 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.637 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.638 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.638 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.512 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.347 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.348 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.348 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.348 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.349 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.349 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.350 I llama_model_loader: - type  f32:  194 tensors
0.00.025.350 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.350 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.584 I llm_load_vocab: special tokens cache size = 25
0.00.051.667 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.670 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.670 I llm_load_print_meta: arch             = gptneox
0.00.051.670 I llm_load_print_meta: vocab type       = BPE
0.00.051.670 I llm_load_print_meta: n_vocab          = 50304
0.00.051.671 I llm_load_print_meta: n_merges         = 50009
0.00.051.671 I llm_load_print_meta: vocab_only       = 0
0.00.051.671 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.671 I llm_load_print_meta: n_embd           = 2048
0.00.051.671 I llm_load_print_meta: n_layer          = 24
0.00.051.674 I llm_load_print_meta: n_head           = 16
0.00.051.675 I llm_load_print_meta: n_head_kv        = 16
0.00.051.675 I llm_load_print_meta: n_rot            = 32
0.00.051.676 I llm_load_print_meta: n_swa            = 0
0.00.051.676 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.676 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.677 I llm_load_print_meta: n_gqa            = 1
0.00.051.677 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.680 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.681 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.681 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.681 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.681 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.683 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.683 I llm_load_print_meta: n_ff             = 8192
0.00.051.684 I llm_load_print_meta: n_expert         = 0
0.00.051.684 I llm_load_print_meta: n_expert_used    = 0
0.00.051.687 I llm_load_print_meta: causal attn      = 1
0.00.051.687 I llm_load_print_meta: pooling type     = 0
0.00.051.688 I llm_load_print_meta: rope type        = 2
0.00.051.688 I llm_load_print_meta: rope scaling     = linear
0.00.051.689 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.689 I llm_load_print_meta: freq_scale_train = 1
0.00.051.689 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.690 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.690 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.690 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.690 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.690 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.690 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.703 I llm_load_print_meta: model type       = 1.4B
0.00.051.703 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.703 I llm_load_print_meta: model params     = 1.41 B
0.00.051.704 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.704 I llm_load_print_meta: general.name     = 1.4B
0.00.051.704 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.704 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.704 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.705 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.705 I llm_load_print_meta: LF token         = 128 ''
0.00.051.705 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.705 I llm_load_print_meta: max token length = 1024
0.00.053.705 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.705 I llm_load_tensors: offloading output layer to GPU
0.00.053.705 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.715 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.716 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.642 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.643 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.643 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.643 I llama_new_context_with_model: n_batch       = 2048
0.00.054.643 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.643 I llama_new_context_with_model: flash_attn    = 0
0.00.054.644 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.644 I llama_new_context_with_model: freq_scale    = 1
0.00.054.645 I ggml_metal_init: allocating
0.00.054.650 I ggml_metal_init: found device: Apple M4
0.00.054.652 I ggml_metal_init: picking default device: Apple M4
0.00.055.235 I ggml_metal_init: using embedded metal library
0.00.057.231 I ggml_metal_init: GPU name:   Apple M4
0.00.057.233 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.233 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.234 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.234 I ggml_metal_init: simdgroup reduction   = true
0.00.057.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.234 I ggml_metal_init: has bfloat            = true
0.00.057.236 I ggml_metal_init: use bfloat            = true
0.00.057.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.237 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.445 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.451 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.469 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.481 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.483 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.483 I llama_new_context_with_model: graph nodes  = 967
0.00.086.483 I llama_new_context_with_model: graph splits = 2
0.00.086.496 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.189 I main: llama threadpool init, n_threads = 4
0.00.722.224 I 
0.00.722.242 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.722.242 I 
0.00.722.476 I sampler seed: 1234
0.00.722.479 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.722.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.722.492 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.722.492 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.449.460 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64137.31 tokens per second)
0.01.449.460 I llama_perf_context_print:        load time =     711.77 ms
0.01.449.461 I llama_perf_context_print: prompt eval time =      36.30 ms /     7 tokens (    5.19 ms per token,   192.82 tokens per second)
0.01.449.462 I llama_perf_context_print:        eval time =     687.77 ms /    63 runs   (   10.92 ms per token,    91.60 tokens per second)
0.01.449.462 I llama_perf_context_print:       total time =     727.27 ms /    70 tokens
0.01.449.638 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.109s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.947 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.701 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.707 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.708 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.708 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.710 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.711 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.711 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.711 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.712 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.715 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.716 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.616 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.498 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.498 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.498 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.499 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.499 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.500 I llama_model_loader: - type  f32:  194 tensors
0.00.023.500 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.500 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.472 I llm_load_vocab: special tokens cache size = 25
0.00.050.574 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.577 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.577 I llm_load_print_meta: arch             = gptneox
0.00.050.577 I llm_load_print_meta: vocab type       = BPE
0.00.050.578 I llm_load_print_meta: n_vocab          = 50304
0.00.050.578 I llm_load_print_meta: n_merges         = 50009
0.00.050.578 I llm_load_print_meta: vocab_only       = 0
0.00.050.578 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.578 I llm_load_print_meta: n_embd           = 2048
0.00.050.578 I llm_load_print_meta: n_layer          = 24
0.00.050.581 I llm_load_print_meta: n_head           = 16
0.00.050.581 I llm_load_print_meta: n_head_kv        = 16
0.00.050.582 I llm_load_print_meta: n_rot            = 32
0.00.050.582 I llm_load_print_meta: n_swa            = 0
0.00.050.582 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.582 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.584 I llm_load_print_meta: n_gqa            = 1
0.00.050.585 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.586 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.586 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.587 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.587 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.587 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.587 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.588 I llm_load_print_meta: n_ff             = 8192
0.00.050.588 I llm_load_print_meta: n_expert         = 0
0.00.050.588 I llm_load_print_meta: n_expert_used    = 0
0.00.050.588 I llm_load_print_meta: causal attn      = 1
0.00.050.588 I llm_load_print_meta: pooling type     = 0
0.00.050.589 I llm_load_print_meta: rope type        = 2
0.00.050.589 I llm_load_print_meta: rope scaling     = linear
0.00.050.589 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.594 I llm_load_print_meta: freq_scale_train = 1
0.00.050.594 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.595 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.596 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.596 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.596 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.596 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.596 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.608 I llm_load_print_meta: model type       = 1.4B
0.00.050.609 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.609 I llm_load_print_meta: model params     = 1.41 B
0.00.050.609 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.610 I llm_load_print_meta: general.name     = 1.4B
0.00.050.610 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.610 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.610 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.610 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.611 I llm_load_print_meta: LF token         = 128 ''
0.00.050.611 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.611 I llm_load_print_meta: max token length = 1024
0.00.052.617 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.617 I llm_load_tensors: offloading output layer to GPU
0.00.052.617 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.627 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.628 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.551 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.552 I llama_new_context_with_model: n_ctx         = 128
0.00.053.552 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.552 I llama_new_context_with_model: n_batch       = 128
0.00.053.552 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.552 I llama_new_context_with_model: flash_attn    = 0
0.00.053.553 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.553 I llama_new_context_with_model: freq_scale    = 1
0.00.053.553 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.554 I ggml_metal_init: allocating
0.00.053.559 I ggml_metal_init: found device: Apple M4
0.00.053.561 I ggml_metal_init: picking default device: Apple M4
0.00.054.084 I ggml_metal_init: using embedded metal library
0.00.055.990 I ggml_metal_init: GPU name:   Apple M4
0.00.055.991 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.992 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.992 I ggml_metal_init: simdgroup reduction   = true
0.00.055.993 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.993 I ggml_metal_init: has bfloat            = true
0.00.055.993 I ggml_metal_init: use bfloat            = true
0.00.055.993 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.994 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.864 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.869 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.884 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.729 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.730 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.730 I llama_new_context_with_model: graph nodes  = 967
0.00.065.731 I llama_new_context_with_model: graph splits = 2
0.00.065.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.334 I 
0.00.678.351 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.678.358 I perplexity: tokenizing the input ..
0.00.686.095 I perplexity: tokenization took 7.735 ms
0.00.686.106 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.676 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.809.920 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.809.941 I llama_perf_context_print:        load time =     669.38 ms
0.00.809.942 I llama_perf_context_print: prompt eval time =     122.35 ms /   128 tokens (    0.96 ms per token,  1046.20 tokens per second)
0.00.809.943 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.943 I llama_perf_context_print:       total time =     131.61 ms /   129 tokens
0.00.810.356 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.077s
sys	0m0.115s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.122 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.046 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.055 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.056 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.857 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.858 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.858 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.858 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.859 I llama_model_loader: - type  f32:  194 tensors
0.00.024.859 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.860 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.640 I llm_load_vocab: special tokens cache size = 25
0.00.051.573 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.575 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.575 I llm_load_print_meta: arch             = gptneox
0.00.051.576 I llm_load_print_meta: vocab type       = BPE
0.00.051.576 I llm_load_print_meta: n_vocab          = 50304
0.00.051.576 I llm_load_print_meta: n_merges         = 50009
0.00.051.576 I llm_load_print_meta: vocab_only       = 0
0.00.051.576 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.577 I llm_load_print_meta: n_embd           = 2048
0.00.051.577 I llm_load_print_meta: n_layer          = 24
0.00.051.579 I llm_load_print_meta: n_head           = 16
0.00.051.580 I llm_load_print_meta: n_head_kv        = 16
0.00.051.580 I llm_load_print_meta: n_rot            = 32
0.00.051.580 I llm_load_print_meta: n_swa            = 0
0.00.051.580 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.581 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.581 I llm_load_print_meta: n_gqa            = 1
0.00.051.582 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.583 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.586 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.586 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.586 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.586 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.586 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.587 I llm_load_print_meta: n_ff             = 8192
0.00.051.587 I llm_load_print_meta: n_expert         = 0
0.00.051.587 I llm_load_print_meta: n_expert_used    = 0
0.00.051.588 I llm_load_print_meta: causal attn      = 1
0.00.051.589 I llm_load_print_meta: pooling type     = 0
0.00.051.589 I llm_load_print_meta: rope type        = 2
0.00.051.590 I llm_load_print_meta: rope scaling     = linear
0.00.051.590 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.590 I llm_load_print_meta: freq_scale_train = 1
0.00.051.591 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.591 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.591 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.591 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.591 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.591 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.591 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.597 I llm_load_print_meta: model type       = 1.4B
0.00.051.598 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.598 I llm_load_print_meta: model params     = 1.41 B
0.00.051.599 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.599 I llm_load_print_meta: general.name     = 1.4B
0.00.051.599 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.600 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.600 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.600 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.600 I llm_load_print_meta: LF token         = 128 ''
0.00.051.600 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.601 I llm_load_print_meta: max token length = 1024
0.00.053.107 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.107 I llm_load_tensors: offloading output layer to GPU
0.00.053.107 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.111 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.112 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.899 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.899 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.900 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.900 I llama_new_context_with_model: n_batch       = 2048
0.00.053.900 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.900 I llama_new_context_with_model: flash_attn    = 0
0.00.053.900 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.901 I llama_new_context_with_model: freq_scale    = 1
0.00.053.901 I ggml_metal_init: allocating
0.00.053.904 I ggml_metal_init: found device: Apple M4
0.00.053.906 I ggml_metal_init: picking default device: Apple M4
0.00.054.429 I ggml_metal_init: using embedded metal library
0.00.056.332 I ggml_metal_init: GPU name:   Apple M4
0.00.056.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.334 I ggml_metal_init: simdgroup reduction   = true
0.00.056.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.334 I ggml_metal_init: has bfloat            = true
0.00.056.334 I ggml_metal_init: use bfloat            = true
0.00.056.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.473 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.478 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.494 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.402 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.404 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.404 I llama_new_context_with_model: graph nodes  = 967
0.00.083.404 I llama_new_context_with_model: graph splits = 2
0.00.083.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.749 I main: llama threadpool init, n_threads = 4
0.00.760.785 I 
0.00.760.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.760.803 I 
0.00.760.962 I sampler seed: 1234
0.00.760.966 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.996 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.998 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.998 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.552.385 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61792.86 tokens per second)
0.01.552.386 I llama_perf_context_print:        load time =     751.62 ms
0.01.552.387 I llama_perf_context_print: prompt eval time =      36.56 ms /     7 tokens (    5.22 ms per token,   191.48 tokens per second)
0.01.552.388 I llama_perf_context_print:        eval time =     751.93 ms /    63 runs   (   11.94 ms per token,    83.78 tokens per second)
0.01.552.388 I llama_perf_context_print:       total time =     791.64 ms /    70 tokens
0.01.552.560 I ggml_metal_free: deallocating

real	0m1.568s
user	0m0.109s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.908 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.748 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.749 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.749 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.751 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.753 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.754 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.599 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.699 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.605 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.607 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.607 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.608 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.608 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.608 I llama_model_loader: - type  f32:  194 tensors
0.00.023.609 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.609 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.525 I llm_load_vocab: special tokens cache size = 25
0.00.050.623 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.626 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.627 I llm_load_print_meta: arch             = gptneox
0.00.050.627 I llm_load_print_meta: vocab type       = BPE
0.00.050.627 I llm_load_print_meta: n_vocab          = 50304
0.00.050.627 I llm_load_print_meta: n_merges         = 50009
0.00.050.628 I llm_load_print_meta: vocab_only       = 0
0.00.050.628 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.628 I llm_load_print_meta: n_embd           = 2048
0.00.050.628 I llm_load_print_meta: n_layer          = 24
0.00.050.631 I llm_load_print_meta: n_head           = 16
0.00.050.632 I llm_load_print_meta: n_head_kv        = 16
0.00.050.632 I llm_load_print_meta: n_rot            = 32
0.00.050.632 I llm_load_print_meta: n_swa            = 0
0.00.050.632 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.633 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.633 I llm_load_print_meta: n_gqa            = 1
0.00.050.634 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.635 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.635 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.637 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.638 I llm_load_print_meta: n_ff             = 8192
0.00.050.638 I llm_load_print_meta: n_expert         = 0
0.00.050.639 I llm_load_print_meta: n_expert_used    = 0
0.00.050.639 I llm_load_print_meta: causal attn      = 1
0.00.050.639 I llm_load_print_meta: pooling type     = 0
0.00.050.639 I llm_load_print_meta: rope type        = 2
0.00.050.639 I llm_load_print_meta: rope scaling     = linear
0.00.050.641 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.641 I llm_load_print_meta: freq_scale_train = 1
0.00.050.642 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.642 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.642 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.642 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.642 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.642 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.642 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.655 I llm_load_print_meta: model type       = 1.4B
0.00.050.655 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.656 I llm_load_print_meta: model params     = 1.41 B
0.00.050.656 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.656 I llm_load_print_meta: general.name     = 1.4B
0.00.050.657 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.657 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.657 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.657 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.657 I llm_load_print_meta: LF token         = 128 ''
0.00.050.658 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.658 I llm_load_print_meta: max token length = 1024
0.00.052.699 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.699 I llm_load_tensors: offloading output layer to GPU
0.00.052.699 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.709 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.710 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.695 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.695 I llama_new_context_with_model: n_ctx         = 128
0.00.053.696 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.696 I llama_new_context_with_model: n_batch       = 128
0.00.053.696 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.696 I llama_new_context_with_model: flash_attn    = 0
0.00.053.697 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.697 I llama_new_context_with_model: freq_scale    = 1
0.00.053.697 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.697 I ggml_metal_init: allocating
0.00.053.701 I ggml_metal_init: found device: Apple M4
0.00.053.703 I ggml_metal_init: picking default device: Apple M4
0.00.054.262 I ggml_metal_init: using embedded metal library
0.00.056.195 I ggml_metal_init: GPU name:   Apple M4
0.00.056.197 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.197 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.198 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.198 I ggml_metal_init: simdgroup reduction   = true
0.00.056.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.198 I ggml_metal_init: has bfloat            = true
0.00.056.198 I ggml_metal_init: use bfloat            = true
0.00.056.199 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.558 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.562 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.576 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.548 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.550 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.550 I llama_new_context_with_model: graph nodes  = 967
0.00.066.550 I llama_new_context_with_model: graph splits = 2
0.00.066.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.169 I 
0.00.717.191 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.717.195 I perplexity: tokenizing the input ..
0.00.724.853 I perplexity: tokenization took 7.657 ms
0.00.724.863 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.836 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.861.076 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.861.099 I llama_perf_context_print:        load time =     708.26 ms
0.00.861.102 I llama_perf_context_print: prompt eval time =     134.74 ms /   128 tokens (    1.05 ms per token,   949.96 tokens per second)
0.00.861.103 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.103 I llama_perf_context_print:       total time =     143.93 ms /   129 tokens
0.00.861.558 I ggml_metal_free: deallocating

real	0m0.878s
user	0m0.077s
sys	0m0.117s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.011.088 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.392 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.403 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.403 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.407 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.343 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.402 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.281 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.282 I llama_model_loader: - type  f32:  194 tensors
0.00.026.282 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.283 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.274 I llm_load_vocab: special tokens cache size = 25
0.00.053.381 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.384 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.384 I llm_load_print_meta: arch             = gptneox
0.00.053.385 I llm_load_print_meta: vocab type       = BPE
0.00.053.385 I llm_load_print_meta: n_vocab          = 50304
0.00.053.385 I llm_load_print_meta: n_merges         = 50009
0.00.053.385 I llm_load_print_meta: vocab_only       = 0
0.00.053.386 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.386 I llm_load_print_meta: n_embd           = 2048
0.00.053.386 I llm_load_print_meta: n_layer          = 24
0.00.053.388 I llm_load_print_meta: n_head           = 16
0.00.053.389 I llm_load_print_meta: n_head_kv        = 16
0.00.053.389 I llm_load_print_meta: n_rot            = 32
0.00.053.391 I llm_load_print_meta: n_swa            = 0
0.00.053.391 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.391 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.392 I llm_load_print_meta: n_gqa            = 1
0.00.053.393 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.393 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.394 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.394 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.394 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.395 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.395 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.395 I llm_load_print_meta: n_ff             = 8192
0.00.053.396 I llm_load_print_meta: n_expert         = 0
0.00.053.396 I llm_load_print_meta: n_expert_used    = 0
0.00.053.396 I llm_load_print_meta: causal attn      = 1
0.00.053.396 I llm_load_print_meta: pooling type     = 0
0.00.053.396 I llm_load_print_meta: rope type        = 2
0.00.053.396 I llm_load_print_meta: rope scaling     = linear
0.00.053.397 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.398 I llm_load_print_meta: freq_scale_train = 1
0.00.053.398 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.398 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.398 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.399 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.399 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.399 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.399 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.407 I llm_load_print_meta: model type       = 1.4B
0.00.053.408 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.408 I llm_load_print_meta: model params     = 1.41 B
0.00.053.409 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.409 I llm_load_print_meta: general.name     = 1.4B
0.00.053.409 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.409 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.409 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.411 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.411 I llm_load_print_meta: LF token         = 128 ''
0.00.053.411 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.411 I llm_load_print_meta: max token length = 1024
0.00.055.185 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.185 I llm_load_tensors: offloading output layer to GPU
0.00.055.185 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.190 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.191 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.056.124 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.125 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.125 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.125 I llama_new_context_with_model: n_batch       = 2048
0.00.056.125 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.125 I llama_new_context_with_model: flash_attn    = 0
0.00.056.126 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.126 I llama_new_context_with_model: freq_scale    = 1
0.00.056.126 I ggml_metal_init: allocating
0.00.056.130 I ggml_metal_init: found device: Apple M4
0.00.056.132 I ggml_metal_init: picking default device: Apple M4
0.00.056.678 I ggml_metal_init: using embedded metal library
0.00.058.628 I ggml_metal_init: GPU name:   Apple M4
0.00.058.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.630 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.631 I ggml_metal_init: simdgroup reduction   = true
0.00.058.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.631 I ggml_metal_init: has bfloat            = true
0.00.058.631 I ggml_metal_init: use bfloat            = true
0.00.058.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.632 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.233 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.243 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.260 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.309 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.311 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.311 I llama_new_context_with_model: graph nodes  = 967
0.00.088.311 I llama_new_context_with_model: graph splits = 2
0.00.088.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.231 I main: llama threadpool init, n_threads = 4
0.00.717.274 I 
0.00.717.294 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.717.294 I 
0.00.717.525 I sampler seed: 1234
0.00.717.531 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.568 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.579 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.589 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.559.501 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.559.502 I llama_perf_context_print:        load time =     706.14 ms
0.01.559.502 I llama_perf_context_print: prompt eval time =      36.61 ms /     7 tokens (    5.23 ms per token,   191.18 tokens per second)
0.01.559.503 I llama_perf_context_print:        eval time =     802.31 ms /    63 runs   (   12.74 ms per token,    78.52 tokens per second)
0.01.559.503 I llama_perf_context_print:       total time =     842.27 ms /    70 tokens
0.01.559.681 I ggml_metal_free: deallocating

real	0m1.580s
user	0m0.110s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.600 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.035 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.044 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.044 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.045 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.047 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.047 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.047 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.871 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.911 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.749 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.749 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.749 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.750 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.750 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.750 I llama_model_loader: - type  f32:  194 tensors
0.00.022.751 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.751 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.708 I llm_load_vocab: special tokens cache size = 25
0.00.048.878 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.880 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.881 I llm_load_print_meta: arch             = gptneox
0.00.048.881 I llm_load_print_meta: vocab type       = BPE
0.00.048.881 I llm_load_print_meta: n_vocab          = 50304
0.00.048.882 I llm_load_print_meta: n_merges         = 50009
0.00.048.882 I llm_load_print_meta: vocab_only       = 0
0.00.048.882 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.882 I llm_load_print_meta: n_embd           = 2048
0.00.048.882 I llm_load_print_meta: n_layer          = 24
0.00.048.885 I llm_load_print_meta: n_head           = 16
0.00.048.886 I llm_load_print_meta: n_head_kv        = 16
0.00.048.888 I llm_load_print_meta: n_rot            = 32
0.00.048.888 I llm_load_print_meta: n_swa            = 0
0.00.048.888 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.888 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.889 I llm_load_print_meta: n_gqa            = 1
0.00.048.890 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.895 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.895 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.896 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.896 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.896 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.896 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.897 I llm_load_print_meta: n_ff             = 8192
0.00.048.897 I llm_load_print_meta: n_expert         = 0
0.00.048.898 I llm_load_print_meta: n_expert_used    = 0
0.00.048.898 I llm_load_print_meta: causal attn      = 1
0.00.048.898 I llm_load_print_meta: pooling type     = 0
0.00.048.898 I llm_load_print_meta: rope type        = 2
0.00.048.898 I llm_load_print_meta: rope scaling     = linear
0.00.048.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.899 I llm_load_print_meta: freq_scale_train = 1
0.00.048.899 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.899 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.900 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.900 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.900 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.900 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.912 I llm_load_print_meta: model type       = 1.4B
0.00.048.913 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.913 I llm_load_print_meta: model params     = 1.41 B
0.00.048.913 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.914 I llm_load_print_meta: general.name     = 1.4B
0.00.048.914 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.914 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.915 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.915 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.916 I llm_load_print_meta: LF token         = 128 ''
0.00.048.916 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.916 I llm_load_print_meta: max token length = 1024
0.00.050.482 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.482 I llm_load_tensors: offloading output layer to GPU
0.00.050.482 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.492 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.493 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.306 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.307 I llama_new_context_with_model: n_ctx         = 128
0.00.051.308 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.308 I llama_new_context_with_model: n_batch       = 128
0.00.051.308 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.308 I llama_new_context_with_model: flash_attn    = 0
0.00.051.308 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.309 I llama_new_context_with_model: freq_scale    = 1
0.00.051.309 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.309 I ggml_metal_init: allocating
0.00.051.315 I ggml_metal_init: found device: Apple M4
0.00.051.317 I ggml_metal_init: picking default device: Apple M4
0.00.051.846 I ggml_metal_init: using embedded metal library
0.00.053.777 I ggml_metal_init: GPU name:   Apple M4
0.00.053.779 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.779 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.780 I ggml_metal_init: simdgroup reduction   = true
0.00.053.780 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.780 I ggml_metal_init: has bfloat            = true
0.00.053.780 I ggml_metal_init: use bfloat            = true
0.00.053.781 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.816 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.818 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.832 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.729 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.730 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.731 I llama_new_context_with_model: graph nodes  = 967
0.00.063.731 I llama_new_context_with_model: graph splits = 2
0.00.063.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.577 I 
0.00.592.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.592.617 I perplexity: tokenizing the input ..
0.00.600.563 I perplexity: tokenization took 7.944 ms
0.00.600.576 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.735.693 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.736.967 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.736.982 I llama_perf_context_print:        load time =     583.97 ms
0.00.736.983 I llama_perf_context_print: prompt eval time =     134.89 ms /   128 tokens (    1.05 ms per token,   948.91 tokens per second)
0.00.736.985 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.736.985 I llama_perf_context_print:       total time =     144.41 ms /   129 tokens
0.00.737.389 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.076s
sys	0m0.115s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.900 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.498 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.502 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.509 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.517 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.517 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.310 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.199 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.200 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.200 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.201 I llama_model_loader: - type  f32:  194 tensors
0.00.025.201 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.201 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.201 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.361 I llm_load_vocab: special tokens cache size = 25
0.00.051.513 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.516 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.516 I llm_load_print_meta: arch             = gptneox
0.00.051.517 I llm_load_print_meta: vocab type       = BPE
0.00.051.517 I llm_load_print_meta: n_vocab          = 50304
0.00.051.517 I llm_load_print_meta: n_merges         = 50009
0.00.051.517 I llm_load_print_meta: vocab_only       = 0
0.00.051.518 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.518 I llm_load_print_meta: n_embd           = 2048
0.00.051.518 I llm_load_print_meta: n_layer          = 24
0.00.051.520 I llm_load_print_meta: n_head           = 16
0.00.051.521 I llm_load_print_meta: n_head_kv        = 16
0.00.051.521 I llm_load_print_meta: n_rot            = 32
0.00.051.522 I llm_load_print_meta: n_swa            = 0
0.00.051.522 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.522 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.523 I llm_load_print_meta: n_gqa            = 1
0.00.051.523 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.524 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.525 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.525 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.525 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.526 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.526 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.527 I llm_load_print_meta: n_ff             = 8192
0.00.051.527 I llm_load_print_meta: n_expert         = 0
0.00.051.527 I llm_load_print_meta: n_expert_used    = 0
0.00.051.527 I llm_load_print_meta: causal attn      = 1
0.00.051.527 I llm_load_print_meta: pooling type     = 0
0.00.051.527 I llm_load_print_meta: rope type        = 2
0.00.051.528 I llm_load_print_meta: rope scaling     = linear
0.00.051.528 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.528 I llm_load_print_meta: freq_scale_train = 1
0.00.051.528 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.529 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.529 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.529 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.529 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.529 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.530 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.542 I llm_load_print_meta: model type       = 1.4B
0.00.051.543 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.543 I llm_load_print_meta: model params     = 1.41 B
0.00.051.543 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.543 I llm_load_print_meta: general.name     = 1.4B
0.00.051.544 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.544 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.544 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.544 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.544 I llm_load_print_meta: LF token         = 128 ''
0.00.051.545 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.545 I llm_load_print_meta: max token length = 1024
0.00.053.421 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.421 I llm_load_tensors: offloading output layer to GPU
0.00.053.422 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.432 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.433 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.403 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.404 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.404 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.404 I llama_new_context_with_model: n_batch       = 2048
0.00.054.404 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.405 I llama_new_context_with_model: flash_attn    = 0
0.00.054.405 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.405 I llama_new_context_with_model: freq_scale    = 1
0.00.054.406 I ggml_metal_init: allocating
0.00.054.409 I ggml_metal_init: found device: Apple M4
0.00.054.411 I ggml_metal_init: picking default device: Apple M4
0.00.054.987 I ggml_metal_init: using embedded metal library
0.00.056.899 I ggml_metal_init: GPU name:   Apple M4
0.00.056.901 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.901 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.901 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.902 I ggml_metal_init: simdgroup reduction   = true
0.00.056.902 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.902 I ggml_metal_init: has bfloat            = true
0.00.056.902 I ggml_metal_init: use bfloat            = true
0.00.056.903 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.909 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.917 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.942 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.934 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.936 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.936 I llama_new_context_with_model: graph nodes  = 967
0.00.086.936 I llama_new_context_with_model: graph splits = 2
0.00.086.948 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.101 I main: llama threadpool init, n_threads = 4
0.00.439.137 I 
0.00.439.161 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.439.162 I 
0.00.439.323 I sampler seed: 1234
0.00.439.328 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.439.364 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.439.369 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.439.369 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.129.023 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.129.024 I llama_perf_context_print:        load time =     428.20 ms
0.01.129.024 I llama_perf_context_print: prompt eval time =      35.89 ms /     7 tokens (    5.13 ms per token,   195.06 tokens per second)
0.01.129.025 I llama_perf_context_print:        eval time =     650.74 ms /    63 runs   (   10.33 ms per token,    96.81 tokens per second)
0.01.129.025 I llama_perf_context_print:       total time =     689.93 ms /    70 tokens
0.01.129.205 I ggml_metal_free: deallocating

real	0m1.148s
user	0m0.108s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.981 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.572 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.579 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.581 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.582 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.582 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.582 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.584 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.584 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.584 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.310 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.218 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.220 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.220 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.220 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.221 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.221 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.222 I llama_model_loader: - type  f32:  194 tensors
0.00.024.222 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.222 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.222 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.297 I llm_load_vocab: special tokens cache size = 25
0.00.050.344 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.348 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.348 I llm_load_print_meta: arch             = gptneox
0.00.050.348 I llm_load_print_meta: vocab type       = BPE
0.00.050.349 I llm_load_print_meta: n_vocab          = 50304
0.00.050.349 I llm_load_print_meta: n_merges         = 50009
0.00.050.349 I llm_load_print_meta: vocab_only       = 0
0.00.050.349 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.352 I llm_load_print_meta: n_embd           = 2048
0.00.050.352 I llm_load_print_meta: n_layer          = 24
0.00.050.355 I llm_load_print_meta: n_head           = 16
0.00.050.356 I llm_load_print_meta: n_head_kv        = 16
0.00.050.356 I llm_load_print_meta: n_rot            = 32
0.00.050.356 I llm_load_print_meta: n_swa            = 0
0.00.050.356 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.356 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.357 I llm_load_print_meta: n_gqa            = 1
0.00.050.358 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.359 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.359 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.359 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.361 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.361 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.361 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.362 I llm_load_print_meta: n_ff             = 8192
0.00.050.362 I llm_load_print_meta: n_expert         = 0
0.00.050.362 I llm_load_print_meta: n_expert_used    = 0
0.00.050.362 I llm_load_print_meta: causal attn      = 1
0.00.050.362 I llm_load_print_meta: pooling type     = 0
0.00.050.362 I llm_load_print_meta: rope type        = 2
0.00.050.363 I llm_load_print_meta: rope scaling     = linear
0.00.050.363 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.363 I llm_load_print_meta: freq_scale_train = 1
0.00.050.364 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.364 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.364 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.364 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.364 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.364 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.364 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.376 I llm_load_print_meta: model type       = 1.4B
0.00.050.377 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.377 I llm_load_print_meta: model params     = 1.41 B
0.00.050.377 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.377 I llm_load_print_meta: general.name     = 1.4B
0.00.050.378 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.378 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.378 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.378 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: LF token         = 128 ''
0.00.050.379 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: max token length = 1024
0.00.052.282 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.283 I llm_load_tensors: offloading output layer to GPU
0.00.052.283 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.293 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.294 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.227 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.228 I llama_new_context_with_model: n_ctx         = 128
0.00.053.228 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.228 I llama_new_context_with_model: n_batch       = 128
0.00.053.228 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.228 I llama_new_context_with_model: flash_attn    = 0
0.00.053.229 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.229 I llama_new_context_with_model: freq_scale    = 1
0.00.053.229 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.230 I ggml_metal_init: allocating
0.00.053.235 I ggml_metal_init: found device: Apple M4
0.00.053.238 I ggml_metal_init: picking default device: Apple M4
0.00.053.768 I ggml_metal_init: using embedded metal library
0.00.055.734 I ggml_metal_init: GPU name:   Apple M4
0.00.055.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.736 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.736 I ggml_metal_init: simdgroup reduction   = true
0.00.055.736 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.736 I ggml_metal_init: has bfloat            = true
0.00.055.737 I ggml_metal_init: use bfloat            = true
0.00.055.737 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.739 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.767 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.769 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.783 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.670 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.671 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.671 I llama_new_context_with_model: graph nodes  = 967
0.00.065.671 I llama_new_context_with_model: graph splits = 2
0.00.065.684 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.396.293 I 
0.00.396.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.396.313 I perplexity: tokenizing the input ..
0.00.403.534 I perplexity: tokenization took 7.22 ms
0.00.403.546 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.536.095 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.537.464 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.537.481 I llama_perf_context_print:        load time =     386.31 ms
0.00.537.482 I llama_perf_context_print: prompt eval time =     132.33 ms /   128 tokens (    1.03 ms per token,   967.29 tokens per second)
0.00.537.485 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.537.486 I llama_perf_context_print:       total time =     141.19 ms /   129 tokens
0.00.537.909 I ggml_metal_free: deallocating

real	0m0.553s
user	0m0.075s
sys	0m0.072s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.191 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.688 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.700 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.700 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.702 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.702 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.702 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.582 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.414 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.416 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.416 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.416 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.416 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.417 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.417 I llama_model_loader: - type  f32:  194 tensors
0.00.025.418 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.418 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.418 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.418 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.797 I llm_load_vocab: special tokens cache size = 25
0.00.051.817 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.819 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.820 I llm_load_print_meta: arch             = gptneox
0.00.051.820 I llm_load_print_meta: vocab type       = BPE
0.00.051.820 I llm_load_print_meta: n_vocab          = 50304
0.00.051.820 I llm_load_print_meta: n_merges         = 50009
0.00.051.820 I llm_load_print_meta: vocab_only       = 0
0.00.051.821 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.821 I llm_load_print_meta: n_embd           = 2048
0.00.051.821 I llm_load_print_meta: n_layer          = 24
0.00.051.824 I llm_load_print_meta: n_head           = 16
0.00.051.825 I llm_load_print_meta: n_head_kv        = 16
0.00.051.825 I llm_load_print_meta: n_rot            = 32
0.00.051.825 I llm_load_print_meta: n_swa            = 0
0.00.051.825 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.826 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.827 I llm_load_print_meta: n_gqa            = 1
0.00.051.828 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.828 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.829 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.829 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.829 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.830 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.830 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.831 I llm_load_print_meta: n_ff             = 8192
0.00.051.831 I llm_load_print_meta: n_expert         = 0
0.00.051.831 I llm_load_print_meta: n_expert_used    = 0
0.00.051.831 I llm_load_print_meta: causal attn      = 1
0.00.051.831 I llm_load_print_meta: pooling type     = 0
0.00.051.831 I llm_load_print_meta: rope type        = 2
0.00.051.832 I llm_load_print_meta: rope scaling     = linear
0.00.051.832 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.832 I llm_load_print_meta: freq_scale_train = 1
0.00.051.833 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.833 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.833 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.833 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.835 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.835 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.835 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.846 I llm_load_print_meta: model type       = 1.4B
0.00.051.853 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.855 I llm_load_print_meta: model params     = 1.41 B
0.00.051.856 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.856 I llm_load_print_meta: general.name     = 1.4B
0.00.051.859 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.861 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.861 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.862 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.862 I llm_load_print_meta: LF token         = 128 ''
0.00.051.862 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.863 I llm_load_print_meta: max token length = 1024
0.00.053.479 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.479 I llm_load_tensors: offloading output layer to GPU
0.00.053.479 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.489 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.490 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.364 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.365 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.365 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.365 I llama_new_context_with_model: n_batch       = 2048
0.00.054.365 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.366 I llama_new_context_with_model: flash_attn    = 0
0.00.054.366 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.366 I llama_new_context_with_model: freq_scale    = 1
0.00.054.367 I ggml_metal_init: allocating
0.00.054.371 I ggml_metal_init: found device: Apple M4
0.00.054.373 I ggml_metal_init: picking default device: Apple M4
0.00.054.923 I ggml_metal_init: using embedded metal library
0.00.056.851 I ggml_metal_init: GPU name:   Apple M4
0.00.056.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.853 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.854 I ggml_metal_init: simdgroup reduction   = true
0.00.056.854 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.854 I ggml_metal_init: has bfloat            = true
0.00.056.854 I ggml_metal_init: use bfloat            = true
0.00.056.854 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.856 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.480 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.487 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.505 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.439 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.440 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.440 I llama_new_context_with_model: graph nodes  = 967
0.00.085.441 I llama_new_context_with_model: graph splits = 2
0.00.085.462 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.435 I main: llama threadpool init, n_threads = 4
0.00.558.475 I 
0.00.558.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.558.497 I 
0.00.558.725 I sampler seed: 1234
0.00.558.729 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.558.765 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.558.767 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.558.768 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.310.542 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.310.543 I llama_perf_context_print:        load time =     547.24 ms
0.01.310.544 I llama_perf_context_print: prompt eval time =      42.54 ms /     7 tokens (    6.08 ms per token,   164.57 tokens per second)
0.01.310.545 I llama_perf_context_print:        eval time =     706.09 ms /    63 runs   (   11.21 ms per token,    89.22 tokens per second)
0.01.310.545 I llama_perf_context_print:       total time =     752.11 ms /    70 tokens
0.01.310.722 I ggml_metal_free: deallocating

real	0m1.326s
user	0m0.109s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.898 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.512 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.513 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.514 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.514 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.515 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.515 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.516 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.516 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.518 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.518 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.519 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.520 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.520 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.482 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.417 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.418 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.418 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.419 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.419 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.420 I llama_model_loader: - type  f32:  194 tensors
0.00.023.420 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.421 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.421 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.421 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.299 I llm_load_vocab: special tokens cache size = 25
0.00.050.391 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.394 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.394 I llm_load_print_meta: arch             = gptneox
0.00.050.394 I llm_load_print_meta: vocab type       = BPE
0.00.050.395 I llm_load_print_meta: n_vocab          = 50304
0.00.050.395 I llm_load_print_meta: n_merges         = 50009
0.00.050.395 I llm_load_print_meta: vocab_only       = 0
0.00.050.395 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.395 I llm_load_print_meta: n_embd           = 2048
0.00.050.396 I llm_load_print_meta: n_layer          = 24
0.00.050.399 I llm_load_print_meta: n_head           = 16
0.00.050.399 I llm_load_print_meta: n_head_kv        = 16
0.00.050.400 I llm_load_print_meta: n_rot            = 32
0.00.050.402 I llm_load_print_meta: n_swa            = 0
0.00.050.402 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.402 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.403 I llm_load_print_meta: n_gqa            = 1
0.00.050.404 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.405 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.405 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.405 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.406 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.406 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.406 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.407 I llm_load_print_meta: n_ff             = 8192
0.00.050.407 I llm_load_print_meta: n_expert         = 0
0.00.050.407 I llm_load_print_meta: n_expert_used    = 0
0.00.050.407 I llm_load_print_meta: causal attn      = 1
0.00.050.407 I llm_load_print_meta: pooling type     = 0
0.00.050.408 I llm_load_print_meta: rope type        = 2
0.00.050.408 I llm_load_print_meta: rope scaling     = linear
0.00.050.408 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.409 I llm_load_print_meta: freq_scale_train = 1
0.00.050.409 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.409 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.409 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.409 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.410 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.410 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.410 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.422 I llm_load_print_meta: model type       = 1.4B
0.00.050.422 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.422 I llm_load_print_meta: model params     = 1.41 B
0.00.050.423 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.423 I llm_load_print_meta: general.name     = 1.4B
0.00.050.423 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.424 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.425 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.425 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.425 I llm_load_print_meta: LF token         = 128 ''
0.00.050.425 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.425 I llm_load_print_meta: max token length = 1024
0.00.052.351 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.352 I llm_load_tensors: offloading output layer to GPU
0.00.052.352 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.362 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.363 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.301 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.302 I llama_new_context_with_model: n_ctx         = 128
0.00.053.302 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.303 I llama_new_context_with_model: n_batch       = 128
0.00.053.303 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.303 I llama_new_context_with_model: flash_attn    = 0
0.00.053.303 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.304 I llama_new_context_with_model: freq_scale    = 1
0.00.053.304 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.304 I ggml_metal_init: allocating
0.00.053.310 I ggml_metal_init: found device: Apple M4
0.00.053.312 I ggml_metal_init: picking default device: Apple M4
0.00.053.836 I ggml_metal_init: using embedded metal library
0.00.055.797 I ggml_metal_init: GPU name:   Apple M4
0.00.055.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.799 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.799 I ggml_metal_init: simdgroup reduction   = true
0.00.055.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.799 I ggml_metal_init: has bfloat            = true
0.00.055.800 I ggml_metal_init: use bfloat            = true
0.00.055.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.801 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.937 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.944 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.960 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.887 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.888 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.889 I llama_new_context_with_model: graph nodes  = 967
0.00.065.889 I llama_new_context_with_model: graph splits = 2
0.00.065.901 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.919 I 
0.00.505.946 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.505.951 I perplexity: tokenizing the input ..
0.00.514.158 I perplexity: tokenization took 8.206 ms
0.00.514.172 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.646.262 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.428 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.443 I llama_perf_context_print:        load time =     497.02 ms
0.00.647.444 I llama_perf_context_print: prompt eval time =     131.84 ms /   128 tokens (    1.03 ms per token,   970.87 tokens per second)
0.00.647.444 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.445 I llama_perf_context_print:       total time =     141.53 ms /   129 tokens
0.00.647.874 I ggml_metal_free: deallocating

real	0m0.662s
user	0m0.078s
sys	0m0.099s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.782 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.132 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.137 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.138 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.139 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.141 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.141 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.141 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.142 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.142 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.142 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.144 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.145 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.965 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.054 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.944 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.946 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.946 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.947 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.947 I llama_model_loader: - type  f32:  194 tensors
0.00.024.947 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.948 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.948 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.896 I llm_load_vocab: special tokens cache size = 25
0.00.050.896 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.898 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.899 I llm_load_print_meta: arch             = gptneox
0.00.050.899 I llm_load_print_meta: vocab type       = BPE
0.00.050.899 I llm_load_print_meta: n_vocab          = 50304
0.00.050.899 I llm_load_print_meta: n_merges         = 50009
0.00.050.900 I llm_load_print_meta: vocab_only       = 0
0.00.050.900 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.900 I llm_load_print_meta: n_embd           = 2048
0.00.050.900 I llm_load_print_meta: n_layer          = 24
0.00.050.903 I llm_load_print_meta: n_head           = 16
0.00.050.904 I llm_load_print_meta: n_head_kv        = 16
0.00.050.904 I llm_load_print_meta: n_rot            = 32
0.00.050.904 I llm_load_print_meta: n_swa            = 0
0.00.050.904 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.904 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.905 I llm_load_print_meta: n_gqa            = 1
0.00.050.906 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.907 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.908 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.908 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.908 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.908 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.908 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.909 I llm_load_print_meta: n_ff             = 8192
0.00.050.909 I llm_load_print_meta: n_expert         = 0
0.00.050.909 I llm_load_print_meta: n_expert_used    = 0
0.00.050.910 I llm_load_print_meta: causal attn      = 1
0.00.050.910 I llm_load_print_meta: pooling type     = 0
0.00.050.910 I llm_load_print_meta: rope type        = 2
0.00.050.910 I llm_load_print_meta: rope scaling     = linear
0.00.050.911 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.911 I llm_load_print_meta: freq_scale_train = 1
0.00.050.911 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.911 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.911 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.912 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.912 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.912 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.912 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.918 I llm_load_print_meta: model type       = 1.4B
0.00.050.918 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.923 I llm_load_print_meta: model params     = 1.41 B
0.00.050.924 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.925 I llm_load_print_meta: general.name     = 1.4B
0.00.050.925 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.925 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.926 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.926 I llm_load_print_meta: LF token         = 128 ''
0.00.050.926 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.928 I llm_load_print_meta: max token length = 1024
0.00.052.383 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.383 I llm_load_tensors: offloading output layer to GPU
0.00.052.384 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.388 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.389 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.229 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.230 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.230 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.230 I llama_new_context_with_model: n_batch       = 2048
0.00.053.230 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.231 I llama_new_context_with_model: flash_attn    = 0
0.00.053.231 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.231 I llama_new_context_with_model: freq_scale    = 1
0.00.053.232 I ggml_metal_init: allocating
0.00.053.236 I ggml_metal_init: found device: Apple M4
0.00.053.239 I ggml_metal_init: picking default device: Apple M4
0.00.053.756 I ggml_metal_init: using embedded metal library
0.00.055.650 I ggml_metal_init: GPU name:   Apple M4
0.00.055.651 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.652 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.652 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.652 I ggml_metal_init: simdgroup reduction   = true
0.00.055.652 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.653 I ggml_metal_init: has bfloat            = true
0.00.055.653 I ggml_metal_init: use bfloat            = true
0.00.055.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.962 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.968 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.984 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.996 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.998 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.998 I llama_new_context_with_model: graph nodes  = 967
0.00.083.998 I llama_new_context_with_model: graph splits = 2
0.00.084.019 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.933 I main: llama threadpool init, n_threads = 4
0.00.624.971 I 
0.00.625.009 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.625.011 I 
0.00.625.228 I sampler seed: 1234
0.00.625.232 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.243 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.243 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.245 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.381.600 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62943.26 tokens per second)
0.01.381.600 I llama_perf_context_print:        load time =     614.15 ms
0.01.381.601 I llama_perf_context_print: prompt eval time =      36.48 ms /     7 tokens (    5.21 ms per token,   191.89 tokens per second)
0.01.381.602 I llama_perf_context_print:        eval time =     717.04 ms /    63 runs   (   11.38 ms per token,    87.86 tokens per second)
0.01.381.602 I llama_perf_context_print:       total time =     756.67 ms /    70 tokens
0.01.381.773 I ggml_metal_free: deallocating

real	0m1.403s
user	0m0.110s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.197 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.813 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.818 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.820 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.821 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.822 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.824 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.675 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.741 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.609 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.610 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.610 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.611 I llama_model_loader: - type  f32:  194 tensors
0.00.023.611 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.612 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.612 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.675 I llm_load_vocab: special tokens cache size = 25
0.00.049.839 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.842 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.842 I llm_load_print_meta: arch             = gptneox
0.00.049.842 I llm_load_print_meta: vocab type       = BPE
0.00.049.843 I llm_load_print_meta: n_vocab          = 50304
0.00.049.843 I llm_load_print_meta: n_merges         = 50009
0.00.049.843 I llm_load_print_meta: vocab_only       = 0
0.00.049.843 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.843 I llm_load_print_meta: n_embd           = 2048
0.00.049.843 I llm_load_print_meta: n_layer          = 24
0.00.049.847 I llm_load_print_meta: n_head           = 16
0.00.049.848 I llm_load_print_meta: n_head_kv        = 16
0.00.049.848 I llm_load_print_meta: n_rot            = 32
0.00.049.848 I llm_load_print_meta: n_swa            = 0
0.00.049.848 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.848 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.849 I llm_load_print_meta: n_gqa            = 1
0.00.049.850 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.851 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.851 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.851 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.852 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.852 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.852 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.853 I llm_load_print_meta: n_ff             = 8192
0.00.049.853 I llm_load_print_meta: n_expert         = 0
0.00.049.853 I llm_load_print_meta: n_expert_used    = 0
0.00.049.853 I llm_load_print_meta: causal attn      = 1
0.00.049.853 I llm_load_print_meta: pooling type     = 0
0.00.049.853 I llm_load_print_meta: rope type        = 2
0.00.049.854 I llm_load_print_meta: rope scaling     = linear
0.00.049.854 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.854 I llm_load_print_meta: freq_scale_train = 1
0.00.049.855 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.855 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.855 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.855 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.855 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.855 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.856 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.867 I llm_load_print_meta: model type       = 1.4B
0.00.049.867 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.868 I llm_load_print_meta: model params     = 1.41 B
0.00.049.870 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.870 I llm_load_print_meta: general.name     = 1.4B
0.00.049.870 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: LF token         = 128 ''
0.00.049.871 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: max token length = 1024
0.00.051.818 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.818 I llm_load_tensors: offloading output layer to GPU
0.00.051.819 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.829 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.830 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.713 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.714 I llama_new_context_with_model: n_ctx         = 128
0.00.052.714 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.714 I llama_new_context_with_model: n_batch       = 128
0.00.052.714 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.714 I llama_new_context_with_model: flash_attn    = 0
0.00.052.715 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.715 I llama_new_context_with_model: freq_scale    = 1
0.00.052.716 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.716 I ggml_metal_init: allocating
0.00.052.722 I ggml_metal_init: found device: Apple M4
0.00.052.724 I ggml_metal_init: picking default device: Apple M4
0.00.053.261 I ggml_metal_init: using embedded metal library
0.00.056.413 I ggml_metal_init: GPU name:   Apple M4
0.00.056.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.415 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.416 I ggml_metal_init: simdgroup reduction   = true
0.00.056.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.416 I ggml_metal_init: has bfloat            = true
0.00.056.416 I ggml_metal_init: use bfloat            = true
0.00.056.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.417 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.381 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.385 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.400 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.281 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.282 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.283 I llama_new_context_with_model: graph nodes  = 967
0.00.066.283 I llama_new_context_with_model: graph splits = 2
0.00.066.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.153 I 
0.00.583.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.583.174 I perplexity: tokenizing the input ..
0.00.591.083 I perplexity: tokenization took 7.908 ms
0.00.591.097 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.077 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.726.330 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.726.343 I llama_perf_context_print:        load time =     573.95 ms
0.00.726.344 I llama_perf_context_print: prompt eval time =     133.75 ms /   128 tokens (    1.04 ms per token,   957.04 tokens per second)
0.00.726.346 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.726.347 I llama_perf_context_print:       total time =     143.19 ms /   129 tokens
0.00.726.623 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.076s
sys	0m0.115s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.388 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.765 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.770 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.773 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.774 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.775 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.775 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.775 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.776 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.776 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.776 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.777 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.778 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.779 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.779 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.543 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.544 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.545 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.545 I llama_model_loader: - type  f32:  194 tensors
0.00.024.545 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.546 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.689 I llm_load_vocab: special tokens cache size = 25
0.00.051.894 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.897 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.897 I llm_load_print_meta: arch             = gptneox
0.00.051.898 I llm_load_print_meta: vocab type       = BPE
0.00.051.898 I llm_load_print_meta: n_vocab          = 50304
0.00.051.898 I llm_load_print_meta: n_merges         = 50009
0.00.051.898 I llm_load_print_meta: vocab_only       = 0
0.00.051.899 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.899 I llm_load_print_meta: n_embd           = 2048
0.00.051.899 I llm_load_print_meta: n_layer          = 24
0.00.051.902 I llm_load_print_meta: n_head           = 16
0.00.051.902 I llm_load_print_meta: n_head_kv        = 16
0.00.051.903 I llm_load_print_meta: n_rot            = 32
0.00.051.903 I llm_load_print_meta: n_swa            = 0
0.00.051.903 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.903 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.904 I llm_load_print_meta: n_gqa            = 1
0.00.051.905 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.905 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.906 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.906 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.906 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.909 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.909 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.909 I llm_load_print_meta: n_ff             = 8192
0.00.051.910 I llm_load_print_meta: n_expert         = 0
0.00.051.910 I llm_load_print_meta: n_expert_used    = 0
0.00.051.910 I llm_load_print_meta: causal attn      = 1
0.00.051.912 I llm_load_print_meta: pooling type     = 0
0.00.051.912 I llm_load_print_meta: rope type        = 2
0.00.051.912 I llm_load_print_meta: rope scaling     = linear
0.00.051.912 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.912 I llm_load_print_meta: freq_scale_train = 1
0.00.051.913 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.913 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.913 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.913 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.913 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.913 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.914 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.925 I llm_load_print_meta: model type       = 1.4B
0.00.051.925 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.926 I llm_load_print_meta: model params     = 1.41 B
0.00.051.926 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.926 I llm_load_print_meta: general.name     = 1.4B
0.00.051.927 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.927 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.927 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.927 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.928 I llm_load_print_meta: LF token         = 128 ''
0.00.051.928 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.928 I llm_load_print_meta: max token length = 1024
0.00.053.487 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.487 I llm_load_tensors: offloading output layer to GPU
0.00.053.487 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.497 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.498 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.296 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.297 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.298 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.298 I llama_new_context_with_model: n_batch       = 2048
0.00.054.298 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.298 I llama_new_context_with_model: flash_attn    = 0
0.00.054.299 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.299 I llama_new_context_with_model: freq_scale    = 1
0.00.054.299 I ggml_metal_init: allocating
0.00.054.302 I ggml_metal_init: found device: Apple M4
0.00.054.305 I ggml_metal_init: picking default device: Apple M4
0.00.054.827 I ggml_metal_init: using embedded metal library
0.00.056.751 I ggml_metal_init: GPU name:   Apple M4
0.00.056.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.753 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.753 I ggml_metal_init: simdgroup reduction   = true
0.00.056.753 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.754 I ggml_metal_init: has bfloat            = true
0.00.056.754 I ggml_metal_init: use bfloat            = true
0.00.056.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.756 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.912 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.918 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.936 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.018 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.019 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.020 I llama_new_context_with_model: graph nodes  = 967
0.00.086.020 I llama_new_context_with_model: graph splits = 2
0.00.086.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.871 I main: llama threadpool init, n_threads = 4
0.00.707.914 I 
0.00.707.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.707.939 I 
0.00.708.102 I sampler seed: 1234
0.00.708.106 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.708.122 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.708.122 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.708.122 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.550.738 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.550.739 I llama_perf_context_print:        load time =     698.48 ms
0.01.550.739 I llama_perf_context_print: prompt eval time =      38.67 ms /     7 tokens (    5.52 ms per token,   181.00 tokens per second)
0.01.550.740 I llama_perf_context_print:        eval time =     800.99 ms /    63 runs   (   12.71 ms per token,    78.65 tokens per second)
0.01.550.743 I llama_perf_context_print:       total time =     842.87 ms /    70 tokens
0.01.550.920 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.024 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.852 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.857 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.858 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.858 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.858 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.859 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.859 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.860 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.860 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.860 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.861 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.861 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.863 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.864 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.865 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.705 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.769 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.602 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.602 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.603 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.603 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.603 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.604 I llama_model_loader: - type  f32:  194 tensors
0.00.023.604 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.604 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.666 I llm_load_vocab: special tokens cache size = 25
0.00.049.692 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.694 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.694 I llm_load_print_meta: arch             = gptneox
0.00.049.695 I llm_load_print_meta: vocab type       = BPE
0.00.049.695 I llm_load_print_meta: n_vocab          = 50304
0.00.049.695 I llm_load_print_meta: n_merges         = 50009
0.00.049.695 I llm_load_print_meta: vocab_only       = 0
0.00.049.696 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.696 I llm_load_print_meta: n_embd           = 2048
0.00.049.696 I llm_load_print_meta: n_layer          = 24
0.00.049.699 I llm_load_print_meta: n_head           = 16
0.00.049.699 I llm_load_print_meta: n_head_kv        = 16
0.00.049.700 I llm_load_print_meta: n_rot            = 32
0.00.049.700 I llm_load_print_meta: n_swa            = 0
0.00.049.700 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.700 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.701 I llm_load_print_meta: n_gqa            = 1
0.00.049.702 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.702 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.703 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.703 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.703 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.703 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.704 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.704 I llm_load_print_meta: n_ff             = 8192
0.00.049.705 I llm_load_print_meta: n_expert         = 0
0.00.049.705 I llm_load_print_meta: n_expert_used    = 0
0.00.049.705 I llm_load_print_meta: causal attn      = 1
0.00.049.705 I llm_load_print_meta: pooling type     = 0
0.00.049.705 I llm_load_print_meta: rope type        = 2
0.00.049.705 I llm_load_print_meta: rope scaling     = linear
0.00.049.706 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.706 I llm_load_print_meta: freq_scale_train = 1
0.00.049.706 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.707 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.707 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.707 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.707 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.707 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.707 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.719 I llm_load_print_meta: model type       = 1.4B
0.00.049.719 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.720 I llm_load_print_meta: model params     = 1.41 B
0.00.049.720 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.720 I llm_load_print_meta: general.name     = 1.4B
0.00.049.721 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.721 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.721 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.721 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.721 I llm_load_print_meta: LF token         = 128 ''
0.00.049.722 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.722 I llm_load_print_meta: max token length = 1024
0.00.051.734 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.734 I llm_load_tensors: offloading output layer to GPU
0.00.051.734 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.744 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.745 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.641 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.642 I llama_new_context_with_model: n_ctx         = 128
0.00.052.642 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.642 I llama_new_context_with_model: n_batch       = 128
0.00.052.642 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.642 I llama_new_context_with_model: flash_attn    = 0
0.00.052.643 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.643 I llama_new_context_with_model: freq_scale    = 1
0.00.052.643 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.644 I ggml_metal_init: allocating
0.00.052.647 I ggml_metal_init: found device: Apple M4
0.00.052.649 I ggml_metal_init: picking default device: Apple M4
0.00.053.160 I ggml_metal_init: using embedded metal library
0.00.055.077 I ggml_metal_init: GPU name:   Apple M4
0.00.055.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.079 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.080 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.080 I ggml_metal_init: simdgroup reduction   = true
0.00.055.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.080 I ggml_metal_init: has bfloat            = true
0.00.055.080 I ggml_metal_init: use bfloat            = true
0.00.055.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.359 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.364 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.379 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.304 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.305 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.306 I llama_new_context_with_model: graph nodes  = 967
0.00.065.306 I llama_new_context_with_model: graph splits = 2
0.00.065.318 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.977 I 
0.00.647.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.648.004 I perplexity: tokenizing the input ..
0.00.655.669 I perplexity: tokenization took 7.665 ms
0.00.655.682 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.300 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.797.462 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.797.483 I llama_perf_context_print:        load time =     638.95 ms
0.00.797.484 I llama_perf_context_print: prompt eval time =     140.39 ms /   128 tokens (    1.10 ms per token,   911.75 tokens per second)
0.00.797.485 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.486 I llama_perf_context_print:       total time =     149.51 ms /   129 tokens
0.00.797.903 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.076s
sys	0m0.119s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.319 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.281 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.285 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.287 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.290 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.290 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.290 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.291 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.291 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.294 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.294 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.159 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.035 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.036 I llama_model_loader: - type  f32:  194 tensors
0.00.026.036 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.332 I llm_load_vocab: special tokens cache size = 25
0.00.052.454 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.457 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.458 I llm_load_print_meta: arch             = gptneox
0.00.052.458 I llm_load_print_meta: vocab type       = BPE
0.00.052.458 I llm_load_print_meta: n_vocab          = 50304
0.00.052.458 I llm_load_print_meta: n_merges         = 50009
0.00.052.458 I llm_load_print_meta: vocab_only       = 0
0.00.052.459 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.459 I llm_load_print_meta: n_embd           = 2048
0.00.052.459 I llm_load_print_meta: n_layer          = 24
0.00.052.462 I llm_load_print_meta: n_head           = 16
0.00.052.462 I llm_load_print_meta: n_head_kv        = 16
0.00.052.462 I llm_load_print_meta: n_rot            = 32
0.00.052.463 I llm_load_print_meta: n_swa            = 0
0.00.052.463 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.463 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.464 I llm_load_print_meta: n_gqa            = 1
0.00.052.464 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.465 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.466 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.466 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.466 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.466 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.466 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.467 I llm_load_print_meta: n_ff             = 8192
0.00.052.467 I llm_load_print_meta: n_expert         = 0
0.00.052.467 I llm_load_print_meta: n_expert_used    = 0
0.00.052.468 I llm_load_print_meta: causal attn      = 1
0.00.052.469 I llm_load_print_meta: pooling type     = 0
0.00.052.469 I llm_load_print_meta: rope type        = 2
0.00.052.469 I llm_load_print_meta: rope scaling     = linear
0.00.052.469 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.470 I llm_load_print_meta: freq_scale_train = 1
0.00.052.470 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.470 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.470 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.471 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.471 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.471 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.471 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.482 I llm_load_print_meta: model type       = 1.4B
0.00.052.483 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.483 I llm_load_print_meta: model params     = 1.41 B
0.00.052.483 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.483 I llm_load_print_meta: general.name     = 1.4B
0.00.052.484 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.484 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.484 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.484 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.486 I llm_load_print_meta: LF token         = 128 ''
0.00.052.487 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.487 I llm_load_print_meta: max token length = 1024
0.00.054.538 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.538 I llm_load_tensors: offloading output layer to GPU
0.00.054.538 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.548 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.549 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.516 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.517 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.517 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.517 I llama_new_context_with_model: n_batch       = 2048
0.00.055.517 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.517 I llama_new_context_with_model: flash_attn    = 0
0.00.055.518 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.518 I llama_new_context_with_model: freq_scale    = 1
0.00.055.518 I ggml_metal_init: allocating
0.00.055.524 I ggml_metal_init: found device: Apple M4
0.00.055.527 I ggml_metal_init: picking default device: Apple M4
0.00.056.050 I ggml_metal_init: using embedded metal library
0.00.057.999 I ggml_metal_init: GPU name:   Apple M4
0.00.058.000 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.001 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.001 I ggml_metal_init: simdgroup reduction   = true
0.00.058.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.003 I ggml_metal_init: has bfloat            = true
0.00.058.003 I ggml_metal_init: use bfloat            = true
0.00.058.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.004 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.268 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.273 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.290 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.287 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.288 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.289 I llama_new_context_with_model: graph nodes  = 967
0.00.086.289 I llama_new_context_with_model: graph splits = 2
0.00.086.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.733 I main: llama threadpool init, n_threads = 4
0.00.764.765 I 
0.00.764.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.764.783 I 
0.00.764.996 I sampler seed: 1234
0.00.765.001 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.765.048 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.765.065 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.765.065 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.638.600 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61525.13 tokens per second)
0.01.638.600 I llama_perf_context_print:        load time =     754.41 ms
0.01.638.601 I llama_perf_context_print: prompt eval time =      38.54 ms /     7 tokens (    5.51 ms per token,   181.64 tokens per second)
0.01.638.602 I llama_perf_context_print:        eval time =     832.14 ms /    63 runs   (   13.21 ms per token,    75.71 tokens per second)
0.01.638.605 I llama_perf_context_print:       total time =     873.87 ms /    70 tokens
0.01.638.770 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.109s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4178 (84e1c33c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.690 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.290 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.296 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.296 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.101 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.863 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.864 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.865 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.865 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.866 I llama_model_loader: - type  f32:  194 tensors
0.00.023.866 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.923 I llm_load_vocab: special tokens cache size = 25
0.00.049.720 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.723 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.723 I llm_load_print_meta: arch             = gptneox
0.00.049.723 I llm_load_print_meta: vocab type       = BPE
0.00.049.723 I llm_load_print_meta: n_vocab          = 50304
0.00.049.724 I llm_load_print_meta: n_merges         = 50009
0.00.049.724 I llm_load_print_meta: vocab_only       = 0
0.00.049.724 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.724 I llm_load_print_meta: n_embd           = 2048
0.00.049.724 I llm_load_print_meta: n_layer          = 24
0.00.049.727 I llm_load_print_meta: n_head           = 16
0.00.049.727 I llm_load_print_meta: n_head_kv        = 16
0.00.049.727 I llm_load_print_meta: n_rot            = 32
0.00.049.728 I llm_load_print_meta: n_swa            = 0
0.00.049.729 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.730 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.730 I llm_load_print_meta: n_gqa            = 1
0.00.049.731 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.732 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.732 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.733 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.733 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.733 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.733 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.734 I llm_load_print_meta: n_ff             = 8192
0.00.049.734 I llm_load_print_meta: n_expert         = 0
0.00.049.734 I llm_load_print_meta: n_expert_used    = 0
0.00.049.734 I llm_load_print_meta: causal attn      = 1
0.00.049.735 I llm_load_print_meta: pooling type     = 0
0.00.049.735 I llm_load_print_meta: rope type        = 2
0.00.049.735 I llm_load_print_meta: rope scaling     = linear
0.00.049.735 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.736 I llm_load_print_meta: freq_scale_train = 1
0.00.049.736 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.736 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.736 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.737 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.737 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.737 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.737 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.749 I llm_load_print_meta: model type       = 1.4B
0.00.049.749 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.750 I llm_load_print_meta: model params     = 1.41 B
0.00.049.750 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.750 I llm_load_print_meta: general.name     = 1.4B
0.00.049.750 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.750 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.751 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.751 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.751 I llm_load_print_meta: LF token         = 128 ''
0.00.049.751 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.751 I llm_load_print_meta: max token length = 1024
0.00.051.766 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.767 I llm_load_tensors: offloading output layer to GPU
0.00.051.767 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.777 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.778 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.671 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.672 I llama_new_context_with_model: n_ctx         = 128
0.00.052.672 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.672 I llama_new_context_with_model: n_batch       = 128
0.00.052.672 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.673 I llama_new_context_with_model: flash_attn    = 0
0.00.052.673 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.673 I llama_new_context_with_model: freq_scale    = 1
0.00.052.674 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.674 I ggml_metal_init: allocating
0.00.052.680 I ggml_metal_init: found device: Apple M4
0.00.052.682 I ggml_metal_init: picking default device: Apple M4
0.00.053.230 I ggml_metal_init: using embedded metal library
0.00.055.166 I ggml_metal_init: GPU name:   Apple M4
0.00.055.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.168 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.169 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.169 I ggml_metal_init: simdgroup reduction   = true
0.00.055.169 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.169 I ggml_metal_init: has bfloat            = true
0.00.055.169 I ggml_metal_init: use bfloat            = true
0.00.055.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.170 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.127 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.132 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.147 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.025 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.026 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.026 I llama_new_context_with_model: graph nodes  = 967
0.00.065.027 I llama_new_context_with_model: graph splits = 2
0.00.065.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.124.665 I 
0.00.124.685 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.124.688 I perplexity: tokenizing the input ..
0.00.131.909 I perplexity: tokenization took 7.22 ms
0.00.131.923 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.271.385 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.272.677 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.272.690 I llama_perf_context_print:        load time =     114.97 ms
0.00.272.691 I llama_perf_context_print: prompt eval time =     139.21 ms /   128 tokens (    1.09 ms per token,   919.49 tokens per second)
0.00.272.692 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.272.695 I llama_perf_context_print:       total time =     148.03 ms /   129 tokens
0.00.273.031 I ggml_metal_free: deallocating

real	0m0.290s
user	0m0.075s
sys	0m0.036s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4178 (84e1c33c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11fe0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11fe0a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11fe0adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11fe0b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11fe0b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11fe0bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11fe0c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11fe0ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11fe0cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11fe0d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11fe0d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11fe0dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11fe0ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11fe0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11fe0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11fe100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11fe10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11fe10f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11fe11640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11fe11e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11fe12530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11fe12c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11fe13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11fe13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11fe14330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11fe145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11fe14c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11fe15870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11fe15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11fe16070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11fe16510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11fe167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11fe17060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11fe175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11fe17860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11fe17d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11fe181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11fe18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11fe18ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11fe18f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11fe19420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11fe198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11fe19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11fe1a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11fe1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11fe1aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11fe1b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11fe1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11fe1c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11fe1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11fe1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11fe1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11fe1d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11fe1de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11fe1e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11fe1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11fe1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11fe1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11fe1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11fe20050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11fe20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11fe207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11fe20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11fe210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11fe21590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11fe21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11fe21ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11fe22370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11fe22810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11fe22cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11fe23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11fe235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11fe23a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11fe23f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11fe243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11fe24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11fe24d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11fe251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11fe25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11fe25af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11fe25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11fe26430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11fe268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11fe26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11fe27210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11fe276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11fe27b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11fe27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11fe28490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11fe28930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11fe28dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11fe29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11fe29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11fe29bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11fe2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11fe2a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11fe2a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11fe1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11fe2afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11fe2b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11fe2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11fe2bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11fe2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11fe2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11fe2cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11fe2d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11fe2d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11fe2d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11fe2de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11fe2e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11fe2e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11fe2ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11fe2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11fe2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11fe2f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11fe2fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11fe30320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11fe307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11fe30c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11fe31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11fe315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11fe31a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11fe31ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11fe32380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11fe32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11fe32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11fe33160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11fe33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11fe33aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11fe33f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11fe343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11fe34880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11fe34d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11fe351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11fe35660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11fe35b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11fe35fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11fe36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11fe368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11fe36d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11fe37220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11fe376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11fe37b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11fe38000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11fe384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11fe38940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11fe38de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11fe39280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11fe39720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11fe39bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11fe3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11fe3a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11fe3a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11fe3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11fe3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11fe3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11fe3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11fe3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11fe3c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11fe3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11fe3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11fe3d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11fe3dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11fe3e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11fe3ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11fe3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11fe3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11fe3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11fe402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11fe40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11fe40d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11fe412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11fe41800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11fe41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11fe422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11fe427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11fe42d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11fe43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11fe437e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11fe43d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11fe44280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11fe447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11fe44d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11fe45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11fe457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11fe45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11fe46260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11fe467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11fe46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11fe47250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11fe477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11fe47cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11fe48240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11fe48790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11fe48ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11fe49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11fe49780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11fe49cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11fe4a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11fe4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11fe4acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11fe4b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11fe4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11fe4bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11fe4c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11fe4c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11fe4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11fe4d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11fe4d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11fe4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11fe4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11fe4e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11fe4ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11fe4f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11fe4f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11fe4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11fe501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11fe50710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11fe50c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11fe511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11fe51700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11fe51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11fe521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11fe526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11fe52b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11fe53030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11fe534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11fe53970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11fe53e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11fe542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11fe54750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11fe54bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11fe55090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11fe55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11fe559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11fe55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11fe56310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11fe56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11fe56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11fe576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11fe57dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11fe584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11fe587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11fe58db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11fe593c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.137.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131405a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131405eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131406320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131406790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131406c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131407070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1314074e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131407950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131407dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131408230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1314086a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131408d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1314098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13140a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13140a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13140af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13140b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13140bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13140c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13140cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13140d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13140da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13140e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13140e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13140efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13140f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13140f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13140f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13140fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131410290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131410700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131410c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1314110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131411360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1314117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131411c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1314120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131412520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131412990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131412e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131413270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1314136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131413b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131413fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131414430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1314148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131414d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131415180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1314155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131415a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131415ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131416340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1314167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131416c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131417090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131417500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131417a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131417f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1314183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131418850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131418cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131419130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1314195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131419a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131419e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13141a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13141a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13141abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13141b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13141b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13141b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13141bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13141c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13141c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13141cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13141cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13141d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13141d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13141dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13141e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13141e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13141e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13141ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13141f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13141f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13141fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131420020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131420490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131420900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131420d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1314211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131421650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131421ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131421f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1314223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131422810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131422c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1314230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131423560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1314239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131423e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1314242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131424720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131424b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131425000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131425470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1314258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131425d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1314261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131426630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131426aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131426f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131427380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1314277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131427c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1314280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131428540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1314289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131428e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131429290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131429700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131429b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131429fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13142a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13142a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13142ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13142b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13142b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13142ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13142bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13142c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13142c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13142cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13142d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13142d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13142d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13142de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13142e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13142e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13142eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13142efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13142f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13142f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13142fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131430180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1314305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131430a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131430ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131431340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1314317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131431c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131432090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131432500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131432970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131432de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131433250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1314336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131433b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131433fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131434410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131434880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131434cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131435160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1314355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131435a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131435eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131436320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131436eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131437170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131437430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1314378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131437d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131438180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1314385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131438a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131438ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131439340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1314397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131439c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13143a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13143a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13143a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13143ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13143b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13143b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13143bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13143bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13143c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13143c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13143ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13143d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13143d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13143da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13143deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13143e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13143e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13143ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13143f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13143f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13143f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13143fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131440230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1314406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131440b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131440f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1314413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131441860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131441cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131442140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1314425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131442a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131442e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131443300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131443770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131443be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131444050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1314444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131444930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131444da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131445210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131445680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131445af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131445f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1314463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131446840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131446cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131447120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131447590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131447a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131447e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1314482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131448750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131448bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131449030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1314494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131449910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131449d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13144a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13144ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13144b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13144bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13144c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13144c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13144c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13144cc80 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131304df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131305260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1313056d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131305b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131305fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131306420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131306890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131306d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131307170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1313075e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131307a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131308150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131308c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131309420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131309c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13130a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13130aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13130b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13130b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13130bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13130c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13130ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13130d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13130dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13130e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13130e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13130e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13130ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13130f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13130f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13130fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13130fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131310460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131310720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131310b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131311000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131311470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1313118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131311d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1313121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131312630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131312aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131312f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131313380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1313137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131313c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1313140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131314540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1313149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131314e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131315290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131315700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131315b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131315fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131316450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1313168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131316e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131317330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1313177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131317c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131318080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1313184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131318960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131318dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131319240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1313196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131319b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131319f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13131a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13131a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13131ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13131b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13131b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13131ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13131bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13131c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13131c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13131cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13131d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13131d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13131d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13131ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13131e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13131e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13131eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13131ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13131f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13131f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13131fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131320130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1313205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131320a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131320e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1313212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131321760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131321bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131322040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1313224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131322920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131322d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131323200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131323670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131323ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131323f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1313243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131324830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131324ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131325110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131325580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1313259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131325e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1313262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131326740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131326bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131327020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131327490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131327900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131327d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1313281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131328650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131328ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131328f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1313293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131329810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131329c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13132a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13132a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13132a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13132ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13132b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13132b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13132bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13132c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13132c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13132c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13132cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13132d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13132d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13132daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13132df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13132e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13132e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13132ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13132f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13132f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13132f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13132fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131330290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131330700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131330b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131330fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131331450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1313318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131331d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1313321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131332610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131332a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131332ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131333360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1313337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131333c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1313340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131334520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131334990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131334e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131335270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1313356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131336270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131336530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1313367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131336c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1313370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131337540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1313379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131337e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131338290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131338700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131338b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131338fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131339450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1313398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131339d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13133a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13133a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13133aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13133aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13133b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13133b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13133bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13133c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13133c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13133c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13133ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13133d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13133d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13133db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13133dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13133e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13133e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13133ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13133f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13133f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13133fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13133fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131340340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1313407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131340c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131341090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131341500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131341970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131341de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131342250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1313426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131342b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131342fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131343410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131343880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131343cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131344160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1313445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131344a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131344eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131345320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131345790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131345c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131346070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1313464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131346950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131346dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131347230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1313476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131347b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131347f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1313483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131348860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131348cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131349140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1313495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13134a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13134a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13134af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13134b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13134b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13134bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13134c040 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.772s
user	0m0.288s
sys	0m0.291s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4178 (84e1c33c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a80abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a80b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a80b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a80be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a80c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a80c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a80cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a80d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a80daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a80dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a80e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a80e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a80f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a80fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a8104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a810bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a811310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a811a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a812150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a812920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a813040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a813760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a813e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a814e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a815100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a815710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a816380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a8168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a816b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a817020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a8172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a817b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a8180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a818370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a818810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a818cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a819150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a8195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a819a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a819f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a81a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a81a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a81ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a81afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a81b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a81bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a81c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a81cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a81d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a81d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a81dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a81e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a81e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a81f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a81f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a81faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a81fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a820370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a820b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a820e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a8212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a821760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a821c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a8220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a822540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a8229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a822e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a823320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a8237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a823c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a824100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a8245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a824a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a824ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a825380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a825820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a825cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a826160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a826600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a826aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a826f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a8273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a827880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a827d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a8281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a828660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a828b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a828fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a829440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a8298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a829d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a82a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a82a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a82ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a82b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a82b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a81c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a82baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a82bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a82c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a82c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a82cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a82d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a82d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a82db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a82dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a82e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a82e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a82edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a82f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a82f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a82fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a830050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a8304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a830990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a830e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a8312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a831770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a831c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a8320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a832550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a8329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a832e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a833330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a8337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a833c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a834110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a8345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a834a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a834ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a835390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a835830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a835cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a836170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a836610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a836ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a836f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a8373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a837890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a837d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a8381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a838670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a838b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a838fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a839450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a8398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a839d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a83a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a83a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a83ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a83b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a83b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a83ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a83bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a83c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a83c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a83ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a83d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a83d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a83dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a83e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a83eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a83f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a83f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a83fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a8400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a840880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a840dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a841320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a841870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a842310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a842860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a842db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a843300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a843850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a843da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a8442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a844840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a844d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a8452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a845830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a845d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a8462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a846820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a846d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a8472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a847810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a847d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a8482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a848800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a848d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a8492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a8497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a849d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a84a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a84a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a84ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a84b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a84b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a84bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a84c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a84c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a84cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a84d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a84d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a84dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a84e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a84e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a84ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a84f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a84f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a84fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a850230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a850780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a850cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a851220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a851770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a851cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a852210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a852760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a852cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a853200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a8536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a853b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a853fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a854480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a854920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a854dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a855260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a855700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a855ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a856040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a8564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a856980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a856e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a857370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a857a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a8581b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a8588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a858ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a8592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a8598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a859ed0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.091.098 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139e06ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139e06f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139e073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139e07820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139e07c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139e08100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139e08570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139e089e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139e08e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139e092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139e09730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139e09df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139e0a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139e0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139e0b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139e0bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139e0c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139e0ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139e0d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139e0dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139e0e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139e0eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139e0f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139e0f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139e100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139e10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139e10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139e10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139e10f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139e11390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139e11890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139e11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139e12210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139e124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139e12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139e12db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139e13310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139e13810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139e13d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139e14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139e14710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139e14c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139e15110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139e15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139e15b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139e15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139e163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139e16860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139e16cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139e17140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139e175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139e17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139e17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139e18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139e18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139e18f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139e193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139e196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139e19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139e1a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139e1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139e1ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139e1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139e1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139e1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139e1c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139e1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139e1c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139e1ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139e1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139e1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139e1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139e1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139e1e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139e1ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139e1eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139e1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139e1f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139e1fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139e20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139e205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139e20a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139e20f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139e213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139e21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139e21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139e22180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139e22620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139e22ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139e22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139e23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139e238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139e23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139e241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139e24680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139e24b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139e24fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139e25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139e25900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139e25da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139e26240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139e266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139e26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139e27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139e274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139e27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139e27e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139e282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139e28740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139e28be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139e29080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139e29520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139e299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139e29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139e2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139e2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139e2ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139e2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139e2b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139e2ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139e2bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139e2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139e2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139e2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139e2d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139e2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139e2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139e2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139e2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139e2e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139e2ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139e2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139e2f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139e2fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139e2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139e30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139e308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139e30d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139e31200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139e316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139e31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139e31fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139e32480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139e32920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139e32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139e33260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139e33700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139e33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139e34040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139e344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139e34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139e34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139e352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139e35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139e35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139e362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139e36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139e36ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139e370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139e376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139e37cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139e38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139e38910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139e39100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139e395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139e39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139e39ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139e3a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139e3abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139e3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139e3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139e3bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139e3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139e3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139e3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139e3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139e3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139e3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139e3e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139e3e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139e3eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139e3f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139e3f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139e3fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139e400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139e40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139e40b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139e410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139e41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139e41b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139e420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139e42610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139e42b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139e430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139e43600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139e43b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139e440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139e445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139e44b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139e45090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139e455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139e45b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139e46080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139e465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139e46b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139e47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139e475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139e47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139e48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139e485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139e48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139e49050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139e495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139e49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139e4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139e4a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139e4aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139e4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139e4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139e4bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139e4c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139e4c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139e4cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139e4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139e4d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139e4d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139e4ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139e4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139e4e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139e4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139e4f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139e4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139e4f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139e4fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139e502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139e50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139e50c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139e51180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139e518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139e51fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139e526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139e52e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139e530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139e536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139e53ce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139e06970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139e06de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139e07250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139e076c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139e07b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139e07fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139e08410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139e08880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139e08cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139e09160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139e095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139e09bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139e0a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139e0ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139e0b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139e0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139e0c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139e0c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139e0cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139e0d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139e0e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139e0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139e0ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139e0f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139e0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139e10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139e104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139e10940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139e10db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139e11220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139e11690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139e11b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139e11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139e12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139e126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139e12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139e12f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139e133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139e13860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139e13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139e14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139e145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139e14a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139e14e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139e15300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139e15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139e15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139e16050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139e164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139e16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139e16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139e17210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139e17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139e17af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139e17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139e183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139e18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139e18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139e19120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139e19590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139e19a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139e19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139e1a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139e1a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139e1abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139e1b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139e1b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139e1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139e1bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139e1c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139e1c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139e1cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139e1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139e1d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139e1d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139e1dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139e1e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139e1e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139e1e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139e1ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139e1f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139e1f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139e1fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139e20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139e20480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139e208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139e20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139e211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139e21640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139e21ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139e21f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139e22390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139e22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139e22c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139e230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139e23550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139e239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139e23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139e242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139e24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139e24b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139e24ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139e25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139e258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139e25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139e261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139e26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139e26a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139e26f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139e27370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139e277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139e27c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139e280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139e28530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139e289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139e28e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139e29280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139e296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139e29b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139e29fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139e2a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139e2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139e2ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139e2b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139e2b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139e2ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139e2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139e2c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139e2c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139e2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139e2d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139e2d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139e2d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139e2ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139e2e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139e2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139e2eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139e2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139e2f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139e2f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139e2fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139e30170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139e305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139e30a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139e30ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139e31330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139e317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139e31c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139e32080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139e324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139e32960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139e32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139e33240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139e336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139e33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139e33f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139e34400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139e34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139e34ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139e35150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139e355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139e35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139e35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139e36310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139e36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139e36bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139e37060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139e377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139e37c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139e380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139e38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139e389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139e38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139e39280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139e396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139e39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139e39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139e3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139e3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139e3ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139e3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139e3b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139e3ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139e3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139e3c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139e3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139e3cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139e3d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139e3d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139e3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139e3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139e3e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139e3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139e3eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139e3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139e3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139e3f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139e3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139e40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139e405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139e40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139e40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139e41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a80bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a80c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a80b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a80c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a80a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a80abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a849cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a84a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a84a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a84aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a84ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a84b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a84b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a84bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a84c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a84c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a84c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a84cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a84d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a84d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a84dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a84df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a84e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a84e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a84ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a84f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a84f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a84f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a84fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a8502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a850730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a850ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a851010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a851480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a8518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a851fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a8526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a852dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a8534b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a853920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a853d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a854200 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.940s
user	0m0.239s
sys	0m0.143s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.53 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.12 sec
        1.14 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
