Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.577s
user	0m0.873s
sys	0m1.233s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-sampling
[ 50%] Built target test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Built target test-json-schema-to-grammar
[ 51%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Built target test-log
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Built target test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Built target test-backend-ops
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-gguf
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Built target test-autorelease
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-quantize-perf
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-eval-callback
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-gguf-split
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Built target llama-bench
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookahead
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-lookup-merge
[ 83%] Generating loading.html.hpp
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 85%] Built target llama-parallel
[ 85%] Built target llama-cli
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Built target llama-quantize
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-run
[ 91%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-speculative-simple
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Built target llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.978s
user	0m6.204s
sys	0m9.768s

main: quantize time =  2153.43 ms
main:    total time =  2153.44 ms

main: quantize time =  1339.59 ms
main:    total time =  1339.59 ms

main: quantize time =  1706.09 ms
main:    total time =  1706.09 ms

main: quantize time =  1381.63 ms
main:    total time =  1381.63 ms

main: quantize time =  2447.83 ms
main:    total time =  2447.83 ms

main: quantize time =  4989.75 ms
main:    total time =  4989.75 ms

main: quantize time =  5844.92 ms
main:    total time =  5844.92 ms

main: quantize time =  7145.57 ms
main:    total time =  7145.57 ms

main: quantize time =  5904.54 ms
main:    total time =  5904.54 ms

main: quantize time =  4513.34 ms
main:    total time =  4513.34 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.207 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.312 I main: llama backend init
0.00.000.318 I main: load the model and apply lora adapter, if any
0.00.047.298 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.061.222 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.061.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.061.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.061.245 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.061.246 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.061.247 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.061.248 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.061.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.061.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.061.252 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.061.252 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.061.253 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.061.254 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.061.255 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.061.257 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.061.258 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.061.259 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.069.805 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.072.038 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.987 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.078.989 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.989 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.990 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.991 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.992 I llama_model_loader: - type  f32:  194 tensors
0.00.078.992 I llama_model_loader: - type  f16:   98 tensors
0.00.078.993 I print_info: file format = GGUF V3 (latest)
0.00.078.995 I print_info: file type   = all F32 (guessed)
0.00.078.996 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.242 I load: special tokens cache size = 25
0.00.112.989 I load: token to piece cache size = 0.2984 MB
0.00.112.992 I print_info: arch             = gptneox
0.00.112.992 I print_info: vocab_only       = 0
0.00.112.992 I print_info: n_ctx_train      = 2048
0.00.112.992 I print_info: n_embd           = 2048
0.00.112.993 I print_info: n_layer          = 24
0.00.112.996 I print_info: n_head           = 16
0.00.112.997 I print_info: n_head_kv        = 16
0.00.112.997 I print_info: n_rot            = 32
0.00.112.997 I print_info: n_swa            = 0
0.00.112.997 I print_info: n_embd_head_k    = 128
0.00.112.998 I print_info: n_embd_head_v    = 128
0.00.112.998 I print_info: n_gqa            = 1
0.00.112.999 I print_info: n_embd_k_gqa     = 2048
0.00.112.999 I print_info: n_embd_v_gqa     = 2048
0.00.113.000 I print_info: f_norm_eps       = 1.0e-05
0.00.113.000 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.113.000 I print_info: f_clamp_kqv      = 0.0e+00
0.00.113.001 I print_info: f_max_alibi_bias = 0.0e+00
0.00.113.003 I print_info: f_logit_scale    = 0.0e+00
0.00.113.003 I print_info: n_ff             = 8192
0.00.113.004 I print_info: n_expert         = 0
0.00.113.004 I print_info: n_expert_used    = 0
0.00.113.004 I print_info: causal attn      = 1
0.00.113.005 I print_info: pooling type     = 0
0.00.113.005 I print_info: rope type        = 2
0.00.113.005 I print_info: rope scaling     = linear
0.00.113.006 I print_info: freq_base_train  = 10000.0
0.00.113.006 I print_info: freq_scale_train = 1
0.00.113.006 I print_info: n_ctx_orig_yarn  = 2048
0.00.113.006 I print_info: rope_finetuned   = unknown
0.00.113.006 I print_info: ssm_d_conv       = 0
0.00.113.006 I print_info: ssm_d_inner      = 0
0.00.113.007 I print_info: ssm_d_state      = 0
0.00.113.007 I print_info: ssm_dt_rank      = 0
0.00.113.007 I print_info: ssm_dt_b_c_rms   = 0
0.00.113.007 I print_info: model type       = 1.4B
0.00.113.007 I print_info: model params     = 1.41 B
0.00.113.008 I print_info: general.name     = 1.4B
0.00.113.012 I print_info: vocab type       = BPE
0.00.113.012 I print_info: n_vocab          = 50304
0.00.113.012 I print_info: n_merges         = 50009
0.00.113.012 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.113.013 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.113.013 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.113.013 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.113.013 I print_info: LF token         = 128 'Ä'
0.00.113.013 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.113.014 I print_info: max token length = 1024
0.00.115.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.115.564 I load_tensors: offloading output layer to GPU
0.00.115.564 I load_tensors: offloaded 25/25 layers to GPU
0.00.115.583 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.115.584 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.115.865 I llama_init_from_model: n_seq_max     = 1
0.00.115.866 I llama_init_from_model: n_ctx         = 2048
0.00.115.866 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.115.866 I llama_init_from_model: n_batch       = 2048
0.00.115.867 I llama_init_from_model: n_ubatch      = 512
0.00.115.867 I llama_init_from_model: flash_attn    = 0
0.00.115.867 I llama_init_from_model: freq_base     = 10000.0
0.00.115.867 I llama_init_from_model: freq_scale    = 1
0.00.115.868 I ggml_metal_init: allocating
0.00.115.871 I ggml_metal_init: found device: Apple M4
0.00.115.873 I ggml_metal_init: picking default device: Apple M4
0.00.116.569 I ggml_metal_init: using embedded metal library
0.00.133.251 I ggml_metal_init: GPU name:   Apple M4
0.00.133.253 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.133.253 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.133.254 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.133.254 I ggml_metal_init: simdgroup reduction   = true
0.00.133.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.133.254 I ggml_metal_init: has bfloat            = true
0.00.133.254 I ggml_metal_init: use bfloat            = true
0.00.133.255 I ggml_metal_init: hasUnifiedMemory      = true
0.00.133.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.203.114 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.226.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.226.009 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.226.034 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.226.995 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.226.997 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.226.998 I llama_init_from_model: graph nodes  = 967
0.00.226.998 I llama_init_from_model: graph splits = 2
0.00.227.003 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.227.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.227.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.313.516 I main: llama threadpool init, n_threads = 4
0.00.313.560 I 
0.00.313.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.313.582 I 
0.00.313.652 I sampler seed: 1234
0.00.313.657 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.313.682 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.313.683 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.313.683 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.161.912 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.02.161.912 I llama_perf_context_print:        load time =     266.20 ms
0.02.161.913 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.77 tokens per second)
0.02.161.914 I llama_perf_context_print:        eval time =    1791.10 ms /    63 runs   (   28.43 ms per token,    35.17 tokens per second)
0.02.161.914 I llama_perf_context_print:       total time =    1848.40 ms /    70 tokens
0.02.162.131 I ggml_metal_free: deallocating

real	0m2.475s
user	0m0.144s
sys	0m0.109s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.009.815 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.718 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.034.724 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.731 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.732 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.732 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.732 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.733 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.734 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.734 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.735 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.735 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.736 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.738 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.739 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.739 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.368 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.370 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.371 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.043.371 I llama_model_loader: - type  f32:  194 tensors
0.00.043.372 I llama_model_loader: - type q8_0:   98 tensors
0.00.043.372 I print_info: file format = GGUF V3 (latest)
0.00.043.373 I print_info: file type   = Q8_0
0.00.043.375 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.067.361 I load: special tokens cache size = 25
0.00.075.851 I load: token to piece cache size = 0.2984 MB
0.00.075.855 I print_info: arch             = gptneox
0.00.075.855 I print_info: vocab_only       = 0
0.00.075.856 I print_info: n_ctx_train      = 2048
0.00.075.856 I print_info: n_embd           = 2048
0.00.075.856 I print_info: n_layer          = 24
0.00.075.862 I print_info: n_head           = 16
0.00.075.865 I print_info: n_head_kv        = 16
0.00.075.866 I print_info: n_rot            = 32
0.00.075.866 I print_info: n_swa            = 0
0.00.075.866 I print_info: n_embd_head_k    = 128
0.00.075.868 I print_info: n_embd_head_v    = 128
0.00.075.868 I print_info: n_gqa            = 1
0.00.075.869 I print_info: n_embd_k_gqa     = 2048
0.00.075.870 I print_info: n_embd_v_gqa     = 2048
0.00.075.871 I print_info: f_norm_eps       = 1.0e-05
0.00.075.871 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.872 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.872 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.873 I print_info: f_logit_scale    = 0.0e+00
0.00.075.873 I print_info: n_ff             = 8192
0.00.075.873 I print_info: n_expert         = 0
0.00.075.874 I print_info: n_expert_used    = 0
0.00.075.874 I print_info: causal attn      = 1
0.00.075.874 I print_info: pooling type     = 0
0.00.075.874 I print_info: rope type        = 2
0.00.075.874 I print_info: rope scaling     = linear
0.00.075.875 I print_info: freq_base_train  = 10000.0
0.00.075.875 I print_info: freq_scale_train = 1
0.00.075.875 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.876 I print_info: rope_finetuned   = unknown
0.00.075.876 I print_info: ssm_d_conv       = 0
0.00.075.876 I print_info: ssm_d_inner      = 0
0.00.075.876 I print_info: ssm_d_state      = 0
0.00.075.876 I print_info: ssm_dt_rank      = 0
0.00.075.878 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.879 I print_info: model type       = 1.4B
0.00.075.879 I print_info: model params     = 1.41 B
0.00.075.879 I print_info: general.name     = 1.4B
0.00.075.880 I print_info: vocab type       = BPE
0.00.075.880 I print_info: n_vocab          = 50304
0.00.075.880 I print_info: n_merges         = 50009
0.00.075.881 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.881 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.881 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.881 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.882 I print_info: LF token         = 128 'Ä'
0.00.075.882 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.882 I print_info: max token length = 1024
0.00.078.795 I load_tensors: offloading 24 repeating layers to GPU
0.00.078.795 I load_tensors: offloading output layer to GPU
0.00.078.795 I load_tensors: offloaded 25/25 layers to GPU
0.00.078.808 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.078.809 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.079.235 I llama_init_from_model: n_seq_max     = 1
0.00.079.236 I llama_init_from_model: n_ctx         = 2048
0.00.079.237 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.079.237 I llama_init_from_model: n_batch       = 2048
0.00.079.237 I llama_init_from_model: n_ubatch      = 512
0.00.079.237 I llama_init_from_model: flash_attn    = 0
0.00.079.238 I llama_init_from_model: freq_base     = 10000.0
0.00.079.238 I llama_init_from_model: freq_scale    = 1
0.00.079.239 I ggml_metal_init: allocating
0.00.079.243 I ggml_metal_init: found device: Apple M4
0.00.079.245 I ggml_metal_init: picking default device: Apple M4
0.00.080.165 I ggml_metal_init: using embedded metal library
0.00.084.010 I ggml_metal_init: GPU name:   Apple M4
0.00.084.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.084.013 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.084.014 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.084.014 I ggml_metal_init: simdgroup reduction   = true
0.00.084.014 I ggml_metal_init: simdgroup matrix mul. = true
0.00.084.014 I ggml_metal_init: has bfloat            = true
0.00.084.015 I ggml_metal_init: use bfloat            = true
0.00.084.015 I ggml_metal_init: hasUnifiedMemory      = true
0.00.084.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.024 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.123.748 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.755 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.782 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.125.065 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.125.068 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.125.068 I llama_init_from_model: graph nodes  = 967
0.00.125.068 I llama_init_from_model: graph splits = 2
0.00.125.072 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.125.188 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.125.189 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.417.365 I main: llama threadpool init, n_threads = 4
0.01.417.414 I 
0.01.417.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.417.437 I 
0.01.417.662 I sampler seed: 1234
0.01.417.667 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.417.704 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.417.707 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.417.707 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.511.912 I llama_perf_sampler_print:    sampling time =       1.54 ms /    71 runs   (    0.02 ms per token, 46073.98 tokens per second)
0.02.511.913 I llama_perf_context_print:        load time =    1407.54 ms
0.02.511.914 I llama_perf_context_print: prompt eval time =      45.47 ms /     7 tokens (    6.50 ms per token,   153.95 tokens per second)
0.02.511.914 I llama_perf_context_print:        eval time =    1046.12 ms /    63 runs   (   16.61 ms per token,    60.22 tokens per second)
0.02.511.915 I llama_perf_context_print:       total time =    1094.55 ms /    70 tokens
0.02.512.165 I ggml_metal_free: deallocating

real	0m2.533s
user	0m0.122s
sys	0m0.225s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.017.282 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.946 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.951 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.953 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.953 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.953 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.954 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.954 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.956 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.957 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.957 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.959 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.960 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.397 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.564 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.134 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.135 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.137 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.048.137 I llama_model_loader: - type  f32:  194 tensors
0.00.048.138 I llama_model_loader: - type q4_0:   97 tensors
0.00.048.138 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.138 I print_info: file format = GGUF V3 (latest)
0.00.048.139 I print_info: file type   = Q4_0
0.00.048.140 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.075.408 I load: special tokens cache size = 25
0.00.086.719 I load: token to piece cache size = 0.2984 MB
0.00.086.723 I print_info: arch             = gptneox
0.00.086.724 I print_info: vocab_only       = 0
0.00.086.724 I print_info: n_ctx_train      = 2048
0.00.086.724 I print_info: n_embd           = 2048
0.00.086.724 I print_info: n_layer          = 24
0.00.086.728 I print_info: n_head           = 16
0.00.086.730 I print_info: n_head_kv        = 16
0.00.086.730 I print_info: n_rot            = 32
0.00.086.730 I print_info: n_swa            = 0
0.00.086.730 I print_info: n_embd_head_k    = 128
0.00.086.731 I print_info: n_embd_head_v    = 128
0.00.086.732 I print_info: n_gqa            = 1
0.00.086.733 I print_info: n_embd_k_gqa     = 2048
0.00.086.737 I print_info: n_embd_v_gqa     = 2048
0.00.086.738 I print_info: f_norm_eps       = 1.0e-05
0.00.086.739 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.739 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.739 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.740 I print_info: f_logit_scale    = 0.0e+00
0.00.086.741 I print_info: n_ff             = 8192
0.00.086.741 I print_info: n_expert         = 0
0.00.086.741 I print_info: n_expert_used    = 0
0.00.086.742 I print_info: causal attn      = 1
0.00.086.742 I print_info: pooling type     = 0
0.00.086.742 I print_info: rope type        = 2
0.00.086.742 I print_info: rope scaling     = linear
0.00.086.743 I print_info: freq_base_train  = 10000.0
0.00.086.744 I print_info: freq_scale_train = 1
0.00.086.744 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.744 I print_info: rope_finetuned   = unknown
0.00.086.744 I print_info: ssm_d_conv       = 0
0.00.086.745 I print_info: ssm_d_inner      = 0
0.00.086.745 I print_info: ssm_d_state      = 0
0.00.086.747 I print_info: ssm_dt_rank      = 0
0.00.086.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.747 I print_info: model type       = 1.4B
0.00.086.748 I print_info: model params     = 1.41 B
0.00.086.748 I print_info: general.name     = 1.4B
0.00.086.749 I print_info: vocab type       = BPE
0.00.086.751 I print_info: n_vocab          = 50304
0.00.086.751 I print_info: n_merges         = 50009
0.00.086.751 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.752 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.752 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.752 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.753 I print_info: LF token         = 128 'Ä'
0.00.086.753 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.753 I print_info: max token length = 1024
0.00.089.646 I load_tensors: offloading 24 repeating layers to GPU
0.00.089.647 I load_tensors: offloading output layer to GPU
0.00.089.647 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.659 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.089.661 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.090.150 I llama_init_from_model: n_seq_max     = 1
0.00.090.152 I llama_init_from_model: n_ctx         = 2048
0.00.090.152 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.090.152 I llama_init_from_model: n_batch       = 2048
0.00.090.153 I llama_init_from_model: n_ubatch      = 512
0.00.090.153 I llama_init_from_model: flash_attn    = 0
0.00.090.153 I llama_init_from_model: freq_base     = 10000.0
0.00.090.154 I llama_init_from_model: freq_scale    = 1
0.00.090.154 I ggml_metal_init: allocating
0.00.090.159 I ggml_metal_init: found device: Apple M4
0.00.090.162 I ggml_metal_init: picking default device: Apple M4
0.00.091.087 I ggml_metal_init: using embedded metal library
0.00.094.987 I ggml_metal_init: GPU name:   Apple M4
0.00.094.990 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.991 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.991 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.991 I ggml_metal_init: simdgroup reduction   = true
0.00.094.992 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.992 I ggml_metal_init: has bfloat            = true
0.00.094.992 I ggml_metal_init: use bfloat            = true
0.00.094.992 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.116 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.133.782 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.133.789 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.133.812 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.134.853 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.134.855 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.134.856 I llama_init_from_model: graph nodes  = 967
0.00.134.856 I llama_init_from_model: graph splits = 2
0.00.134.859 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.134.975 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.134.976 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.552 I main: llama threadpool init, n_threads = 4
0.00.794.675 I 
0.00.794.732 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.733 I 
0.00.795.249 I sampler seed: 1234
0.00.795.256 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.287 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.290 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.290 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.482.932 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.482.933 I llama_perf_context_print:        load time =     777.26 ms
0.01.482.933 I llama_perf_context_print: prompt eval time =      40.45 ms /     7 tokens (    5.78 ms per token,   173.06 tokens per second)
0.01.482.934 I llama_perf_context_print:        eval time =     644.17 ms /    63 runs   (   10.22 ms per token,    97.80 tokens per second)
0.01.482.934 I llama_perf_context_print:       total time =     688.39 ms /    70 tokens
0.01.483.150 I ggml_metal_free: deallocating

real	0m1.503s
user	0m0.136s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.011.662 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.835 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.839 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.843 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.845 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.848 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.848 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.510 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.202 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.203 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.204 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.204 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.205 I llama_model_loader: - type  f32:  194 tensors
0.00.029.205 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.206 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.206 I print_info: file format = GGUF V3 (latest)
0.00.029.207 I print_info: file type   = Q4_1
0.00.029.208 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.972 I load: special tokens cache size = 25
0.00.053.977 I load: token to piece cache size = 0.2984 MB
0.00.053.980 I print_info: arch             = gptneox
0.00.053.981 I print_info: vocab_only       = 0
0.00.053.981 I print_info: n_ctx_train      = 2048
0.00.053.981 I print_info: n_embd           = 2048
0.00.053.981 I print_info: n_layer          = 24
0.00.053.984 I print_info: n_head           = 16
0.00.053.985 I print_info: n_head_kv        = 16
0.00.053.985 I print_info: n_rot            = 32
0.00.053.985 I print_info: n_swa            = 0
0.00.053.986 I print_info: n_embd_head_k    = 128
0.00.053.988 I print_info: n_embd_head_v    = 128
0.00.053.989 I print_info: n_gqa            = 1
0.00.053.990 I print_info: n_embd_k_gqa     = 2048
0.00.053.990 I print_info: n_embd_v_gqa     = 2048
0.00.053.991 I print_info: f_norm_eps       = 1.0e-05
0.00.053.991 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.992 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.992 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.992 I print_info: f_logit_scale    = 0.0e+00
0.00.053.993 I print_info: n_ff             = 8192
0.00.053.993 I print_info: n_expert         = 0
0.00.053.993 I print_info: n_expert_used    = 0
0.00.053.994 I print_info: causal attn      = 1
0.00.053.996 I print_info: pooling type     = 0
0.00.053.996 I print_info: rope type        = 2
0.00.053.997 I print_info: rope scaling     = linear
0.00.053.997 I print_info: freq_base_train  = 10000.0
0.00.053.997 I print_info: freq_scale_train = 1
0.00.053.997 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.998 I print_info: rope_finetuned   = unknown
0.00.053.998 I print_info: ssm_d_conv       = 0
0.00.053.998 I print_info: ssm_d_inner      = 0
0.00.053.998 I print_info: ssm_d_state      = 0
0.00.053.998 I print_info: ssm_dt_rank      = 0
0.00.053.998 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.002 I print_info: model type       = 1.4B
0.00.054.003 I print_info: model params     = 1.41 B
0.00.054.003 I print_info: general.name     = 1.4B
0.00.054.004 I print_info: vocab type       = BPE
0.00.054.004 I print_info: n_vocab          = 50304
0.00.054.004 I print_info: n_merges         = 50009
0.00.054.004 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.004 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.005 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.005 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.006 I print_info: LF token         = 128 'Ä'
0.00.054.006 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.006 I print_info: max token length = 1024
0.00.055.901 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.901 I load_tensors: offloading output layer to GPU
0.00.055.901 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.911 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.912 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.056.197 I llama_init_from_model: n_seq_max     = 1
0.00.056.197 I llama_init_from_model: n_ctx         = 2048
0.00.056.198 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.198 I llama_init_from_model: n_batch       = 2048
0.00.056.198 I llama_init_from_model: n_ubatch      = 512
0.00.056.198 I llama_init_from_model: flash_attn    = 0
0.00.056.198 I llama_init_from_model: freq_base     = 10000.0
0.00.056.199 I llama_init_from_model: freq_scale    = 1
0.00.056.199 I ggml_metal_init: allocating
0.00.056.202 I ggml_metal_init: found device: Apple M4
0.00.056.204 I ggml_metal_init: picking default device: Apple M4
0.00.056.790 I ggml_metal_init: using embedded metal library
0.00.059.098 I ggml_metal_init: GPU name:   Apple M4
0.00.059.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.101 I ggml_metal_init: simdgroup reduction   = true
0.00.059.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.101 I ggml_metal_init: has bfloat            = true
0.00.059.101 I ggml_metal_init: use bfloat            = true
0.00.059.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.990 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.560 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.566 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.587 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.891 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.091.893 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.091.893 I llama_init_from_model: graph nodes  = 967
0.00.091.894 I llama_init_from_model: graph splits = 2
0.00.091.896 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.071 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.072 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.477 I main: llama threadpool init, n_threads = 4
0.00.764.522 I 
0.00.764.542 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.542 I 
0.00.764.777 I sampler seed: 1234
0.00.764.782 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.793 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.793 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.793 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.503.732 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62226.12 tokens per second)
0.01.503.733 I llama_perf_context_print:        load time =     752.81 ms
0.01.503.733 I llama_perf_context_print: prompt eval time =      45.93 ms /     7 tokens (    6.56 ms per token,   152.40 tokens per second)
0.01.503.735 I llama_perf_context_print:        eval time =     690.09 ms /    63 runs   (   10.95 ms per token,    91.29 tokens per second)
0.01.503.735 I llama_perf_context_print:       total time =     739.26 ms /    70 tokens
0.01.503.955 I ggml_metal_free: deallocating

real	0m1.521s
user	0m0.109s
sys	0m0.143s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.658 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.670 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.681 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.682 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.683 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.684 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.684 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.685 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.687 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.405 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.129 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.129 I llama_model_loader: - type  f32:  194 tensors
0.00.026.130 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.130 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.130 I print_info: file format = GGUF V3 (latest)
0.00.026.131 I print_info: file type   = Q5_0
0.00.026.132 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.363 I load: special tokens cache size = 25
0.00.051.355 I load: token to piece cache size = 0.2984 MB
0.00.051.358 I print_info: arch             = gptneox
0.00.051.358 I print_info: vocab_only       = 0
0.00.051.359 I print_info: n_ctx_train      = 2048
0.00.051.359 I print_info: n_embd           = 2048
0.00.051.359 I print_info: n_layer          = 24
0.00.051.362 I print_info: n_head           = 16
0.00.051.363 I print_info: n_head_kv        = 16
0.00.051.363 I print_info: n_rot            = 32
0.00.051.363 I print_info: n_swa            = 0
0.00.051.366 I print_info: n_embd_head_k    = 128
0.00.051.366 I print_info: n_embd_head_v    = 128
0.00.051.367 I print_info: n_gqa            = 1
0.00.051.367 I print_info: n_embd_k_gqa     = 2048
0.00.051.368 I print_info: n_embd_v_gqa     = 2048
0.00.051.369 I print_info: f_norm_eps       = 1.0e-05
0.00.051.369 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.369 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.370 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.371 I print_info: f_logit_scale    = 0.0e+00
0.00.051.372 I print_info: n_ff             = 8192
0.00.051.372 I print_info: n_expert         = 0
0.00.051.372 I print_info: n_expert_used    = 0
0.00.051.372 I print_info: causal attn      = 1
0.00.051.373 I print_info: pooling type     = 0
0.00.051.373 I print_info: rope type        = 2
0.00.051.373 I print_info: rope scaling     = linear
0.00.051.374 I print_info: freq_base_train  = 10000.0
0.00.051.374 I print_info: freq_scale_train = 1
0.00.051.374 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.374 I print_info: rope_finetuned   = unknown
0.00.051.375 I print_info: ssm_d_conv       = 0
0.00.051.375 I print_info: ssm_d_inner      = 0
0.00.051.375 I print_info: ssm_d_state      = 0
0.00.051.376 I print_info: ssm_dt_rank      = 0
0.00.051.377 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.377 I print_info: model type       = 1.4B
0.00.051.378 I print_info: model params     = 1.41 B
0.00.051.378 I print_info: general.name     = 1.4B
0.00.051.378 I print_info: vocab type       = BPE
0.00.051.379 I print_info: n_vocab          = 50304
0.00.051.379 I print_info: n_merges         = 50009
0.00.051.379 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.379 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.379 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.380 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.380 I print_info: LF token         = 128 'Ä'
0.00.051.380 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.380 I print_info: max token length = 1024
0.00.053.385 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.385 I load_tensors: offloading output layer to GPU
0.00.053.385 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.396 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.397 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.684 I llama_init_from_model: n_seq_max     = 1
0.00.053.684 I llama_init_from_model: n_ctx         = 2048
0.00.053.685 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.685 I llama_init_from_model: n_batch       = 2048
0.00.053.685 I llama_init_from_model: n_ubatch      = 512
0.00.053.685 I llama_init_from_model: flash_attn    = 0
0.00.053.686 I llama_init_from_model: freq_base     = 10000.0
0.00.053.686 I llama_init_from_model: freq_scale    = 1
0.00.053.686 I ggml_metal_init: allocating
0.00.053.690 I ggml_metal_init: found device: Apple M4
0.00.053.692 I ggml_metal_init: picking default device: Apple M4
0.00.054.276 I ggml_metal_init: using embedded metal library
0.00.056.918 I ggml_metal_init: GPU name:   Apple M4
0.00.056.920 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.920 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.920 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.920 I ggml_metal_init: simdgroup reduction   = true
0.00.056.921 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.921 I ggml_metal_init: has bfloat            = true
0.00.056.921 I ggml_metal_init: use bfloat            = true
0.00.056.921 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.922 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.634 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.812 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.822 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.843 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.901 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.902 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.903 I llama_init_from_model: graph nodes  = 967
0.00.086.903 I llama_init_from_model: graph splits = 2
0.00.086.906 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.054 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.049 I main: llama threadpool init, n_threads = 4
0.00.780.087 I 
0.00.780.116 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.116 I 
0.00.780.349 I sampler seed: 1234
0.00.780.353 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.364 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.365 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.365 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.569.527 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.569.527 I llama_perf_context_print:        load time =     771.39 ms
0.01.569.528 I llama_perf_context_print: prompt eval time =      43.08 ms /     7 tokens (    6.15 ms per token,   162.48 tokens per second)
0.01.569.529 I llama_perf_context_print:        eval time =     743.11 ms /    63 runs   (   11.80 ms per token,    84.78 tokens per second)
0.01.569.529 I llama_perf_context_print:       total time =     789.48 ms /    70 tokens
0.01.569.784 I ggml_metal_free: deallocating

real	0m1.586s
user	0m0.108s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.889 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.241 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.242 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.242 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.245 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.246 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.246 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.247 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.248 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.252 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.050 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.940 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.942 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.943 I llama_model_loader: - type  f32:  194 tensors
0.00.025.943 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.943 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.944 I print_info: file format = GGUF V3 (latest)
0.00.025.944 I print_info: file type   = Q5_1
0.00.025.948 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.243 I load: special tokens cache size = 25
0.00.051.192 I load: token to piece cache size = 0.2984 MB
0.00.051.195 I print_info: arch             = gptneox
0.00.051.196 I print_info: vocab_only       = 0
0.00.051.196 I print_info: n_ctx_train      = 2048
0.00.051.196 I print_info: n_embd           = 2048
0.00.051.196 I print_info: n_layer          = 24
0.00.051.199 I print_info: n_head           = 16
0.00.051.200 I print_info: n_head_kv        = 16
0.00.051.200 I print_info: n_rot            = 32
0.00.051.201 I print_info: n_swa            = 0
0.00.051.201 I print_info: n_embd_head_k    = 128
0.00.051.201 I print_info: n_embd_head_v    = 128
0.00.051.202 I print_info: n_gqa            = 1
0.00.051.203 I print_info: n_embd_k_gqa     = 2048
0.00.051.203 I print_info: n_embd_v_gqa     = 2048
0.00.051.204 I print_info: f_norm_eps       = 1.0e-05
0.00.051.204 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.206 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.207 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.207 I print_info: f_logit_scale    = 0.0e+00
0.00.051.207 I print_info: n_ff             = 8192
0.00.051.208 I print_info: n_expert         = 0
0.00.051.208 I print_info: n_expert_used    = 0
0.00.051.211 I print_info: causal attn      = 1
0.00.051.211 I print_info: pooling type     = 0
0.00.051.211 I print_info: rope type        = 2
0.00.051.211 I print_info: rope scaling     = linear
0.00.051.212 I print_info: freq_base_train  = 10000.0
0.00.051.212 I print_info: freq_scale_train = 1
0.00.051.212 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.212 I print_info: rope_finetuned   = unknown
0.00.051.212 I print_info: ssm_d_conv       = 0
0.00.051.213 I print_info: ssm_d_inner      = 0
0.00.051.213 I print_info: ssm_d_state      = 0
0.00.051.213 I print_info: ssm_dt_rank      = 0
0.00.051.213 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.213 I print_info: model type       = 1.4B
0.00.051.217 I print_info: model params     = 1.41 B
0.00.051.217 I print_info: general.name     = 1.4B
0.00.051.218 I print_info: vocab type       = BPE
0.00.051.218 I print_info: n_vocab          = 50304
0.00.051.218 I print_info: n_merges         = 50009
0.00.051.218 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.219 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.219 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.219 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.219 I print_info: LF token         = 128 'Ä'
0.00.051.221 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.221 I print_info: max token length = 1024
0.00.053.211 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.212 I load_tensors: offloading output layer to GPU
0.00.053.212 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.222 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.224 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.500 I llama_init_from_model: n_seq_max     = 1
0.00.053.501 I llama_init_from_model: n_ctx         = 2048
0.00.053.501 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.501 I llama_init_from_model: n_batch       = 2048
0.00.053.501 I llama_init_from_model: n_ubatch      = 512
0.00.053.501 I llama_init_from_model: flash_attn    = 0
0.00.053.502 I llama_init_from_model: freq_base     = 10000.0
0.00.053.502 I llama_init_from_model: freq_scale    = 1
0.00.053.502 I ggml_metal_init: allocating
0.00.053.506 I ggml_metal_init: found device: Apple M4
0.00.053.508 I ggml_metal_init: picking default device: Apple M4
0.00.054.104 I ggml_metal_init: using embedded metal library
0.00.056.464 I ggml_metal_init: GPU name:   Apple M4
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.467 I ggml_metal_init: simdgroup reduction   = true
0.00.056.467 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.467 I ggml_metal_init: has bfloat            = true
0.00.056.467 I ggml_metal_init: use bfloat            = true
0.00.056.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.295 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.358 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.369 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.390 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.380 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.381 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.382 I llama_init_from_model: graph nodes  = 967
0.00.087.382 I llama_init_from_model: graph splits = 2
0.00.087.385 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.516 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.551 I main: llama threadpool init, n_threads = 4
0.00.726.591 I 
0.00.726.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.613 I 
0.00.726.903 I sampler seed: 1234
0.00.726.907 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.726.918 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.726.918 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.726.919 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.554.637 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.554.638 I llama_perf_context_print:        load time =     716.66 ms
0.01.554.639 I llama_perf_context_print: prompt eval time =      42.16 ms /     7 tokens (    6.02 ms per token,   166.05 tokens per second)
0.01.554.639 I llama_perf_context_print:        eval time =     782.51 ms /    63 runs   (   12.42 ms per token,    80.51 tokens per second)
0.01.554.640 I llama_perf_context_print:       total time =     828.09 ms /    70 tokens
0.01.554.867 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.109s
sys	0m0.163s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.380 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.919 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.926 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.926 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.927 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.927 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.927 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.928 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.929 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.929 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.929 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.934 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.934 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.721 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.498 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.498 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.498 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.499 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.499 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.500 I llama_model_loader: - type  f32:  194 tensors
0.00.024.500 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.500 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.500 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.501 I print_info: file format = GGUF V3 (latest)
0.00.024.501 I print_info: file type   = Q2_K - Medium
0.00.024.502 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.892 I load: special tokens cache size = 25
0.00.049.920 I load: token to piece cache size = 0.2984 MB
0.00.049.923 I print_info: arch             = gptneox
0.00.049.923 I print_info: vocab_only       = 0
0.00.049.923 I print_info: n_ctx_train      = 2048
0.00.049.923 I print_info: n_embd           = 2048
0.00.049.923 I print_info: n_layer          = 24
0.00.049.927 I print_info: n_head           = 16
0.00.049.927 I print_info: n_head_kv        = 16
0.00.049.928 I print_info: n_rot            = 32
0.00.049.928 I print_info: n_swa            = 0
0.00.049.928 I print_info: n_embd_head_k    = 128
0.00.049.928 I print_info: n_embd_head_v    = 128
0.00.049.929 I print_info: n_gqa            = 1
0.00.049.930 I print_info: n_embd_k_gqa     = 2048
0.00.049.930 I print_info: n_embd_v_gqa     = 2048
0.00.049.931 I print_info: f_norm_eps       = 1.0e-05
0.00.049.931 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.931 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.933 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.933 I print_info: f_logit_scale    = 0.0e+00
0.00.049.936 I print_info: n_ff             = 8192
0.00.049.936 I print_info: n_expert         = 0
0.00.049.936 I print_info: n_expert_used    = 0
0.00.049.936 I print_info: causal attn      = 1
0.00.049.937 I print_info: pooling type     = 0
0.00.049.937 I print_info: rope type        = 2
0.00.049.937 I print_info: rope scaling     = linear
0.00.049.939 I print_info: freq_base_train  = 10000.0
0.00.049.939 I print_info: freq_scale_train = 1
0.00.049.939 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.939 I print_info: rope_finetuned   = unknown
0.00.049.939 I print_info: ssm_d_conv       = 0
0.00.049.940 I print_info: ssm_d_inner      = 0
0.00.049.940 I print_info: ssm_d_state      = 0
0.00.049.940 I print_info: ssm_dt_rank      = 0
0.00.049.940 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.940 I print_info: model type       = 1.4B
0.00.049.941 I print_info: model params     = 1.41 B
0.00.049.941 I print_info: general.name     = 1.4B
0.00.049.945 I print_info: vocab type       = BPE
0.00.049.945 I print_info: n_vocab          = 50304
0.00.049.946 I print_info: n_merges         = 50009
0.00.049.946 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.946 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.946 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.946 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.948 I print_info: LF token         = 128 'Ä'
0.00.049.948 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.948 I print_info: max token length = 1024
0.00.051.563 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.563 I load_tensors: offloading output layer to GPU
0.00.051.563 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.573 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.574 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.860 I llama_init_from_model: n_seq_max     = 1
0.00.051.861 I llama_init_from_model: n_ctx         = 2048
0.00.051.861 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.862 I llama_init_from_model: n_batch       = 2048
0.00.051.862 I llama_init_from_model: n_ubatch      = 512
0.00.051.862 I llama_init_from_model: flash_attn    = 0
0.00.051.862 I llama_init_from_model: freq_base     = 10000.0
0.00.051.863 I llama_init_from_model: freq_scale    = 1
0.00.051.863 I ggml_metal_init: allocating
0.00.051.866 I ggml_metal_init: found device: Apple M4
0.00.051.868 I ggml_metal_init: picking default device: Apple M4
0.00.052.492 I ggml_metal_init: using embedded metal library
0.00.054.874 I ggml_metal_init: GPU name:   Apple M4
0.00.054.875 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.876 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.876 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.876 I ggml_metal_init: simdgroup reduction   = true
0.00.054.876 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.876 I ggml_metal_init: has bfloat            = true
0.00.054.877 I ggml_metal_init: use bfloat            = true
0.00.054.877 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.878 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.899 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.240 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.247 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.270 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.305 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.306 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.306 I llama_init_from_model: graph nodes  = 967
0.00.085.307 I llama_init_from_model: graph splits = 2
0.00.085.310 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.449 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.431.056 I main: llama threadpool init, n_threads = 4
0.00.431.105 I 
0.00.431.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.431.129 I 
0.00.431.291 I sampler seed: 1234
0.00.431.296 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.431.306 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.431.306 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.431.306 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.121.206 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.121.207 I llama_perf_context_print:        load time =     421.67 ms
0.01.121.208 I llama_perf_context_print: prompt eval time =      35.90 ms /     7 tokens (    5.13 ms per token,   194.97 tokens per second)
0.01.121.209 I llama_perf_context_print:        eval time =     650.97 ms /    63 runs   (   10.33 ms per token,    96.78 tokens per second)
0.01.121.209 I llama_perf_context_print:       total time =     690.16 ms /    70 tokens
0.01.121.467 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.109s
sys	0m0.100s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.013.327 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.822 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.020.827 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.830 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.830 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.831 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.834 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.834 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.834 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.835 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.835 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.836 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.838 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.838 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.838 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.597 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.431 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.432 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.433 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.029.434 I llama_model_loader: - type  f32:  194 tensors
0.00.029.434 I llama_model_loader: - type q3_K:   25 tensors
0.00.029.434 I llama_model_loader: - type q4_K:   71 tensors
0.00.029.434 I llama_model_loader: - type q5_K:    1 tensors
0.00.029.435 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.435 I print_info: file format = GGUF V3 (latest)
0.00.029.436 I print_info: file type   = Q3_K - Medium
0.00.029.438 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.048.843 I load: special tokens cache size = 25
0.00.054.574 I load: token to piece cache size = 0.2984 MB
0.00.054.577 I print_info: arch             = gptneox
0.00.054.577 I print_info: vocab_only       = 0
0.00.054.578 I print_info: n_ctx_train      = 2048
0.00.054.578 I print_info: n_embd           = 2048
0.00.054.578 I print_info: n_layer          = 24
0.00.054.581 I print_info: n_head           = 16
0.00.054.582 I print_info: n_head_kv        = 16
0.00.054.582 I print_info: n_rot            = 32
0.00.054.582 I print_info: n_swa            = 0
0.00.054.582 I print_info: n_embd_head_k    = 128
0.00.054.583 I print_info: n_embd_head_v    = 128
0.00.054.583 I print_info: n_gqa            = 1
0.00.054.584 I print_info: n_embd_k_gqa     = 2048
0.00.054.585 I print_info: n_embd_v_gqa     = 2048
0.00.054.585 I print_info: f_norm_eps       = 1.0e-05
0.00.054.586 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.586 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.586 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.586 I print_info: f_logit_scale    = 0.0e+00
0.00.054.587 I print_info: n_ff             = 8192
0.00.054.588 I print_info: n_expert         = 0
0.00.054.589 I print_info: n_expert_used    = 0
0.00.054.589 I print_info: causal attn      = 1
0.00.054.589 I print_info: pooling type     = 0
0.00.054.591 I print_info: rope type        = 2
0.00.054.591 I print_info: rope scaling     = linear
0.00.054.592 I print_info: freq_base_train  = 10000.0
0.00.054.592 I print_info: freq_scale_train = 1
0.00.054.592 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.592 I print_info: rope_finetuned   = unknown
0.00.054.593 I print_info: ssm_d_conv       = 0
0.00.054.593 I print_info: ssm_d_inner      = 0
0.00.054.593 I print_info: ssm_d_state      = 0
0.00.054.593 I print_info: ssm_dt_rank      = 0
0.00.054.593 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.593 I print_info: model type       = 1.4B
0.00.054.594 I print_info: model params     = 1.41 B
0.00.054.596 I print_info: general.name     = 1.4B
0.00.054.596 I print_info: vocab type       = BPE
0.00.054.596 I print_info: n_vocab          = 50304
0.00.054.598 I print_info: n_merges         = 50009
0.00.054.598 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.598 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.598 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.599 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.599 I print_info: LF token         = 128 'Ä'
0.00.054.599 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.599 I print_info: max token length = 1024
0.00.056.281 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.281 I load_tensors: offloading output layer to GPU
0.00.056.281 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.292 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.056.293 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.056.586 I llama_init_from_model: n_seq_max     = 1
0.00.056.587 I llama_init_from_model: n_ctx         = 2048
0.00.056.587 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.587 I llama_init_from_model: n_batch       = 2048
0.00.056.587 I llama_init_from_model: n_ubatch      = 512
0.00.056.587 I llama_init_from_model: flash_attn    = 0
0.00.056.588 I llama_init_from_model: freq_base     = 10000.0
0.00.056.588 I llama_init_from_model: freq_scale    = 1
0.00.056.589 I ggml_metal_init: allocating
0.00.056.592 I ggml_metal_init: found device: Apple M4
0.00.056.594 I ggml_metal_init: picking default device: Apple M4
0.00.057.200 I ggml_metal_init: using embedded metal library
0.00.059.573 I ggml_metal_init: GPU name:   Apple M4
0.00.059.574 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.575 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.575 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.575 I ggml_metal_init: simdgroup reduction   = true
0.00.059.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.575 I ggml_metal_init: has bfloat            = true
0.00.059.575 I ggml_metal_init: use bfloat            = true
0.00.059.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.381 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.623 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.631 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.650 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.669 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.671 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.671 I llama_init_from_model: graph nodes  = 967
0.00.089.671 I llama_init_from_model: graph splits = 2
0.00.089.674 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.809 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.809 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.028 I main: llama threadpool init, n_threads = 4
0.00.541.075 I 
0.00.541.110 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.112 I 
0.00.541.273 I sampler seed: 1234
0.00.541.278 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.541.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.541.346 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.541.346 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.309.234 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.309.236 I llama_perf_context_print:        load time =     527.70 ms
0.01.309.236 I llama_perf_context_print: prompt eval time =      40.62 ms /     7 tokens (    5.80 ms per token,   172.35 tokens per second)
0.01.309.237 I llama_perf_context_print:        eval time =     724.17 ms /    63 runs   (   11.49 ms per token,    87.00 tokens per second)
0.01.309.237 I llama_perf_context_print:       total time =     768.21 ms /    70 tokens
0.01.309.442 I ggml_metal_free: deallocating

real	0m1.325s
user	0m0.109s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.286 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.464 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.470 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.479 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.483 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.483 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.261 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.154 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.155 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.156 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.156 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.156 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.157 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.157 I llama_model_loader: - type  f32:  194 tensors
0.00.027.158 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.158 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.158 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.159 I print_info: file format = GGUF V3 (latest)
0.00.027.159 I print_info: file type   = Q4_K - Medium
0.00.027.160 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.401 I load: special tokens cache size = 25
0.00.052.335 I load: token to piece cache size = 0.2984 MB
0.00.052.338 I print_info: arch             = gptneox
0.00.052.338 I print_info: vocab_only       = 0
0.00.052.339 I print_info: n_ctx_train      = 2048
0.00.052.339 I print_info: n_embd           = 2048
0.00.052.339 I print_info: n_layer          = 24
0.00.052.342 I print_info: n_head           = 16
0.00.052.343 I print_info: n_head_kv        = 16
0.00.052.343 I print_info: n_rot            = 32
0.00.052.343 I print_info: n_swa            = 0
0.00.052.343 I print_info: n_embd_head_k    = 128
0.00.052.344 I print_info: n_embd_head_v    = 128
0.00.052.344 I print_info: n_gqa            = 1
0.00.052.347 I print_info: n_embd_k_gqa     = 2048
0.00.052.348 I print_info: n_embd_v_gqa     = 2048
0.00.052.349 I print_info: f_norm_eps       = 1.0e-05
0.00.052.349 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.349 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.349 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.350 I print_info: f_logit_scale    = 0.0e+00
0.00.052.352 I print_info: n_ff             = 8192
0.00.052.352 I print_info: n_expert         = 0
0.00.052.353 I print_info: n_expert_used    = 0
0.00.052.353 I print_info: causal attn      = 1
0.00.052.353 I print_info: pooling type     = 0
0.00.052.353 I print_info: rope type        = 2
0.00.052.353 I print_info: rope scaling     = linear
0.00.052.354 I print_info: freq_base_train  = 10000.0
0.00.052.354 I print_info: freq_scale_train = 1
0.00.052.354 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.354 I print_info: rope_finetuned   = unknown
0.00.052.355 I print_info: ssm_d_conv       = 0
0.00.052.355 I print_info: ssm_d_inner      = 0
0.00.052.355 I print_info: ssm_d_state      = 0
0.00.052.355 I print_info: ssm_dt_rank      = 0
0.00.052.355 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.355 I print_info: model type       = 1.4B
0.00.052.356 I print_info: model params     = 1.41 B
0.00.052.356 I print_info: general.name     = 1.4B
0.00.052.357 I print_info: vocab type       = BPE
0.00.052.357 I print_info: n_vocab          = 50304
0.00.052.361 I print_info: n_merges         = 50009
0.00.052.361 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.361 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.361 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.361 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.363 I print_info: LF token         = 128 'Ä'
0.00.052.363 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.363 I print_info: max token length = 1024
0.00.054.010 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.010 I load_tensors: offloading output layer to GPU
0.00.054.010 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.021 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.022 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.318 I llama_init_from_model: n_seq_max     = 1
0.00.054.319 I llama_init_from_model: n_ctx         = 2048
0.00.054.319 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.320 I llama_init_from_model: n_batch       = 2048
0.00.054.320 I llama_init_from_model: n_ubatch      = 512
0.00.054.320 I llama_init_from_model: flash_attn    = 0
0.00.054.321 I llama_init_from_model: freq_base     = 10000.0
0.00.054.321 I llama_init_from_model: freq_scale    = 1
0.00.054.321 I ggml_metal_init: allocating
0.00.054.325 I ggml_metal_init: found device: Apple M4
0.00.054.327 I ggml_metal_init: picking default device: Apple M4
0.00.054.915 I ggml_metal_init: using embedded metal library
0.00.057.268 I ggml_metal_init: GPU name:   Apple M4
0.00.057.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.270 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.271 I ggml_metal_init: simdgroup reduction   = true
0.00.057.271 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.271 I ggml_metal_init: has bfloat            = true
0.00.057.271 I ggml_metal_init: use bfloat            = true
0.00.057.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.272 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.074 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.619 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.629 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.650 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.721 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.723 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.723 I llama_init_from_model: graph nodes  = 967
0.00.087.723 I llama_init_from_model: graph splits = 2
0.00.087.726 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.871 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.872 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.741 I main: llama threadpool init, n_threads = 4
0.00.622.786 I 
0.00.622.822 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.823 I 
0.00.622.978 I sampler seed: 1234
0.00.622.985 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.623.038 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.623.042 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.623.042 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.402.792 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.402.793 I llama_perf_context_print:        load time =     612.45 ms
0.01.402.794 I llama_perf_context_print: prompt eval time =      47.18 ms /     7 tokens (    6.74 ms per token,   148.38 tokens per second)
0.01.402.795 I llama_perf_context_print:        eval time =     729.40 ms /    63 runs   (   11.58 ms per token,    86.37 tokens per second)
0.01.402.795 I llama_perf_context_print:       total time =     780.05 ms /    70 tokens
0.01.403.007 I ggml_metal_free: deallocating

real	0m1.422s
user	0m0.110s
sys	0m0.144s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.720 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.084 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.089 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.091 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.094 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.098 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.102 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.107 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.107 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.107 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.605 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.606 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.606 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.607 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.607 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.608 I llama_model_loader: - type  f32:  194 tensors
0.00.025.608 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.608 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.609 I print_info: file format = GGUF V3 (latest)
0.00.025.609 I print_info: file type   = Q5_K - Medium
0.00.025.610 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.002 I load: special tokens cache size = 25
0.00.050.075 I load: token to piece cache size = 0.2984 MB
0.00.050.078 I print_info: arch             = gptneox
0.00.050.078 I print_info: vocab_only       = 0
0.00.050.078 I print_info: n_ctx_train      = 2048
0.00.050.079 I print_info: n_embd           = 2048
0.00.050.079 I print_info: n_layer          = 24
0.00.050.082 I print_info: n_head           = 16
0.00.050.083 I print_info: n_head_kv        = 16
0.00.050.083 I print_info: n_rot            = 32
0.00.050.085 I print_info: n_swa            = 0
0.00.050.085 I print_info: n_embd_head_k    = 128
0.00.050.085 I print_info: n_embd_head_v    = 128
0.00.050.086 I print_info: n_gqa            = 1
0.00.050.087 I print_info: n_embd_k_gqa     = 2048
0.00.050.088 I print_info: n_embd_v_gqa     = 2048
0.00.050.088 I print_info: f_norm_eps       = 1.0e-05
0.00.050.088 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.089 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.089 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.089 I print_info: f_logit_scale    = 0.0e+00
0.00.050.090 I print_info: n_ff             = 8192
0.00.050.090 I print_info: n_expert         = 0
0.00.050.090 I print_info: n_expert_used    = 0
0.00.050.091 I print_info: causal attn      = 1
0.00.050.092 I print_info: pooling type     = 0
0.00.050.092 I print_info: rope type        = 2
0.00.050.093 I print_info: rope scaling     = linear
0.00.050.093 I print_info: freq_base_train  = 10000.0
0.00.050.093 I print_info: freq_scale_train = 1
0.00.050.094 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.094 I print_info: rope_finetuned   = unknown
0.00.050.096 I print_info: ssm_d_conv       = 0
0.00.050.096 I print_info: ssm_d_inner      = 0
0.00.050.096 I print_info: ssm_d_state      = 0
0.00.050.096 I print_info: ssm_dt_rank      = 0
0.00.050.096 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.096 I print_info: model type       = 1.4B
0.00.050.097 I print_info: model params     = 1.41 B
0.00.050.097 I print_info: general.name     = 1.4B
0.00.050.098 I print_info: vocab type       = BPE
0.00.050.098 I print_info: n_vocab          = 50304
0.00.050.098 I print_info: n_merges         = 50009
0.00.050.098 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.098 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.100 I print_info: LF token         = 128 'Ä'
0.00.050.101 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: max token length = 1024
0.00.052.058 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.058 I load_tensors: offloading output layer to GPU
0.00.052.059 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.069 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.070 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.351 I llama_init_from_model: n_seq_max     = 1
0.00.052.352 I llama_init_from_model: n_ctx         = 2048
0.00.052.352 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.352 I llama_init_from_model: n_batch       = 2048
0.00.052.353 I llama_init_from_model: n_ubatch      = 512
0.00.052.353 I llama_init_from_model: flash_attn    = 0
0.00.052.353 I llama_init_from_model: freq_base     = 10000.0
0.00.052.353 I llama_init_from_model: freq_scale    = 1
0.00.052.354 I ggml_metal_init: allocating
0.00.052.356 I ggml_metal_init: found device: Apple M4
0.00.052.358 I ggml_metal_init: picking default device: Apple M4
0.00.052.938 I ggml_metal_init: using embedded metal library
0.00.055.297 I ggml_metal_init: GPU name:   Apple M4
0.00.055.299 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.299 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.300 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.300 I ggml_metal_init: simdgroup reduction   = true
0.00.055.300 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.300 I ggml_metal_init: has bfloat            = true
0.00.055.300 I ggml_metal_init: use bfloat            = true
0.00.055.301 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.301 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.953 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.008 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.026 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.109 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.111 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.112 I llama_init_from_model: graph nodes  = 967
0.00.085.112 I llama_init_from_model: graph splits = 2
0.00.085.115 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.246 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.300 I main: llama threadpool init, n_threads = 4
0.00.683.343 I 
0.00.683.385 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.386 I 
0.00.683.619 I sampler seed: 1234
0.00.683.623 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.634 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.634 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.635 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.534.375 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.534.376 I llama_perf_context_print:        load time =     673.58 ms
0.01.534.377 I llama_perf_context_print: prompt eval time =      51.59 ms /     7 tokens (    7.37 ms per token,   135.70 tokens per second)
0.01.534.378 I llama_perf_context_print:        eval time =     796.28 ms /    63 runs   (   12.64 ms per token,    79.12 tokens per second)
0.01.534.378 I llama_perf_context_print:       total time =     851.08 ms /    70 tokens
0.01.534.645 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.108s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.079 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.107 I main: llama backend init
0.00.000.109 I main: load the model and apply lora adapter, if any
0.00.008.709 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.894 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.020.901 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.903 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.905 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.906 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.906 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.906 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.907 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.908 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.908 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.908 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.909 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.909 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.910 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.913 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.914 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.827 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.889 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.834 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.836 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.836 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.837 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.837 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.029.838 I llama_model_loader: - type  f32:  194 tensors
0.00.029.838 I llama_model_loader: - type q6_K:   98 tensors
0.00.029.839 I print_info: file format = GGUF V3 (latest)
0.00.029.839 I print_info: file type   = Q6_K
0.00.029.840 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.049.836 I load: special tokens cache size = 25
0.00.056.117 I load: token to piece cache size = 0.2984 MB
0.00.056.123 I print_info: arch             = gptneox
0.00.056.126 I print_info: vocab_only       = 0
0.00.056.126 I print_info: n_ctx_train      = 2048
0.00.056.127 I print_info: n_embd           = 2048
0.00.056.127 I print_info: n_layer          = 24
0.00.056.131 I print_info: n_head           = 16
0.00.056.132 I print_info: n_head_kv        = 16
0.00.056.133 I print_info: n_rot            = 32
0.00.056.133 I print_info: n_swa            = 0
0.00.056.134 I print_info: n_embd_head_k    = 128
0.00.056.134 I print_info: n_embd_head_v    = 128
0.00.056.135 I print_info: n_gqa            = 1
0.00.056.135 I print_info: n_embd_k_gqa     = 2048
0.00.056.136 I print_info: n_embd_v_gqa     = 2048
0.00.056.137 I print_info: f_norm_eps       = 1.0e-05
0.00.056.137 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.137 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.141 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.141 I print_info: f_logit_scale    = 0.0e+00
0.00.056.142 I print_info: n_ff             = 8192
0.00.056.142 I print_info: n_expert         = 0
0.00.056.143 I print_info: n_expert_used    = 0
0.00.056.143 I print_info: causal attn      = 1
0.00.056.143 I print_info: pooling type     = 0
0.00.056.143 I print_info: rope type        = 2
0.00.056.145 I print_info: rope scaling     = linear
0.00.056.147 I print_info: freq_base_train  = 10000.0
0.00.056.147 I print_info: freq_scale_train = 1
0.00.056.147 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.147 I print_info: rope_finetuned   = unknown
0.00.056.147 I print_info: ssm_d_conv       = 0
0.00.056.148 I print_info: ssm_d_inner      = 0
0.00.056.148 I print_info: ssm_d_state      = 0
0.00.056.148 I print_info: ssm_dt_rank      = 0
0.00.056.148 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.148 I print_info: model type       = 1.4B
0.00.056.149 I print_info: model params     = 1.41 B
0.00.056.149 I print_info: general.name     = 1.4B
0.00.056.149 I print_info: vocab type       = BPE
0.00.056.150 I print_info: n_vocab          = 50304
0.00.056.150 I print_info: n_merges         = 50009
0.00.056.150 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.150 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.150 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.151 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.151 I print_info: LF token         = 128 'Ä'
0.00.056.151 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.151 I print_info: max token length = 1024
0.00.058.212 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.212 I load_tensors: offloading output layer to GPU
0.00.058.212 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.223 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.058.225 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.058.629 I llama_init_from_model: n_seq_max     = 1
0.00.058.630 I llama_init_from_model: n_ctx         = 2048
0.00.058.630 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.630 I llama_init_from_model: n_batch       = 2048
0.00.058.630 I llama_init_from_model: n_ubatch      = 512
0.00.058.630 I llama_init_from_model: flash_attn    = 0
0.00.058.631 I llama_init_from_model: freq_base     = 10000.0
0.00.058.631 I llama_init_from_model: freq_scale    = 1
0.00.058.632 I ggml_metal_init: allocating
0.00.058.636 I ggml_metal_init: found device: Apple M4
0.00.058.638 I ggml_metal_init: picking default device: Apple M4
0.00.059.297 I ggml_metal_init: using embedded metal library
0.00.061.687 I ggml_metal_init: GPU name:   Apple M4
0.00.061.689 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.689 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.690 I ggml_metal_init: simdgroup reduction   = true
0.00.061.690 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.690 I ggml_metal_init: has bfloat            = true
0.00.061.691 I ggml_metal_init: use bfloat            = true
0.00.061.691 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.382 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.012 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.043 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.998 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.092.000 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.092.000 I llama_init_from_model: graph nodes  = 967
0.00.092.000 I llama_init_from_model: graph splits = 2
0.00.092.008 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.574 I main: llama threadpool init, n_threads = 4
0.00.762.670 I 
0.00.762.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.718 I 
0.00.763.036 I sampler seed: 1234
0.00.763.045 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.066 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.070 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.070 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.645.980 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51226.55 tokens per second)
0.01.645.980 I llama_perf_context_print:        load time =     753.85 ms
0.01.645.981 I llama_perf_context_print: prompt eval time =      55.08 ms /     7 tokens (    7.87 ms per token,   127.08 tokens per second)
0.01.645.982 I llama_perf_context_print:        eval time =     824.63 ms /    63 runs   (   13.09 ms per token,    76.40 tokens per second)
0.01.645.982 I llama_perf_context_print:       total time =     883.41 ms /    70 tokens
0.01.646.248 I ggml_metal_free: deallocating

real	0m1.703s
user	0m0.129s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.604 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.603 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.772 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.776 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.778 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.779 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.780 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.782 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.783 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.786 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.786 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.787 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.076 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.260 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.262 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.263 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.263 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.263 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.264 I llama_model_loader: - type  f32:  194 tensors
0.00.053.264 I llama_model_loader: - type  f16:   98 tensors
0.00.053.265 I print_info: file format = GGUF V3 (latest)
0.00.053.266 I print_info: file type   = all F32 (guessed)
0.00.053.267 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.634 I load: special tokens cache size = 25
0.00.085.825 I load: token to piece cache size = 0.2984 MB
0.00.085.828 I print_info: arch             = gptneox
0.00.085.828 I print_info: vocab_only       = 0
0.00.085.829 I print_info: n_ctx_train      = 2048
0.00.085.829 I print_info: n_embd           = 2048
0.00.085.829 I print_info: n_layer          = 24
0.00.085.832 I print_info: n_head           = 16
0.00.085.833 I print_info: n_head_kv        = 16
0.00.085.834 I print_info: n_rot            = 32
0.00.085.835 I print_info: n_swa            = 0
0.00.085.835 I print_info: n_embd_head_k    = 128
0.00.085.836 I print_info: n_embd_head_v    = 128
0.00.085.837 I print_info: n_gqa            = 1
0.00.085.837 I print_info: n_embd_k_gqa     = 2048
0.00.085.840 I print_info: n_embd_v_gqa     = 2048
0.00.085.840 I print_info: f_norm_eps       = 1.0e-05
0.00.085.841 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.841 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.841 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.841 I print_info: f_logit_scale    = 0.0e+00
0.00.085.842 I print_info: n_ff             = 8192
0.00.085.842 I print_info: n_expert         = 0
0.00.085.842 I print_info: n_expert_used    = 0
0.00.085.842 I print_info: causal attn      = 1
0.00.085.842 I print_info: pooling type     = 0
0.00.085.842 I print_info: rope type        = 2
0.00.085.843 I print_info: rope scaling     = linear
0.00.085.843 I print_info: freq_base_train  = 10000.0
0.00.085.844 I print_info: freq_scale_train = 1
0.00.085.847 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.847 I print_info: rope_finetuned   = unknown
0.00.085.848 I print_info: ssm_d_conv       = 0
0.00.085.848 I print_info: ssm_d_inner      = 0
0.00.085.848 I print_info: ssm_d_state      = 0
0.00.085.848 I print_info: ssm_dt_rank      = 0
0.00.085.848 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.849 I print_info: model type       = 1.4B
0.00.085.850 I print_info: model params     = 1.41 B
0.00.085.850 I print_info: general.name     = 1.4B
0.00.085.850 I print_info: vocab type       = BPE
0.00.085.851 I print_info: n_vocab          = 50304
0.00.085.851 I print_info: n_merges         = 50009
0.00.085.851 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.851 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.851 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.851 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.852 I print_info: LF token         = 128 'Ä'
0.00.085.852 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.852 I print_info: max token length = 1024
0.00.088.496 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.497 I load_tensors: offloading output layer to GPU
0.00.088.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.508 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.509 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.909 I llama_init_from_model: n_seq_max     = 1
0.00.088.909 I llama_init_from_model: n_ctx         = 128
0.00.088.910 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.910 I llama_init_from_model: n_batch       = 128
0.00.088.910 I llama_init_from_model: n_ubatch      = 128
0.00.088.910 I llama_init_from_model: flash_attn    = 0
0.00.088.911 I llama_init_from_model: freq_base     = 10000.0
0.00.088.911 I llama_init_from_model: freq_scale    = 1
0.00.088.911 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.912 I ggml_metal_init: allocating
0.00.088.915 I ggml_metal_init: found device: Apple M4
0.00.088.917 I ggml_metal_init: picking default device: Apple M4
0.00.089.522 I ggml_metal_init: using embedded metal library
0.00.092.088 I ggml_metal_init: GPU name:   Apple M4
0.00.092.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.090 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.090 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.091 I ggml_metal_init: simdgroup reduction   = true
0.00.092.091 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.091 I ggml_metal_init: has bfloat            = true
0.00.092.091 I ggml_metal_init: use bfloat            = true
0.00.092.092 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.092 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.175 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.590 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.595 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.611 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.512 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.108.513 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.108.513 I llama_init_from_model: graph nodes  = 967
0.00.108.513 I llama_init_from_model: graph splits = 2
0.00.108.514 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.514 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.132.852 I 
0.01.132.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.132.885 I perplexity: tokenizing the input ..
0.01.145.984 I perplexity: tokenization took 13.096 ms
0.01.145.989 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.267.012 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.268.697 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.268.748 I llama_perf_context_print:        load time =    1111.24 ms
0.01.268.749 I llama_perf_context_print: prompt eval time =     120.16 ms /   128 tokens (    0.94 ms per token,  1065.28 tokens per second)
0.01.268.751 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.268.751 I llama_perf_context_print:       total time =     135.90 ms /   129 tokens
0.01.269.373 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.122s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.144 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.601 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.835 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.845 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.846 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.848 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.849 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.852 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.853 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.036 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.038 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.039 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.039 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.039 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.040 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.040 I llama_model_loader: - type  f32:  194 tensors
0.00.033.041 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.041 I print_info: file format = GGUF V3 (latest)
0.00.033.042 I print_info: file type   = Q8_0
0.00.033.044 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.880 I load: special tokens cache size = 25
0.00.061.757 I load: token to piece cache size = 0.2984 MB
0.00.061.761 I print_info: arch             = gptneox
0.00.061.761 I print_info: vocab_only       = 0
0.00.061.761 I print_info: n_ctx_train      = 2048
0.00.061.762 I print_info: n_embd           = 2048
0.00.061.762 I print_info: n_layer          = 24
0.00.061.766 I print_info: n_head           = 16
0.00.061.767 I print_info: n_head_kv        = 16
0.00.061.767 I print_info: n_rot            = 32
0.00.061.768 I print_info: n_swa            = 0
0.00.061.768 I print_info: n_embd_head_k    = 128
0.00.061.768 I print_info: n_embd_head_v    = 128
0.00.061.769 I print_info: n_gqa            = 1
0.00.061.769 I print_info: n_embd_k_gqa     = 2048
0.00.061.770 I print_info: n_embd_v_gqa     = 2048
0.00.061.771 I print_info: f_norm_eps       = 1.0e-05
0.00.061.771 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.771 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.772 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.772 I print_info: f_logit_scale    = 0.0e+00
0.00.061.773 I print_info: n_ff             = 8192
0.00.061.773 I print_info: n_expert         = 0
0.00.061.773 I print_info: n_expert_used    = 0
0.00.061.773 I print_info: causal attn      = 1
0.00.061.773 I print_info: pooling type     = 0
0.00.061.773 I print_info: rope type        = 2
0.00.061.774 I print_info: rope scaling     = linear
0.00.061.776 I print_info: freq_base_train  = 10000.0
0.00.061.776 I print_info: freq_scale_train = 1
0.00.061.777 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.777 I print_info: rope_finetuned   = unknown
0.00.061.777 I print_info: ssm_d_conv       = 0
0.00.061.777 I print_info: ssm_d_inner      = 0
0.00.061.777 I print_info: ssm_d_state      = 0
0.00.061.777 I print_info: ssm_dt_rank      = 0
0.00.061.778 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.778 I print_info: model type       = 1.4B
0.00.061.778 I print_info: model params     = 1.41 B
0.00.061.778 I print_info: general.name     = 1.4B
0.00.061.779 I print_info: vocab type       = BPE
0.00.061.779 I print_info: n_vocab          = 50304
0.00.061.781 I print_info: n_merges         = 50009
0.00.061.781 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.781 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.782 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.782 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.782 I print_info: LF token         = 128 'Ä'
0.00.061.782 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.783 I print_info: max token length = 1024
0.00.064.081 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.081 I load_tensors: offloading output layer to GPU
0.00.064.081 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.093 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.094 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.384 I llama_init_from_model: n_seq_max     = 1
0.00.064.385 I llama_init_from_model: n_ctx         = 128
0.00.064.385 I llama_init_from_model: n_ctx_per_seq = 128
0.00.064.385 I llama_init_from_model: n_batch       = 128
0.00.064.385 I llama_init_from_model: n_ubatch      = 128
0.00.064.386 I llama_init_from_model: flash_attn    = 0
0.00.064.386 I llama_init_from_model: freq_base     = 10000.0
0.00.064.386 I llama_init_from_model: freq_scale    = 1
0.00.064.387 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.387 I ggml_metal_init: allocating
0.00.064.391 I ggml_metal_init: found device: Apple M4
0.00.064.393 I ggml_metal_init: picking default device: Apple M4
0.00.065.037 I ggml_metal_init: using embedded metal library
0.00.067.926 I ggml_metal_init: GPU name:   Apple M4
0.00.067.928 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.929 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.929 I ggml_metal_init: simdgroup reduction   = true
0.00.067.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.929 I ggml_metal_init: has bfloat            = true
0.00.067.929 I ggml_metal_init: use bfloat            = true
0.00.067.930 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.931 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.685 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.067 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.070 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.085 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.169 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.170 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.171 I llama_init_from_model: graph nodes  = 967
0.00.080.171 I llama_init_from_model: graph splits = 2
0.00.080.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.173 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.984.943 I 
0.00.984.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.984.970 I perplexity: tokenizing the input ..
0.00.993.228 I perplexity: tokenization took 8.257 ms
0.00.993.232 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.117.487 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.118.652 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.118.683 I llama_perf_context_print:        load time =     973.34 ms
0.01.118.684 I llama_perf_context_print: prompt eval time =     124.02 ms /   128 tokens (    0.97 ms per token,  1032.11 tokens per second)
0.01.118.685 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.118.685 I llama_perf_context_print:       total time =     133.74 ms /   129 tokens
0.01.119.231 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.090s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.218 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.101 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.102 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.102 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.103 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.103 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.897 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.727 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.728 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.729 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.730 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.730 I llama_model_loader: - type  f32:  194 tensors
0.00.025.730 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.731 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.731 I print_info: file format = GGUF V3 (latest)
0.00.025.732 I print_info: file type   = Q4_0
0.00.025.732 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.875 I load: special tokens cache size = 25
0.00.050.822 I load: token to piece cache size = 0.2984 MB
0.00.050.825 I print_info: arch             = gptneox
0.00.050.825 I print_info: vocab_only       = 0
0.00.050.826 I print_info: n_ctx_train      = 2048
0.00.050.826 I print_info: n_embd           = 2048
0.00.050.826 I print_info: n_layer          = 24
0.00.050.829 I print_info: n_head           = 16
0.00.050.829 I print_info: n_head_kv        = 16
0.00.050.831 I print_info: n_rot            = 32
0.00.050.832 I print_info: n_swa            = 0
0.00.050.832 I print_info: n_embd_head_k    = 128
0.00.050.832 I print_info: n_embd_head_v    = 128
0.00.050.833 I print_info: n_gqa            = 1
0.00.050.834 I print_info: n_embd_k_gqa     = 2048
0.00.050.839 I print_info: n_embd_v_gqa     = 2048
0.00.050.839 I print_info: f_norm_eps       = 1.0e-05
0.00.050.840 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.840 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.840 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.840 I print_info: f_logit_scale    = 0.0e+00
0.00.050.841 I print_info: n_ff             = 8192
0.00.050.841 I print_info: n_expert         = 0
0.00.050.841 I print_info: n_expert_used    = 0
0.00.050.842 I print_info: causal attn      = 1
0.00.050.842 I print_info: pooling type     = 0
0.00.050.842 I print_info: rope type        = 2
0.00.050.842 I print_info: rope scaling     = linear
0.00.050.844 I print_info: freq_base_train  = 10000.0
0.00.050.846 I print_info: freq_scale_train = 1
0.00.050.846 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.846 I print_info: rope_finetuned   = unknown
0.00.050.846 I print_info: ssm_d_conv       = 0
0.00.050.846 I print_info: ssm_d_inner      = 0
0.00.050.847 I print_info: ssm_d_state      = 0
0.00.050.847 I print_info: ssm_dt_rank      = 0
0.00.050.847 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.852 I print_info: model type       = 1.4B
0.00.050.854 I print_info: model params     = 1.41 B
0.00.050.856 I print_info: general.name     = 1.4B
0.00.050.856 I print_info: vocab type       = BPE
0.00.050.856 I print_info: n_vocab          = 50304
0.00.050.857 I print_info: n_merges         = 50009
0.00.050.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.857 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.857 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.857 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.858 I print_info: LF token         = 128 'Ä'
0.00.050.858 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.858 I print_info: max token length = 1024
0.00.052.763 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.764 I load_tensors: offloading output layer to GPU
0.00.052.764 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.774 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.776 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.051 I llama_init_from_model: n_seq_max     = 1
0.00.053.052 I llama_init_from_model: n_ctx         = 128
0.00.053.052 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.052 I llama_init_from_model: n_batch       = 128
0.00.053.052 I llama_init_from_model: n_ubatch      = 128
0.00.053.052 I llama_init_from_model: flash_attn    = 0
0.00.053.053 I llama_init_from_model: freq_base     = 10000.0
0.00.053.053 I llama_init_from_model: freq_scale    = 1
0.00.053.053 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.054 I ggml_metal_init: allocating
0.00.053.057 I ggml_metal_init: found device: Apple M4
0.00.053.058 I ggml_metal_init: picking default device: Apple M4
0.00.053.620 I ggml_metal_init: using embedded metal library
0.00.055.972 I ggml_metal_init: GPU name:   Apple M4
0.00.055.973 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.973 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.974 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.974 I ggml_metal_init: simdgroup reduction   = true
0.00.055.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.974 I ggml_metal_init: has bfloat            = true
0.00.055.974 I ggml_metal_init: use bfloat            = true
0.00.055.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.060 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.293 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.295 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.308 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.219 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.220 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.220 I llama_init_from_model: graph nodes  = 967
0.00.068.221 I llama_init_from_model: graph splits = 2
0.00.068.222 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.222 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.292 I 
0.00.632.321 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.324 I perplexity: tokenizing the input ..
0.00.639.943 I perplexity: tokenization took 7.617 ms
0.00.639.947 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.762.463 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.763.608 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.763.640 I llama_perf_context_print:        load time =     622.07 ms
0.00.763.641 I llama_perf_context_print: prompt eval time =     122.28 ms /   128 tokens (    0.96 ms per token,  1046.74 tokens per second)
0.00.763.642 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.763.642 I llama_perf_context_print:       total time =     131.35 ms /   129 tokens
0.00.764.124 I ggml_metal_free: deallocating

real	0m0.779s
user	0m0.077s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.780 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.934 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.935 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.935 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.936 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.937 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.942 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.687 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.750 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.459 I llama_model_loader: - type  f32:  194 tensors
0.00.024.459 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.459 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.460 I print_info: file format = GGUF V3 (latest)
0.00.024.460 I print_info: file type   = Q4_1
0.00.024.461 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.619 I load: special tokens cache size = 25
0.00.049.389 I load: token to piece cache size = 0.2984 MB
0.00.049.392 I print_info: arch             = gptneox
0.00.049.393 I print_info: vocab_only       = 0
0.00.049.393 I print_info: n_ctx_train      = 2048
0.00.049.393 I print_info: n_embd           = 2048
0.00.049.393 I print_info: n_layer          = 24
0.00.049.396 I print_info: n_head           = 16
0.00.049.397 I print_info: n_head_kv        = 16
0.00.049.397 I print_info: n_rot            = 32
0.00.049.397 I print_info: n_swa            = 0
0.00.049.397 I print_info: n_embd_head_k    = 128
0.00.049.398 I print_info: n_embd_head_v    = 128
0.00.049.398 I print_info: n_gqa            = 1
0.00.049.399 I print_info: n_embd_k_gqa     = 2048
0.00.049.400 I print_info: n_embd_v_gqa     = 2048
0.00.049.402 I print_info: f_norm_eps       = 1.0e-05
0.00.049.402 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.402 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.402 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.403 I print_info: f_logit_scale    = 0.0e+00
0.00.049.403 I print_info: n_ff             = 8192
0.00.049.403 I print_info: n_expert         = 0
0.00.049.404 I print_info: n_expert_used    = 0
0.00.049.404 I print_info: causal attn      = 1
0.00.049.406 I print_info: pooling type     = 0
0.00.049.406 I print_info: rope type        = 2
0.00.049.406 I print_info: rope scaling     = linear
0.00.049.407 I print_info: freq_base_train  = 10000.0
0.00.049.407 I print_info: freq_scale_train = 1
0.00.049.407 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.407 I print_info: rope_finetuned   = unknown
0.00.049.407 I print_info: ssm_d_conv       = 0
0.00.049.408 I print_info: ssm_d_inner      = 0
0.00.049.408 I print_info: ssm_d_state      = 0
0.00.049.408 I print_info: ssm_dt_rank      = 0
0.00.049.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.408 I print_info: model type       = 1.4B
0.00.049.409 I print_info: model params     = 1.41 B
0.00.049.409 I print_info: general.name     = 1.4B
0.00.049.409 I print_info: vocab type       = BPE
0.00.049.409 I print_info: n_vocab          = 50304
0.00.049.410 I print_info: n_merges         = 50009
0.00.049.410 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.410 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.410 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.410 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.411 I print_info: LF token         = 128 'Ä'
0.00.049.411 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.411 I print_info: max token length = 1024
0.00.051.384 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.384 I load_tensors: offloading output layer to GPU
0.00.051.384 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.395 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.396 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.675 I llama_init_from_model: n_seq_max     = 1
0.00.051.676 I llama_init_from_model: n_ctx         = 128
0.00.051.676 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.676 I llama_init_from_model: n_batch       = 128
0.00.051.676 I llama_init_from_model: n_ubatch      = 128
0.00.051.676 I llama_init_from_model: flash_attn    = 0
0.00.051.677 I llama_init_from_model: freq_base     = 10000.0
0.00.051.677 I llama_init_from_model: freq_scale    = 1
0.00.051.677 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.678 I ggml_metal_init: allocating
0.00.051.680 I ggml_metal_init: found device: Apple M4
0.00.051.682 I ggml_metal_init: picking default device: Apple M4
0.00.052.257 I ggml_metal_init: using embedded metal library
0.00.054.824 I ggml_metal_init: GPU name:   Apple M4
0.00.054.826 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.826 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.826 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.827 I ggml_metal_init: simdgroup reduction   = true
0.00.054.827 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.827 I ggml_metal_init: has bfloat            = true
0.00.054.827 I ggml_metal_init: use bfloat            = true
0.00.054.827 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.828 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.858 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.236 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.238 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.252 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.208 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.209 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.209 I llama_init_from_model: graph nodes  = 967
0.00.068.209 I llama_init_from_model: graph splits = 2
0.00.068.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.211 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.306 I 
0.00.766.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.353 I perplexity: tokenizing the input ..
0.00.774.231 I perplexity: tokenization took 7.876 ms
0.00.774.235 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.896.803 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.897.973 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.898.007 I llama_perf_context_print:        load time =     757.52 ms
0.00.898.008 I llama_perf_context_print: prompt eval time =     122.34 ms /   128 tokens (    0.96 ms per token,  1046.25 tokens per second)
0.00.898.008 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.898.009 I llama_perf_context_print:       total time =     131.70 ms /   129 tokens
0.00.898.515 I ggml_metal_free: deallocating

real	0m0.914s
user	0m0.077s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.300 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.994 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.998 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.000 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.001 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.001 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.003 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.003 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.005 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.005 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.005 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.006 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.007 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.424 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.424 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.425 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.426 I llama_model_loader: - type  f32:  194 tensors
0.00.024.426 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.427 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.427 I print_info: file format = GGUF V3 (latest)
0.00.024.428 I print_info: file type   = Q5_0
0.00.024.429 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.042.793 I load: special tokens cache size = 25
0.00.048.544 I load: token to piece cache size = 0.2984 MB
0.00.048.546 I print_info: arch             = gptneox
0.00.048.547 I print_info: vocab_only       = 0
0.00.048.547 I print_info: n_ctx_train      = 2048
0.00.048.547 I print_info: n_embd           = 2048
0.00.048.547 I print_info: n_layer          = 24
0.00.048.550 I print_info: n_head           = 16
0.00.048.551 I print_info: n_head_kv        = 16
0.00.048.551 I print_info: n_rot            = 32
0.00.048.551 I print_info: n_swa            = 0
0.00.048.551 I print_info: n_embd_head_k    = 128
0.00.048.552 I print_info: n_embd_head_v    = 128
0.00.048.553 I print_info: n_gqa            = 1
0.00.048.554 I print_info: n_embd_k_gqa     = 2048
0.00.048.556 I print_info: n_embd_v_gqa     = 2048
0.00.048.557 I print_info: f_norm_eps       = 1.0e-05
0.00.048.557 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.557 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.557 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.558 I print_info: f_logit_scale    = 0.0e+00
0.00.048.559 I print_info: n_ff             = 8192
0.00.048.559 I print_info: n_expert         = 0
0.00.048.559 I print_info: n_expert_used    = 0
0.00.048.559 I print_info: causal attn      = 1
0.00.048.559 I print_info: pooling type     = 0
0.00.048.559 I print_info: rope type        = 2
0.00.048.565 I print_info: rope scaling     = linear
0.00.048.566 I print_info: freq_base_train  = 10000.0
0.00.048.566 I print_info: freq_scale_train = 1
0.00.048.566 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.566 I print_info: rope_finetuned   = unknown
0.00.048.567 I print_info: ssm_d_conv       = 0
0.00.048.567 I print_info: ssm_d_inner      = 0
0.00.048.567 I print_info: ssm_d_state      = 0
0.00.048.567 I print_info: ssm_dt_rank      = 0
0.00.048.567 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.568 I print_info: model type       = 1.4B
0.00.048.568 I print_info: model params     = 1.41 B
0.00.048.568 I print_info: general.name     = 1.4B
0.00.048.569 I print_info: vocab type       = BPE
0.00.048.569 I print_info: n_vocab          = 50304
0.00.048.569 I print_info: n_merges         = 50009
0.00.048.570 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.571 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.571 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.571 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.572 I print_info: LF token         = 128 'Ä'
0.00.048.572 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.572 I print_info: max token length = 1024
0.00.050.534 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.534 I load_tensors: offloading output layer to GPU
0.00.050.534 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.544 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.050.546 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.050.844 I llama_init_from_model: n_seq_max     = 1
0.00.050.845 I llama_init_from_model: n_ctx         = 128
0.00.050.845 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.846 I llama_init_from_model: n_batch       = 128
0.00.050.846 I llama_init_from_model: n_ubatch      = 128
0.00.050.846 I llama_init_from_model: flash_attn    = 0
0.00.050.846 I llama_init_from_model: freq_base     = 10000.0
0.00.050.847 I llama_init_from_model: freq_scale    = 1
0.00.050.847 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.847 I ggml_metal_init: allocating
0.00.050.851 I ggml_metal_init: found device: Apple M4
0.00.050.853 I ggml_metal_init: picking default device: Apple M4
0.00.051.409 I ggml_metal_init: using embedded metal library
0.00.053.717 I ggml_metal_init: GPU name:   Apple M4
0.00.053.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.718 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.719 I ggml_metal_init: simdgroup reduction   = true
0.00.053.719 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.719 I ggml_metal_init: has bfloat            = true
0.00.053.720 I ggml_metal_init: use bfloat            = true
0.00.053.720 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.721 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.124 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.380 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.383 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.396 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.336 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.337 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.337 I llama_init_from_model: graph nodes  = 967
0.00.065.337 I llama_init_from_model: graph splits = 2
0.00.065.338 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.338 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.206 I 
0.00.705.253 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.258 I perplexity: tokenizing the input ..
0.00.713.118 I perplexity: tokenization took 7.859 ms
0.00.713.122 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.852 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.849.074 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.849.101 I llama_perf_context_print:        load time =     695.90 ms
0.00.849.102 I llama_perf_context_print: prompt eval time =     134.50 ms /   128 tokens (    1.05 ms per token,   951.64 tokens per second)
0.00.849.103 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.103 I llama_perf_context_print:       total time =     143.90 ms /   129 tokens
0.00.849.586 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.076s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.929 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.623 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.628 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.630 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.630 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.630 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.631 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.631 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.632 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.633 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.633 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.637 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.637 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.053 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.055 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.055 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.056 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.056 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.056 I llama_model_loader: - type  f32:  194 tensors
0.00.024.057 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.057 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.058 I print_info: file format = GGUF V3 (latest)
0.00.024.058 I print_info: file type   = Q5_1
0.00.024.059 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.274 I load: special tokens cache size = 25
0.00.049.443 I load: token to piece cache size = 0.2984 MB
0.00.049.446 I print_info: arch             = gptneox
0.00.049.446 I print_info: vocab_only       = 0
0.00.049.446 I print_info: n_ctx_train      = 2048
0.00.049.446 I print_info: n_embd           = 2048
0.00.049.447 I print_info: n_layer          = 24
0.00.049.449 I print_info: n_head           = 16
0.00.049.450 I print_info: n_head_kv        = 16
0.00.049.451 I print_info: n_rot            = 32
0.00.049.452 I print_info: n_swa            = 0
0.00.049.452 I print_info: n_embd_head_k    = 128
0.00.049.452 I print_info: n_embd_head_v    = 128
0.00.049.453 I print_info: n_gqa            = 1
0.00.049.454 I print_info: n_embd_k_gqa     = 2048
0.00.049.454 I print_info: n_embd_v_gqa     = 2048
0.00.049.457 I print_info: f_norm_eps       = 1.0e-05
0.00.049.457 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.457 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.458 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.458 I print_info: f_logit_scale    = 0.0e+00
0.00.049.459 I print_info: n_ff             = 8192
0.00.049.459 I print_info: n_expert         = 0
0.00.049.459 I print_info: n_expert_used    = 0
0.00.049.461 I print_info: causal attn      = 1
0.00.049.461 I print_info: pooling type     = 0
0.00.049.461 I print_info: rope type        = 2
0.00.049.461 I print_info: rope scaling     = linear
0.00.049.462 I print_info: freq_base_train  = 10000.0
0.00.049.462 I print_info: freq_scale_train = 1
0.00.049.462 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.463 I print_info: rope_finetuned   = unknown
0.00.049.463 I print_info: ssm_d_conv       = 0
0.00.049.463 I print_info: ssm_d_inner      = 0
0.00.049.463 I print_info: ssm_d_state      = 0
0.00.049.463 I print_info: ssm_dt_rank      = 0
0.00.049.463 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.467 I print_info: model type       = 1.4B
0.00.049.468 I print_info: model params     = 1.41 B
0.00.049.468 I print_info: general.name     = 1.4B
0.00.049.469 I print_info: vocab type       = BPE
0.00.049.469 I print_info: n_vocab          = 50304
0.00.049.469 I print_info: n_merges         = 50009
0.00.049.472 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.472 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.472 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.473 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.473 I print_info: LF token         = 128 'Ä'
0.00.049.474 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.475 I print_info: max token length = 1024
0.00.051.469 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.469 I load_tensors: offloading output layer to GPU
0.00.051.469 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.480 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.481 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.774 I llama_init_from_model: n_seq_max     = 1
0.00.051.775 I llama_init_from_model: n_ctx         = 128
0.00.051.775 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.775 I llama_init_from_model: n_batch       = 128
0.00.051.775 I llama_init_from_model: n_ubatch      = 128
0.00.051.775 I llama_init_from_model: flash_attn    = 0
0.00.051.776 I llama_init_from_model: freq_base     = 10000.0
0.00.051.776 I llama_init_from_model: freq_scale    = 1
0.00.051.776 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.777 I ggml_metal_init: allocating
0.00.051.779 I ggml_metal_init: found device: Apple M4
0.00.051.781 I ggml_metal_init: picking default device: Apple M4
0.00.052.360 I ggml_metal_init: using embedded metal library
0.00.054.687 I ggml_metal_init: GPU name:   Apple M4
0.00.054.689 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.689 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.689 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.690 I ggml_metal_init: simdgroup reduction   = true
0.00.054.690 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.690 I ggml_metal_init: has bfloat            = true
0.00.054.690 I ggml_metal_init: use bfloat            = true
0.00.054.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.691 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.270 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.598 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.601 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.614 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.568 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.569 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.569 I llama_init_from_model: graph nodes  = 967
0.00.066.569 I llama_init_from_model: graph splits = 2
0.00.066.571 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.355 I 
0.00.636.416 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.425 I perplexity: tokenizing the input ..
0.00.644.149 I perplexity: tokenization took 7.723 ms
0.00.644.152 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.885 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.780.050 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.780.088 I llama_perf_context_print:        load time =     627.42 ms
0.00.780.089 I llama_perf_context_print: prompt eval time =     134.51 ms /   128 tokens (    1.05 ms per token,   951.62 tokens per second)
0.00.780.090 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.090 I llama_perf_context_print:       total time =     143.74 ms /   129 tokens
0.00.780.602 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.077s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.983 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.596 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.603 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.605 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.605 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.606 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.606 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.606 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.608 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.608 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.611 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.611 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.613 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.614 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.342 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.037 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.038 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.039 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.039 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.039 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.040 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.040 I llama_model_loader: - type  f32:  194 tensors
0.00.025.040 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.040 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.041 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.041 I print_info: file format = GGUF V3 (latest)
0.00.025.041 I print_info: file type   = Q2_K - Medium
0.00.025.042 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.565 I load: special tokens cache size = 25
0.00.049.408 I load: token to piece cache size = 0.2984 MB
0.00.049.411 I print_info: arch             = gptneox
0.00.049.412 I print_info: vocab_only       = 0
0.00.049.412 I print_info: n_ctx_train      = 2048
0.00.049.412 I print_info: n_embd           = 2048
0.00.049.412 I print_info: n_layer          = 24
0.00.049.415 I print_info: n_head           = 16
0.00.049.416 I print_info: n_head_kv        = 16
0.00.049.416 I print_info: n_rot            = 32
0.00.049.417 I print_info: n_swa            = 0
0.00.049.417 I print_info: n_embd_head_k    = 128
0.00.049.417 I print_info: n_embd_head_v    = 128
0.00.049.420 I print_info: n_gqa            = 1
0.00.049.421 I print_info: n_embd_k_gqa     = 2048
0.00.049.422 I print_info: n_embd_v_gqa     = 2048
0.00.049.422 I print_info: f_norm_eps       = 1.0e-05
0.00.049.423 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.423 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.423 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.423 I print_info: f_logit_scale    = 0.0e+00
0.00.049.424 I print_info: n_ff             = 8192
0.00.049.424 I print_info: n_expert         = 0
0.00.049.425 I print_info: n_expert_used    = 0
0.00.049.425 I print_info: causal attn      = 1
0.00.049.425 I print_info: pooling type     = 0
0.00.049.425 I print_info: rope type        = 2
0.00.049.427 I print_info: rope scaling     = linear
0.00.049.427 I print_info: freq_base_train  = 10000.0
0.00.049.427 I print_info: freq_scale_train = 1
0.00.049.427 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.428 I print_info: rope_finetuned   = unknown
0.00.049.428 I print_info: ssm_d_conv       = 0
0.00.049.428 I print_info: ssm_d_inner      = 0
0.00.049.428 I print_info: ssm_d_state      = 0
0.00.049.428 I print_info: ssm_dt_rank      = 0
0.00.049.428 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.428 I print_info: model type       = 1.4B
0.00.049.429 I print_info: model params     = 1.41 B
0.00.049.429 I print_info: general.name     = 1.4B
0.00.049.430 I print_info: vocab type       = BPE
0.00.049.430 I print_info: n_vocab          = 50304
0.00.049.430 I print_info: n_merges         = 50009
0.00.049.430 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.430 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.431 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.431 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.431 I print_info: LF token         = 128 'Ä'
0.00.049.431 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.432 I print_info: max token length = 1024
0.00.051.289 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.289 I load_tensors: offloading output layer to GPU
0.00.051.289 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.300 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.301 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.580 I llama_init_from_model: n_seq_max     = 1
0.00.051.581 I llama_init_from_model: n_ctx         = 128
0.00.051.581 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.581 I llama_init_from_model: n_batch       = 128
0.00.051.581 I llama_init_from_model: n_ubatch      = 128
0.00.051.582 I llama_init_from_model: flash_attn    = 0
0.00.051.582 I llama_init_from_model: freq_base     = 10000.0
0.00.051.582 I llama_init_from_model: freq_scale    = 1
0.00.051.582 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.583 I ggml_metal_init: allocating
0.00.051.585 I ggml_metal_init: found device: Apple M4
0.00.051.587 I ggml_metal_init: picking default device: Apple M4
0.00.052.173 I ggml_metal_init: using embedded metal library
0.00.054.492 I ggml_metal_init: GPU name:   Apple M4
0.00.054.494 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.494 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.495 I ggml_metal_init: simdgroup reduction   = true
0.00.054.495 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.495 I ggml_metal_init: has bfloat            = true
0.00.054.495 I ggml_metal_init: use bfloat            = true
0.00.054.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.076 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.358 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.363 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.390 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.258 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.259 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.259 I llama_init_from_model: graph nodes  = 967
0.00.065.259 I llama_init_from_model: graph splits = 2
0.00.065.260 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.260 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.373.693 I 
0.00.373.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.373.742 I perplexity: tokenizing the input ..
0.00.381.321 I perplexity: tokenization took 7.578 ms
0.00.381.325 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.513.541 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.514.720 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.514.745 I llama_perf_context_print:        load time =     363.71 ms
0.00.514.747 I llama_perf_context_print: prompt eval time =     131.99 ms /   128 tokens (    1.03 ms per token,   969.76 tokens per second)
0.00.514.748 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.514.748 I llama_perf_context_print:       total time =     141.05 ms /   129 tokens
0.00.515.152 I ggml_metal_free: deallocating

real	0m0.529s
user	0m0.075s
sys	0m0.065s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.726 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.623 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.631 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.634 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.635 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.635 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.635 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.636 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.636 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.636 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.639 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.639 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.054 I llama_model_loader: - type  f32:  194 tensors
0.00.024.054 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.054 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.054 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.054 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.055 I print_info: file format = GGUF V3 (latest)
0.00.024.055 I print_info: file type   = Q3_K - Medium
0.00.024.056 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.445 I load: special tokens cache size = 25
0.00.048.411 I load: token to piece cache size = 0.2984 MB
0.00.048.414 I print_info: arch             = gptneox
0.00.048.414 I print_info: vocab_only       = 0
0.00.048.415 I print_info: n_ctx_train      = 2048
0.00.048.415 I print_info: n_embd           = 2048
0.00.048.415 I print_info: n_layer          = 24
0.00.048.418 I print_info: n_head           = 16
0.00.048.418 I print_info: n_head_kv        = 16
0.00.048.418 I print_info: n_rot            = 32
0.00.048.419 I print_info: n_swa            = 0
0.00.048.419 I print_info: n_embd_head_k    = 128
0.00.048.419 I print_info: n_embd_head_v    = 128
0.00.048.420 I print_info: n_gqa            = 1
0.00.048.421 I print_info: n_embd_k_gqa     = 2048
0.00.048.421 I print_info: n_embd_v_gqa     = 2048
0.00.048.422 I print_info: f_norm_eps       = 1.0e-05
0.00.048.422 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.423 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.423 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.423 I print_info: f_logit_scale    = 0.0e+00
0.00.048.424 I print_info: n_ff             = 8192
0.00.048.424 I print_info: n_expert         = 0
0.00.048.424 I print_info: n_expert_used    = 0
0.00.048.424 I print_info: causal attn      = 1
0.00.048.424 I print_info: pooling type     = 0
0.00.048.424 I print_info: rope type        = 2
0.00.048.425 I print_info: rope scaling     = linear
0.00.048.425 I print_info: freq_base_train  = 10000.0
0.00.048.426 I print_info: freq_scale_train = 1
0.00.048.426 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.426 I print_info: rope_finetuned   = unknown
0.00.048.427 I print_info: ssm_d_conv       = 0
0.00.048.427 I print_info: ssm_d_inner      = 0
0.00.048.427 I print_info: ssm_d_state      = 0
0.00.048.427 I print_info: ssm_dt_rank      = 0
0.00.048.427 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.427 I print_info: model type       = 1.4B
0.00.048.428 I print_info: model params     = 1.41 B
0.00.048.429 I print_info: general.name     = 1.4B
0.00.048.429 I print_info: vocab type       = BPE
0.00.048.429 I print_info: n_vocab          = 50304
0.00.048.430 I print_info: n_merges         = 50009
0.00.048.430 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.430 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.430 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.431 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.431 I print_info: LF token         = 128 'Ä'
0.00.048.431 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.431 I print_info: max token length = 1024
0.00.050.325 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.325 I load_tensors: offloading output layer to GPU
0.00.050.325 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.336 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.337 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.628 I llama_init_from_model: n_seq_max     = 1
0.00.050.629 I llama_init_from_model: n_ctx         = 128
0.00.050.629 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.629 I llama_init_from_model: n_batch       = 128
0.00.050.629 I llama_init_from_model: n_ubatch      = 128
0.00.050.630 I llama_init_from_model: flash_attn    = 0
0.00.050.630 I llama_init_from_model: freq_base     = 10000.0
0.00.050.630 I llama_init_from_model: freq_scale    = 1
0.00.050.631 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.631 I ggml_metal_init: allocating
0.00.050.633 I ggml_metal_init: found device: Apple M4
0.00.050.635 I ggml_metal_init: picking default device: Apple M4
0.00.051.207 I ggml_metal_init: using embedded metal library
0.00.053.678 I ggml_metal_init: GPU name:   Apple M4
0.00.053.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.681 I ggml_metal_init: simdgroup reduction   = true
0.00.053.681 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.681 I ggml_metal_init: has bfloat            = true
0.00.053.681 I ggml_metal_init: use bfloat            = true
0.00.053.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.682 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.036 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.260 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.262 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.275 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.203 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.204 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.205 I llama_init_from_model: graph nodes  = 967
0.00.065.205 I llama_init_from_model: graph splits = 2
0.00.065.206 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.529.426 I 
0.00.529.456 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.529.460 I perplexity: tokenizing the input ..
0.00.537.219 I perplexity: tokenization took 7.757 ms
0.00.537.222 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.669.452 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.670.622 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.670.647 I llama_perf_context_print:        load time =     520.69 ms
0.00.670.648 I llama_perf_context_print: prompt eval time =     131.99 ms /   128 tokens (    1.03 ms per token,   969.77 tokens per second)
0.00.670.649 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.670.649 I llama_perf_context_print:       total time =     141.22 ms /   129 tokens
0.00.671.179 I ggml_metal_free: deallocating

real	0m0.685s
user	0m0.076s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.749 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.512 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.513 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.514 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.514 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.515 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.515 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.518 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.518 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.518 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.519 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.522 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.522 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.347 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.414 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.203 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.204 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.205 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.205 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.206 I llama_model_loader: - type  f32:  194 tensors
0.00.025.206 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.206 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.206 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.207 I print_info: file format = GGUF V3 (latest)
0.00.025.207 I print_info: file type   = Q4_K - Medium
0.00.025.208 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.427 I load: special tokens cache size = 25
0.00.050.252 I load: token to piece cache size = 0.2984 MB
0.00.050.255 I print_info: arch             = gptneox
0.00.050.255 I print_info: vocab_only       = 0
0.00.050.256 I print_info: n_ctx_train      = 2048
0.00.050.256 I print_info: n_embd           = 2048
0.00.050.256 I print_info: n_layer          = 24
0.00.050.259 I print_info: n_head           = 16
0.00.050.260 I print_info: n_head_kv        = 16
0.00.050.260 I print_info: n_rot            = 32
0.00.050.260 I print_info: n_swa            = 0
0.00.050.260 I print_info: n_embd_head_k    = 128
0.00.050.260 I print_info: n_embd_head_v    = 128
0.00.050.261 I print_info: n_gqa            = 1
0.00.050.262 I print_info: n_embd_k_gqa     = 2048
0.00.050.263 I print_info: n_embd_v_gqa     = 2048
0.00.050.263 I print_info: f_norm_eps       = 1.0e-05
0.00.050.264 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.264 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.264 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.264 I print_info: f_logit_scale    = 0.0e+00
0.00.050.265 I print_info: n_ff             = 8192
0.00.050.265 I print_info: n_expert         = 0
0.00.050.265 I print_info: n_expert_used    = 0
0.00.050.265 I print_info: causal attn      = 1
0.00.050.266 I print_info: pooling type     = 0
0.00.050.266 I print_info: rope type        = 2
0.00.050.266 I print_info: rope scaling     = linear
0.00.050.266 I print_info: freq_base_train  = 10000.0
0.00.050.267 I print_info: freq_scale_train = 1
0.00.050.267 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.267 I print_info: rope_finetuned   = unknown
0.00.050.267 I print_info: ssm_d_conv       = 0
0.00.050.268 I print_info: ssm_d_inner      = 0
0.00.050.268 I print_info: ssm_d_state      = 0
0.00.050.268 I print_info: ssm_dt_rank      = 0
0.00.050.268 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.268 I print_info: model type       = 1.4B
0.00.050.269 I print_info: model params     = 1.41 B
0.00.050.269 I print_info: general.name     = 1.4B
0.00.050.269 I print_info: vocab type       = BPE
0.00.050.269 I print_info: n_vocab          = 50304
0.00.050.270 I print_info: n_merges         = 50009
0.00.050.270 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.270 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.270 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.271 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.271 I print_info: LF token         = 128 'Ä'
0.00.050.271 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.271 I print_info: max token length = 1024
0.00.052.253 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.253 I load_tensors: offloading output layer to GPU
0.00.052.254 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.264 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.265 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.562 I llama_init_from_model: n_seq_max     = 1
0.00.052.562 I llama_init_from_model: n_ctx         = 128
0.00.052.563 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.563 I llama_init_from_model: n_batch       = 128
0.00.052.563 I llama_init_from_model: n_ubatch      = 128
0.00.052.563 I llama_init_from_model: flash_attn    = 0
0.00.052.564 I llama_init_from_model: freq_base     = 10000.0
0.00.052.564 I llama_init_from_model: freq_scale    = 1
0.00.052.564 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.565 I ggml_metal_init: allocating
0.00.052.568 I ggml_metal_init: found device: Apple M4
0.00.052.570 I ggml_metal_init: picking default device: Apple M4
0.00.053.121 I ggml_metal_init: using embedded metal library
0.00.055.485 I ggml_metal_init: GPU name:   Apple M4
0.00.055.486 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.487 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.487 I ggml_metal_init: simdgroup reduction   = true
0.00.055.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.487 I ggml_metal_init: has bfloat            = true
0.00.055.487 I ggml_metal_init: use bfloat            = true
0.00.055.488 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.098 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.339 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.341 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.366 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.341 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.342 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.342 I llama_init_from_model: graph nodes  = 967
0.00.067.342 I llama_init_from_model: graph splits = 2
0.00.067.343 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.343 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.709 I 
0.00.571.742 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.748 I perplexity: tokenizing the input ..
0.00.579.924 I perplexity: tokenization took 8.174 ms
0.00.579.947 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.714.239 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.715.473 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.715.501 I llama_perf_context_print:        load time =     561.96 ms
0.00.715.503 I llama_perf_context_print: prompt eval time =     134.06 ms /   128 tokens (    1.05 ms per token,   954.76 tokens per second)
0.00.715.507 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.715.508 I llama_perf_context_print:       total time =     143.79 ms /   129 tokens
0.00.715.960 I ggml_metal_free: deallocating

real	0m0.730s
user	0m0.077s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.917 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.806 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.808 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.813 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.818 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.579 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.264 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.265 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.266 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.266 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.266 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.267 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.267 I llama_model_loader: - type  f32:  194 tensors
0.00.024.268 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.268 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.268 I print_info: file format = GGUF V3 (latest)
0.00.024.269 I print_info: file type   = Q5_K - Medium
0.00.024.270 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.042.589 I load: special tokens cache size = 25
0.00.048.631 I load: token to piece cache size = 0.2984 MB
0.00.048.634 I print_info: arch             = gptneox
0.00.048.634 I print_info: vocab_only       = 0
0.00.048.634 I print_info: n_ctx_train      = 2048
0.00.048.634 I print_info: n_embd           = 2048
0.00.048.635 I print_info: n_layer          = 24
0.00.048.637 I print_info: n_head           = 16
0.00.048.638 I print_info: n_head_kv        = 16
0.00.048.640 I print_info: n_rot            = 32
0.00.048.640 I print_info: n_swa            = 0
0.00.048.640 I print_info: n_embd_head_k    = 128
0.00.048.640 I print_info: n_embd_head_v    = 128
0.00.048.641 I print_info: n_gqa            = 1
0.00.048.642 I print_info: n_embd_k_gqa     = 2048
0.00.048.647 I print_info: n_embd_v_gqa     = 2048
0.00.048.648 I print_info: f_norm_eps       = 1.0e-05
0.00.048.649 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.650 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.650 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.650 I print_info: f_logit_scale    = 0.0e+00
0.00.048.651 I print_info: n_ff             = 8192
0.00.048.651 I print_info: n_expert         = 0
0.00.048.651 I print_info: n_expert_used    = 0
0.00.048.651 I print_info: causal attn      = 1
0.00.048.651 I print_info: pooling type     = 0
0.00.048.651 I print_info: rope type        = 2
0.00.048.652 I print_info: rope scaling     = linear
0.00.048.654 I print_info: freq_base_train  = 10000.0
0.00.048.655 I print_info: freq_scale_train = 1
0.00.048.656 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.656 I print_info: rope_finetuned   = unknown
0.00.048.656 I print_info: ssm_d_conv       = 0
0.00.048.656 I print_info: ssm_d_inner      = 0
0.00.048.656 I print_info: ssm_d_state      = 0
0.00.048.656 I print_info: ssm_dt_rank      = 0
0.00.048.657 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.657 I print_info: model type       = 1.4B
0.00.048.658 I print_info: model params     = 1.41 B
0.00.048.658 I print_info: general.name     = 1.4B
0.00.048.658 I print_info: vocab type       = BPE
0.00.048.658 I print_info: n_vocab          = 50304
0.00.048.659 I print_info: n_merges         = 50009
0.00.048.660 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.660 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.660 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.660 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.660 I print_info: LF token         = 128 'Ä'
0.00.048.661 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.661 I print_info: max token length = 1024
0.00.050.431 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.431 I load_tensors: offloading output layer to GPU
0.00.050.431 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.437 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.438 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.050.708 I llama_init_from_model: n_seq_max     = 1
0.00.050.709 I llama_init_from_model: n_ctx         = 128
0.00.050.709 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.709 I llama_init_from_model: n_batch       = 128
0.00.050.709 I llama_init_from_model: n_ubatch      = 128
0.00.050.709 I llama_init_from_model: flash_attn    = 0
0.00.050.710 I llama_init_from_model: freq_base     = 10000.0
0.00.050.710 I llama_init_from_model: freq_scale    = 1
0.00.050.710 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.711 I ggml_metal_init: allocating
0.00.050.713 I ggml_metal_init: found device: Apple M4
0.00.050.715 I ggml_metal_init: picking default device: Apple M4
0.00.051.274 I ggml_metal_init: using embedded metal library
0.00.053.652 I ggml_metal_init: GPU name:   Apple M4
0.00.053.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.654 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.654 I ggml_metal_init: simdgroup reduction   = true
0.00.053.654 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.654 I ggml_metal_init: has bfloat            = true
0.00.053.654 I ggml_metal_init: use bfloat            = true
0.00.053.655 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.655 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.727 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.997 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.001 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.014 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.832 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.833 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.834 I llama_init_from_model: graph nodes  = 967
0.00.064.834 I llama_init_from_model: graph splits = 2
0.00.064.835 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.835 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.538 I 
0.00.631.563 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.567 I perplexity: tokenizing the input ..
0.00.639.720 I perplexity: tokenization took 8.151 ms
0.00.639.725 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.217 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.781.410 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.781.435 I llama_perf_context_print:        load time =     622.62 ms
0.00.781.436 I llama_perf_context_print: prompt eval time =     140.27 ms /   128 tokens (    1.10 ms per token,   912.55 tokens per second)
0.00.781.437 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.442 I llama_perf_context_print:       total time =     149.90 ms /   129 tokens
0.00.781.884 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.076s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.915 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.804 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.819 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.822 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.823 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.824 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.824 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.502 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.218 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.218 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.218 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.219 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.219 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.220 I llama_model_loader: - type  f32:  194 tensors
0.00.025.220 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.220 I print_info: file format = GGUF V3 (latest)
0.00.025.221 I print_info: file type   = Q6_K
0.00.025.222 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.293 I load: special tokens cache size = 25
0.00.050.052 I load: token to piece cache size = 0.2984 MB
0.00.050.055 I print_info: arch             = gptneox
0.00.050.055 I print_info: vocab_only       = 0
0.00.050.055 I print_info: n_ctx_train      = 2048
0.00.050.056 I print_info: n_embd           = 2048
0.00.050.056 I print_info: n_layer          = 24
0.00.050.058 I print_info: n_head           = 16
0.00.050.059 I print_info: n_head_kv        = 16
0.00.050.059 I print_info: n_rot            = 32
0.00.050.060 I print_info: n_swa            = 0
0.00.050.060 I print_info: n_embd_head_k    = 128
0.00.050.060 I print_info: n_embd_head_v    = 128
0.00.050.061 I print_info: n_gqa            = 1
0.00.050.061 I print_info: n_embd_k_gqa     = 2048
0.00.050.064 I print_info: n_embd_v_gqa     = 2048
0.00.050.065 I print_info: f_norm_eps       = 1.0e-05
0.00.050.067 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.067 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.067 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.067 I print_info: f_logit_scale    = 0.0e+00
0.00.050.068 I print_info: n_ff             = 8192
0.00.050.068 I print_info: n_expert         = 0
0.00.050.068 I print_info: n_expert_used    = 0
0.00.050.068 I print_info: causal attn      = 1
0.00.050.068 I print_info: pooling type     = 0
0.00.050.069 I print_info: rope type        = 2
0.00.050.070 I print_info: rope scaling     = linear
0.00.050.070 I print_info: freq_base_train  = 10000.0
0.00.050.070 I print_info: freq_scale_train = 1
0.00.050.071 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.071 I print_info: rope_finetuned   = unknown
0.00.050.071 I print_info: ssm_d_conv       = 0
0.00.050.071 I print_info: ssm_d_inner      = 0
0.00.050.071 I print_info: ssm_d_state      = 0
0.00.050.071 I print_info: ssm_dt_rank      = 0
0.00.050.071 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.072 I print_info: model type       = 1.4B
0.00.050.072 I print_info: model params     = 1.41 B
0.00.050.072 I print_info: general.name     = 1.4B
0.00.050.073 I print_info: vocab type       = BPE
0.00.050.073 I print_info: n_vocab          = 50304
0.00.050.073 I print_info: n_merges         = 50009
0.00.050.073 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.074 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.074 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.074 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.074 I print_info: LF token         = 128 'Ä'
0.00.050.075 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.075 I print_info: max token length = 1024
0.00.052.079 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.079 I load_tensors: offloading output layer to GPU
0.00.052.079 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.090 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.091 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.400 I llama_init_from_model: n_seq_max     = 1
0.00.052.401 I llama_init_from_model: n_ctx         = 128
0.00.052.401 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.401 I llama_init_from_model: n_batch       = 128
0.00.052.402 I llama_init_from_model: n_ubatch      = 128
0.00.052.402 I llama_init_from_model: flash_attn    = 0
0.00.052.402 I llama_init_from_model: freq_base     = 10000.0
0.00.052.402 I llama_init_from_model: freq_scale    = 1
0.00.052.403 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.403 I ggml_metal_init: allocating
0.00.052.406 I ggml_metal_init: found device: Apple M4
0.00.052.408 I ggml_metal_init: picking default device: Apple M4
0.00.052.968 I ggml_metal_init: using embedded metal library
0.00.055.280 I ggml_metal_init: GPU name:   Apple M4
0.00.055.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.281 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.282 I ggml_metal_init: simdgroup reduction   = true
0.00.055.282 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.282 I ggml_metal_init: has bfloat            = true
0.00.055.282 I ggml_metal_init: use bfloat            = true
0.00.055.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.283 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.851 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.186 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.191 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.207 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.028 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.029 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.029 I llama_init_from_model: graph nodes  = 967
0.00.067.029 I llama_init_from_model: graph splits = 2
0.00.067.030 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.864 I 
0.00.387.888 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.387.891 I perplexity: tokenizing the input ..
0.00.395.524 I perplexity: tokenization took 7.632 ms
0.00.395.530 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.535.967 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.537.234 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.537.267 I llama_perf_context_print:        load time =     377.94 ms
0.00.537.268 I llama_perf_context_print: prompt eval time =     140.21 ms /   128 tokens (    1.10 ms per token,   912.94 tokens per second)
0.00.537.268 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.537.269 I llama_perf_context_print:       total time =     149.40 ms /   129 tokens
0.00.537.771 I ggml_metal_free: deallocating

real	0m0.553s
user	0m0.077s
sys	0m0.078s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.285 I build: 4475 (84a44815) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.038.907 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.052.863 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.052.882 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.052.886 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.052.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.052.902 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.052.903 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.052.904 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.052.910 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.052.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.052.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.052.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.052.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.052.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.052.914 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.052.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.052.918 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.052.919 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.060.007 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.062.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.071.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.071.773 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.071.774 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.071.775 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.071.775 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.071.776 I llama_model_loader: - type  f32:  194 tensors
0.00.071.777 I llama_model_loader: - type  f16:   98 tensors
0.00.071.781 I print_info: file format = GGUF V3 (latest)
0.00.071.782 I print_info: file type   = all F32 (guessed)
0.00.071.784 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.105.847 I load: special tokens cache size = 25
0.00.113.432 I load: token to piece cache size = 0.2984 MB
0.00.113.435 I print_info: arch             = gptneox
0.00.113.435 I print_info: vocab_only       = 0
0.00.113.435 I print_info: n_ctx_train      = 2048
0.00.113.436 I print_info: n_embd           = 2048
0.00.113.436 I print_info: n_layer          = 24
0.00.113.439 I print_info: n_head           = 16
0.00.113.442 I print_info: n_head_kv        = 16
0.00.113.442 I print_info: n_rot            = 32
0.00.113.442 I print_info: n_swa            = 0
0.00.113.442 I print_info: n_embd_head_k    = 128
0.00.113.442 I print_info: n_embd_head_v    = 128
0.00.113.443 I print_info: n_gqa            = 1
0.00.113.444 I print_info: n_embd_k_gqa     = 2048
0.00.113.445 I print_info: n_embd_v_gqa     = 2048
0.00.113.445 I print_info: f_norm_eps       = 1.0e-05
0.00.113.445 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.113.446 I print_info: f_clamp_kqv      = 0.0e+00
0.00.113.446 I print_info: f_max_alibi_bias = 0.0e+00
0.00.113.446 I print_info: f_logit_scale    = 0.0e+00
0.00.113.447 I print_info: n_ff             = 8192
0.00.113.448 I print_info: n_expert         = 0
0.00.113.448 I print_info: n_expert_used    = 0
0.00.113.448 I print_info: causal attn      = 1
0.00.113.448 I print_info: pooling type     = 0
0.00.113.448 I print_info: rope type        = 2
0.00.113.448 I print_info: rope scaling     = linear
0.00.113.449 I print_info: freq_base_train  = 10000.0
0.00.113.449 I print_info: freq_scale_train = 1
0.00.113.449 I print_info: n_ctx_orig_yarn  = 2048
0.00.113.450 I print_info: rope_finetuned   = unknown
0.00.113.450 I print_info: ssm_d_conv       = 0
0.00.113.450 I print_info: ssm_d_inner      = 0
0.00.113.450 I print_info: ssm_d_state      = 0
0.00.113.450 I print_info: ssm_dt_rank      = 0
0.00.113.451 I print_info: ssm_dt_b_c_rms   = 0
0.00.113.451 I print_info: model type       = 1.4B
0.00.113.451 I print_info: model params     = 1.41 B
0.00.113.451 I print_info: general.name     = 1.4B
0.00.113.452 I print_info: vocab type       = BPE
0.00.113.452 I print_info: n_vocab          = 50304
0.00.113.452 I print_info: n_merges         = 50009
0.00.113.452 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.113.453 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.113.453 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.113.453 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.113.453 I print_info: LF token         = 128 'Ä'
0.00.113.454 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.113.454 I print_info: max token length = 1024
0.00.115.671 I load_tensors: offloading 24 repeating layers to GPU
0.00.115.671 I load_tensors: offloading output layer to GPU
0.00.115.672 I load_tensors: offloaded 25/25 layers to GPU
0.00.115.677 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.115.678 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.116.057 I llama_init_from_model: n_seq_max     = 1
0.00.116.058 I llama_init_from_model: n_ctx         = 128
0.00.116.058 I llama_init_from_model: n_ctx_per_seq = 128
0.00.116.058 I llama_init_from_model: n_batch       = 128
0.00.116.058 I llama_init_from_model: n_ubatch      = 128
0.00.116.058 I llama_init_from_model: flash_attn    = 0
0.00.116.059 I llama_init_from_model: freq_base     = 10000.0
0.00.116.059 I llama_init_from_model: freq_scale    = 1
0.00.116.060 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.116.060 I ggml_metal_init: allocating
0.00.116.063 I ggml_metal_init: found device: Apple M4
0.00.116.065 I ggml_metal_init: picking default device: Apple M4
0.00.116.732 I ggml_metal_init: using embedded metal library
0.00.119.538 I ggml_metal_init: GPU name:   Apple M4
0.00.119.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.119.540 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.119.541 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.119.541 I ggml_metal_init: simdgroup reduction   = true
0.00.119.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.119.541 I ggml_metal_init: has bfloat            = true
0.00.119.541 I ggml_metal_init: use bfloat            = true
0.00.119.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.119.542 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.128.852 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.130.089 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.130.095 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.130.110 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.130.991 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.130.992 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.130.993 I llama_init_from_model: graph nodes  = 967
0.00.130.993 I llama_init_from_model: graph splits = 2
0.00.130.994 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.130.994 I 
0.00.131.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.131.016 I compute_imatrix: tokenizing the input ..
0.00.137.977 I compute_imatrix: tokenization took 6.961 ms
0.00.137.979 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.711.883 I compute_imatrix: 1.57 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.715.185 I llama_perf_context_print:        load time =    1672.97 ms
0.01.715.186 I llama_perf_context_print: prompt eval time =    1573.27 ms /   128 tokens (   12.29 ms per token,    81.36 tokens per second)
0.01.715.187 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.715.188 I llama_perf_context_print:       total time =    1676.26 ms /   129 tokens
0.01.715.713 I ggml_metal_free: deallocating

real	0m1.902s
user	0m0.186s
sys	0m0.240s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4475 (84a44815)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10340a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10340aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10340aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10340b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10340bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10340c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10340c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10340cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10340d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10340d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10340dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10340e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10340ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10340f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10340fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x103410310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x103410a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x103411150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x103411870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x103412040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x103412760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x103412e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1034135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x103413e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x103414560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x103414820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x103414e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x103415aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x103415fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1034162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x103416740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x103416a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x103417290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1034177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x103417a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x103417f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1034183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x103418870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x103418d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1034191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x103419650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x103419af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x103419f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10341a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10341a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10341ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10341b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10341bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10341c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10341c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10341ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10341d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10341da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10341e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10341e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10341ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10341f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10341f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10341fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x103420280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x103420540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1034209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x103420e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x103421320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1034217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x103421c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x103422100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1034225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x103422a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x103422ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x103423380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x103423820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x103423cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x103424210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x103424760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x103424cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x103425200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x103425750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x103425ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1034261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x103426740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x103426c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1034271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x103427730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x103427c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1034281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x103428720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x103428c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1034291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x103429710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x103429c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10342a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10342a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10342ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10342b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10342b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10342bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10341b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10342c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10342c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10342cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10342d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10342d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10342dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10342e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10342e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10342ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10342f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10342f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10342fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1034302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x103430820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x103430d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x103431210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1034316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x103431b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x103431ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x103432490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x103432930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x103432dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x103433270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x103433710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x103433bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x103434050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1034344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x103434990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x103434e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1034352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x103435770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x103435c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1034360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x103436550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1034369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x103436e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x103437330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1034377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x103437c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x103438110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1034385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x103438a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x103438ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x103439390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x103439830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x103439cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10343a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10343a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10343aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10343af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10343b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10343b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10343bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10343c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10343c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10343cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10343cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10343d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10343d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10343dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10343e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10343e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10343eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10343f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10343f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10343f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10343fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x103440290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x103440730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x103440bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x103441070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x103441510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1034419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x103441e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1034422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x103442790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x103442c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1034430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x103443570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x103443a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x103443eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x103444350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1034447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x103444c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x103445130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1034455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x103445a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x103445f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1034463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x103446850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x103446cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x103447190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x103447630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x103447ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x103447f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1034484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x103448a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x103448f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1034494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x103449770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x103449d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10344a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10344a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10344b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10344b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10344b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10344bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10344c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10344cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10344d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10344d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10344dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10344e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10344e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10344ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10344f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10344f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10344fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x103450270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1034507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x103450d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x103451260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1034517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x103451d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x103452250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1034527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x103452cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x103453240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x103453790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x103453ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x103454230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x103454780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x103454cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x103455220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x103455770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x103455cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x103456210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x103456760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x103456cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x103457200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x103457750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x103457ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1034581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x103458740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x103458c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1034591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x103459730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x103459c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10345a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10345a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10345ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10345b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10345b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10345bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10345c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10345c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10345cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10345d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10345d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10345dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10345e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10345e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10345ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10345f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10345f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10345fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x103460170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1034606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x103460c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1034610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x103461550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1034619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x103461e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x103462330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1034627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x103462c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x103463110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1034635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x103463a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x103463ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x103464390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x103464830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x103464cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x103465170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1034656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x103465de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x103466500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x103466c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x103467340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x103467600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x103467df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1034680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1034686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.157.054 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.157.058 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117304b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117304f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117305400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117305870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117305ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117306150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1173065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117306a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117306ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117307310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117307780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117307e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117308990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117309140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117309950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11730a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11730a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11730aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11730b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11730bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11730c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11730cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11730d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11730d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11730e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11730e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11730e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11730ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11730ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11730f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11730f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11730fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117310180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117310440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1173108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117310d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117311190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117311600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117311a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117311ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117312350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1173127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117312c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1173130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117313510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117313980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117313df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117314260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1173146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117314b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117314fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117315420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117315890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117315d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117316170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1173165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117316b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117317050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1173174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117317930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117317da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117318210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117318680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117318af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117318f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1173193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117319840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117319cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11731a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11731a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11731aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11731ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11731b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11731b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11731bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11731c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11731c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11731c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11731cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11731d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11731d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11731dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11731df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11731e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11731e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11731ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11731f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11731f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11731f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11731fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1173202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117320730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117320ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117321010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117321480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1173218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117321d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1173221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117322640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117322ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117322f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117323390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117323800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117323c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1173240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117324550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1173249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117324e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1173252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117325710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117325b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117325ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117326460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1173268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117326d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1173271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117327620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117327a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117327f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117328370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1173287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117328c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1173290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117329530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1173299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117329e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11732a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11732a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11732ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11732afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11732b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11732b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11732bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11732c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11732c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11732ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11732cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11732d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11732d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11732dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11732e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11732e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11732e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11732edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11732f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11732f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11732fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11732ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117330420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117330890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117330d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117331170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1173315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117331a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117331ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117332330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1173327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117332c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117333080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1173334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117333960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117333dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117334240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1173346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117334b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117334f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117335bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117335e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117336140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1173365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117336a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117336e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117337300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117337770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117337be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117338050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1173384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117338930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117338da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117339210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117339680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117339af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117339f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11733a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11733a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11733acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11733b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11733b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11733ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11733be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11733c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11733c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11733cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11733d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11733d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11733d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11733dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11733e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11733e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11733ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11733ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11733f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11733f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11733fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117340290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117340700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117340b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117340fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117341500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117341a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117342580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117342840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117342e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1173433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117343980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117343f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117344500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117344ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117345080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117345640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117345c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1173461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117346780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117346d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117347300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1173478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117347e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117348440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117348a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117348fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117349580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117349b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11734a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11734a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11734ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11734b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11734b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11734bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11734c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11734c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11734cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11734d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11734da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11734e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11734e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11734ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11734f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11734f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11734fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1173502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117350880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117350e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117351400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1173519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117351f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117352540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117352b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1173530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117353680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117353c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117354200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1173547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117354d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117355340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117355900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117355ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117356480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117356a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117356f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117357440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117357940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117357e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117358340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117358840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117358d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117359240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117359740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117359c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11735a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11735a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11735ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11735b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11735b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11735bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11735c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11735cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11735d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11735d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11735df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11735e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11735e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x103468370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10344a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x103449a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10344a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10341d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10341d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10341f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10344c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x103414ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10341b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10341bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10341c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10341a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10341cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x103413ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10341fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10342c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1034678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x103416cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x103416f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10344c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10344ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1034150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1034153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x103415670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x103468b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x103468de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1034690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x103469360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x103469620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1034698e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x103469ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x103469e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10346a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10346a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10346a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10346a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10346ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10346aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10346b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10346b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10346b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10346b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10346bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10346bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10346c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10346c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10346c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10346ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10346cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10346cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10346d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10346d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10346d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10346dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10346dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10346e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10346e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10346e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10346e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10346eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10346ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10346f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10346f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10346f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10346f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10346fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10346fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x103470160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x103470420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1034706e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1034709a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x103470c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x103470f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1034711e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1034714a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x103471760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x103471a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x103471ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x103471fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x103472260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x103472520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1034727e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x103472aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x103472d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x103473020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1034732e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1034735a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x103473860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x103473b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x103473de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1034740a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x103474360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x103474620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1034748e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x103474ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x103474e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x103475120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1034753e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1034756a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x103475960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x103475c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x103475ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1034761a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x103476460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x103476720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1034769e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x103476ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x103476f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x103477220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1034774e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1034777a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x103477a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x103477d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x103477fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1034782a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x103478560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x103478820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x103478ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x103478da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x103479060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x103479320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1034795e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1034798a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x103479b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x103479e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10347a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10347a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10347a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10347a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10347abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10347aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10347b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10347b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10347b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10347b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10347bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10347bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10347c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10347c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10347c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10347ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10347cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10347cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10347d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10347d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10347d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10347daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10347dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10347e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10347e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10347e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10347e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10347eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10347ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10347f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10347f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10347f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10347f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10347fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10347fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x103480120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1034803e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1034806a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x103480960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x103480c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x103480ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1034811a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x103481460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x103481720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1034819e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x103481ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x103481f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x103482220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1034824e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1034827a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x103482a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x103482d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x103482fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1034832a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x103483560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x103483820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x103483ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x103483da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x103484060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x103484320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1034845e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1034848a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x103484b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x103484e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1034850e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1034853a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x103485660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x103485920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x103485be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x103485ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x103486160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x103486420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1034866e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1034869a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x103486c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x103486f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1034871e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1034874a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x103487760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x103487a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x103487ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x103487fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x103488260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x103488520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x103488af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x103489040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x103489590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x103489ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10348a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10348a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10348aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10348b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10348b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10348bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10348c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10348c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10348cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10348d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10348d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10348daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10348dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10348e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10348ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10348efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10348f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10348fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10348ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x103490520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x103490a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x103490fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x103491510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x103491a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x103491fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x103492500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x103492a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x103492fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1034934f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x103493a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x103493f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1034944e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x103494a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x103494f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1034954d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x103495a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x103495f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1034964c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x103496a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x103496f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1034974b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x103497a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x103497f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1034984a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1034989f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x103498f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x103499490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1034999e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x103499f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10349a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10349a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10349af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10349b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10349b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10349b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10349bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10349c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10349c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10349ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10349ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10349d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10349d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10349dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10349e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10349e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10349e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10349ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10349f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10349f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10349fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1034a07c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1034a0ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1034a1600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1034a18c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1034a1d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1034a2330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1034a2940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.943s
user	0m0.302s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4475 (84a44815)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f70d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f70dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f70e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f70e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f70ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f70f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f70f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f70fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f710410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f710e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f711310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f711e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f7125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f712df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f714350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f714a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f715240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f715960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f716080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f7167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f717040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f717760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f717a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f718030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f718ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f7191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f7194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f719940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f719c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f71a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f71a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f71ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f71b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f71b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f71ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f71bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f71c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f71ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f71d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f71d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f71d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f71df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f71e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f71ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f71f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f71fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f720060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f720c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f721290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f721a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f721f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f7223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f722680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f722c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f723480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f723be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f724080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f724520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f7249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f724e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f7257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f725c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f7260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f726580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f726a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f726ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f727960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f728400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f728ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f7293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f729940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f72a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f72a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f72ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f72b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f72b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f72be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f72c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f72c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f72ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f72d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f72d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f72de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f72e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f72e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f72ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f71eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f72f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f72fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f72ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f730500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f730a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f730fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f7314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f731a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f731f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f7324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f732a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f732f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f7334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f733a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f733f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f734410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f7348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f734d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f7351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f735690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f735b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f735fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f736470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f736910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f736db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f7376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f737b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f738030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f7384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f738970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f738e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f7392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f739750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f739bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f73a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f73a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f73a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f73ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f73b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f73b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f73bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f73c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f73c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f73ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f73ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f73d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f73d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f73dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f73e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f73e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f73ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f73ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f73f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f73f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f73fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f7401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f740650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f740af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f740f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f741430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f7418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f741d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f742210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f7426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f742b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f742ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f743490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f743930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f743dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f744270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f744710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f744bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f745050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f7454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f745990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f745e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f7462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f746770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f746c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f7470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f747550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f7479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f747e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f748330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f7487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f748c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f749110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f7495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f749a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f749ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f74a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f74a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f74acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f74b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f74b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f74bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f74c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f74c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f74c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f74cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f74d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f74dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f74e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f74e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f74eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f74f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f74f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f74ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f7503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f750840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f750ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f7519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f751f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f752480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f7529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f752f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f753470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f7539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f753f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f754460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f7549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f754f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f7559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f755ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f756440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f756990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f756ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f757430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f757980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f757ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f758420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f758970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f758ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f759410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f759960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f759eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f75a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f75a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f75aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f75b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f75b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f75be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f75c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f75c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f75ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f75d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f75d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f75de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f75e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f75e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f75ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f75f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f75f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f75fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f7603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f7608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f760e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f761390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f7618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f761e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f762380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f7628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f762e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f763370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f7638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f763e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f7642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f764750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f764bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f765090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f765530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f7659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f766310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f7667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f766c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f7670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f767590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f767a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f767ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f768370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f7688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f768fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f769700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f769e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f76a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f76a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f76aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f76b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f76b8c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.094.263 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f608800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f608c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f6090e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f609550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f6099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f609e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f60a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f60a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f60ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f60aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f60b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f60bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f60c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f60ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f60d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f60dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f60e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f60eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f60f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f60f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f610100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f610820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f610f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f611660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f611d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f612300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f612770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f612be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f613050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f613550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f613a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f613ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f614190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f614600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f614a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f614fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f6154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f6159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f615ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f6163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f6168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f616dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f6172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f6177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f617c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f6180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f618520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f618990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f618e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f619270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f6196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f619b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f619fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f61a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f61ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f61b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f61b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f61b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f61c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f61c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f61caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f61cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f61d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f61d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f61dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f61e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f61e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f61eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f61efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f61f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f61f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f61fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f6202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f620820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f620d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f6212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f621810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f621d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f6222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f622800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f622d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f6232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f6237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f624290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f6247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f625280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f6257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f625d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f626270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f6267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f626d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f627260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f6277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f627d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f6287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f628cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f629790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f629ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f62a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f62a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f62acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f62b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f62b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f62bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f62c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f62c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f62ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f62d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f62d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f62db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f62dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f62e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f62e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f62edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f62f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f62f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f62fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f630040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f6304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f630980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f630e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f6312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f631c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f6320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f632540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f6329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f632e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f6337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f633c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f634100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f6345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f634a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f634ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f635380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f635820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f636160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f636600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f636aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f636f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f6373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f637880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f637d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f6381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f638660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f638b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f639440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f6398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f639d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f63a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f63a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f63ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f63b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f63b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f63b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f63bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f63c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f63c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f63cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f63d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f63d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f63d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f63de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f63e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f63e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f63ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f63f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f63f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f63fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f63fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f640340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f6407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f640c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f641120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f6415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f641a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f6423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f642840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f642ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f643180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f643620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f643ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f643f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f644400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f644950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f644ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f6453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f645940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f645c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f646210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f646820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f646e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f647620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f647ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f647d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f648390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f6489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f649190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f649630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f649ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f649f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f64a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f64ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f64b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f64b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f64bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f64c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f64c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f64cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f64d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f64d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f64dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f64e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f64e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f64ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f64f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f64f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f64fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f650170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f6506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f650c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f651160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f6516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f651c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f652150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f6526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f652bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f653140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f653690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f653be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f654130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f654680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f654bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f655120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f655670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f655bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f656110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f656660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f656bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f657100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f657650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f657ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f6580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f658640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f658b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f6590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f659630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f659b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f65a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f65a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f65ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f65b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f65b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f65bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f65c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f65c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f65cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f65d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f65d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f65d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f65de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f65e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f65e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f65ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f65f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f65f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f65fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f65fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f660380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f660820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f660cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f661160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f661600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f661b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f662270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f662990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f6630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f6637d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f663a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f664280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f664540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f664b50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1308044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1308056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1308063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x130808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1308092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x130809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13080a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13080a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13080af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13080b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13080be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13080c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13080cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13080d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13080dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13080dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13080e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13080e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13080e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13080edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13080f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13080f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13080fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13080fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1308102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1308114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1308133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1308149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1308152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x130816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1308177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1308180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1308189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1308196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x130819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x130819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13081a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13081a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13081ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13081b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13081b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13081ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13081bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13081c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13081c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13081cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13081d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13081d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13081d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13081ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13081e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13081e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13081eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13081efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13081f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13081f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13081fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1308205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1308217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1308224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1308253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1308269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1308272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x130828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1308291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x130829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x130829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13082a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13082a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13082ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13082b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13082b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13082b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13082be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13082c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13082c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13082cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13082d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13082d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13082d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13082dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13082e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13082e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13082eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13082ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13082f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13082f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13082fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1308300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1308309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1308328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1308331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1308347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1308350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x130835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x130835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1308366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1308378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1308385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x130839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1308397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13083a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13083a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13083a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13083ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13083b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13083b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13083bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13083bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13083c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13083c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13083ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13083d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13083d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13083da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13083deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13083e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13083e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13083ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13083f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13083f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13083f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13083fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1308406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1308424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1308436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1308455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1308474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1308493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x130849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13084a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13084a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13084aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13084ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13084b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13084b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13084bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13084c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13084c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13084c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13084cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13084d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13084d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13084dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13084df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13084e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13084e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13084eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13084f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13084f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13084f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13084fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1308502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1308521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1308533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1308540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1308549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x130854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1308552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1308568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1308576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1308579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x130857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.926s
user	0m0.243s
sys	0m0.138s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
