### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.18 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.88 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.11 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  191.53 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.89 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.99 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 252.26 sec*proc (29 tests)

Total Test time (real) = 252.27 sec

real	4m12.302s
user	8m33.817s
sys	0m7.372s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.13 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.78 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.39 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.53 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.38 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.06 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.37 sec*proc (29 tests)

Total Test time (real) =  54.38 sec

real	0m54.383s
user	1m16.194s
sys	0m6.208s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.073 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.041 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.727 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.016.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.735 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.016.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.736 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.016.736 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.016.737 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.016.738 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.016.738 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.016.741 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.016.741 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.016.742 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.016.749 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.016.750 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.016.751 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.016.751 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.016.751 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.016.752 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.016.752 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.020.286 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.021.163 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.021.164 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.021.165 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.021.165 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.021.165 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.021.166 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.021.166 I llama_model_loader: - type  f32:  124 tensors
0.00.021.167 I llama_model_loader: - type  f16:   73 tensors
0.00.021.168 I print_info: file format = GGUF V3 (latest)
0.00.021.168 I print_info: file type   = F16
0.00.021.169 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.024.339 I load: special tokens cache size = 5
0.00.025.993 I load: token to piece cache size = 0.2032 MB
0.00.025.996 I print_info: arch             = bert
0.00.025.997 I print_info: vocab_only       = 0
0.00.025.997 I print_info: n_ctx_train      = 512
0.00.025.997 I print_info: n_embd           = 384
0.00.025.997 I print_info: n_layer          = 12
0.00.026.001 I print_info: n_head           = 12
0.00.026.001 I print_info: n_head_kv        = 12
0.00.026.001 I print_info: n_rot            = 32
0.00.026.002 I print_info: n_swa            = 0
0.00.026.002 I print_info: n_embd_head_k    = 32
0.00.026.002 I print_info: n_embd_head_v    = 32
0.00.026.003 I print_info: n_gqa            = 1
0.00.026.003 I print_info: n_embd_k_gqa     = 384
0.00.026.004 I print_info: n_embd_v_gqa     = 384
0.00.026.005 I print_info: f_norm_eps       = 1.0e-12
0.00.026.005 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.026.005 I print_info: f_clamp_kqv      = 0.0e+00
0.00.026.005 I print_info: f_max_alibi_bias = 0.0e+00
0.00.026.005 I print_info: f_logit_scale    = 0.0e+00
0.00.026.006 I print_info: n_ff             = 1536
0.00.026.006 I print_info: n_expert         = 0
0.00.026.006 I print_info: n_expert_used    = 0
0.00.026.007 I print_info: causal attn      = 0
0.00.026.007 I print_info: pooling type     = 2
0.00.026.007 I print_info: rope type        = 2
0.00.026.007 I print_info: rope scaling     = linear
0.00.026.007 I print_info: freq_base_train  = 10000.0
0.00.026.008 I print_info: freq_scale_train = 1
0.00.026.008 I print_info: n_ctx_orig_yarn  = 512
0.00.026.008 I print_info: rope_finetuned   = unknown
0.00.026.008 I print_info: ssm_d_conv       = 0
0.00.026.009 I print_info: ssm_d_inner      = 0
0.00.026.009 I print_info: ssm_d_state      = 0
0.00.026.009 I print_info: ssm_dt_rank      = 0
0.00.026.009 I print_info: ssm_dt_b_c_rms   = 0
0.00.026.010 I print_info: model type       = 33M
0.00.026.011 I print_info: model params     = 33.21 M
0.00.026.011 I print_info: general.name     = Bge Small
0.00.026.011 I print_info: vocab type       = WPM
0.00.026.011 I print_info: n_vocab          = 30522
0.00.026.012 I print_info: n_merges         = 0
0.00.026.012 I print_info: BOS token        = 101 '[CLS]'
0.00.026.014 I print_info: UNK token        = 100 '[UNK]'
0.00.026.014 I print_info: SEP token        = 102 '[SEP]'
0.00.026.014 I print_info: PAD token        = 0 '[PAD]'
0.00.026.014 I print_info: MASK token       = 103 '[MASK]'
0.00.026.015 I print_info: LF token         = 0 '[PAD]'
0.00.026.015 I print_info: max token length = 21
0.00.028.423 I load_tensors: offloading 12 repeating layers to GPU
0.00.028.425 I load_tensors: offloading output layer to GPU
0.00.028.425 I load_tensors: offloaded 13/13 layers to GPU
0.00.028.445 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.028.447 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.028.627 I llama_init_from_model: n_seq_max     = 1
0.00.028.628 I llama_init_from_model: n_ctx         = 512
0.00.028.628 I llama_init_from_model: n_ctx_per_seq = 512
0.00.028.628 I llama_init_from_model: n_batch       = 2048
0.00.028.629 I llama_init_from_model: n_ubatch      = 2048
0.00.028.629 I llama_init_from_model: flash_attn    = 0
0.00.028.629 I llama_init_from_model: freq_base     = 10000.0
0.00.028.629 I llama_init_from_model: freq_scale    = 1
0.00.028.630 I ggml_metal_init: allocating
0.00.028.634 I ggml_metal_init: found device: Apple M4
0.00.028.639 I ggml_metal_init: picking default device: Apple M4
0.00.029.251 I ggml_metal_init: using embedded metal library
0.00.032.438 I ggml_metal_init: GPU name:   Apple M4
0.00.032.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.032.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.032.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.032.442 I ggml_metal_init: simdgroup reduction   = true
0.00.032.442 I ggml_metal_init: simdgroup matrix mul. = true
0.00.032.442 I ggml_metal_init: has residency sets    = true
0.00.032.442 I ggml_metal_init: has bfloat            = true
0.00.032.443 I ggml_metal_init: use bfloat            = true
0.00.032.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.032.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.042.480 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.043.118 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.043.121 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.043.141 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.044.233 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.044.234 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.044.235 I llama_init_from_model: graph nodes  = 429
0.00.044.235 I llama_init_from_model: graph splits = 2
0.00.044.236 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.044.236 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.048.725 I 
0.00.048.756 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.049.334 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.053.725 I llama_perf_context_print:        load time =      35.68 ms
0.00.053.726 I llama_perf_context_print: prompt eval time =       4.26 ms /     9 tokens (    0.47 ms per token,  2113.17 tokens per second)
0.00.053.727 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.053.727 I llama_perf_context_print:       total time =       5.00 ms /    10 tokens
0.00.053.938 I ggml_metal_free: deallocating

real	0m0.233s
user	0m0.040s
sys	0m0.027s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.040 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.744 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.109 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.113 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.114 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.115 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.115 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.116 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.116 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.117 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.117 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.118 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.118 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.118 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.121 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.121 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.122 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.122 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.122 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.123 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.441 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.093 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.094 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.094 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.095 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.095 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.095 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.095 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.096 I llama_model_loader: - type  f32:  124 tensors
0.00.014.096 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.097 I print_info: file format = GGUF V3 (latest)
0.00.014.097 I print_info: file type   = Q8_0
0.00.014.100 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.319 I load: special tokens cache size = 5
0.00.017.496 I load: token to piece cache size = 0.2032 MB
0.00.017.499 I print_info: arch             = bert
0.00.017.500 I print_info: vocab_only       = 0
0.00.017.500 I print_info: n_ctx_train      = 512
0.00.017.500 I print_info: n_embd           = 384
0.00.017.500 I print_info: n_layer          = 12
0.00.017.503 I print_info: n_head           = 12
0.00.017.504 I print_info: n_head_kv        = 12
0.00.017.504 I print_info: n_rot            = 32
0.00.017.504 I print_info: n_swa            = 0
0.00.017.507 I print_info: n_embd_head_k    = 32
0.00.017.507 I print_info: n_embd_head_v    = 32
0.00.017.508 I print_info: n_gqa            = 1
0.00.017.508 I print_info: n_embd_k_gqa     = 384
0.00.017.509 I print_info: n_embd_v_gqa     = 384
0.00.017.509 I print_info: f_norm_eps       = 1.0e-12
0.00.017.510 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.511 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.511 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.515 I print_info: f_logit_scale    = 0.0e+00
0.00.017.516 I print_info: n_ff             = 1536
0.00.017.516 I print_info: n_expert         = 0
0.00.017.517 I print_info: n_expert_used    = 0
0.00.017.517 I print_info: causal attn      = 0
0.00.017.517 I print_info: pooling type     = 2
0.00.017.517 I print_info: rope type        = 2
0.00.017.518 I print_info: rope scaling     = linear
0.00.017.518 I print_info: freq_base_train  = 10000.0
0.00.017.518 I print_info: freq_scale_train = 1
0.00.017.518 I print_info: n_ctx_orig_yarn  = 512
0.00.017.519 I print_info: rope_finetuned   = unknown
0.00.017.519 I print_info: ssm_d_conv       = 0
0.00.017.520 I print_info: ssm_d_inner      = 0
0.00.017.526 I print_info: ssm_d_state      = 0
0.00.017.527 I print_info: ssm_dt_rank      = 0
0.00.017.527 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.527 I print_info: model type       = 33M
0.00.017.528 I print_info: model params     = 33.21 M
0.00.017.529 I print_info: general.name     = Bge Small
0.00.017.529 I print_info: vocab type       = WPM
0.00.017.530 I print_info: n_vocab          = 30522
0.00.017.530 I print_info: n_merges         = 0
0.00.017.530 I print_info: BOS token        = 101 '[CLS]'
0.00.017.530 I print_info: UNK token        = 100 '[UNK]'
0.00.017.531 I print_info: SEP token        = 102 '[SEP]'
0.00.017.531 I print_info: PAD token        = 0 '[PAD]'
0.00.017.531 I print_info: MASK token       = 103 '[MASK]'
0.00.017.531 I print_info: LF token         = 0 '[PAD]'
0.00.017.534 I print_info: max token length = 21
0.00.019.198 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.199 I load_tensors: offloading output layer to GPU
0.00.019.199 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.205 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.206 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.019.353 I llama_init_from_model: n_seq_max     = 1
0.00.019.354 I llama_init_from_model: n_ctx         = 512
0.00.019.355 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.355 I llama_init_from_model: n_batch       = 2048
0.00.019.355 I llama_init_from_model: n_ubatch      = 2048
0.00.019.355 I llama_init_from_model: flash_attn    = 0
0.00.019.356 I llama_init_from_model: freq_base     = 10000.0
0.00.019.356 I llama_init_from_model: freq_scale    = 1
0.00.019.357 I ggml_metal_init: allocating
0.00.019.360 I ggml_metal_init: found device: Apple M4
0.00.019.364 I ggml_metal_init: picking default device: Apple M4
0.00.019.841 I ggml_metal_init: using embedded metal library
0.00.022.237 I ggml_metal_init: GPU name:   Apple M4
0.00.022.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.240 I ggml_metal_init: simdgroup reduction   = true
0.00.022.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.240 I ggml_metal_init: has residency sets    = true
0.00.022.240 I ggml_metal_init: has bfloat            = true
0.00.022.240 I ggml_metal_init: use bfloat            = true
0.00.022.241 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.495 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.099 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.101 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.115 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.109 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.110 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.110 I llama_init_from_model: graph nodes  = 429
0.00.034.111 I llama_init_from_model: graph splits = 2
0.00.034.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.133 I 
0.00.038.160 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.673 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.137 I llama_perf_context_print:        load time =      29.38 ms
0.00.043.138 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2076.60 tokens per second)
0.00.043.139 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.139 I llama_perf_context_print:       total time =       5.01 ms /    10 tokens
0.00.043.350 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.263 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.640 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.715 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.722 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.723 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.726 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.727 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.727 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.728 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.729 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.730 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.731 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.731 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.735 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.736 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.737 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.046.724 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.048.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.509 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.053.511 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.511 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.053.512 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.053.512 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.053.513 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.053.513 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.053.513 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.053.514 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.053.514 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.053.514 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.053.515 I llama_model_loader: - type  f32:   40 tensors
0.00.053.515 I llama_model_loader: - type  f16:   30 tensors
0.00.053.516 I print_info: file format = GGUF V3 (latest)
0.00.053.518 I print_info: file type   = F16
0.00.053.519 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.057.781 W load: empty token at index 5
0.00.062.933 W load: model vocab missing newline token, using special_pad_id instead
0.00.064.477 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.064.515 I load: special tokens cache size = 5
0.00.327.748 I load: token to piece cache size = 1.5060 MB
0.00.327.753 I print_info: arch             = jina-bert-v2
0.00.327.754 I print_info: vocab_only       = 0
0.00.327.754 I print_info: n_ctx_train      = 8192
0.00.327.754 I print_info: n_embd           = 384
0.00.327.754 I print_info: n_layer          = 4
0.00.327.761 I print_info: n_head           = 12
0.00.327.762 I print_info: n_head_kv        = 12
0.00.327.762 I print_info: n_rot            = 32
0.00.327.762 I print_info: n_swa            = 0
0.00.327.762 I print_info: n_embd_head_k    = 32
0.00.327.764 I print_info: n_embd_head_v    = 32
0.00.327.764 I print_info: n_gqa            = 1
0.00.327.765 I print_info: n_embd_k_gqa     = 384
0.00.327.766 I print_info: n_embd_v_gqa     = 384
0.00.327.766 I print_info: f_norm_eps       = 1.0e-12
0.00.327.767 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.327.767 I print_info: f_clamp_kqv      = 0.0e+00
0.00.327.768 I print_info: f_max_alibi_bias = 8.0e+00
0.00.327.768 I print_info: f_logit_scale    = 0.0e+00
0.00.327.771 I print_info: n_ff             = 1536
0.00.327.771 I print_info: n_expert         = 0
0.00.327.771 I print_info: n_expert_used    = 0
0.00.327.771 I print_info: causal attn      = 0
0.00.327.771 I print_info: pooling type     = -1
0.00.327.771 I print_info: rope type        = -1
0.00.327.772 I print_info: rope scaling     = linear
0.00.327.772 I print_info: freq_base_train  = 10000.0
0.00.327.773 I print_info: freq_scale_train = 1
0.00.327.773 I print_info: n_ctx_orig_yarn  = 8192
0.00.327.773 I print_info: rope_finetuned   = unknown
0.00.327.774 I print_info: ssm_d_conv       = 0
0.00.327.774 I print_info: ssm_d_inner      = 0
0.00.327.774 I print_info: ssm_d_state      = 0
0.00.327.775 I print_info: ssm_dt_rank      = 0
0.00.327.775 I print_info: ssm_dt_b_c_rms   = 0
0.00.327.775 I print_info: model type       = 33M
0.00.327.775 I print_info: model params     = 32.90 M
0.00.327.775 I print_info: general.name     = Jina Bert Implementation
0.00.327.776 I print_info: vocab type       = BPE
0.00.327.777 I print_info: n_vocab          = 61056
0.00.327.777 I print_info: n_merges         = 39382
0.00.327.777 I print_info: BOS token        = 0 '<s>'
0.00.327.777 I print_info: EOS token        = 2 '</s>'
0.00.327.777 I print_info: UNK token        = 3 '<unk>'
0.00.327.778 I print_info: SEP token        = 2 '</s>'
0.00.327.778 I print_info: PAD token        = 1 '<pad>'
0.00.327.786 I print_info: MASK token       = 4 '<mask>'
0.00.327.789 I print_info: EOG token        = 2 '</s>'
0.00.327.789 I print_info: max token length = 45
0.00.329.988 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.989 I load_tensors: offloading output layer to GPU
0.00.329.989 I load_tensors: offloaded 5/5 layers to GPU
0.00.330.014 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.330.015 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.330.300 I llama_init_from_model: n_seq_max     = 1
0.00.330.301 I llama_init_from_model: n_ctx         = 8192
0.00.330.301 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.330.301 I llama_init_from_model: n_batch       = 2048
0.00.330.301 I llama_init_from_model: n_ubatch      = 2048
0.00.330.302 I llama_init_from_model: flash_attn    = 0
0.00.330.302 I llama_init_from_model: freq_base     = 10000.0
0.00.330.302 I llama_init_from_model: freq_scale    = 1
0.00.330.303 I ggml_metal_init: allocating
0.00.330.307 I ggml_metal_init: found device: Apple M4
0.00.330.310 I ggml_metal_init: picking default device: Apple M4
0.00.331.180 I ggml_metal_init: using embedded metal library
0.00.333.998 I ggml_metal_init: GPU name:   Apple M4
0.00.334.000 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.334.000 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.334.001 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.334.001 I ggml_metal_init: simdgroup reduction   = true
0.00.334.001 I ggml_metal_init: simdgroup matrix mul. = true
0.00.334.001 I ggml_metal_init: has residency sets    = true
0.00.334.001 I ggml_metal_init: has bfloat            = true
0.00.334.001 I ggml_metal_init: use bfloat            = true
0.00.334.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.334.002 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.343.478 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.346.435 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.346.436 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.346.458 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.353.375 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.353.377 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.353.377 I llama_init_from_model: graph nodes  = 154
0.00.353.377 I llama_init_from_model: graph splits = 2
0.00.353.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.629 I 
0.00.360.658 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.749 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.750 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.753 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.753 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.763 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.763 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.288 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.876 I llama_perf_context_print:        load time =     334.98 ms
0.00.364.877 I llama_perf_context_print: prompt eval time =       3.58 ms /    62 tokens (    0.06 ms per token, 17318.44 tokens per second)
0.00.364.878 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.878 I llama_perf_context_print:       total time =       4.25 ms /    63 tokens
0.00.365.056 I ggml_metal_free: deallocating

real	0m1.081s
user	0m0.332s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.144 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.322 I main: llama backend init
0.00.000.328 I main: load the model and apply lora adapter, if any
0.00.084.025 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.096.352 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.096.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.096.369 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.096.370 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.096.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.096.375 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.096.375 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.096.390 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.096.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.096.392 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.096.392 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.096.393 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.096.393 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.096.395 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.096.399 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.096.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.096.404 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.103.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.105.391 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.112.107 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.112.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.112.121 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.112.122 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.112.123 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.112.125 I llama_model_loader: - type  f32:  194 tensors
0.00.112.125 I llama_model_loader: - type  f16:   98 tensors
0.00.112.127 I print_info: file format = GGUF V3 (latest)
0.00.112.129 I print_info: file type   = all F32 (guessed)
0.00.112.133 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.130.123 I load: special tokens cache size = 25
0.00.140.100 I load: token to piece cache size = 0.2984 MB
0.00.140.106 I print_info: arch             = gptneox
0.00.140.106 I print_info: vocab_only       = 0
0.00.140.106 I print_info: n_ctx_train      = 2048
0.00.140.107 I print_info: n_embd           = 2048
0.00.140.107 I print_info: n_layer          = 24
0.00.140.113 I print_info: n_head           = 16
0.00.140.114 I print_info: n_head_kv        = 16
0.00.140.115 I print_info: n_rot            = 32
0.00.140.118 I print_info: n_swa            = 0
0.00.140.118 I print_info: n_embd_head_k    = 128
0.00.140.118 I print_info: n_embd_head_v    = 128
0.00.140.119 I print_info: n_gqa            = 1
0.00.140.120 I print_info: n_embd_k_gqa     = 2048
0.00.140.121 I print_info: n_embd_v_gqa     = 2048
0.00.140.122 I print_info: f_norm_eps       = 1.0e-05
0.00.140.122 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.140.123 I print_info: f_clamp_kqv      = 0.0e+00
0.00.140.124 I print_info: f_max_alibi_bias = 0.0e+00
0.00.140.125 I print_info: f_logit_scale    = 0.0e+00
0.00.140.126 I print_info: n_ff             = 8192
0.00.140.126 I print_info: n_expert         = 0
0.00.140.126 I print_info: n_expert_used    = 0
0.00.140.126 I print_info: causal attn      = 1
0.00.140.126 I print_info: pooling type     = 0
0.00.140.126 I print_info: rope type        = 2
0.00.140.127 I print_info: rope scaling     = linear
0.00.140.127 I print_info: freq_base_train  = 10000.0
0.00.140.128 I print_info: freq_scale_train = 1
0.00.140.128 I print_info: n_ctx_orig_yarn  = 2048
0.00.140.128 I print_info: rope_finetuned   = unknown
0.00.140.128 I print_info: ssm_d_conv       = 0
0.00.140.128 I print_info: ssm_d_inner      = 0
0.00.140.128 I print_info: ssm_d_state      = 0
0.00.140.129 I print_info: ssm_dt_rank      = 0
0.00.140.129 I print_info: ssm_dt_b_c_rms   = 0
0.00.140.129 I print_info: model type       = 1.4B
0.00.140.129 I print_info: model params     = 1.41 B
0.00.140.130 I print_info: general.name     = 1.4B
0.00.140.135 I print_info: vocab type       = BPE
0.00.140.135 I print_info: n_vocab          = 50304
0.00.140.136 I print_info: n_merges         = 50009
0.00.140.136 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.140.136 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.140.136 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.140.137 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.140.137 I print_info: LF token         = 187 'Ċ'
0.00.140.139 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.140.139 I print_info: max token length = 1024
0.00.186.648 I load_tensors: offloading 24 repeating layers to GPU
0.00.186.652 I load_tensors: offloading output layer to GPU
0.00.186.652 I load_tensors: offloaded 25/25 layers to GPU
0.00.186.676 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.186.678 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.187.158 I llama_init_from_model: n_seq_max     = 1
0.00.187.159 I llama_init_from_model: n_ctx         = 2048
0.00.187.159 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.187.159 I llama_init_from_model: n_batch       = 2048
0.00.187.160 I llama_init_from_model: n_ubatch      = 512
0.00.187.160 I llama_init_from_model: flash_attn    = 0
0.00.187.160 I llama_init_from_model: freq_base     = 10000.0
0.00.187.161 I llama_init_from_model: freq_scale    = 1
0.00.187.161 I ggml_metal_init: allocating
0.00.187.189 I ggml_metal_init: found device: Apple M4
0.00.187.193 I ggml_metal_init: picking default device: Apple M4
0.00.187.791 I ggml_metal_init: using embedded metal library
0.00.196.915 I ggml_metal_init: GPU name:   Apple M4
0.00.196.917 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.196.917 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.196.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.196.918 I ggml_metal_init: simdgroup reduction   = true
0.00.196.918 I ggml_metal_init: simdgroup matrix mul. = true
0.00.196.918 I ggml_metal_init: has residency sets    = true
0.00.196.918 I ggml_metal_init: has bfloat            = true
0.00.196.918 I ggml_metal_init: use bfloat            = true
0.00.196.919 I ggml_metal_init: hasUnifiedMemory      = true
0.00.196.919 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.219.866 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.247.449 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.247.457 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.247.502 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.251.442 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.251.445 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.251.445 I llama_init_from_model: graph nodes  = 967
0.00.251.446 I llama_init_from_model: graph splits = 2
0.00.251.452 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.251.575 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.251.575 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.317.767 I main: llama threadpool init, n_threads = 4
0.00.317.809 I 
0.00.317.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.317.846 I 
0.00.318.024 I sampler seed: 1234
0.00.318.028 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.318.053 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.318.053 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.318.053 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.151.972 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60118.54 tokens per second)
0.02.151.972 I llama_perf_context_print:        load time =     232.69 ms
0.02.151.973 I llama_perf_context_print: prompt eval time =      43.86 ms /     7 tokens (    6.27 ms per token,   159.61 tokens per second)
0.02.151.974 I llama_perf_context_print:        eval time =    1787.29 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.151.974 I llama_perf_context_print:       total time =    1835.25 ms /    70 tokens
0.02.152.192 I ggml_metal_free: deallocating

real	0m2.470s
user	0m0.132s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.768 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.290 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.984 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.989 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.991 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.998 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.998 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.998 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.000 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.003 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.004 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.004 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.005 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.006 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.007 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.010 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.011 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.012 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.696 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.122 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.124 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.125 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.125 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.125 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.126 I llama_model_loader: - type  f32:  194 tensors
0.00.058.127 I llama_model_loader: - type  f16:   98 tensors
0.00.058.127 I print_info: file format = GGUF V3 (latest)
0.00.058.128 I print_info: file type   = all F32 (guessed)
0.00.058.129 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.811 I load: special tokens cache size = 25
0.00.077.401 I load: token to piece cache size = 0.2984 MB
0.00.077.405 I print_info: arch             = gptneox
0.00.077.405 I print_info: vocab_only       = 0
0.00.077.405 I print_info: n_ctx_train      = 2048
0.00.077.405 I print_info: n_embd           = 2048
0.00.077.405 I print_info: n_layer          = 24
0.00.077.408 I print_info: n_head           = 16
0.00.077.409 I print_info: n_head_kv        = 16
0.00.077.409 I print_info: n_rot            = 32
0.00.077.409 I print_info: n_swa            = 0
0.00.077.410 I print_info: n_embd_head_k    = 128
0.00.077.410 I print_info: n_embd_head_v    = 128
0.00.077.410 I print_info: n_gqa            = 1
0.00.077.411 I print_info: n_embd_k_gqa     = 2048
0.00.077.414 I print_info: n_embd_v_gqa     = 2048
0.00.077.414 I print_info: f_norm_eps       = 1.0e-05
0.00.077.415 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.415 I print_info: f_logit_scale    = 0.0e+00
0.00.077.416 I print_info: n_ff             = 8192
0.00.077.421 I print_info: n_expert         = 0
0.00.077.421 I print_info: n_expert_used    = 0
0.00.077.421 I print_info: causal attn      = 1
0.00.077.422 I print_info: pooling type     = 0
0.00.077.422 I print_info: rope type        = 2
0.00.077.422 I print_info: rope scaling     = linear
0.00.077.423 I print_info: freq_base_train  = 10000.0
0.00.077.426 I print_info: freq_scale_train = 1
0.00.077.426 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.426 I print_info: rope_finetuned   = unknown
0.00.077.426 I print_info: ssm_d_conv       = 0
0.00.077.426 I print_info: ssm_d_inner      = 0
0.00.077.427 I print_info: ssm_d_state      = 0
0.00.077.427 I print_info: ssm_dt_rank      = 0
0.00.077.427 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.428 I print_info: model type       = 1.4B
0.00.077.429 I print_info: model params     = 1.41 B
0.00.077.429 I print_info: general.name     = 1.4B
0.00.077.429 I print_info: vocab type       = BPE
0.00.077.429 I print_info: n_vocab          = 50304
0.00.077.431 I print_info: n_merges         = 50009
0.00.077.431 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.431 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.431 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.431 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.432 I print_info: LF token         = 187 'Ċ'
0.00.077.434 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.434 I print_info: max token length = 1024
0.01.431.141 I load_tensors: offloading 24 repeating layers to GPU
0.01.431.147 I load_tensors: offloading output layer to GPU
0.01.431.147 I load_tensors: offloaded 25/25 layers to GPU
0.01.431.186 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.431.188 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.431.847 I llama_init_from_model: n_seq_max     = 1
0.01.431.849 I llama_init_from_model: n_ctx         = 128
0.01.431.850 I llama_init_from_model: n_ctx_per_seq = 128
0.01.431.850 I llama_init_from_model: n_batch       = 128
0.01.431.850 I llama_init_from_model: n_ubatch      = 128
0.01.431.850 I llama_init_from_model: flash_attn    = 0
0.01.431.851 I llama_init_from_model: freq_base     = 10000.0
0.01.431.851 I llama_init_from_model: freq_scale    = 1
0.01.431.851 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.431.856 I ggml_metal_init: allocating
0.01.431.935 I ggml_metal_init: found device: Apple M4
0.01.431.942 I ggml_metal_init: picking default device: Apple M4
0.01.433.077 I ggml_metal_init: using embedded metal library
0.01.437.233 I ggml_metal_init: GPU name:   Apple M4
0.01.437.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.437.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.437.237 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.437.237 I ggml_metal_init: simdgroup reduction   = true
0.01.437.237 I ggml_metal_init: simdgroup matrix mul. = true
0.01.437.238 I ggml_metal_init: has residency sets    = true
0.01.437.238 I ggml_metal_init: has bfloat            = true
0.01.437.238 I ggml_metal_init: use bfloat            = true
0.01.437.239 I ggml_metal_init: hasUnifiedMemory      = true
0.01.437.240 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.448.682 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.450.472 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.450.475 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.450.504 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.452.193 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.452.195 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.452.195 I llama_init_from_model: graph nodes  = 967
0.01.452.195 I llama_init_from_model: graph splits = 2
0.01.452.197 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.452.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.486.942 I 
0.01.486.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.486.985 I perplexity: tokenizing the input ..
0.01.490.991 I perplexity: tokenization took 4.004 ms
0.01.490.995 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.621.079 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.622.431 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.622.462 I llama_perf_context_print:        load time =    1462.64 ms
0.01.622.462 I llama_perf_context_print: prompt eval time =     129.85 ms /   128 tokens (    1.01 ms per token,   985.75 tokens per second)
0.01.622.463 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.622.463 I llama_perf_context_print:       total time =     135.52 ms /   129 tokens
0.01.622.814 I ggml_metal_free: deallocating

real	0m1.818s
user	0m0.098s
sys	0m0.245s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.514 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.525 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.526 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.526 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.527 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.527 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.528 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.532 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.576 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.932 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.933 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.934 I llama_model_loader: - type  f32:  194 tensors
0.00.036.934 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.935 I print_info: file format = GGUF V3 (latest)
0.00.036.936 I print_info: file type   = Q8_0
0.00.036.937 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.928 I load: special tokens cache size = 25
0.00.051.919 I load: token to piece cache size = 0.2984 MB
0.00.051.923 I print_info: arch             = gptneox
0.00.051.923 I print_info: vocab_only       = 0
0.00.051.923 I print_info: n_ctx_train      = 2048
0.00.051.924 I print_info: n_embd           = 2048
0.00.051.924 I print_info: n_layer          = 24
0.00.051.930 I print_info: n_head           = 16
0.00.051.930 I print_info: n_head_kv        = 16
0.00.051.933 I print_info: n_rot            = 32
0.00.051.934 I print_info: n_swa            = 0
0.00.051.934 I print_info: n_embd_head_k    = 128
0.00.051.936 I print_info: n_embd_head_v    = 128
0.00.051.937 I print_info: n_gqa            = 1
0.00.051.938 I print_info: n_embd_k_gqa     = 2048
0.00.051.939 I print_info: n_embd_v_gqa     = 2048
0.00.051.940 I print_info: f_norm_eps       = 1.0e-05
0.00.051.941 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.941 I print_info: f_logit_scale    = 0.0e+00
0.00.051.942 I print_info: n_ff             = 8192
0.00.051.942 I print_info: n_expert         = 0
0.00.051.942 I print_info: n_expert_used    = 0
0.00.051.943 I print_info: causal attn      = 1
0.00.051.943 I print_info: pooling type     = 0
0.00.051.943 I print_info: rope type        = 2
0.00.051.943 I print_info: rope scaling     = linear
0.00.051.944 I print_info: freq_base_train  = 10000.0
0.00.051.944 I print_info: freq_scale_train = 1
0.00.051.945 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.945 I print_info: rope_finetuned   = unknown
0.00.051.945 I print_info: ssm_d_conv       = 0
0.00.051.946 I print_info: ssm_d_inner      = 0
0.00.051.946 I print_info: ssm_d_state      = 0
0.00.051.946 I print_info: ssm_dt_rank      = 0
0.00.051.946 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.946 I print_info: model type       = 1.4B
0.00.051.946 I print_info: model params     = 1.41 B
0.00.051.947 I print_info: general.name     = 1.4B
0.00.051.947 I print_info: vocab type       = BPE
0.00.051.948 I print_info: n_vocab          = 50304
0.00.051.949 I print_info: n_merges         = 50009
0.00.051.949 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.949 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.949 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.949 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.950 I print_info: LF token         = 187 'Ċ'
0.00.051.950 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.950 I print_info: max token length = 1024
0.01.107.607 I load_tensors: offloading 24 repeating layers to GPU
0.01.107.612 I load_tensors: offloading output layer to GPU
0.01.107.613 I load_tensors: offloaded 25/25 layers to GPU
0.01.107.636 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.107.637 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.108.327 I llama_init_from_model: n_seq_max     = 1
0.01.108.329 I llama_init_from_model: n_ctx         = 2048
0.01.108.329 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.108.330 I llama_init_from_model: n_batch       = 2048
0.01.108.330 I llama_init_from_model: n_ubatch      = 512
0.01.108.330 I llama_init_from_model: flash_attn    = 0
0.01.108.331 I llama_init_from_model: freq_base     = 10000.0
0.01.108.331 I llama_init_from_model: freq_scale    = 1
0.01.108.333 I ggml_metal_init: allocating
0.01.108.341 I ggml_metal_init: found device: Apple M4
0.01.108.349 I ggml_metal_init: picking default device: Apple M4
0.01.109.600 I ggml_metal_init: using embedded metal library
0.01.115.181 I ggml_metal_init: GPU name:   Apple M4
0.01.115.184 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.115.185 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.115.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.115.186 I ggml_metal_init: simdgroup reduction   = true
0.01.115.186 I ggml_metal_init: simdgroup matrix mul. = true
0.01.115.186 I ggml_metal_init: has residency sets    = true
0.01.115.187 I ggml_metal_init: has bfloat            = true
0.01.115.187 I ggml_metal_init: use bfloat            = true
0.01.115.188 I ggml_metal_init: hasUnifiedMemory      = true
0.01.115.189 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.131.955 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.185.953 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.185.960 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.185.994 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.189.919 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.189.921 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.189.921 I llama_init_from_model: graph nodes  = 967
0.01.189.922 I llama_init_from_model: graph splits = 2
0.01.189.927 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.190.053 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.190.053 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.244.656 I main: llama threadpool init, n_threads = 4
0.01.244.701 I 
0.01.244.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.244.726 I 
0.01.244.907 I sampler seed: 1234
0.01.244.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.244.924 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.244.924 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.244.924 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.340.090 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.02.340.091 I llama_perf_context_print:        load time =    1233.83 ms
0.02.340.091 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.12 tokens per second)
0.02.340.092 I llama_perf_context_print:        eval time =    1043.36 ms /    63 runs   (   16.56 ms per token,    60.38 tokens per second)
0.02.340.092 I llama_perf_context_print:       total time =    1096.35 ms /    70 tokens
0.02.340.311 I ggml_metal_free: deallocating

real	0m2.360s
user	0m0.109s
sys	0m0.281s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.331 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.405 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.289 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.294 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.296 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.297 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.297 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.297 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.298 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.299 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.300 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.300 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.300 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.124 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.133 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.063 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.065 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.065 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.066 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.066 I llama_model_loader: - type  f32:  194 tensors
0.00.026.067 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.067 I print_info: file format = GGUF V3 (latest)
0.00.026.068 I print_info: file type   = Q8_0
0.00.026.069 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.269 I load: special tokens cache size = 25
0.00.040.399 I load: token to piece cache size = 0.2984 MB
0.00.040.403 I print_info: arch             = gptneox
0.00.040.403 I print_info: vocab_only       = 0
0.00.040.404 I print_info: n_ctx_train      = 2048
0.00.040.404 I print_info: n_embd           = 2048
0.00.040.404 I print_info: n_layer          = 24
0.00.040.407 I print_info: n_head           = 16
0.00.040.408 I print_info: n_head_kv        = 16
0.00.040.408 I print_info: n_rot            = 32
0.00.040.409 I print_info: n_swa            = 0
0.00.040.409 I print_info: n_embd_head_k    = 128
0.00.040.409 I print_info: n_embd_head_v    = 128
0.00.040.410 I print_info: n_gqa            = 1
0.00.040.410 I print_info: n_embd_k_gqa     = 2048
0.00.040.411 I print_info: n_embd_v_gqa     = 2048
0.00.040.412 I print_info: f_norm_eps       = 1.0e-05
0.00.040.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.414 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.414 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.414 I print_info: f_logit_scale    = 0.0e+00
0.00.040.415 I print_info: n_ff             = 8192
0.00.040.415 I print_info: n_expert         = 0
0.00.040.415 I print_info: n_expert_used    = 0
0.00.040.416 I print_info: causal attn      = 1
0.00.040.416 I print_info: pooling type     = 0
0.00.040.416 I print_info: rope type        = 2
0.00.040.416 I print_info: rope scaling     = linear
0.00.040.417 I print_info: freq_base_train  = 10000.0
0.00.040.418 I print_info: freq_scale_train = 1
0.00.040.418 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.418 I print_info: rope_finetuned   = unknown
0.00.040.418 I print_info: ssm_d_conv       = 0
0.00.040.418 I print_info: ssm_d_inner      = 0
0.00.040.418 I print_info: ssm_d_state      = 0
0.00.040.418 I print_info: ssm_dt_rank      = 0
0.00.040.419 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.419 I print_info: model type       = 1.4B
0.00.040.419 I print_info: model params     = 1.41 B
0.00.040.419 I print_info: general.name     = 1.4B
0.00.040.420 I print_info: vocab type       = BPE
0.00.040.420 I print_info: n_vocab          = 50304
0.00.040.420 I print_info: n_merges         = 50009
0.00.040.420 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.420 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.420 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.421 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.421 I print_info: LF token         = 187 'Ċ'
0.00.040.421 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.421 I print_info: max token length = 1024
0.00.969.114 I load_tensors: offloading 24 repeating layers to GPU
0.00.969.121 I load_tensors: offloading output layer to GPU
0.00.969.121 I load_tensors: offloaded 25/25 layers to GPU
0.00.969.142 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.969.143 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.969.895 I llama_init_from_model: n_seq_max     = 1
0.00.969.902 I llama_init_from_model: n_ctx         = 128
0.00.969.902 I llama_init_from_model: n_ctx_per_seq = 128
0.00.969.902 I llama_init_from_model: n_batch       = 128
0.00.969.906 I llama_init_from_model: n_ubatch      = 128
0.00.969.907 I llama_init_from_model: flash_attn    = 0
0.00.969.908 I llama_init_from_model: freq_base     = 10000.0
0.00.969.909 I llama_init_from_model: freq_scale    = 1
0.00.969.909 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.969.911 I ggml_metal_init: allocating
0.00.969.962 I ggml_metal_init: found device: Apple M4
0.00.969.976 I ggml_metal_init: picking default device: Apple M4
0.00.971.148 I ggml_metal_init: using embedded metal library
0.00.978.141 I ggml_metal_init: GPU name:   Apple M4
0.00.978.149 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.978.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.978.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.978.151 I ggml_metal_init: simdgroup reduction   = true
0.00.978.152 I ggml_metal_init: simdgroup matrix mul. = true
0.00.978.152 I ggml_metal_init: has residency sets    = true
0.00.978.152 I ggml_metal_init: has bfloat            = true
0.00.978.152 I ggml_metal_init: use bfloat            = true
0.00.978.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.978.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.995.892 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.997.608 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.997.611 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.997.638 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.999.260 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.999.261 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.999.261 I llama_init_from_model: graph nodes  = 967
0.00.999.261 I llama_init_from_model: graph splits = 2
0.00.999.263 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.999.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.023.689 I 
0.01.023.727 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.023.730 I perplexity: tokenizing the input ..
0.01.027.449 I perplexity: tokenization took 3.717 ms
0.01.027.452 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.165.025 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.166.455 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.166.472 I llama_perf_context_print:        load time =    1013.28 ms
0.01.166.472 I llama_perf_context_print: prompt eval time =     137.32 ms /   128 tokens (    1.07 ms per token,   932.15 tokens per second)
0.01.166.473 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.166.473 I llama_perf_context_print:       total time =     142.78 ms /   129 tokens
0.01.166.823 I ggml_metal_free: deallocating

real	0m1.183s
user	0m0.071s
sys	0m0.181s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.015.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.162 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.177 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.902 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.100 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.661 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.663 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.663 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.663 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.664 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.664 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.665 I llama_model_loader: - type  f32:  194 tensors
0.00.040.665 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.665 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.666 I print_info: file format = GGUF V3 (latest)
0.00.040.667 I print_info: file type   = Q4_0
0.00.040.668 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.938 I load: special tokens cache size = 25
0.00.059.544 I load: token to piece cache size = 0.2984 MB
0.00.059.549 I print_info: arch             = gptneox
0.00.059.549 I print_info: vocab_only       = 0
0.00.059.549 I print_info: n_ctx_train      = 2048
0.00.059.550 I print_info: n_embd           = 2048
0.00.059.550 I print_info: n_layer          = 24
0.00.059.554 I print_info: n_head           = 16
0.00.059.555 I print_info: n_head_kv        = 16
0.00.059.556 I print_info: n_rot            = 32
0.00.059.556 I print_info: n_swa            = 0
0.00.059.556 I print_info: n_embd_head_k    = 128
0.00.059.556 I print_info: n_embd_head_v    = 128
0.00.059.557 I print_info: n_gqa            = 1
0.00.059.558 I print_info: n_embd_k_gqa     = 2048
0.00.059.559 I print_info: n_embd_v_gqa     = 2048
0.00.059.560 I print_info: f_norm_eps       = 1.0e-05
0.00.059.560 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.564 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.564 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.564 I print_info: f_logit_scale    = 0.0e+00
0.00.059.565 I print_info: n_ff             = 8192
0.00.059.565 I print_info: n_expert         = 0
0.00.059.565 I print_info: n_expert_used    = 0
0.00.059.565 I print_info: causal attn      = 1
0.00.059.566 I print_info: pooling type     = 0
0.00.059.566 I print_info: rope type        = 2
0.00.059.566 I print_info: rope scaling     = linear
0.00.059.567 I print_info: freq_base_train  = 10000.0
0.00.059.567 I print_info: freq_scale_train = 1
0.00.059.567 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.568 I print_info: rope_finetuned   = unknown
0.00.059.568 I print_info: ssm_d_conv       = 0
0.00.059.568 I print_info: ssm_d_inner      = 0
0.00.059.568 I print_info: ssm_d_state      = 0
0.00.059.568 I print_info: ssm_dt_rank      = 0
0.00.059.568 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.569 I print_info: model type       = 1.4B
0.00.059.569 I print_info: model params     = 1.41 B
0.00.059.569 I print_info: general.name     = 1.4B
0.00.059.575 I print_info: vocab type       = BPE
0.00.059.575 I print_info: n_vocab          = 50304
0.00.059.575 I print_info: n_merges         = 50009
0.00.059.575 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.576 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.577 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.578 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.578 I print_info: LF token         = 187 'Ċ'
0.00.059.579 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.579 I print_info: max token length = 1024
0.00.623.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.525 I load_tensors: offloading output layer to GPU
0.00.623.526 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.562 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.623.563 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.624.840 I llama_init_from_model: n_seq_max     = 1
0.00.624.847 I llama_init_from_model: n_ctx         = 2048
0.00.624.847 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.624.848 I llama_init_from_model: n_batch       = 2048
0.00.624.848 I llama_init_from_model: n_ubatch      = 512
0.00.624.848 I llama_init_from_model: flash_attn    = 0
0.00.624.851 I llama_init_from_model: freq_base     = 10000.0
0.00.624.851 I llama_init_from_model: freq_scale    = 1
0.00.624.853 I ggml_metal_init: allocating
0.00.624.925 I ggml_metal_init: found device: Apple M4
0.00.624.939 I ggml_metal_init: picking default device: Apple M4
0.00.626.706 I ggml_metal_init: using embedded metal library
0.00.633.465 I ggml_metal_init: GPU name:   Apple M4
0.00.633.469 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.633.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.633.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.633.472 I ggml_metal_init: simdgroup reduction   = true
0.00.633.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.633.472 I ggml_metal_init: has residency sets    = true
0.00.633.472 I ggml_metal_init: has bfloat            = true
0.00.633.473 I ggml_metal_init: use bfloat            = true
0.00.633.474 I ggml_metal_init: hasUnifiedMemory      = true
0.00.633.475 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.540 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.005 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.705.011 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.705.044 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.709.337 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.709.338 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.709.339 I llama_init_from_model: graph nodes  = 967
0.00.709.339 I llama_init_from_model: graph splits = 2
0.00.709.345 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.709.478 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.538 I main: llama threadpool init, n_threads = 4
0.00.763.591 I 
0.00.763.612 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.615 I 
0.00.763.767 I sampler seed: 1234
0.00.763.771 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.817 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.820 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.820 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.454.259 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.01.454.259 I llama_perf_context_print:        load time =     746.74 ms
0.01.454.260 I llama_perf_context_print: prompt eval time =      49.29 ms /     7 tokens (    7.04 ms per token,   142.01 tokens per second)
0.01.454.261 I llama_perf_context_print:        eval time =     638.19 ms /    63 runs   (   10.13 ms per token,    98.72 tokens per second)
0.01.454.261 I llama_perf_context_print:       total time =     691.63 ms /    70 tokens
0.01.454.494 I ggml_metal_free: deallocating

real	0m1.478s
user	0m0.118s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.228 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.732 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.053 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.061 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.063 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.064 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.065 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.069 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.070 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.127 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.927 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.929 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.930 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.930 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.931 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.931 I llama_model_loader: - type  f32:  194 tensors
0.00.026.932 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.932 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.933 I print_info: file format = GGUF V3 (latest)
0.00.026.933 I print_info: file type   = Q4_0
0.00.026.935 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.231 I load: special tokens cache size = 25
0.00.041.336 I load: token to piece cache size = 0.2984 MB
0.00.041.341 I print_info: arch             = gptneox
0.00.041.341 I print_info: vocab_only       = 0
0.00.041.341 I print_info: n_ctx_train      = 2048
0.00.041.341 I print_info: n_embd           = 2048
0.00.041.342 I print_info: n_layer          = 24
0.00.041.346 I print_info: n_head           = 16
0.00.041.347 I print_info: n_head_kv        = 16
0.00.041.347 I print_info: n_rot            = 32
0.00.041.347 I print_info: n_swa            = 0
0.00.041.348 I print_info: n_embd_head_k    = 128
0.00.041.349 I print_info: n_embd_head_v    = 128
0.00.041.350 I print_info: n_gqa            = 1
0.00.041.351 I print_info: n_embd_k_gqa     = 2048
0.00.041.351 I print_info: n_embd_v_gqa     = 2048
0.00.041.352 I print_info: f_norm_eps       = 1.0e-05
0.00.041.352 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.353 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.353 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.353 I print_info: f_logit_scale    = 0.0e+00
0.00.041.354 I print_info: n_ff             = 8192
0.00.041.354 I print_info: n_expert         = 0
0.00.041.354 I print_info: n_expert_used    = 0
0.00.041.354 I print_info: causal attn      = 1
0.00.041.354 I print_info: pooling type     = 0
0.00.041.354 I print_info: rope type        = 2
0.00.041.355 I print_info: rope scaling     = linear
0.00.041.355 I print_info: freq_base_train  = 10000.0
0.00.041.355 I print_info: freq_scale_train = 1
0.00.041.355 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.355 I print_info: rope_finetuned   = unknown
0.00.041.356 I print_info: ssm_d_conv       = 0
0.00.041.356 I print_info: ssm_d_inner      = 0
0.00.041.356 I print_info: ssm_d_state      = 0
0.00.041.356 I print_info: ssm_dt_rank      = 0
0.00.041.356 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.356 I print_info: model type       = 1.4B
0.00.041.356 I print_info: model params     = 1.41 B
0.00.041.357 I print_info: general.name     = 1.4B
0.00.041.357 I print_info: vocab type       = BPE
0.00.041.357 I print_info: n_vocab          = 50304
0.00.041.357 I print_info: n_merges         = 50009
0.00.041.358 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.358 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.358 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.358 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.358 I print_info: LF token         = 187 'Ċ'
0.00.041.359 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.359 I print_info: max token length = 1024
0.00.595.563 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.585 I load_tensors: offloading output layer to GPU
0.00.595.586 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.621 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.595.623 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.596.813 I llama_init_from_model: n_seq_max     = 1
0.00.596.822 I llama_init_from_model: n_ctx         = 128
0.00.596.822 I llama_init_from_model: n_ctx_per_seq = 128
0.00.596.823 I llama_init_from_model: n_batch       = 128
0.00.596.823 I llama_init_from_model: n_ubatch      = 128
0.00.596.824 I llama_init_from_model: flash_attn    = 0
0.00.596.826 I llama_init_from_model: freq_base     = 10000.0
0.00.596.827 I llama_init_from_model: freq_scale    = 1
0.00.596.827 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.829 I ggml_metal_init: allocating
0.00.596.928 I ggml_metal_init: found device: Apple M4
0.00.596.945 I ggml_metal_init: picking default device: Apple M4
0.00.598.787 I ggml_metal_init: using embedded metal library
0.00.604.726 I ggml_metal_init: GPU name:   Apple M4
0.00.604.736 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.737 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.738 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.739 I ggml_metal_init: simdgroup reduction   = true
0.00.604.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.740 I ggml_metal_init: has residency sets    = true
0.00.604.740 I ggml_metal_init: has bfloat            = true
0.00.604.740 I ggml_metal_init: use bfloat            = true
0.00.604.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.300 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.218 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.628.223 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.628.268 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.612 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.631.614 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.631.615 I llama_init_from_model: graph nodes  = 967
0.00.631.615 I llama_init_from_model: graph splits = 2
0.00.631.619 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.631.619 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.993 I 
0.00.658.079 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.088 I perplexity: tokenizing the input ..
0.00.664.934 I perplexity: tokenization took 6.842 ms
0.00.664.943 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.802.233 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.803.545 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.803.563 I llama_perf_context_print:        load time =     647.25 ms
0.00.803.564 I llama_perf_context_print: prompt eval time =     136.36 ms /   128 tokens (    1.07 ms per token,   938.67 tokens per second)
0.00.803.565 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.566 I llama_perf_context_print:       total time =     145.58 ms /   129 tokens
0.00.804.014 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.081s
sys	0m0.115s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.981 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.145 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.150 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.160 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.162 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.163 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.007 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.989 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.991 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.991 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.991 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.992 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.992 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.992 I llama_model_loader: - type  f32:  194 tensors
0.00.035.993 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.993 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.993 I print_info: file format = GGUF V3 (latest)
0.00.035.994 I print_info: file type   = Q4_1
0.00.035.995 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.851 I load: special tokens cache size = 25
0.00.051.417 I load: token to piece cache size = 0.2984 MB
0.00.051.420 I print_info: arch             = gptneox
0.00.051.420 I print_info: vocab_only       = 0
0.00.051.420 I print_info: n_ctx_train      = 2048
0.00.051.420 I print_info: n_embd           = 2048
0.00.051.421 I print_info: n_layer          = 24
0.00.051.423 I print_info: n_head           = 16
0.00.051.424 I print_info: n_head_kv        = 16
0.00.051.424 I print_info: n_rot            = 32
0.00.051.426 I print_info: n_swa            = 0
0.00.051.426 I print_info: n_embd_head_k    = 128
0.00.051.426 I print_info: n_embd_head_v    = 128
0.00.051.427 I print_info: n_gqa            = 1
0.00.051.427 I print_info: n_embd_k_gqa     = 2048
0.00.051.428 I print_info: n_embd_v_gqa     = 2048
0.00.051.428 I print_info: f_norm_eps       = 1.0e-05
0.00.051.429 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.429 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.429 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.429 I print_info: f_logit_scale    = 0.0e+00
0.00.051.430 I print_info: n_ff             = 8192
0.00.051.430 I print_info: n_expert         = 0
0.00.051.430 I print_info: n_expert_used    = 0
0.00.051.430 I print_info: causal attn      = 1
0.00.051.430 I print_info: pooling type     = 0
0.00.051.430 I print_info: rope type        = 2
0.00.051.430 I print_info: rope scaling     = linear
0.00.051.434 I print_info: freq_base_train  = 10000.0
0.00.051.434 I print_info: freq_scale_train = 1
0.00.051.435 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.435 I print_info: rope_finetuned   = unknown
0.00.051.435 I print_info: ssm_d_conv       = 0
0.00.051.435 I print_info: ssm_d_inner      = 0
0.00.051.435 I print_info: ssm_d_state      = 0
0.00.051.435 I print_info: ssm_dt_rank      = 0
0.00.051.436 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.436 I print_info: model type       = 1.4B
0.00.051.438 I print_info: model params     = 1.41 B
0.00.051.438 I print_info: general.name     = 1.4B
0.00.051.439 I print_info: vocab type       = BPE
0.00.051.439 I print_info: n_vocab          = 50304
0.00.051.439 I print_info: n_merges         = 50009
0.00.051.440 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.440 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.440 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.440 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.440 I print_info: LF token         = 187 'Ċ'
0.00.051.441 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.441 I print_info: max token length = 1024
0.00.727.560 I load_tensors: offloading 24 repeating layers to GPU
0.00.727.576 I load_tensors: offloading output layer to GPU
0.00.727.576 I load_tensors: offloaded 25/25 layers to GPU
0.00.727.608 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.727.610 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.728.911 I llama_init_from_model: n_seq_max     = 1
0.00.728.915 I llama_init_from_model: n_ctx         = 2048
0.00.728.916 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.728.916 I llama_init_from_model: n_batch       = 2048
0.00.728.917 I llama_init_from_model: n_ubatch      = 512
0.00.728.917 I llama_init_from_model: flash_attn    = 0
0.00.728.919 I llama_init_from_model: freq_base     = 10000.0
0.00.728.919 I llama_init_from_model: freq_scale    = 1
0.00.728.922 I ggml_metal_init: allocating
0.00.729.000 I ggml_metal_init: found device: Apple M4
0.00.729.014 I ggml_metal_init: picking default device: Apple M4
0.00.730.783 I ggml_metal_init: using embedded metal library
0.00.737.176 I ggml_metal_init: GPU name:   Apple M4
0.00.737.181 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.737.182 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.737.183 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.737.184 I ggml_metal_init: simdgroup reduction   = true
0.00.737.184 I ggml_metal_init: simdgroup matrix mul. = true
0.00.737.184 I ggml_metal_init: has residency sets    = true
0.00.737.184 I ggml_metal_init: has bfloat            = true
0.00.737.185 I ggml_metal_init: use bfloat            = true
0.00.737.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.737.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.755.700 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.814.320 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.814.326 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.814.362 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.818.688 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.818.691 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.818.691 I llama_init_from_model: graph nodes  = 967
0.00.818.692 I llama_init_from_model: graph splits = 2
0.00.818.698 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.818.831 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.818.831 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.873.223 I main: llama threadpool init, n_threads = 4
0.00.873.269 I 
0.00.873.292 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.873.294 I 
0.00.873.444 I sampler seed: 1234
0.00.873.449 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.873.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.873.505 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.873.505 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.612.934 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.612.935 I llama_perf_context_print:        load time =     863.34 ms
0.01.612.936 I llama_perf_context_print: prompt eval time =      49.17 ms /     7 tokens (    7.02 ms per token,   142.37 tokens per second)
0.01.612.936 I llama_perf_context_print:        eval time =     687.51 ms /    63 runs   (   10.91 ms per token,    91.63 tokens per second)
0.01.612.937 I llama_perf_context_print:       total time =     740.61 ms /    70 tokens
0.01.613.184 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.112s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.195 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.675 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.690 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.693 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.694 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.694 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.694 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.695 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.696 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.697 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.697 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.699 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.700 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.700 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.251 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.313 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.314 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.315 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.315 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.315 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.316 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.316 I llama_model_loader: - type  f32:  194 tensors
0.00.026.317 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.317 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.318 I print_info: file format = GGUF V3 (latest)
0.00.026.322 I print_info: file type   = Q4_1
0.00.026.324 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.996 I load: special tokens cache size = 25
0.00.039.701 I load: token to piece cache size = 0.2984 MB
0.00.039.703 I print_info: arch             = gptneox
0.00.039.704 I print_info: vocab_only       = 0
0.00.039.704 I print_info: n_ctx_train      = 2048
0.00.039.704 I print_info: n_embd           = 2048
0.00.039.704 I print_info: n_layer          = 24
0.00.039.707 I print_info: n_head           = 16
0.00.039.708 I print_info: n_head_kv        = 16
0.00.039.708 I print_info: n_rot            = 32
0.00.039.711 I print_info: n_swa            = 0
0.00.039.711 I print_info: n_embd_head_k    = 128
0.00.039.711 I print_info: n_embd_head_v    = 128
0.00.039.712 I print_info: n_gqa            = 1
0.00.039.713 I print_info: n_embd_k_gqa     = 2048
0.00.039.713 I print_info: n_embd_v_gqa     = 2048
0.00.039.714 I print_info: f_norm_eps       = 1.0e-05
0.00.039.715 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.715 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.715 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.719 I print_info: f_logit_scale    = 0.0e+00
0.00.039.720 I print_info: n_ff             = 8192
0.00.039.720 I print_info: n_expert         = 0
0.00.039.721 I print_info: n_expert_used    = 0
0.00.039.721 I print_info: causal attn      = 1
0.00.039.722 I print_info: pooling type     = 0
0.00.039.722 I print_info: rope type        = 2
0.00.039.722 I print_info: rope scaling     = linear
0.00.039.722 I print_info: freq_base_train  = 10000.0
0.00.039.723 I print_info: freq_scale_train = 1
0.00.039.723 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.723 I print_info: rope_finetuned   = unknown
0.00.039.723 I print_info: ssm_d_conv       = 0
0.00.039.723 I print_info: ssm_d_inner      = 0
0.00.039.724 I print_info: ssm_d_state      = 0
0.00.039.724 I print_info: ssm_dt_rank      = 0
0.00.039.724 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.724 I print_info: model type       = 1.4B
0.00.039.724 I print_info: model params     = 1.41 B
0.00.039.725 I print_info: general.name     = 1.4B
0.00.039.725 I print_info: vocab type       = BPE
0.00.039.725 I print_info: n_vocab          = 50304
0.00.039.725 I print_info: n_merges         = 50009
0.00.039.726 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.729 I print_info: LF token         = 187 'Ċ'
0.00.039.729 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.729 I print_info: max token length = 1024
0.00.651.902 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.921 I load_tensors: offloading output layer to GPU
0.00.651.921 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.957 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.651.958 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.653.237 I llama_init_from_model: n_seq_max     = 1
0.00.653.243 I llama_init_from_model: n_ctx         = 128
0.00.653.244 I llama_init_from_model: n_ctx_per_seq = 128
0.00.653.249 I llama_init_from_model: n_batch       = 128
0.00.653.249 I llama_init_from_model: n_ubatch      = 128
0.00.653.250 I llama_init_from_model: flash_attn    = 0
0.00.653.252 I llama_init_from_model: freq_base     = 10000.0
0.00.653.253 I llama_init_from_model: freq_scale    = 1
0.00.653.253 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.653.258 I ggml_metal_init: allocating
0.00.653.361 I ggml_metal_init: found device: Apple M4
0.00.653.376 I ggml_metal_init: picking default device: Apple M4
0.00.655.232 I ggml_metal_init: using embedded metal library
0.00.660.983 I ggml_metal_init: GPU name:   Apple M4
0.00.660.989 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.660.990 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.660.991 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.660.992 I ggml_metal_init: simdgroup reduction   = true
0.00.660.993 I ggml_metal_init: simdgroup matrix mul. = true
0.00.660.993 I ggml_metal_init: has residency sets    = true
0.00.660.993 I ggml_metal_init: has bfloat            = true
0.00.660.993 I ggml_metal_init: use bfloat            = true
0.00.660.995 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.230 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.682 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.683.686 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.683.725 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.792 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.686.794 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.686.794 I llama_init_from_model: graph nodes  = 967
0.00.686.795 I llama_init_from_model: graph splits = 2
0.00.686.798 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.686.798 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.276 I 
0.00.710.332 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.337 I perplexity: tokenizing the input ..
0.00.716.772 I perplexity: tokenization took 6.432 ms
0.00.716.779 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.098 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.850.434 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.850.459 I llama_perf_context_print:        load time =     701.07 ms
0.00.850.460 I llama_perf_context_print: prompt eval time =     131.70 ms /   128 tokens (    1.03 ms per token,   971.90 tokens per second)
0.00.850.463 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.464 I llama_perf_context_print:       total time =     140.19 ms /   129 tokens
0.00.850.834 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.079s
sys	0m0.139s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.787 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.621 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.628 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.629 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.629 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.631 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.634 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.634 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.635 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.639 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.639 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.489 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.327 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.328 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.328 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.329 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.329 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.330 I llama_model_loader: - type  f32:  194 tensors
0.00.027.330 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.330 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.331 I print_info: file format = GGUF V3 (latest)
0.00.027.331 I print_info: file type   = Q5_0
0.00.027.332 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.475 I load: special tokens cache size = 25
0.00.041.463 I load: token to piece cache size = 0.2984 MB
0.00.041.466 I print_info: arch             = gptneox
0.00.041.466 I print_info: vocab_only       = 0
0.00.041.466 I print_info: n_ctx_train      = 2048
0.00.041.466 I print_info: n_embd           = 2048
0.00.041.466 I print_info: n_layer          = 24
0.00.041.469 I print_info: n_head           = 16
0.00.041.470 I print_info: n_head_kv        = 16
0.00.041.470 I print_info: n_rot            = 32
0.00.041.472 I print_info: n_swa            = 0
0.00.041.472 I print_info: n_embd_head_k    = 128
0.00.041.472 I print_info: n_embd_head_v    = 128
0.00.041.473 I print_info: n_gqa            = 1
0.00.041.474 I print_info: n_embd_k_gqa     = 2048
0.00.041.475 I print_info: n_embd_v_gqa     = 2048
0.00.041.475 I print_info: f_norm_eps       = 1.0e-05
0.00.041.476 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.476 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.476 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.476 I print_info: f_logit_scale    = 0.0e+00
0.00.041.477 I print_info: n_ff             = 8192
0.00.041.477 I print_info: n_expert         = 0
0.00.041.477 I print_info: n_expert_used    = 0
0.00.041.479 I print_info: causal attn      = 1
0.00.041.479 I print_info: pooling type     = 0
0.00.041.480 I print_info: rope type        = 2
0.00.041.483 I print_info: rope scaling     = linear
0.00.041.483 I print_info: freq_base_train  = 10000.0
0.00.041.484 I print_info: freq_scale_train = 1
0.00.041.484 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.484 I print_info: rope_finetuned   = unknown
0.00.041.484 I print_info: ssm_d_conv       = 0
0.00.041.484 I print_info: ssm_d_inner      = 0
0.00.041.484 I print_info: ssm_d_state      = 0
0.00.041.485 I print_info: ssm_dt_rank      = 0
0.00.041.485 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.485 I print_info: model type       = 1.4B
0.00.041.485 I print_info: model params     = 1.41 B
0.00.041.487 I print_info: general.name     = 1.4B
0.00.041.487 I print_info: vocab type       = BPE
0.00.041.487 I print_info: n_vocab          = 50304
0.00.041.487 I print_info: n_merges         = 50009
0.00.041.488 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.488 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.488 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.488 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.488 I print_info: LF token         = 187 'Ċ'
0.00.041.489 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.490 I print_info: max token length = 1024
0.00.707.908 I load_tensors: offloading 24 repeating layers to GPU
0.00.707.925 I load_tensors: offloading output layer to GPU
0.00.707.925 I load_tensors: offloaded 25/25 layers to GPU
0.00.707.959 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.707.961 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.709.382 I llama_init_from_model: n_seq_max     = 1
0.00.709.386 I llama_init_from_model: n_ctx         = 2048
0.00.709.386 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.709.387 I llama_init_from_model: n_batch       = 2048
0.00.709.387 I llama_init_from_model: n_ubatch      = 512
0.00.709.388 I llama_init_from_model: flash_attn    = 0
0.00.709.390 I llama_init_from_model: freq_base     = 10000.0
0.00.709.390 I llama_init_from_model: freq_scale    = 1
0.00.709.396 I ggml_metal_init: allocating
0.00.709.482 I ggml_metal_init: found device: Apple M4
0.00.709.497 I ggml_metal_init: picking default device: Apple M4
0.00.711.339 I ggml_metal_init: using embedded metal library
0.00.717.811 I ggml_metal_init: GPU name:   Apple M4
0.00.717.815 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.816 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.817 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.817 I ggml_metal_init: simdgroup reduction   = true
0.00.717.818 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.818 I ggml_metal_init: has residency sets    = true
0.00.717.818 I ggml_metal_init: has bfloat            = true
0.00.717.818 I ggml_metal_init: use bfloat            = true
0.00.717.819 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.820 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.735.321 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.789.315 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.789.322 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.789.359 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.794.581 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.794.584 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.794.584 I llama_init_from_model: graph nodes  = 967
0.00.794.584 I llama_init_from_model: graph splits = 2
0.00.794.590 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.794.710 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.794.711 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.184 I main: llama threadpool init, n_threads = 4
0.00.855.229 I 
0.00.855.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.855.257 I 
0.00.855.408 I sampler seed: 1234
0.00.855.412 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.855.461 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.855.464 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.855.464 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.651.153 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49546.41 tokens per second)
0.01.651.155 I llama_perf_context_print:        load time =     843.49 ms
0.01.651.156 I llama_perf_context_print: prompt eval time =      47.66 ms /     7 tokens (    6.81 ms per token,   146.89 tokens per second)
0.01.651.157 I llama_perf_context_print:        eval time =     745.20 ms /    63 runs   (   11.83 ms per token,    84.54 tokens per second)
0.01.651.157 I llama_perf_context_print:       total time =     796.88 ms /    70 tokens
0.01.651.432 I ggml_metal_free: deallocating

real	0m1.669s
user	0m0.110s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.234 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.636 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.643 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.645 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.646 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.646 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.646 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.647 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.648 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.648 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.651 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.654 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.654 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.411 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.311 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.312 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.313 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.313 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.313 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.314 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.315 I llama_model_loader: - type  f32:  194 tensors
0.00.026.315 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.315 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.316 I print_info: file format = GGUF V3 (latest)
0.00.026.316 I print_info: file type   = Q5_0
0.00.026.319 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.278 I load: special tokens cache size = 25
0.00.040.191 I load: token to piece cache size = 0.2984 MB
0.00.040.196 I print_info: arch             = gptneox
0.00.040.196 I print_info: vocab_only       = 0
0.00.040.196 I print_info: n_ctx_train      = 2048
0.00.040.197 I print_info: n_embd           = 2048
0.00.040.197 I print_info: n_layer          = 24
0.00.040.201 I print_info: n_head           = 16
0.00.040.202 I print_info: n_head_kv        = 16
0.00.040.202 I print_info: n_rot            = 32
0.00.040.202 I print_info: n_swa            = 0
0.00.040.205 I print_info: n_embd_head_k    = 128
0.00.040.205 I print_info: n_embd_head_v    = 128
0.00.040.206 I print_info: n_gqa            = 1
0.00.040.206 I print_info: n_embd_k_gqa     = 2048
0.00.040.207 I print_info: n_embd_v_gqa     = 2048
0.00.040.208 I print_info: f_norm_eps       = 1.0e-05
0.00.040.208 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.208 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.208 I print_info: f_logit_scale    = 0.0e+00
0.00.040.210 I print_info: n_ff             = 8192
0.00.040.211 I print_info: n_expert         = 0
0.00.040.211 I print_info: n_expert_used    = 0
0.00.040.211 I print_info: causal attn      = 1
0.00.040.211 I print_info: pooling type     = 0
0.00.040.211 I print_info: rope type        = 2
0.00.040.212 I print_info: rope scaling     = linear
0.00.040.212 I print_info: freq_base_train  = 10000.0
0.00.040.213 I print_info: freq_scale_train = 1
0.00.040.213 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.214 I print_info: rope_finetuned   = unknown
0.00.040.214 I print_info: ssm_d_conv       = 0
0.00.040.214 I print_info: ssm_d_inner      = 0
0.00.040.214 I print_info: ssm_d_state      = 0
0.00.040.214 I print_info: ssm_dt_rank      = 0
0.00.040.214 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.215 I print_info: model type       = 1.4B
0.00.040.215 I print_info: model params     = 1.41 B
0.00.040.215 I print_info: general.name     = 1.4B
0.00.040.216 I print_info: vocab type       = BPE
0.00.040.217 I print_info: n_vocab          = 50304
0.00.040.217 I print_info: n_merges         = 50009
0.00.040.217 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.218 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.218 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.218 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.218 I print_info: LF token         = 187 'Ċ'
0.00.040.218 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.218 I print_info: max token length = 1024
0.00.720.850 I load_tensors: offloading 24 repeating layers to GPU
0.00.720.872 I load_tensors: offloading output layer to GPU
0.00.720.873 I load_tensors: offloaded 25/25 layers to GPU
0.00.720.909 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.720.911 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.722.177 I llama_init_from_model: n_seq_max     = 1
0.00.722.183 I llama_init_from_model: n_ctx         = 128
0.00.722.184 I llama_init_from_model: n_ctx_per_seq = 128
0.00.722.185 I llama_init_from_model: n_batch       = 128
0.00.722.185 I llama_init_from_model: n_ubatch      = 128
0.00.722.185 I llama_init_from_model: flash_attn    = 0
0.00.722.188 I llama_init_from_model: freq_base     = 10000.0
0.00.722.188 I llama_init_from_model: freq_scale    = 1
0.00.722.189 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.722.191 I ggml_metal_init: allocating
0.00.722.284 I ggml_metal_init: found device: Apple M4
0.00.722.300 I ggml_metal_init: picking default device: Apple M4
0.00.724.170 I ggml_metal_init: using embedded metal library
0.00.731.115 I ggml_metal_init: GPU name:   Apple M4
0.00.731.123 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.731.124 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.731.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.731.128 I ggml_metal_init: simdgroup reduction   = true
0.00.731.128 I ggml_metal_init: simdgroup matrix mul. = true
0.00.731.129 I ggml_metal_init: has residency sets    = true
0.00.731.129 I ggml_metal_init: has bfloat            = true
0.00.731.129 I ggml_metal_init: use bfloat            = true
0.00.731.130 I ggml_metal_init: hasUnifiedMemory      = true
0.00.731.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.749.384 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.753.153 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.753.157 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.753.200 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.472 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.756.473 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.756.474 I llama_init_from_model: graph nodes  = 967
0.00.756.474 I llama_init_from_model: graph splits = 2
0.00.756.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.756.477 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.985 I 
0.00.788.047 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.052 I perplexity: tokenizing the input ..
0.00.794.479 I perplexity: tokenization took 6.423 ms
0.00.794.485 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.942.843 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.944.207 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.944.232 I llama_perf_context_print:        load time =     777.74 ms
0.00.944.233 I llama_perf_context_print: prompt eval time =     147.37 ms /   128 tokens (    1.15 ms per token,   868.59 tokens per second)
0.00.944.234 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.944.234 I llama_perf_context_print:       total time =     156.25 ms /   129 tokens
0.00.944.636 I ggml_metal_free: deallocating

real	0m0.960s
user	0m0.080s
sys	0m0.156s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.795 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.368 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.369 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.370 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.370 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.370 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.371 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.377 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.378 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.378 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.379 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.379 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.383 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.383 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.254 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.106 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.108 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.109 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.109 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.109 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.110 I llama_model_loader: - type  f32:  194 tensors
0.00.026.111 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.111 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.112 I print_info: file format = GGUF V3 (latest)
0.00.026.112 I print_info: file type   = Q5_1
0.00.026.113 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.131 I load: special tokens cache size = 25
0.00.040.113 I load: token to piece cache size = 0.2984 MB
0.00.040.120 I print_info: arch             = gptneox
0.00.040.120 I print_info: vocab_only       = 0
0.00.040.121 I print_info: n_ctx_train      = 2048
0.00.040.121 I print_info: n_embd           = 2048
0.00.040.121 I print_info: n_layer          = 24
0.00.040.125 I print_info: n_head           = 16
0.00.040.126 I print_info: n_head_kv        = 16
0.00.040.126 I print_info: n_rot            = 32
0.00.040.126 I print_info: n_swa            = 0
0.00.040.127 I print_info: n_embd_head_k    = 128
0.00.040.127 I print_info: n_embd_head_v    = 128
0.00.040.127 I print_info: n_gqa            = 1
0.00.040.128 I print_info: n_embd_k_gqa     = 2048
0.00.040.128 I print_info: n_embd_v_gqa     = 2048
0.00.040.129 I print_info: f_norm_eps       = 1.0e-05
0.00.040.129 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.130 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.131 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.133 I print_info: f_logit_scale    = 0.0e+00
0.00.040.134 I print_info: n_ff             = 8192
0.00.040.134 I print_info: n_expert         = 0
0.00.040.134 I print_info: n_expert_used    = 0
0.00.040.134 I print_info: causal attn      = 1
0.00.040.134 I print_info: pooling type     = 0
0.00.040.134 I print_info: rope type        = 2
0.00.040.135 I print_info: rope scaling     = linear
0.00.040.135 I print_info: freq_base_train  = 10000.0
0.00.040.135 I print_info: freq_scale_train = 1
0.00.040.135 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.137 I print_info: rope_finetuned   = unknown
0.00.040.137 I print_info: ssm_d_conv       = 0
0.00.040.137 I print_info: ssm_d_inner      = 0
0.00.040.137 I print_info: ssm_d_state      = 0
0.00.040.137 I print_info: ssm_dt_rank      = 0
0.00.040.137 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.137 I print_info: model type       = 1.4B
0.00.040.138 I print_info: model params     = 1.41 B
0.00.040.138 I print_info: general.name     = 1.4B
0.00.040.138 I print_info: vocab type       = BPE
0.00.040.139 I print_info: n_vocab          = 50304
0.00.040.139 I print_info: n_merges         = 50009
0.00.040.139 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.139 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.139 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.139 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.140 I print_info: LF token         = 187 'Ċ'
0.00.040.140 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.140 I print_info: max token length = 1024
0.00.590.006 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.019 I load_tensors: offloading output layer to GPU
0.00.590.020 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.054 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.590.060 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.591.456 I llama_init_from_model: n_seq_max     = 1
0.00.591.465 I llama_init_from_model: n_ctx         = 2048
0.00.591.466 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.591.466 I llama_init_from_model: n_batch       = 2048
0.00.591.467 I llama_init_from_model: n_ubatch      = 512
0.00.591.467 I llama_init_from_model: flash_attn    = 0
0.00.591.469 I llama_init_from_model: freq_base     = 10000.0
0.00.591.478 I llama_init_from_model: freq_scale    = 1
0.00.591.481 I ggml_metal_init: allocating
0.00.591.535 I ggml_metal_init: found device: Apple M4
0.00.591.549 I ggml_metal_init: picking default device: Apple M4
0.00.593.135 I ggml_metal_init: using embedded metal library
0.00.599.577 I ggml_metal_init: GPU name:   Apple M4
0.00.599.581 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.582 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.583 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.584 I ggml_metal_init: simdgroup reduction   = true
0.00.599.584 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.584 I ggml_metal_init: has residency sets    = true
0.00.599.584 I ggml_metal_init: has bfloat            = true
0.00.599.585 I ggml_metal_init: use bfloat            = true
0.00.599.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.272 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.877 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.667.885 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.667.920 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.330 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.672.333 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.672.333 I llama_init_from_model: graph nodes  = 967
0.00.672.333 I llama_init_from_model: graph splits = 2
0.00.672.340 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.672.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.672.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.701 I main: llama threadpool init, n_threads = 4
0.00.727.744 I 
0.00.727.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.770 I 
0.00.727.941 I sampler seed: 1234
0.00.727.945 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.727.986 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.727.990 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.727.990 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.566.805 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.566.805 I llama_perf_context_print:        load time =     716.99 ms
0.01.566.806 I llama_perf_context_print: prompt eval time =      42.04 ms /     7 tokens (    6.01 ms per token,   166.50 tokens per second)
0.01.566.807 I llama_perf_context_print:        eval time =     793.75 ms /    63 runs   (   12.60 ms per token,    79.37 tokens per second)
0.01.566.807 I llama_perf_context_print:       total time =     840.02 ms /    70 tokens
0.01.567.030 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.109s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.372 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.791 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.797 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.799 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.799 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.800 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.802 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.807 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.807 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.809 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.810 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.810 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.817 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.727 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.727 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.728 I llama_model_loader: - type  f32:  194 tensors
0.00.025.728 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.729 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.730 I print_info: file format = GGUF V3 (latest)
0.00.025.730 I print_info: file type   = Q5_1
0.00.025.731 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.861 I load: special tokens cache size = 25
0.00.039.918 I load: token to piece cache size = 0.2984 MB
0.00.039.920 I print_info: arch             = gptneox
0.00.039.921 I print_info: vocab_only       = 0
0.00.039.921 I print_info: n_ctx_train      = 2048
0.00.039.921 I print_info: n_embd           = 2048
0.00.039.921 I print_info: n_layer          = 24
0.00.039.925 I print_info: n_head           = 16
0.00.039.925 I print_info: n_head_kv        = 16
0.00.039.926 I print_info: n_rot            = 32
0.00.039.926 I print_info: n_swa            = 0
0.00.039.929 I print_info: n_embd_head_k    = 128
0.00.039.929 I print_info: n_embd_head_v    = 128
0.00.039.930 I print_info: n_gqa            = 1
0.00.039.930 I print_info: n_embd_k_gqa     = 2048
0.00.039.931 I print_info: n_embd_v_gqa     = 2048
0.00.039.932 I print_info: f_norm_eps       = 1.0e-05
0.00.039.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.932 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.932 I print_info: f_logit_scale    = 0.0e+00
0.00.039.933 I print_info: n_ff             = 8192
0.00.039.933 I print_info: n_expert         = 0
0.00.039.933 I print_info: n_expert_used    = 0
0.00.039.934 I print_info: causal attn      = 1
0.00.039.934 I print_info: pooling type     = 0
0.00.039.935 I print_info: rope type        = 2
0.00.039.935 I print_info: rope scaling     = linear
0.00.039.936 I print_info: freq_base_train  = 10000.0
0.00.039.936 I print_info: freq_scale_train = 1
0.00.039.936 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.936 I print_info: rope_finetuned   = unknown
0.00.039.936 I print_info: ssm_d_conv       = 0
0.00.039.937 I print_info: ssm_d_inner      = 0
0.00.039.937 I print_info: ssm_d_state      = 0
0.00.039.937 I print_info: ssm_dt_rank      = 0
0.00.039.937 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.937 I print_info: model type       = 1.4B
0.00.039.938 I print_info: model params     = 1.41 B
0.00.039.938 I print_info: general.name     = 1.4B
0.00.039.938 I print_info: vocab type       = BPE
0.00.039.939 I print_info: n_vocab          = 50304
0.00.039.939 I print_info: n_merges         = 50009
0.00.039.939 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.939 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.939 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.939 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.940 I print_info: LF token         = 187 'Ċ'
0.00.039.940 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.940 I print_info: max token length = 1024
0.00.616.749 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.770 I load_tensors: offloading output layer to GPU
0.00.616.770 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.810 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.616.811 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.618.092 I llama_init_from_model: n_seq_max     = 1
0.00.618.098 I llama_init_from_model: n_ctx         = 128
0.00.618.099 I llama_init_from_model: n_ctx_per_seq = 128
0.00.618.102 I llama_init_from_model: n_batch       = 128
0.00.618.102 I llama_init_from_model: n_ubatch      = 128
0.00.618.103 I llama_init_from_model: flash_attn    = 0
0.00.618.105 I llama_init_from_model: freq_base     = 10000.0
0.00.618.105 I llama_init_from_model: freq_scale    = 1
0.00.618.106 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.618.113 I ggml_metal_init: allocating
0.00.618.211 I ggml_metal_init: found device: Apple M4
0.00.618.226 I ggml_metal_init: picking default device: Apple M4
0.00.620.111 I ggml_metal_init: using embedded metal library
0.00.626.504 I ggml_metal_init: GPU name:   Apple M4
0.00.626.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.510 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.511 I ggml_metal_init: simdgroup reduction   = true
0.00.626.511 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.511 I ggml_metal_init: has residency sets    = true
0.00.626.512 I ggml_metal_init: has bfloat            = true
0.00.626.512 I ggml_metal_init: use bfloat            = true
0.00.626.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.947 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.632 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.637 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.678 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.650.858 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.650.860 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.650.860 I llama_init_from_model: graph nodes  = 967
0.00.650.861 I llama_init_from_model: graph splits = 2
0.00.650.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.518 I 
0.00.681.579 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.584 I perplexity: tokenizing the input ..
0.00.688.421 I perplexity: tokenization took 6.834 ms
0.00.688.426 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.716 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.838.060 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.838.084 I llama_perf_context_print:        load time =     672.14 ms
0.00.838.086 I llama_perf_context_print: prompt eval time =     147.27 ms /   128 tokens (    1.15 ms per token,   869.18 tokens per second)
0.00.838.087 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.087 I llama_perf_context_print:       total time =     156.57 ms /   129 tokens
0.00.838.481 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.080s
sys	0m0.139s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.993 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.673 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.680 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.681 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.682 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.683 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.684 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.460 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.461 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.461 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.461 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.462 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.462 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.463 I llama_model_loader: - type  f32:  194 tensors
0.00.025.463 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.463 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.463 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.464 I print_info: file format = GGUF V3 (latest)
0.00.025.465 I print_info: file type   = Q2_K - Medium
0.00.025.465 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.551 I load: special tokens cache size = 25
0.00.039.764 I load: token to piece cache size = 0.2984 MB
0.00.039.767 I print_info: arch             = gptneox
0.00.039.767 I print_info: vocab_only       = 0
0.00.039.767 I print_info: n_ctx_train      = 2048
0.00.039.767 I print_info: n_embd           = 2048
0.00.039.767 I print_info: n_layer          = 24
0.00.039.770 I print_info: n_head           = 16
0.00.039.771 I print_info: n_head_kv        = 16
0.00.039.771 I print_info: n_rot            = 32
0.00.039.771 I print_info: n_swa            = 0
0.00.039.771 I print_info: n_embd_head_k    = 128
0.00.039.771 I print_info: n_embd_head_v    = 128
0.00.039.772 I print_info: n_gqa            = 1
0.00.039.773 I print_info: n_embd_k_gqa     = 2048
0.00.039.773 I print_info: n_embd_v_gqa     = 2048
0.00.039.775 I print_info: f_norm_eps       = 1.0e-05
0.00.039.776 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.776 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.776 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.776 I print_info: f_logit_scale    = 0.0e+00
0.00.039.777 I print_info: n_ff             = 8192
0.00.039.777 I print_info: n_expert         = 0
0.00.039.777 I print_info: n_expert_used    = 0
0.00.039.777 I print_info: causal attn      = 1
0.00.039.777 I print_info: pooling type     = 0
0.00.039.778 I print_info: rope type        = 2
0.00.039.779 I print_info: rope scaling     = linear
0.00.039.779 I print_info: freq_base_train  = 10000.0
0.00.039.779 I print_info: freq_scale_train = 1
0.00.039.780 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.780 I print_info: rope_finetuned   = unknown
0.00.039.780 I print_info: ssm_d_conv       = 0
0.00.039.780 I print_info: ssm_d_inner      = 0
0.00.039.780 I print_info: ssm_d_state      = 0
0.00.039.781 I print_info: ssm_dt_rank      = 0
0.00.039.781 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.781 I print_info: model type       = 1.4B
0.00.039.781 I print_info: model params     = 1.41 B
0.00.039.781 I print_info: general.name     = 1.4B
0.00.039.784 I print_info: vocab type       = BPE
0.00.039.784 I print_info: n_vocab          = 50304
0.00.039.784 I print_info: n_merges         = 50009
0.00.039.784 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.784 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.785 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.785 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.785 I print_info: LF token         = 187 'Ċ'
0.00.039.785 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.786 I print_info: max token length = 1024
0.00.343.452 I load_tensors: offloading 24 repeating layers to GPU
0.00.343.465 I load_tensors: offloading output layer to GPU
0.00.343.466 I load_tensors: offloaded 25/25 layers to GPU
0.00.343.501 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.343.502 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.345.147 I llama_init_from_model: n_seq_max     = 1
0.00.345.152 I llama_init_from_model: n_ctx         = 2048
0.00.345.153 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.345.153 I llama_init_from_model: n_batch       = 2048
0.00.345.154 I llama_init_from_model: n_ubatch      = 512
0.00.345.154 I llama_init_from_model: flash_attn    = 0
0.00.345.156 I llama_init_from_model: freq_base     = 10000.0
0.00.345.160 I llama_init_from_model: freq_scale    = 1
0.00.345.166 I ggml_metal_init: allocating
0.00.345.275 I ggml_metal_init: found device: Apple M4
0.00.345.289 I ggml_metal_init: picking default device: Apple M4
0.00.347.173 I ggml_metal_init: using embedded metal library
0.00.352.909 I ggml_metal_init: GPU name:   Apple M4
0.00.352.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.352.922 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.352.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.352.924 I ggml_metal_init: simdgroup reduction   = true
0.00.352.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.352.924 I ggml_metal_init: has residency sets    = true
0.00.352.925 I ggml_metal_init: has bfloat            = true
0.00.352.925 I ggml_metal_init: use bfloat            = true
0.00.352.926 I ggml_metal_init: hasUnifiedMemory      = true
0.00.352.931 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.373.763 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.434.001 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.434.012 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.434.049 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.438.230 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.438.233 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.438.233 I llama_init_from_model: graph nodes  = 967
0.00.438.233 I llama_init_from_model: graph splits = 2
0.00.438.240 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.438.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.438.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.903 I main: llama threadpool init, n_threads = 4
0.00.499.946 I 
0.00.499.969 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.499.970 I 
0.00.500.145 I sampler seed: 1234
0.00.500.150 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.500.161 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.500.161 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.500.161 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.181.732 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.01.181.733 I llama_perf_context_print:        load time =     488.97 ms
0.01.181.733 I llama_perf_context_print: prompt eval time =      41.98 ms /     7 tokens (    6.00 ms per token,   166.76 tokens per second)
0.01.181.735 I llama_perf_context_print:        eval time =     636.79 ms /    63 runs   (   10.11 ms per token,    98.93 tokens per second)
0.01.181.735 I llama_perf_context_print:       total time =     682.77 ms /    70 tokens
0.01.181.961 I ggml_metal_free: deallocating

real	0m1.200s
user	0m0.111s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.274 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.276 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.278 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.279 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.279 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.279 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.280 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.281 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.281 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.282 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.282 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.282 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.283 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.283 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.285 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.285 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.286 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.217 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.150 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.151 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.152 I llama_model_loader: - type  f32:  194 tensors
0.00.026.152 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.152 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.152 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.153 I print_info: file format = GGUF V3 (latest)
0.00.026.154 I print_info: file type   = Q2_K - Medium
0.00.026.154 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.145 I load: special tokens cache size = 25
0.00.040.025 I load: token to piece cache size = 0.2984 MB
0.00.040.028 I print_info: arch             = gptneox
0.00.040.028 I print_info: vocab_only       = 0
0.00.040.028 I print_info: n_ctx_train      = 2048
0.00.040.029 I print_info: n_embd           = 2048
0.00.040.029 I print_info: n_layer          = 24
0.00.040.032 I print_info: n_head           = 16
0.00.040.033 I print_info: n_head_kv        = 16
0.00.040.033 I print_info: n_rot            = 32
0.00.040.033 I print_info: n_swa            = 0
0.00.040.033 I print_info: n_embd_head_k    = 128
0.00.040.033 I print_info: n_embd_head_v    = 128
0.00.040.035 I print_info: n_gqa            = 1
0.00.040.036 I print_info: n_embd_k_gqa     = 2048
0.00.040.037 I print_info: n_embd_v_gqa     = 2048
0.00.040.038 I print_info: f_norm_eps       = 1.0e-05
0.00.040.038 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.038 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.039 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.039 I print_info: f_logit_scale    = 0.0e+00
0.00.040.039 I print_info: n_ff             = 8192
0.00.040.040 I print_info: n_expert         = 0
0.00.040.040 I print_info: n_expert_used    = 0
0.00.040.040 I print_info: causal attn      = 1
0.00.040.040 I print_info: pooling type     = 0
0.00.040.040 I print_info: rope type        = 2
0.00.040.041 I print_info: rope scaling     = linear
0.00.040.041 I print_info: freq_base_train  = 10000.0
0.00.040.041 I print_info: freq_scale_train = 1
0.00.040.041 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.042 I print_info: rope_finetuned   = unknown
0.00.040.042 I print_info: ssm_d_conv       = 0
0.00.040.042 I print_info: ssm_d_inner      = 0
0.00.040.042 I print_info: ssm_d_state      = 0
0.00.040.042 I print_info: ssm_dt_rank      = 0
0.00.040.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.043 I print_info: model type       = 1.4B
0.00.040.043 I print_info: model params     = 1.41 B
0.00.040.043 I print_info: general.name     = 1.4B
0.00.040.044 I print_info: vocab type       = BPE
0.00.040.044 I print_info: n_vocab          = 50304
0.00.040.044 I print_info: n_merges         = 50009
0.00.040.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.045 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.045 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.045 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.045 I print_info: LF token         = 187 'Ċ'
0.00.040.046 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.046 I print_info: max token length = 1024
0.00.351.765 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.788 I load_tensors: offloading output layer to GPU
0.00.351.789 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.822 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.823 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.353.029 I llama_init_from_model: n_seq_max     = 1
0.00.353.038 I llama_init_from_model: n_ctx         = 128
0.00.353.038 I llama_init_from_model: n_ctx_per_seq = 128
0.00.353.039 I llama_init_from_model: n_batch       = 128
0.00.353.039 I llama_init_from_model: n_ubatch      = 128
0.00.353.040 I llama_init_from_model: flash_attn    = 0
0.00.353.042 I llama_init_from_model: freq_base     = 10000.0
0.00.353.042 I llama_init_from_model: freq_scale    = 1
0.00.353.043 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.353.048 I ggml_metal_init: allocating
0.00.353.121 I ggml_metal_init: found device: Apple M4
0.00.353.135 I ggml_metal_init: picking default device: Apple M4
0.00.354.958 I ggml_metal_init: using embedded metal library
0.00.361.283 I ggml_metal_init: GPU name:   Apple M4
0.00.361.297 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.361.298 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.361.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.361.299 I ggml_metal_init: simdgroup reduction   = true
0.00.361.300 I ggml_metal_init: simdgroup matrix mul. = true
0.00.361.300 I ggml_metal_init: has residency sets    = true
0.00.361.300 I ggml_metal_init: has bfloat            = true
0.00.361.300 I ggml_metal_init: use bfloat            = true
0.00.361.303 I ggml_metal_init: hasUnifiedMemory      = true
0.00.361.321 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.383.509 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.387.138 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.387.144 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.387.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.390.728 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.390.731 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.390.731 I llama_init_from_model: graph nodes  = 967
0.00.390.732 I llama_init_from_model: graph splits = 2
0.00.390.735 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.390.735 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.417.103 I 
0.00.417.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.417.181 I perplexity: tokenizing the input ..
0.00.423.859 I perplexity: tokenization took 6.674 ms
0.00.423.866 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.556.916 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.558.255 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.558.282 I llama_perf_context_print:        load time =     406.82 ms
0.00.558.283 I llama_perf_context_print: prompt eval time =     132.08 ms /   128 tokens (    1.03 ms per token,   969.12 tokens per second)
0.00.558.284 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.558.285 I llama_perf_context_print:       total time =     141.18 ms /   129 tokens
0.00.558.715 I ggml_metal_free: deallocating

real	0m0.574s
user	0m0.083s
sys	0m0.098s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.723 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.220 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.225 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.227 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.228 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.228 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.229 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.230 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.232 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.233 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.233 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.237 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.237 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.237 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.304 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.395 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.411 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.412 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.413 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.413 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.413 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.414 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.414 I llama_model_loader: - type  f32:  194 tensors
0.00.025.414 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.414 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.415 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.415 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.415 I print_info: file format = GGUF V3 (latest)
0.00.025.416 I print_info: file type   = Q3_K - Medium
0.00.025.417 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.195 I load: special tokens cache size = 25
0.00.039.090 I load: token to piece cache size = 0.2984 MB
0.00.039.093 I print_info: arch             = gptneox
0.00.039.093 I print_info: vocab_only       = 0
0.00.039.093 I print_info: n_ctx_train      = 2048
0.00.039.093 I print_info: n_embd           = 2048
0.00.039.093 I print_info: n_layer          = 24
0.00.039.096 I print_info: n_head           = 16
0.00.039.097 I print_info: n_head_kv        = 16
0.00.039.098 I print_info: n_rot            = 32
0.00.039.098 I print_info: n_swa            = 0
0.00.039.100 I print_info: n_embd_head_k    = 128
0.00.039.100 I print_info: n_embd_head_v    = 128
0.00.039.101 I print_info: n_gqa            = 1
0.00.039.101 I print_info: n_embd_k_gqa     = 2048
0.00.039.102 I print_info: n_embd_v_gqa     = 2048
0.00.039.103 I print_info: f_norm_eps       = 1.0e-05
0.00.039.103 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.103 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.103 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.104 I print_info: f_logit_scale    = 0.0e+00
0.00.039.104 I print_info: n_ff             = 8192
0.00.039.104 I print_info: n_expert         = 0
0.00.039.104 I print_info: n_expert_used    = 0
0.00.039.107 I print_info: causal attn      = 1
0.00.039.108 I print_info: pooling type     = 0
0.00.039.109 I print_info: rope type        = 2
0.00.039.109 I print_info: rope scaling     = linear
0.00.039.109 I print_info: freq_base_train  = 10000.0
0.00.039.109 I print_info: freq_scale_train = 1
0.00.039.110 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.110 I print_info: rope_finetuned   = unknown
0.00.039.110 I print_info: ssm_d_conv       = 0
0.00.039.110 I print_info: ssm_d_inner      = 0
0.00.039.110 I print_info: ssm_d_state      = 0
0.00.039.110 I print_info: ssm_dt_rank      = 0
0.00.039.110 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.111 I print_info: model type       = 1.4B
0.00.039.111 I print_info: model params     = 1.41 B
0.00.039.111 I print_info: general.name     = 1.4B
0.00.039.112 I print_info: vocab type       = BPE
0.00.039.112 I print_info: n_vocab          = 50304
0.00.039.112 I print_info: n_merges         = 50009
0.00.039.112 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.112 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: LF token         = 187 'Ċ'
0.00.039.113 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.113 I print_info: max token length = 1024
0.00.430.033 I load_tensors: offloading 24 repeating layers to GPU
0.00.430.047 I load_tensors: offloading output layer to GPU
0.00.430.048 I load_tensors: offloaded 25/25 layers to GPU
0.00.430.082 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.430.084 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.431.548 I llama_init_from_model: n_seq_max     = 1
0.00.431.553 I llama_init_from_model: n_ctx         = 2048
0.00.431.554 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.431.554 I llama_init_from_model: n_batch       = 2048
0.00.431.554 I llama_init_from_model: n_ubatch      = 512
0.00.431.555 I llama_init_from_model: flash_attn    = 0
0.00.431.561 I llama_init_from_model: freq_base     = 10000.0
0.00.431.561 I llama_init_from_model: freq_scale    = 1
0.00.431.575 I ggml_metal_init: allocating
0.00.431.653 I ggml_metal_init: found device: Apple M4
0.00.431.667 I ggml_metal_init: picking default device: Apple M4
0.00.433.461 I ggml_metal_init: using embedded metal library
0.00.438.991 I ggml_metal_init: GPU name:   Apple M4
0.00.439.005 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.439.005 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.439.006 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.439.007 I ggml_metal_init: simdgroup reduction   = true
0.00.439.007 I ggml_metal_init: simdgroup matrix mul. = true
0.00.439.008 I ggml_metal_init: has residency sets    = true
0.00.439.008 I ggml_metal_init: has bfloat            = true
0.00.439.008 I ggml_metal_init: use bfloat            = true
0.00.439.010 I ggml_metal_init: hasUnifiedMemory      = true
0.00.439.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.459.559 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.518.701 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.518.711 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.518.748 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.523.200 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.523.202 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.523.202 I llama_init_from_model: graph nodes  = 967
0.00.523.202 I llama_init_from_model: graph splits = 2
0.00.523.207 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.523.323 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.523.323 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.141 I main: llama threadpool init, n_threads = 4
0.00.582.185 I 
0.00.582.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.211 I 
0.00.582.375 I sampler seed: 1234
0.00.582.380 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.582.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.582.433 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.582.433 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.332.947 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.332.948 I llama_perf_context_print:        load time =     572.50 ms
0.01.332.950 I llama_perf_context_print: prompt eval time =      50.22 ms /     7 tokens (    7.17 ms per token,   139.39 tokens per second)
0.01.332.951 I llama_perf_context_print:        eval time =     697.37 ms /    63 runs   (   11.07 ms per token,    90.34 tokens per second)
0.01.332.951 I llama_perf_context_print:       total time =     751.72 ms /    70 tokens
0.01.333.178 I ggml_metal_free: deallocating

real	0m1.350s
user	0m0.112s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.579 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.806 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.808 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.810 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.815 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.818 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.783 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.858 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.859 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.859 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.859 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.860 I llama_model_loader: - type  f32:  194 tensors
0.00.025.861 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.861 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.861 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.861 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.862 I print_info: file format = GGUF V3 (latest)
0.00.025.863 I print_info: file type   = Q3_K - Medium
0.00.025.864 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.203 I load: special tokens cache size = 25
0.00.040.343 I load: token to piece cache size = 0.2984 MB
0.00.040.346 I print_info: arch             = gptneox
0.00.040.347 I print_info: vocab_only       = 0
0.00.040.347 I print_info: n_ctx_train      = 2048
0.00.040.347 I print_info: n_embd           = 2048
0.00.040.347 I print_info: n_layer          = 24
0.00.040.351 I print_info: n_head           = 16
0.00.040.355 I print_info: n_head_kv        = 16
0.00.040.355 I print_info: n_rot            = 32
0.00.040.355 I print_info: n_swa            = 0
0.00.040.355 I print_info: n_embd_head_k    = 128
0.00.040.356 I print_info: n_embd_head_v    = 128
0.00.040.356 I print_info: n_gqa            = 1
0.00.040.357 I print_info: n_embd_k_gqa     = 2048
0.00.040.358 I print_info: n_embd_v_gqa     = 2048
0.00.040.358 I print_info: f_norm_eps       = 1.0e-05
0.00.040.360 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.360 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.360 I print_info: f_logit_scale    = 0.0e+00
0.00.040.361 I print_info: n_ff             = 8192
0.00.040.361 I print_info: n_expert         = 0
0.00.040.361 I print_info: n_expert_used    = 0
0.00.040.362 I print_info: causal attn      = 1
0.00.040.362 I print_info: pooling type     = 0
0.00.040.362 I print_info: rope type        = 2
0.00.040.362 I print_info: rope scaling     = linear
0.00.040.363 I print_info: freq_base_train  = 10000.0
0.00.040.363 I print_info: freq_scale_train = 1
0.00.040.363 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.363 I print_info: rope_finetuned   = unknown
0.00.040.363 I print_info: ssm_d_conv       = 0
0.00.040.364 I print_info: ssm_d_inner      = 0
0.00.040.364 I print_info: ssm_d_state      = 0
0.00.040.364 I print_info: ssm_dt_rank      = 0
0.00.040.364 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.366 I print_info: model type       = 1.4B
0.00.040.366 I print_info: model params     = 1.41 B
0.00.040.366 I print_info: general.name     = 1.4B
0.00.040.367 I print_info: vocab type       = BPE
0.00.040.367 I print_info: n_vocab          = 50304
0.00.040.367 I print_info: n_merges         = 50009
0.00.040.368 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.368 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: LF token         = 187 'Ċ'
0.00.040.369 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: max token length = 1024
0.00.446.693 I load_tensors: offloading 24 repeating layers to GPU
0.00.446.715 I load_tensors: offloading output layer to GPU
0.00.446.715 I load_tensors: offloaded 25/25 layers to GPU
0.00.446.753 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.446.754 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.447.997 I llama_init_from_model: n_seq_max     = 1
0.00.448.003 I llama_init_from_model: n_ctx         = 128
0.00.448.003 I llama_init_from_model: n_ctx_per_seq = 128
0.00.448.004 I llama_init_from_model: n_batch       = 128
0.00.448.004 I llama_init_from_model: n_ubatch      = 128
0.00.448.005 I llama_init_from_model: flash_attn    = 0
0.00.448.007 I llama_init_from_model: freq_base     = 10000.0
0.00.448.008 I llama_init_from_model: freq_scale    = 1
0.00.448.008 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.448.013 I ggml_metal_init: allocating
0.00.448.112 I ggml_metal_init: found device: Apple M4
0.00.448.127 I ggml_metal_init: picking default device: Apple M4
0.00.449.969 I ggml_metal_init: using embedded metal library
0.00.455.919 I ggml_metal_init: GPU name:   Apple M4
0.00.455.941 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.942 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.943 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.944 I ggml_metal_init: simdgroup reduction   = true
0.00.455.944 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.945 I ggml_metal_init: has residency sets    = true
0.00.455.945 I ggml_metal_init: has bfloat            = true
0.00.455.945 I ggml_metal_init: use bfloat            = true
0.00.455.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.955 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.906 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.479.402 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.479.407 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.479.449 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.482.610 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.482.612 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.482.613 I llama_init_from_model: graph nodes  = 967
0.00.482.613 I llama_init_from_model: graph splits = 2
0.00.482.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.482.617 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.177 I 
0.00.508.236 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.242 I perplexity: tokenizing the input ..
0.00.514.340 I perplexity: tokenization took 6.097 ms
0.00.514.344 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.645.999 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.343 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.369 I llama_perf_context_print:        load time =     498.59 ms
0.00.647.370 I llama_perf_context_print: prompt eval time =     131.39 ms /   128 tokens (    1.03 ms per token,   974.18 tokens per second)
0.00.647.374 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.374 I llama_perf_context_print:       total time =     139.20 ms /   129 tokens
0.00.647.793 I ggml_metal_free: deallocating

real	0m0.662s
user	0m0.080s
sys	0m0.109s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.093 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.724 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.735 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.737 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.738 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.738 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.740 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.740 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.742 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.743 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.743 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.744 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.621 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.444 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.445 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.447 I llama_model_loader: - type  f32:  194 tensors
0.00.025.447 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.448 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.448 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.448 I print_info: file format = GGUF V3 (latest)
0.00.025.449 I print_info: file type   = Q4_K - Medium
0.00.025.450 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.533 I load: special tokens cache size = 25
0.00.039.578 I load: token to piece cache size = 0.2984 MB
0.00.039.581 I print_info: arch             = gptneox
0.00.039.581 I print_info: vocab_only       = 0
0.00.039.581 I print_info: n_ctx_train      = 2048
0.00.039.581 I print_info: n_embd           = 2048
0.00.039.582 I print_info: n_layer          = 24
0.00.039.584 I print_info: n_head           = 16
0.00.039.585 I print_info: n_head_kv        = 16
0.00.039.587 I print_info: n_rot            = 32
0.00.039.587 I print_info: n_swa            = 0
0.00.039.587 I print_info: n_embd_head_k    = 128
0.00.039.588 I print_info: n_embd_head_v    = 128
0.00.039.588 I print_info: n_gqa            = 1
0.00.039.589 I print_info: n_embd_k_gqa     = 2048
0.00.039.590 I print_info: n_embd_v_gqa     = 2048
0.00.039.590 I print_info: f_norm_eps       = 1.0e-05
0.00.039.591 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.591 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.591 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.591 I print_info: f_logit_scale    = 0.0e+00
0.00.039.592 I print_info: n_ff             = 8192
0.00.039.592 I print_info: n_expert         = 0
0.00.039.592 I print_info: n_expert_used    = 0
0.00.039.593 I print_info: causal attn      = 1
0.00.039.593 I print_info: pooling type     = 0
0.00.039.593 I print_info: rope type        = 2
0.00.039.593 I print_info: rope scaling     = linear
0.00.039.594 I print_info: freq_base_train  = 10000.0
0.00.039.594 I print_info: freq_scale_train = 1
0.00.039.594 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.594 I print_info: rope_finetuned   = unknown
0.00.039.595 I print_info: ssm_d_conv       = 0
0.00.039.595 I print_info: ssm_d_inner      = 0
0.00.039.596 I print_info: ssm_d_state      = 0
0.00.039.596 I print_info: ssm_dt_rank      = 0
0.00.039.596 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.596 I print_info: model type       = 1.4B
0.00.039.597 I print_info: model params     = 1.41 B
0.00.039.597 I print_info: general.name     = 1.4B
0.00.039.597 I print_info: vocab type       = BPE
0.00.039.598 I print_info: n_vocab          = 50304
0.00.039.598 I print_info: n_merges         = 50009
0.00.039.598 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.598 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.598 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.600 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.600 I print_info: LF token         = 187 'Ċ'
0.00.039.600 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.601 I print_info: max token length = 1024
0.00.514.738 I load_tensors: offloading 24 repeating layers to GPU
0.00.514.753 I load_tensors: offloading output layer to GPU
0.00.514.754 I load_tensors: offloaded 25/25 layers to GPU
0.00.514.802 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.514.804 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.516.233 I llama_init_from_model: n_seq_max     = 1
0.00.516.237 I llama_init_from_model: n_ctx         = 2048
0.00.516.238 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.516.238 I llama_init_from_model: n_batch       = 2048
0.00.516.238 I llama_init_from_model: n_ubatch      = 512
0.00.516.239 I llama_init_from_model: flash_attn    = 0
0.00.516.241 I llama_init_from_model: freq_base     = 10000.0
0.00.516.241 I llama_init_from_model: freq_scale    = 1
0.00.516.244 I ggml_metal_init: allocating
0.00.516.322 I ggml_metal_init: found device: Apple M4
0.00.516.335 I ggml_metal_init: picking default device: Apple M4
0.00.518.180 I ggml_metal_init: using embedded metal library
0.00.524.854 I ggml_metal_init: GPU name:   Apple M4
0.00.524.859 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.524.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.524.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.524.862 I ggml_metal_init: simdgroup reduction   = true
0.00.524.862 I ggml_metal_init: simdgroup matrix mul. = true
0.00.524.863 I ggml_metal_init: has residency sets    = true
0.00.524.863 I ggml_metal_init: has bfloat            = true
0.00.524.863 I ggml_metal_init: use bfloat            = true
0.00.524.864 I ggml_metal_init: hasUnifiedMemory      = true
0.00.524.866 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.942 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.598.502 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.598.510 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.598.544 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.602.623 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.602.625 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.602.625 I llama_init_from_model: graph nodes  = 967
0.00.602.626 I llama_init_from_model: graph splits = 2
0.00.602.632 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.602.744 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.602.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.653 I main: llama threadpool init, n_threads = 4
0.00.657.693 I 
0.00.657.723 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.723 I 
0.00.657.967 I sampler seed: 1234
0.00.657.977 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.657.993 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.657.994 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.657.995 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.413.763 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.01.413.764 I llama_perf_context_print:        load time =     647.53 ms
0.01.413.765 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.53 tokens per second)
0.01.413.767 I llama_perf_context_print:        eval time =     705.70 ms /    63 runs   (   11.20 ms per token,    89.27 tokens per second)
0.01.413.767 I llama_perf_context_print:       total time =     757.14 ms /    70 tokens
0.01.413.984 I ggml_metal_free: deallocating

real	0m1.431s
user	0m0.110s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.781 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.790 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.791 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.793 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.794 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.751 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.842 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.816 I llama_model_loader: - type  f32:  194 tensors
0.00.026.816 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.816 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.817 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.817 I print_info: file format = GGUF V3 (latest)
0.00.026.818 I print_info: file type   = Q4_K - Medium
0.00.026.819 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.542 I load: special tokens cache size = 25
0.00.040.749 I load: token to piece cache size = 0.2984 MB
0.00.040.752 I print_info: arch             = gptneox
0.00.040.752 I print_info: vocab_only       = 0
0.00.040.752 I print_info: n_ctx_train      = 2048
0.00.040.752 I print_info: n_embd           = 2048
0.00.040.753 I print_info: n_layer          = 24
0.00.040.756 I print_info: n_head           = 16
0.00.040.757 I print_info: n_head_kv        = 16
0.00.040.757 I print_info: n_rot            = 32
0.00.040.760 I print_info: n_swa            = 0
0.00.040.760 I print_info: n_embd_head_k    = 128
0.00.040.760 I print_info: n_embd_head_v    = 128
0.00.040.761 I print_info: n_gqa            = 1
0.00.040.762 I print_info: n_embd_k_gqa     = 2048
0.00.040.762 I print_info: n_embd_v_gqa     = 2048
0.00.040.763 I print_info: f_norm_eps       = 1.0e-05
0.00.040.764 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.764 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.764 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.764 I print_info: f_logit_scale    = 0.0e+00
0.00.040.766 I print_info: n_ff             = 8192
0.00.040.766 I print_info: n_expert         = 0
0.00.040.766 I print_info: n_expert_used    = 0
0.00.040.767 I print_info: causal attn      = 1
0.00.040.767 I print_info: pooling type     = 0
0.00.040.767 I print_info: rope type        = 2
0.00.040.767 I print_info: rope scaling     = linear
0.00.040.767 I print_info: freq_base_train  = 10000.0
0.00.040.768 I print_info: freq_scale_train = 1
0.00.040.768 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.768 I print_info: rope_finetuned   = unknown
0.00.040.768 I print_info: ssm_d_conv       = 0
0.00.040.768 I print_info: ssm_d_inner      = 0
0.00.040.768 I print_info: ssm_d_state      = 0
0.00.040.768 I print_info: ssm_dt_rank      = 0
0.00.040.769 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.773 I print_info: model type       = 1.4B
0.00.040.773 I print_info: model params     = 1.41 B
0.00.040.773 I print_info: general.name     = 1.4B
0.00.040.774 I print_info: vocab type       = BPE
0.00.040.774 I print_info: n_vocab          = 50304
0.00.040.774 I print_info: n_merges         = 50009
0.00.040.774 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.774 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.774 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.776 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.776 I print_info: LF token         = 187 'Ċ'
0.00.040.776 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.777 I print_info: max token length = 1024
0.00.532.947 I load_tensors: offloading 24 repeating layers to GPU
0.00.532.978 I load_tensors: offloading output layer to GPU
0.00.532.980 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.012 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.014 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.534.206 I llama_init_from_model: n_seq_max     = 1
0.00.534.214 I llama_init_from_model: n_ctx         = 128
0.00.534.215 I llama_init_from_model: n_ctx_per_seq = 128
0.00.534.215 I llama_init_from_model: n_batch       = 128
0.00.534.216 I llama_init_from_model: n_ubatch      = 128
0.00.534.216 I llama_init_from_model: flash_attn    = 0
0.00.534.219 I llama_init_from_model: freq_base     = 10000.0
0.00.534.219 I llama_init_from_model: freq_scale    = 1
0.00.534.220 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.534.223 I ggml_metal_init: allocating
0.00.534.315 I ggml_metal_init: found device: Apple M4
0.00.534.330 I ggml_metal_init: picking default device: Apple M4
0.00.536.192 I ggml_metal_init: using embedded metal library
0.00.542.245 I ggml_metal_init: GPU name:   Apple M4
0.00.542.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.542.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.542.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.542.254 I ggml_metal_init: simdgroup reduction   = true
0.00.542.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.542.255 I ggml_metal_init: has residency sets    = true
0.00.542.255 I ggml_metal_init: has bfloat            = true
0.00.542.255 I ggml_metal_init: use bfloat            = true
0.00.542.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.542.267 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.155 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.564.852 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.564.856 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.564.900 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.568.175 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.568.177 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.568.178 I llama_init_from_model: graph nodes  = 967
0.00.568.178 I llama_init_from_model: graph splits = 2
0.00.568.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.568.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.028 I 
0.00.594.088 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.094 I perplexity: tokenizing the input ..
0.00.600.203 I perplexity: tokenization took 6.107 ms
0.00.600.207 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.734.049 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.735.463 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.735.501 I llama_perf_context_print:        load time =     583.20 ms
0.00.735.502 I llama_perf_context_print: prompt eval time =     133.57 ms /   128 tokens (    1.04 ms per token,   958.28 tokens per second)
0.00.735.503 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.735.503 I llama_perf_context_print:       total time =     141.47 ms /   129 tokens
0.00.735.921 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.078s
sys	0m0.134s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.012.187 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.561 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.578 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.337 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.091 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.094 I llama_model_loader: - type  f32:  194 tensors
0.00.028.094 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.094 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.095 I print_info: file format = GGUF V3 (latest)
0.00.028.095 I print_info: file type   = Q5_K - Medium
0.00.028.100 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.036.227 I load: special tokens cache size = 25
0.00.042.350 I load: token to piece cache size = 0.2984 MB
0.00.042.353 I print_info: arch             = gptneox
0.00.042.353 I print_info: vocab_only       = 0
0.00.042.353 I print_info: n_ctx_train      = 2048
0.00.042.353 I print_info: n_embd           = 2048
0.00.042.354 I print_info: n_layer          = 24
0.00.042.356 I print_info: n_head           = 16
0.00.042.357 I print_info: n_head_kv        = 16
0.00.042.357 I print_info: n_rot            = 32
0.00.042.357 I print_info: n_swa            = 0
0.00.042.358 I print_info: n_embd_head_k    = 128
0.00.042.358 I print_info: n_embd_head_v    = 128
0.00.042.359 I print_info: n_gqa            = 1
0.00.042.359 I print_info: n_embd_k_gqa     = 2048
0.00.042.360 I print_info: n_embd_v_gqa     = 2048
0.00.042.362 I print_info: f_norm_eps       = 1.0e-05
0.00.042.363 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.363 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.363 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.363 I print_info: f_logit_scale    = 0.0e+00
0.00.042.364 I print_info: n_ff             = 8192
0.00.042.364 I print_info: n_expert         = 0
0.00.042.364 I print_info: n_expert_used    = 0
0.00.042.364 I print_info: causal attn      = 1
0.00.042.364 I print_info: pooling type     = 0
0.00.042.366 I print_info: rope type        = 2
0.00.042.368 I print_info: rope scaling     = linear
0.00.042.368 I print_info: freq_base_train  = 10000.0
0.00.042.368 I print_info: freq_scale_train = 1
0.00.042.368 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.369 I print_info: rope_finetuned   = unknown
0.00.042.369 I print_info: ssm_d_conv       = 0
0.00.042.369 I print_info: ssm_d_inner      = 0
0.00.042.369 I print_info: ssm_d_state      = 0
0.00.042.369 I print_info: ssm_dt_rank      = 0
0.00.042.369 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.369 I print_info: model type       = 1.4B
0.00.042.370 I print_info: model params     = 1.41 B
0.00.042.370 I print_info: general.name     = 1.4B
0.00.042.371 I print_info: vocab type       = BPE
0.00.042.371 I print_info: n_vocab          = 50304
0.00.042.371 I print_info: n_merges         = 50009
0.00.042.371 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.371 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.373 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.373 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.374 I print_info: LF token         = 187 'Ċ'
0.00.042.374 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.374 I print_info: max token length = 1024
0.00.619.181 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.195 I load_tensors: offloading output layer to GPU
0.00.619.196 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.230 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.619.232 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.620.808 I llama_init_from_model: n_seq_max     = 1
0.00.620.814 I llama_init_from_model: n_ctx         = 2048
0.00.620.814 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.620.815 I llama_init_from_model: n_batch       = 2048
0.00.620.815 I llama_init_from_model: n_ubatch      = 512
0.00.620.815 I llama_init_from_model: flash_attn    = 0
0.00.620.817 I llama_init_from_model: freq_base     = 10000.0
0.00.620.818 I llama_init_from_model: freq_scale    = 1
0.00.620.824 I ggml_metal_init: allocating
0.00.620.908 I ggml_metal_init: found device: Apple M4
0.00.620.923 I ggml_metal_init: picking default device: Apple M4
0.00.622.618 I ggml_metal_init: using embedded metal library
0.00.629.133 I ggml_metal_init: GPU name:   Apple M4
0.00.629.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.139 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.140 I ggml_metal_init: simdgroup reduction   = true
0.00.629.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.140 I ggml_metal_init: has residency sets    = true
0.00.629.141 I ggml_metal_init: has bfloat            = true
0.00.629.141 I ggml_metal_init: use bfloat            = true
0.00.629.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.143 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.105 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.410 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.417 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.451 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.704.001 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.704.002 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.704.003 I llama_init_from_model: graph nodes  = 967
0.00.704.003 I llama_init_from_model: graph splits = 2
0.00.704.008 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.128 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.129 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.482 I main: llama threadpool init, n_threads = 4
0.00.770.526 I 
0.00.770.551 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.552 I 
0.00.770.707 I sampler seed: 1234
0.00.770.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.723 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.723 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.723 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.614.082 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.01.614.083 I llama_perf_context_print:        load time =     757.38 ms
0.01.614.084 I llama_perf_context_print: prompt eval time =      51.23 ms /     7 tokens (    7.32 ms per token,   136.65 tokens per second)
0.01.614.085 I llama_perf_context_print:        eval time =     789.33 ms /    63 runs   (   12.53 ms per token,    79.81 tokens per second)
0.01.614.085 I llama_perf_context_print:       total time =     844.51 ms /    70 tokens
0.01.614.351 I ggml_metal_free: deallocating

real	0m1.633s
user	0m0.108s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.282 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.080 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.086 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.087 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.088 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.088 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.088 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.089 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.090 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.090 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.090 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.091 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.091 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.092 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.092 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.094 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.067 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.121 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.131 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.132 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.133 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.133 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.133 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.134 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.135 I llama_model_loader: - type  f32:  194 tensors
0.00.025.135 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.135 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.136 I print_info: file format = GGUF V3 (latest)
0.00.025.136 I print_info: file type   = Q5_K - Medium
0.00.025.140 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.576 I load: special tokens cache size = 25
0.00.039.740 I load: token to piece cache size = 0.2984 MB
0.00.039.744 I print_info: arch             = gptneox
0.00.039.744 I print_info: vocab_only       = 0
0.00.039.745 I print_info: n_ctx_train      = 2048
0.00.039.745 I print_info: n_embd           = 2048
0.00.039.745 I print_info: n_layer          = 24
0.00.039.750 I print_info: n_head           = 16
0.00.039.751 I print_info: n_head_kv        = 16
0.00.039.751 I print_info: n_rot            = 32
0.00.039.751 I print_info: n_swa            = 0
0.00.039.751 I print_info: n_embd_head_k    = 128
0.00.039.753 I print_info: n_embd_head_v    = 128
0.00.039.754 I print_info: n_gqa            = 1
0.00.039.754 I print_info: n_embd_k_gqa     = 2048
0.00.039.755 I print_info: n_embd_v_gqa     = 2048
0.00.039.755 I print_info: f_norm_eps       = 1.0e-05
0.00.039.757 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.757 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.757 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.757 I print_info: f_logit_scale    = 0.0e+00
0.00.039.759 I print_info: n_ff             = 8192
0.00.039.760 I print_info: n_expert         = 0
0.00.039.760 I print_info: n_expert_used    = 0
0.00.039.760 I print_info: causal attn      = 1
0.00.039.761 I print_info: pooling type     = 0
0.00.039.761 I print_info: rope type        = 2
0.00.039.761 I print_info: rope scaling     = linear
0.00.039.761 I print_info: freq_base_train  = 10000.0
0.00.039.762 I print_info: freq_scale_train = 1
0.00.039.762 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.762 I print_info: rope_finetuned   = unknown
0.00.039.762 I print_info: ssm_d_conv       = 0
0.00.039.762 I print_info: ssm_d_inner      = 0
0.00.039.762 I print_info: ssm_d_state      = 0
0.00.039.762 I print_info: ssm_dt_rank      = 0
0.00.039.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.763 I print_info: model type       = 1.4B
0.00.039.763 I print_info: model params     = 1.41 B
0.00.039.763 I print_info: general.name     = 1.4B
0.00.039.764 I print_info: vocab type       = BPE
0.00.039.764 I print_info: n_vocab          = 50304
0.00.039.764 I print_info: n_merges         = 50009
0.00.039.764 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.765 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.765 I print_info: LF token         = 187 'Ċ'
0.00.039.770 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.770 I print_info: max token length = 1024
0.00.607.413 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.426 I load_tensors: offloading output layer to GPU
0.00.607.427 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.463 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.607.472 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.609.054 I llama_init_from_model: n_seq_max     = 1
0.00.609.057 I llama_init_from_model: n_ctx         = 128
0.00.609.058 I llama_init_from_model: n_ctx_per_seq = 128
0.00.609.061 I llama_init_from_model: n_batch       = 128
0.00.609.062 I llama_init_from_model: n_ubatch      = 128
0.00.609.062 I llama_init_from_model: flash_attn    = 0
0.00.609.064 I llama_init_from_model: freq_base     = 10000.0
0.00.609.064 I llama_init_from_model: freq_scale    = 1
0.00.609.065 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.069 I ggml_metal_init: allocating
0.00.609.086 I ggml_metal_init: found device: Apple M4
0.00.609.099 I ggml_metal_init: picking default device: Apple M4
0.00.610.474 I ggml_metal_init: using embedded metal library
0.00.617.498 I ggml_metal_init: GPU name:   Apple M4
0.00.617.502 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.504 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.505 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.505 I ggml_metal_init: simdgroup reduction   = true
0.00.617.506 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.506 I ggml_metal_init: has residency sets    = true
0.00.617.506 I ggml_metal_init: has bfloat            = true
0.00.617.506 I ggml_metal_init: use bfloat            = true
0.00.617.507 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.512 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.144 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.638.550 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.638.553 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.638.597 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.641.731 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.641.733 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.641.734 I llama_init_from_model: graph nodes  = 967
0.00.641.734 I llama_init_from_model: graph splits = 2
0.00.641.737 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.641.737 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.522 I 
0.00.678.611 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.620 I perplexity: tokenizing the input ..
0.00.685.023 I perplexity: tokenization took 6.401 ms
0.00.685.032 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.522 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.826.852 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.826.876 I llama_perf_context_print:        load time =     669.23 ms
0.00.826.876 I llama_perf_context_print: prompt eval time =     140.09 ms /   128 tokens (    1.09 ms per token,   913.70 tokens per second)
0.00.826.877 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.878 I llama_perf_context_print:       total time =     148.36 ms /   129 tokens
0.00.827.239 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.079s
sys	0m0.146s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.624 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.065 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.071 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.072 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.072 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.072 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.072 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.073 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.074 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.074 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.075 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.075 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.075 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.076 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.078 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.079 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.985 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.900 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.901 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.901 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.901 I llama_model_loader: - type  f32:  194 tensors
0.00.025.902 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.902 I print_info: file format = GGUF V3 (latest)
0.00.025.903 I print_info: file type   = Q6_K
0.00.025.904 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.745 I load: special tokens cache size = 25
0.00.039.755 I load: token to piece cache size = 0.2984 MB
0.00.039.758 I print_info: arch             = gptneox
0.00.039.758 I print_info: vocab_only       = 0
0.00.039.758 I print_info: n_ctx_train      = 2048
0.00.039.758 I print_info: n_embd           = 2048
0.00.039.758 I print_info: n_layer          = 24
0.00.039.761 I print_info: n_head           = 16
0.00.039.762 I print_info: n_head_kv        = 16
0.00.039.762 I print_info: n_rot            = 32
0.00.039.763 I print_info: n_swa            = 0
0.00.039.763 I print_info: n_embd_head_k    = 128
0.00.039.763 I print_info: n_embd_head_v    = 128
0.00.039.764 I print_info: n_gqa            = 1
0.00.039.764 I print_info: n_embd_k_gqa     = 2048
0.00.039.766 I print_info: n_embd_v_gqa     = 2048
0.00.039.766 I print_info: f_norm_eps       = 1.0e-05
0.00.039.767 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.767 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.769 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.769 I print_info: f_logit_scale    = 0.0e+00
0.00.039.769 I print_info: n_ff             = 8192
0.00.039.769 I print_info: n_expert         = 0
0.00.039.770 I print_info: n_expert_used    = 0
0.00.039.770 I print_info: causal attn      = 1
0.00.039.770 I print_info: pooling type     = 0
0.00.039.770 I print_info: rope type        = 2
0.00.039.770 I print_info: rope scaling     = linear
0.00.039.771 I print_info: freq_base_train  = 10000.0
0.00.039.771 I print_info: freq_scale_train = 1
0.00.039.772 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.773 I print_info: rope_finetuned   = unknown
0.00.039.774 I print_info: ssm_d_conv       = 0
0.00.039.774 I print_info: ssm_d_inner      = 0
0.00.039.774 I print_info: ssm_d_state      = 0
0.00.039.775 I print_info: ssm_dt_rank      = 0
0.00.039.775 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.775 I print_info: model type       = 1.4B
0.00.039.775 I print_info: model params     = 1.41 B
0.00.039.775 I print_info: general.name     = 1.4B
0.00.039.776 I print_info: vocab type       = BPE
0.00.039.776 I print_info: n_vocab          = 50304
0.00.039.776 I print_info: n_merges         = 50009
0.00.039.777 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.777 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.781 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.781 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.781 I print_info: LF token         = 187 'Ċ'
0.00.039.781 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.782 I print_info: max token length = 1024
0.00.660.439 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.442 I load_tensors: offloading output layer to GPU
0.00.660.443 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.468 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.660.471 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.661.690 I llama_init_from_model: n_seq_max     = 1
0.00.661.692 I llama_init_from_model: n_ctx         = 2048
0.00.661.693 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.661.693 I llama_init_from_model: n_batch       = 2048
0.00.661.693 I llama_init_from_model: n_ubatch      = 512
0.00.661.694 I llama_init_from_model: flash_attn    = 0
0.00.661.695 I llama_init_from_model: freq_base     = 10000.0
0.00.661.695 I llama_init_from_model: freq_scale    = 1
0.00.661.696 I ggml_metal_init: allocating
0.00.661.742 I ggml_metal_init: found device: Apple M4
0.00.661.754 I ggml_metal_init: picking default device: Apple M4
0.00.663.128 I ggml_metal_init: using embedded metal library
0.00.668.764 I ggml_metal_init: GPU name:   Apple M4
0.00.668.767 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.668.768 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.668.769 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.668.769 I ggml_metal_init: simdgroup reduction   = true
0.00.668.770 I ggml_metal_init: simdgroup matrix mul. = true
0.00.668.770 I ggml_metal_init: has residency sets    = true
0.00.668.770 I ggml_metal_init: has bfloat            = true
0.00.668.770 I ggml_metal_init: use bfloat            = true
0.00.668.771 I ggml_metal_init: hasUnifiedMemory      = true
0.00.668.772 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.684.744 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.941 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.951 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.990 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.145 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.742.148 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.742.148 I llama_init_from_model: graph nodes  = 967
0.00.742.148 I llama_init_from_model: graph splits = 2
0.00.742.154 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.742.288 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.742.288 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.810.983 I main: llama threadpool init, n_threads = 4
0.00.811.024 I 
0.00.811.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.050 I 
0.00.811.224 I sampler seed: 1234
0.00.811.228 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.267 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.268 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.271 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.689.556 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49789.62 tokens per second)
0.01.689.556 I llama_perf_context_print:        load time =     801.39 ms
0.01.689.557 I llama_perf_context_print: prompt eval time =      54.32 ms /     7 tokens (    7.76 ms per token,   128.86 tokens per second)
0.01.689.558 I llama_perf_context_print:        eval time =     821.25 ms /    63 runs   (   13.04 ms per token,    76.71 tokens per second)
0.01.689.558 I llama_perf_context_print:       total time =     879.54 ms /    70 tokens
0.01.689.834 I ggml_metal_free: deallocating

real	0m1.706s
user	0m0.106s
sys	0m0.230s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4616 (84ec8a58) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.430 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.384 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.392 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.393 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.393 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.395 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.395 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.395 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.396 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.396 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.396 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.397 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.400 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.401 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.401 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.060 I llama_model_loader: - type  f32:  194 tensors
0.00.026.061 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.061 I print_info: file format = GGUF V3 (latest)
0.00.026.062 I print_info: file type   = Q6_K
0.00.026.063 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.191 I load: special tokens cache size = 25
0.00.040.071 I load: token to piece cache size = 0.2984 MB
0.00.040.074 I print_info: arch             = gptneox
0.00.040.074 I print_info: vocab_only       = 0
0.00.040.074 I print_info: n_ctx_train      = 2048
0.00.040.074 I print_info: n_embd           = 2048
0.00.040.075 I print_info: n_layer          = 24
0.00.040.078 I print_info: n_head           = 16
0.00.040.080 I print_info: n_head_kv        = 16
0.00.040.080 I print_info: n_rot            = 32
0.00.040.081 I print_info: n_swa            = 0
0.00.040.081 I print_info: n_embd_head_k    = 128
0.00.040.081 I print_info: n_embd_head_v    = 128
0.00.040.082 I print_info: n_gqa            = 1
0.00.040.082 I print_info: n_embd_k_gqa     = 2048
0.00.040.083 I print_info: n_embd_v_gqa     = 2048
0.00.040.084 I print_info: f_norm_eps       = 1.0e-05
0.00.040.084 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.084 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.085 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.085 I print_info: f_logit_scale    = 0.0e+00
0.00.040.085 I print_info: n_ff             = 8192
0.00.040.086 I print_info: n_expert         = 0
0.00.040.086 I print_info: n_expert_used    = 0
0.00.040.086 I print_info: causal attn      = 1
0.00.040.086 I print_info: pooling type     = 0
0.00.040.086 I print_info: rope type        = 2
0.00.040.086 I print_info: rope scaling     = linear
0.00.040.087 I print_info: freq_base_train  = 10000.0
0.00.040.087 I print_info: freq_scale_train = 1
0.00.040.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.088 I print_info: rope_finetuned   = unknown
0.00.040.088 I print_info: ssm_d_conv       = 0
0.00.040.088 I print_info: ssm_d_inner      = 0
0.00.040.088 I print_info: ssm_d_state      = 0
0.00.040.088 I print_info: ssm_dt_rank      = 0
0.00.040.089 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.089 I print_info: model type       = 1.4B
0.00.040.089 I print_info: model params     = 1.41 B
0.00.040.089 I print_info: general.name     = 1.4B
0.00.040.090 I print_info: vocab type       = BPE
0.00.040.090 I print_info: n_vocab          = 50304
0.00.040.090 I print_info: n_merges         = 50009
0.00.040.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.091 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.091 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.091 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.091 I print_info: LF token         = 187 'Ċ'
0.00.040.092 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.092 I print_info: max token length = 1024
0.00.613.216 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.220 I load_tensors: offloading output layer to GPU
0.00.613.221 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.246 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.613.247 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.614.452 I llama_init_from_model: n_seq_max     = 1
0.00.614.454 I llama_init_from_model: n_ctx         = 128
0.00.614.455 I llama_init_from_model: n_ctx_per_seq = 128
0.00.614.455 I llama_init_from_model: n_batch       = 128
0.00.614.456 I llama_init_from_model: n_ubatch      = 128
0.00.614.456 I llama_init_from_model: flash_attn    = 0
0.00.614.457 I llama_init_from_model: freq_base     = 10000.0
0.00.614.457 I llama_init_from_model: freq_scale    = 1
0.00.614.458 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.614.460 I ggml_metal_init: allocating
0.00.614.500 I ggml_metal_init: found device: Apple M4
0.00.614.515 I ggml_metal_init: picking default device: Apple M4
0.00.615.925 I ggml_metal_init: using embedded metal library
0.00.621.840 I ggml_metal_init: GPU name:   Apple M4
0.00.621.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.846 I ggml_metal_init: simdgroup reduction   = true
0.00.621.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.847 I ggml_metal_init: has residency sets    = true
0.00.621.847 I ggml_metal_init: has bfloat            = true
0.00.621.847 I ggml_metal_init: use bfloat            = true
0.00.621.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.852 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.605 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.125 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.129 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.260 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.645.262 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.645.262 I llama_init_from_model: graph nodes  = 967
0.00.645.263 I llama_init_from_model: graph splits = 2
0.00.645.265 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.825 I 
0.00.682.912 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.920 I perplexity: tokenizing the input ..
0.00.689.672 I perplexity: tokenization took 6.748 ms
0.00.689.678 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.325 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.839.674 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.839.698 I llama_perf_context_print:        load time =     672.39 ms
0.00.839.699 I llama_perf_context_print: prompt eval time =     147.96 ms /   128 tokens (    1.16 ms per token,   865.12 tokens per second)
0.00.839.700 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.839.700 I llama_perf_context_print:       total time =     156.88 ms /   129 tokens
0.00.840.072 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.077s
sys	0m0.150s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4616 (84ec8a58)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1599080b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159908570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159908b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1599090d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x159909680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159909c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15990a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15990a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15990ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15990b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15990b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15990bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15990c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15990cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15990d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15990de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15990e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15990ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15990f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15990fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159910290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1599109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1599110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159911970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159912090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159912350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159912960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1599135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159913b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159913dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159914270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159914530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159914dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159915300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1599155c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159915a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159915f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1599163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159916840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159916ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159917180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159917620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159917ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159917f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x159918220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159918830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159918e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159919760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159919d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15991a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15991a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15991afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15991b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15991bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15991c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15991c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15991ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15991cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15991d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15991ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15991e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15991e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15991e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15991ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15991f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15991f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15991fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1599200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159920570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159920a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159920eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159921350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1599217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159921d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159922290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1599227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159922d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159923280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1599237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159923d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159924270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1599247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159924d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159925260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1599257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159925d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159926250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1599267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159926cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159927240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159927790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159927ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159928230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159928780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159928cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159929220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159929770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159919450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159929be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15992a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15992a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15992ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15992b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15992b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15992be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15992c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15992c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15992ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15992d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15992d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15992de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15992e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15992e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15992ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15992f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15992f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15992fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15992ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159930460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159930900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159930da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159931240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1599316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159931b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159932020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1599324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159932960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159932e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1599332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159933740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159933be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159934080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159934520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1599349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159934e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159935300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1599357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159935c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1599360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159936580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159936a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159936ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159937360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159937800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159937ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159938140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1599385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159938a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159938f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1599393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159939860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159939d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15993a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15993a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15993aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15993af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15993b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15993b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15993bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15993c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15993c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15993cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15993cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15993d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15993d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15993ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15993e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15993e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15993eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15993f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15993f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15993f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15993fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1599402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159940760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159940c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1599410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159941540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1599419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159941e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159942320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1599427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159942c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159943100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1599435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159943a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159943ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159944380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159944820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159944cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159945160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159945600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159945aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159945ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159946540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159946a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159946fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1599472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1599478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159947ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1599484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159948cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159949160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159949420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159949a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15994a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15994a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15994acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15994b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15994b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15994bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15994c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15994c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15994cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15994d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15994d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15994dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15994e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15994e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15994ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15994f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15994f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15994fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1599502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159950820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159950d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1599512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159951810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159951d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1599522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159952800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159952d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1599532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1599537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159953d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159954290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1599547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159954d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159955280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1599557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159955d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159956270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1599567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159956d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159957260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1599577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159957d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159958250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1599587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159958cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159959240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159959790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159959ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15995a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15995a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15995acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15995b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15995b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15995bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15995c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15995c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15995ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15995d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15995d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15995dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15995e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15995e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15995ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15995f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15995f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15995f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15995fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159960300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1599607a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159960c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1599610e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159961580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159961a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159961ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159962360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159962800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159962ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1599631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159963910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159964030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159964750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159964e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159965130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159965920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159965be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1599661f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.733.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.733.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c304d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c3051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c305630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c305aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c305f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c306380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c3067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c306c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c3070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c307540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c3079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c3080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c308bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c309370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c309b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c30a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c30a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c30b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c30b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c30bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c30c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c30cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c30d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c30dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c30e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c30e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c30e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c30ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c30f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c30f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c30fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c30ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c3103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c310670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c310ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c310f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c3113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c311830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c311ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c312110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c312580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c3129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c312e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c3132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c313740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c313bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c314020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c314490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c314900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c314d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c3151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c315650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c315ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c315f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c3163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c316810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c316d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c317280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c3176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c317b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c317fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c318440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c3188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c318d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c319190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c319600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c319a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c319ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c31a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c31a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c31ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c31b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c31b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c31b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c31bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c31c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c31c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c31cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c31cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c31d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c31d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c31dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c31e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c31e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c31ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c31eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c31f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c31f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c31fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c320080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c3204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c320960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c320dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c321240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c3216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c321b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c321f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c322400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c322870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c322ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c323150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c3235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c323a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c323ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c324310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c324780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c324bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c325060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c3254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c325940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c325db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c326220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c326690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c326b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c326f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c3273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c327850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c327cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c328130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c3285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c328a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c328e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c3292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c329760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c329bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c32a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c32a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c32a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c32ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c32b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c32b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c32bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c32bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c32c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c32c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c32cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c32d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c32d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c32d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c32de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c32e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c32e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c32ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c32f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c32f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c32f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c32fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c3301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c330650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c330ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c330f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c3313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c331810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c331c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c3320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c332560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c3329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c332e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c3332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c333720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c333b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c334000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c334470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c3348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c334d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c3351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c335df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c3360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c336370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c3367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c336c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c3370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c337530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c3379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c337e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c338280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c3386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c338b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c338fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c339440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c3398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c339d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c33a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c33a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c33aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c33aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c33b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c33b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c33bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c33c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c33c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c33c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c33cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c33d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c33d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c33db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c33dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c33e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c33e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c33ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c33f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c33f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c33fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c340050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c3404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c340930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c340da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c341210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c341730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c341c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c3427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c342a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c343030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c3435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c343bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c344170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c344730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c344cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c3452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c345870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c345e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c3463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c3469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c346f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c347530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c347af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c3480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c348670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c348c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c3491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c3497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c349d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c34a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c34a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c34aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c34b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c34ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c34bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c34c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c34cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c34d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c34d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c34dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c34e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c34e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c34edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c34f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c34f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c34ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c3504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c350ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c351070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c351630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c351bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c3521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c352770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c352d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c3532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c3538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c353e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c354430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c3549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c354fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c355570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c355b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c3560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c3566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c356c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c357170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c357670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c357b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c358070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c358570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c358a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c358f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c359470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c359970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c359e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c35a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c35a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c35ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c35b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c35b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c35c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c35c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c35cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c35d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c35d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c35e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c35e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c35ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13fa044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13fa04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13fa04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13fa05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13fa056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13fa05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13fa05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13fa063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13fa06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13fa06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13fa07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13fa078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13fa083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13fa08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13fa09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13fa09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13fa0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13fa0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13fa0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13fa0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13fa0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13fa0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13fa0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13fa0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13fa0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13fa0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13fa0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13fa0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13fa0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13fa0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13fa0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13fa0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13fa0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13fa0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13fa10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13fa107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13fa10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13fa110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13fa11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13fa119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13fa11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13fa12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13fa12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13fa12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13fa12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13fa13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13fa138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13fa13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13fa141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13fa14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13fa14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13fa14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13fa15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13fa157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13fa15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13fa160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13fa16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13fa16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13fa16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13fa17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13fa17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13fa17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13fa18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13fa185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13fa18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13fa18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13fa19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13fa19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13fa19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13fa1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13fa1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13fa1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13fa1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13fa1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13fa1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13fa1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13fa1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13fa1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13fa1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13fa1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13fa1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13fa1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13fa1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13fa1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13fa1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13fa1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13fa1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13fa1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13fa1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13fa1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13fa1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13fa20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13fa20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13fa20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13fa20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13fa213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13fa21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13fa21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13fa22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13fa22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13fa229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13fa22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13fa232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13fa23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13fa23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13fa24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13fa24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13fa24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13fa24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13fa25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13fa258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13fa25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13fa261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13fa26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13fa26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13fa26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13fa27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13fa277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13fa27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13fa280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13fa28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13fa28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13fa28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13fa29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13fa296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13fa29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13fa29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13fa2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13fa2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13fa2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13fa2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13fa2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13fa2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13fa2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13fa2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13fa2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13fa2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13fa2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13fa2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13fa2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13fa2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13fa2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13fa2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13fa2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13fa2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13fa2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13fa2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13fa2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13fa30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13fa305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13fa30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13fa30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13fa31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13fa31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13fa31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13fa32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13fa324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13fa32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13fa32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13fa33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13fa336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13fa33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13fa33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13fa343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13fa34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13fa34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13fa35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13fa355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13fa35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13fa35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13fa36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13fa36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13fa36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13fa37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13fa374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13fa37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13fa37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13fa38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13fa38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13fa38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13fa38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13fa393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13fa39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13fa39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13fa3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13fa3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13fa3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13fa3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13fa3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13fa3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13fa3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13fa3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13fa3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13fa3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13fa3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13fa3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13fa3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13fa3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13fa3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13fa3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13fa3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13fa3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13fa3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13fa3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13fa3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13fa3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13fa402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13fa40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13fa40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13fa41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13fa41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13fa41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13fa42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13fa42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13fa429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13fa42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13fa432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13fa43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13fa43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13fa44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13fa44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13fa44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13fa44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13fa451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13fa45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13fa45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13fa45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13fa463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13fa46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13fa46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13fa470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13fa47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13fa479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13fa47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13fa482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13fa48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13fa48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13fa49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13fa49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13fa498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13fa49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13fa4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13fa4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13fa4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13fa4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13fa4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13fa4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13fa4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13fa4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13fa4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13fa4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13fa4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13fa4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13fa4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13fa4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13fa4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13fa4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13fa4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13fa4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13fa4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13fa4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13fa4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13fa4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13fa50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13fa507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13fa50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13fa510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13fa51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13fa51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13fa51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13fa52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13fa526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13fa52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13fa52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13fa53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13fa538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13fa53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13fa54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13fa545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13fa54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13fa54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13fa55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13fa557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13fa56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13fa56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13fa57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13fa57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13fa57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13fa57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13fa584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13fa58ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.791s
user	0m0.284s
sys	0m0.326s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4616 (84ec8a58)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146f0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146f0db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146f0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146f0e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146f0ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146f0f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146f0f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146f0fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146f10330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146f10830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146f10d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146f11230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146f11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146f12500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146f12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146f13430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146f13b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146f14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146f14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146f15160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146f15880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146f15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146f166c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146f16f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146f17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146f17940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146f17f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146f18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146f19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146f193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146f19860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146f19b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146f1a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146f1a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146f1abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146f1b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146f1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146f1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146f1be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146f1c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146f1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146f1cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146f1d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146f1d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146f1d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146f1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146f1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146f1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146f1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146f1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146f1ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146f20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146f211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146f219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146f21e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146f222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146f225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146f22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146f233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146f23660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146f23b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146f23fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146f24440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146f248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146f24d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146f25220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146f256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146f25b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146f26000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146f264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146f26940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146f26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146f27330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146f27880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146f27dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146f28320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146f28870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146f28dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146f29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146f29860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146f29db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146f2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146f2a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146f2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146f2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146f2b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146f2bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146f2c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146f2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146f2cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146f2d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146f2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146f2dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146f2e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146f2e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146f2ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146f1ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146f2f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146f2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146f2fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146f30970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146f30ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146f31410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146f31960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146f31eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146f32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146f32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146f32ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146f333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146f33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146f33e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146f34330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146f347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146f34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146f35110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146f355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146f35a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146f35ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146f36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146f36830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146f36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146f37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146f37610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146f37ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146f37f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146f383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146f38890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146f38d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146f391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146f39670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146f39b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146f39fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146f3a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146f3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146f3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146f3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146f3b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146f3bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146f3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146f3c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146f3c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146f3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146f3d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146f3d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146f3dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146f3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146f3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146f3e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146f3ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146f3f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146f3f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146f3fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146f400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146f40570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146f40a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146f40eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146f41350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146f417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146f41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146f42130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146f425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146f42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146f42f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146f433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146f43850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146f43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146f44190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146f44630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146f44ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146f44f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146f45410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146f458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146f45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146f461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146f46690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146f46b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146f46fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146f47470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146f47910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146f47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146f48250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146f486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146f48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146f49030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146f494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146f49970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146f49e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146f4a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146f4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146f4abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146f4b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146f4b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146f4bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146f4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146f4c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146f4c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146f4cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146f4d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146f4dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146f4e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146f4e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146f4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146f4f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146f4f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146f4fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146f50760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146f50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146f513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146f51900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146f51e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146f523a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146f528f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146f52e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146f53390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146f538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146f53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146f54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146f548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146f54e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146f55370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146f558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146f55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146f56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146f568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146f56e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146f57350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146f578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146f57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146f58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146f58de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146f59330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146f59880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146f59dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146f5a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146f5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146f5adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146f5b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146f5b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146f5bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146f5c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146f5c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146f5cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146f5d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146f5d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146f5dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146f5e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146f5e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146f5ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146f5f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146f5f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146f5fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146f602c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146f60810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146f60d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146f612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146f61800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146f61d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146f622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146f627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146f62d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146f63290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146f637e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146f63d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146f641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146f64670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146f64b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146f64fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146f65450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146f658f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146f65d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146f66230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146f666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146f66b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146f67010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146f674b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146f67950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146f67df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146f68290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146f687e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146f68f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146f69620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146f69d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146f6a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146f6a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146f6af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146f6b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146f6b7e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.917 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146f6b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146f4d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146f4cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146f4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146f20850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146f20240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146f22860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146f4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146f17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146f1e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146f1f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146f1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146f1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146f16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146f22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146f2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146f6a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146f19de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146f1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146f4f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146f4dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146f184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146f18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146f6bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146f6bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146f6c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146f6c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146f6c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146f6ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146f6ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146f6cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146f6d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146f6d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146f6d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146f6da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146f6dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146f6e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146f6e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146f6e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146f6e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146f6eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146f6edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146f6f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146f6f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146f6f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146f6f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146f6fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146f6fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146f70100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146f703c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146f70680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146f70940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146f70c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146f70ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146f71180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146f71440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146f71700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146f719c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146f71c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146f71f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146f72200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146f724c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146f72780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146f72a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146f72d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146f72fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146f73280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146f73540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146f73800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146f73ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146f73d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146f74040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146f74300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146f745c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146f74880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146f74b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146f74e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146f750c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146f75380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146f75640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146f75900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146f75bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146f75e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146f76140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146f76400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146f766c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146f76980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146f76c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146f76f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146f771c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146f77480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146f77740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146f77a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146f77cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146f77f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146f78240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146f78500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146f787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146f78a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146f78d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146f79000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146f792c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146f79580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146f79840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146f79b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146f79dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146f7a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146f7a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146f7a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146f7a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146f7ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146f7ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146f7b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146f7b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146f7b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146f7b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146f7bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146f7bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146f7c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146f7c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146f7c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146f7c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146f7cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146f7cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146f7d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146f7d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146f7d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146f7da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146f7dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146f7dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146f7e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146f7e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146f7e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146f7eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146f7ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146f7f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146f7f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146f7f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146f7f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146f7fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146f7fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146f800c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146f80380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146f80640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146f80900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146f80bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146f80e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146f81140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146f81400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146f816c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146f81980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146f81c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146f81f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146f821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146f82480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146f82740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146f82a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146f82cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146f82f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146f83240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146f83500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146f837c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146f83a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146f83d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146f84000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146f842c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146f84580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146f84840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146f84b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146f84dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146f85080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146f85340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146f85600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146f858c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146f85b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146f85e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146f86100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146f863c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146f86680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146f86940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146f86c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146f86ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146f87180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146f87440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146f87700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146f879c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146f87c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146f87f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146f88200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146f884c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146f88780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146f88a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146f88d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146f88fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146f89280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146f89540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146f89800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146f89ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146f89d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146f8a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146f8a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146f8a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146f8a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146f8adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146f8b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146f8b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146f8b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146f8be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146f8c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146f8c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146f8cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146f8d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146f8d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146f8d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146f8dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146f8e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146f8e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146f8eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146f8ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146f8f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146f8f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146f8fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146f900d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146f90540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146f909b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146f90e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146f91290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146f91700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146f91b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146f91fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146f92450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146f928c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146f92d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146f931a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146f93610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146f93a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146f93ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146f94360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146f947d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146f94c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146f950b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146f95520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146f95990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146f95e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146f96270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146f966e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146f96b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146f96fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146f97430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146f978a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146f97d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146f98180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146f985f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146f98a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146f98ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146f99340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146f997b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146f99c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146f9a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146f9a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146f9a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146f9ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146f9b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146f9b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146f9bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146f9bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146f9c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146f9c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146f9ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146f9d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146f9d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146f9da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146f9deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146f9e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146f9e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146f9ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146f9f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146f9f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146f9f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146f9fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146fa0230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146fa0ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146fa13c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146fa1ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146fa2200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146fa24c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146fa2cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146fa2f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146fa3580 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146e08840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146e08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146e09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146e09590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146e09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146e09e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146e0a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146e0a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146e0abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146e0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146e0b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146e0bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146e0c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146e0ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146e0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146e0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146e0e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146e0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146e0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146e0fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146e10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146e10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146e11070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146e11790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146e11eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146e12170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146e12430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146e128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146e12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146e13180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146e13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146e13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146e14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146e142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146e14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146e14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146e15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146e15600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146e15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146e16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146e16500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146e16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146e16f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146e17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146e17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146e17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146e181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146e18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146e18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146e18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146e193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146e19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146e19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146e1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146e1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146e1ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146e1b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146e1b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146e1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146e1c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146e1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146e1cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146e1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146e1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146e1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146e1de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146e1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146e1e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146e1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146e1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146e1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146e1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146e1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146e20400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146e20950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146e20ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146e213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146e21940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146e21e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146e223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146e22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146e22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146e233d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146e23920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146e23e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146e243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146e24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146e24e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146e253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146e25900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146e25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146e263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146e268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146e26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146e27390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146e278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146e27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146e28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146e288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146e28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146e29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146e298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146e29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146e2a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146e2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146e2ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146e2b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146e2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146e2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146e2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146e2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146e2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146e2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146e2d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146e2dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146e2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146e2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146e2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146e2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146e2f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146e2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146e2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146e30170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146e30610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146e30ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146e30f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146e313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146e31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146e31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146e321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146e32670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146e32b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146e32fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146e33450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146e338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146e33d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146e34230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146e346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146e34b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146e35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146e354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146e35950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146e35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146e36290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146e36730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146e36bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146e37070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146e37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146e379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146e37e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146e382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146e38790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146e38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146e390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146e39570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146e39a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146e39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146e3a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146e3a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146e3ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146e3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146e3b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146e3ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146e3bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146e3c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146e3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146e3ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146e3d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146e3d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146e3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146e3df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146e3e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146e3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146e3ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146e3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146e3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146e3fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146e3ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146e40470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146e40910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146e40db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146e41250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146e416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146e41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146e42030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146e424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146e42970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146e42e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146e432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146e43750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146e43bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146e44090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146e44530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146e44a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146e44fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146e45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146e45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146e45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146e46340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146e46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146e46f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146e47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146e47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146e47eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146e484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146e48ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146e492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146e49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146e49c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146e4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146e4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146e4ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146e4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146e4b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146e4bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146e4c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146e4c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146e4cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146e4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146e4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146e4dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146e4e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146e4e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146e4ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146e4f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146e4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146e4fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146e502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146e507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146e50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146e51290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146e517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146e51d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146e52280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146e527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146e52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146e53270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146e537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146e53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146e54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146e547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146e54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146e55250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146e557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146e55cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146e56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146e56790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146e56ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146e57230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146e57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146e57cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146e58220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146e58770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146e58cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146e59210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146e59760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146e59cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146e5a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146e5a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146e5aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146e5b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146e5b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146e5bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146e5c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146e5c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146e5cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146e5d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146e5d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146e5db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146e5dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146e5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146e5e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146e5ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146e5f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146e5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146e5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146e60010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146e604b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146e60950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146e60df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146e61290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146e61730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146e61c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146e623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146e62ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146e631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146e63900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146e63bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146e643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146e64670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146e64c80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.960s
user	0m0.235s
sys	0m0.186s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.76 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.19 sec*proc (2 tests)

Total Test time (real) =   2.20 sec
        2.23 real         0.51 user         0.26 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.55 real         0.13 user         0.08 sys
```
