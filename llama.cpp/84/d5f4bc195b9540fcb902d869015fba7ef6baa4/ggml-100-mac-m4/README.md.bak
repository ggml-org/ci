### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.43 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.60 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.26 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.29 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.91 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.81 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.24 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.88 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.39 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 164.97 sec*proc (29 tests)

Total Test time (real) = 164.98 sec

real	2m44.991s
user	4m39.277s
sys	0m5.684s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.76 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.44 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.44 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.32 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.11 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.27 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.39 sec*proc (29 tests)

Total Test time (real) =  48.40 sec

real	0m48.411s
user	0m54.644s
sys	0m5.207s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.117 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.518 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.980 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.990 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.991 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.992 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.993 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.993 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.995 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.995 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.996 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.997 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.997 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.001 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.001 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.002 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.003 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.003 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.004 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.005 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.453 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.455 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.456 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.456 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.457 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.457 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.458 I llama_model_loader: - type  f32:  124 tensors
0.00.027.458 I llama_model_loader: - type  f16:   73 tensors
0.00.027.459 I print_info: file format = GGUF V3 (latest)
0.00.027.460 I print_info: file type   = F16
0.00.027.461 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.702 I load: special tokens cache size = 5
0.00.033.832 I load: token to piece cache size = 0.2032 MB
0.00.033.836 I print_info: arch             = bert
0.00.033.836 I print_info: vocab_only       = 0
0.00.033.836 I print_info: n_ctx_train      = 512
0.00.033.837 I print_info: n_embd           = 384
0.00.033.837 I print_info: n_layer          = 12
0.00.033.840 I print_info: n_head           = 12
0.00.033.841 I print_info: n_head_kv        = 12
0.00.033.845 I print_info: n_rot            = 32
0.00.033.845 I print_info: n_swa            = 0
0.00.033.845 I print_info: n_embd_head_k    = 32
0.00.033.845 I print_info: n_embd_head_v    = 32
0.00.033.846 I print_info: n_gqa            = 1
0.00.033.847 I print_info: n_embd_k_gqa     = 384
0.00.033.848 I print_info: n_embd_v_gqa     = 384
0.00.033.848 I print_info: f_norm_eps       = 1.0e-12
0.00.033.849 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.849 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.849 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.850 I print_info: f_logit_scale    = 0.0e+00
0.00.033.851 I print_info: n_ff             = 1536
0.00.033.851 I print_info: n_expert         = 0
0.00.033.851 I print_info: n_expert_used    = 0
0.00.033.851 I print_info: causal attn      = 0
0.00.033.852 I print_info: pooling type     = 2
0.00.033.852 I print_info: rope type        = 2
0.00.033.852 I print_info: rope scaling     = linear
0.00.033.853 I print_info: freq_base_train  = 10000.0
0.00.033.853 I print_info: freq_scale_train = 1
0.00.033.853 I print_info: n_ctx_orig_yarn  = 512
0.00.033.854 I print_info: rope_finetuned   = unknown
0.00.033.854 I print_info: ssm_d_conv       = 0
0.00.033.854 I print_info: ssm_d_inner      = 0
0.00.033.854 I print_info: ssm_d_state      = 0
0.00.033.855 I print_info: ssm_dt_rank      = 0
0.00.033.856 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.856 I print_info: model type       = 33M
0.00.033.856 I print_info: model params     = 33.21 M
0.00.033.857 I print_info: general.name     = Bge Small
0.00.033.857 I print_info: vocab type       = WPM
0.00.033.858 I print_info: n_vocab          = 30522
0.00.033.858 I print_info: n_merges         = 0
0.00.033.860 I print_info: BOS token        = 101 '[CLS]'
0.00.033.860 I print_info: UNK token        = 100 '[UNK]'
0.00.033.861 I print_info: SEP token        = 102 '[SEP]'
0.00.033.861 I print_info: PAD token        = 0 '[PAD]'
0.00.033.861 I print_info: MASK token       = 103 '[MASK]'
0.00.033.861 I print_info: LF token         = 0 '[PAD]'
0.00.033.862 I print_info: max token length = 21
0.00.033.864 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.036.890 I load_tensors: offloading 12 repeating layers to GPU
0.00.036.892 I load_tensors: offloading output layer to GPU
0.00.036.892 I load_tensors: offloaded 13/13 layers to GPU
0.00.036.916 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.918 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.218 I llama_init_from_model: n_seq_max     = 1
0.00.037.220 I llama_init_from_model: n_ctx         = 512
0.00.037.220 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.220 I llama_init_from_model: n_batch       = 2048
0.00.037.220 I llama_init_from_model: n_ubatch      = 2048
0.00.037.221 I llama_init_from_model: flash_attn    = 0
0.00.037.221 I llama_init_from_model: freq_base     = 10000.0
0.00.037.222 I llama_init_from_model: freq_scale    = 1
0.00.037.222 I ggml_metal_init: allocating
0.00.037.235 I ggml_metal_init: found device: Apple M4
0.00.037.244 I ggml_metal_init: picking default device: Apple M4
0.00.038.046 I ggml_metal_init: using embedded metal library
0.00.042.126 I ggml_metal_init: GPU name:   Apple M4
0.00.042.129 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.130 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.130 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.131 I ggml_metal_init: simdgroup reduction   = true
0.00.042.131 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.131 I ggml_metal_init: has residency sets    = true
0.00.042.131 I ggml_metal_init: has bfloat            = true
0.00.042.131 I ggml_metal_init: use bfloat            = true
0.00.042.132 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.133 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.178 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.054.910 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.054.912 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.933 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.175 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.177 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.177 I llama_init_from_model: graph nodes  = 429
0.00.056.177 I llama_init_from_model: graph splits = 2
0.00.056.179 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.843 I 
0.00.061.870 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.551 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.067.683 I llama_perf_context_print:        load time =      45.32 ms
0.00.067.684 I llama_perf_context_print: prompt eval time =       4.99 ms /     9 tokens (    0.55 ms per token,  1805.42 tokens per second)
0.00.067.685 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.067.685 I llama_perf_context_print:       total time =       5.84 ms /    10 tokens
0.00.067.813 I ggml_metal_free: deallocating

real	0m0.247s
user	0m0.049s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.043 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.640 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.013.558 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.013.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.563 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.013.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.564 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.013.564 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.013.565 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.013.566 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.013.566 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.013.567 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.013.567 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.013.567 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.013.570 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.013.571 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.013.571 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.013.571 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.013.572 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.013.572 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.016.081 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.776 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.777 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.777 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.778 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.778 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.779 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.016.779 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.016.779 I llama_model_loader: - type  f32:  124 tensors
0.00.016.779 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.780 I print_info: file format = GGUF V3 (latest)
0.00.016.780 I print_info: file type   = Q8_0
0.00.016.781 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.019.434 I load: special tokens cache size = 5
0.00.020.750 I load: token to piece cache size = 0.2032 MB
0.00.020.754 I print_info: arch             = bert
0.00.020.754 I print_info: vocab_only       = 0
0.00.020.754 I print_info: n_ctx_train      = 512
0.00.020.755 I print_info: n_embd           = 384
0.00.020.755 I print_info: n_layer          = 12
0.00.020.758 I print_info: n_head           = 12
0.00.020.759 I print_info: n_head_kv        = 12
0.00.020.759 I print_info: n_rot            = 32
0.00.020.760 I print_info: n_swa            = 0
0.00.020.760 I print_info: n_embd_head_k    = 32
0.00.020.760 I print_info: n_embd_head_v    = 32
0.00.020.761 I print_info: n_gqa            = 1
0.00.020.761 I print_info: n_embd_k_gqa     = 384
0.00.020.762 I print_info: n_embd_v_gqa     = 384
0.00.020.763 I print_info: f_norm_eps       = 1.0e-12
0.00.020.763 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.020.764 I print_info: f_clamp_kqv      = 0.0e+00
0.00.020.764 I print_info: f_max_alibi_bias = 0.0e+00
0.00.020.764 I print_info: f_logit_scale    = 0.0e+00
0.00.020.765 I print_info: n_ff             = 1536
0.00.020.768 I print_info: n_expert         = 0
0.00.020.768 I print_info: n_expert_used    = 0
0.00.020.768 I print_info: causal attn      = 0
0.00.020.768 I print_info: pooling type     = 2
0.00.020.769 I print_info: rope type        = 2
0.00.020.769 I print_info: rope scaling     = linear
0.00.020.769 I print_info: freq_base_train  = 10000.0
0.00.020.769 I print_info: freq_scale_train = 1
0.00.020.769 I print_info: n_ctx_orig_yarn  = 512
0.00.020.770 I print_info: rope_finetuned   = unknown
0.00.020.770 I print_info: ssm_d_conv       = 0
0.00.020.770 I print_info: ssm_d_inner      = 0
0.00.020.770 I print_info: ssm_d_state      = 0
0.00.020.770 I print_info: ssm_dt_rank      = 0
0.00.020.770 I print_info: ssm_dt_b_c_rms   = 0
0.00.020.770 I print_info: model type       = 33M
0.00.020.771 I print_info: model params     = 33.21 M
0.00.020.771 I print_info: general.name     = Bge Small
0.00.020.772 I print_info: vocab type       = WPM
0.00.020.772 I print_info: n_vocab          = 30522
0.00.020.772 I print_info: n_merges         = 0
0.00.020.772 I print_info: BOS token        = 101 '[CLS]'
0.00.020.772 I print_info: UNK token        = 100 '[UNK]'
0.00.020.773 I print_info: SEP token        = 102 '[SEP]'
0.00.020.773 I print_info: PAD token        = 0 '[PAD]'
0.00.020.773 I print_info: MASK token       = 103 '[MASK]'
0.00.020.773 I print_info: LF token         = 0 '[PAD]'
0.00.020.773 I print_info: max token length = 21
0.00.020.774 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.022.821 I load_tensors: offloading 12 repeating layers to GPU
0.00.022.823 I load_tensors: offloading output layer to GPU
0.00.022.823 I load_tensors: offloaded 13/13 layers to GPU
0.00.022.832 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.022.833 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.023.080 I llama_init_from_model: n_seq_max     = 1
0.00.023.080 I llama_init_from_model: n_ctx         = 512
0.00.023.080 I llama_init_from_model: n_ctx_per_seq = 512
0.00.023.081 I llama_init_from_model: n_batch       = 2048
0.00.023.081 I llama_init_from_model: n_ubatch      = 2048
0.00.023.081 I llama_init_from_model: flash_attn    = 0
0.00.023.081 I llama_init_from_model: freq_base     = 10000.0
0.00.023.082 I llama_init_from_model: freq_scale    = 1
0.00.023.082 I ggml_metal_init: allocating
0.00.023.096 I ggml_metal_init: found device: Apple M4
0.00.023.099 I ggml_metal_init: picking default device: Apple M4
0.00.023.732 I ggml_metal_init: using embedded metal library
0.00.026.505 I ggml_metal_init: GPU name:   Apple M4
0.00.026.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.026.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.026.507 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.026.508 I ggml_metal_init: simdgroup reduction   = true
0.00.026.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.026.508 I ggml_metal_init: has residency sets    = true
0.00.026.508 I ggml_metal_init: has bfloat            = true
0.00.026.508 I ggml_metal_init: use bfloat            = true
0.00.026.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.026.516 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.037.261 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.037.935 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.037.937 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.037.951 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.039.112 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.039.113 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.039.113 I llama_init_from_model: graph nodes  = 429
0.00.039.114 I llama_init_from_model: graph splits = 2
0.00.039.116 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.039.116 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.043.660 I 
0.00.043.687 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.044.260 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.048.817 I llama_perf_context_print:        load time =      33.01 ms
0.00.048.818 I llama_perf_context_print: prompt eval time =       4.43 ms /     9 tokens (    0.49 ms per token,  2031.14 tokens per second)
0.00.048.819 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.048.819 I llama_perf_context_print:       total time =       5.16 ms /    10 tokens
0.00.049.027 I ggml_metal_free: deallocating

real	0m0.062s
user	0m0.032s
sys	0m0.018s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.253 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.772 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.221 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.227 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.229 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.231 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.231 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.232 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.233 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.234 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.235 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.235 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.236 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.239 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.240 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.240 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.241 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.939 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.281 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.281 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.282 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.282 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.282 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.283 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.283 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.283 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.284 I llama_model_loader: - type  f32:   40 tensors
0.00.050.284 I llama_model_loader: - type  f16:   30 tensors
0.00.050.285 I print_info: file format = GGUF V3 (latest)
0.00.050.285 I print_info: file type   = F16
0.00.050.287 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.690 W load: empty token at index 5
0.00.060.199 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.782 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.815 I load: special tokens cache size = 5
0.00.324.903 I load: token to piece cache size = 1.5060 MB
0.00.324.910 I print_info: arch             = jina-bert-v2
0.00.324.918 I print_info: vocab_only       = 0
0.00.324.919 I print_info: n_ctx_train      = 8192
0.00.324.921 I print_info: n_embd           = 384
0.00.324.921 I print_info: n_layer          = 4
0.00.324.931 I print_info: n_head           = 12
0.00.324.932 I print_info: n_head_kv        = 12
0.00.324.932 I print_info: n_rot            = 32
0.00.324.932 I print_info: n_swa            = 0
0.00.324.932 I print_info: n_embd_head_k    = 32
0.00.324.933 I print_info: n_embd_head_v    = 32
0.00.324.934 I print_info: n_gqa            = 1
0.00.324.934 I print_info: n_embd_k_gqa     = 384
0.00.324.936 I print_info: n_embd_v_gqa     = 384
0.00.324.937 I print_info: f_norm_eps       = 1.0e-12
0.00.324.939 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.324.940 I print_info: f_clamp_kqv      = 0.0e+00
0.00.324.940 I print_info: f_max_alibi_bias = 8.0e+00
0.00.324.940 I print_info: f_logit_scale    = 0.0e+00
0.00.324.941 I print_info: n_ff             = 1536
0.00.324.941 I print_info: n_expert         = 0
0.00.324.941 I print_info: n_expert_used    = 0
0.00.324.941 I print_info: causal attn      = 0
0.00.324.941 I print_info: pooling type     = -1
0.00.324.941 I print_info: rope type        = -1
0.00.324.941 I print_info: rope scaling     = linear
0.00.324.943 I print_info: freq_base_train  = 10000.0
0.00.324.943 I print_info: freq_scale_train = 1
0.00.324.943 I print_info: n_ctx_orig_yarn  = 8192
0.00.324.943 I print_info: rope_finetuned   = unknown
0.00.324.943 I print_info: ssm_d_conv       = 0
0.00.324.943 I print_info: ssm_d_inner      = 0
0.00.324.944 I print_info: ssm_d_state      = 0
0.00.324.944 I print_info: ssm_dt_rank      = 0
0.00.324.944 I print_info: ssm_dt_b_c_rms   = 0
0.00.324.944 I print_info: model type       = 33M
0.00.324.944 I print_info: model params     = 32.90 M
0.00.324.945 I print_info: general.name     = Jina Bert Implementation
0.00.324.949 I print_info: vocab type       = BPE
0.00.324.949 I print_info: n_vocab          = 61056
0.00.324.949 I print_info: n_merges         = 39382
0.00.324.949 I print_info: BOS token        = 0 '<s>'
0.00.324.950 I print_info: EOS token        = 2 '</s>'
0.00.324.951 I print_info: UNK token        = 3 '<unk>'
0.00.324.951 I print_info: SEP token        = 2 '</s>'
0.00.324.951 I print_info: PAD token        = 1 '<pad>'
0.00.324.951 I print_info: MASK token       = 4 '<mask>'
0.00.324.952 I print_info: EOG token        = 2 '</s>'
0.00.324.953 I print_info: max token length = 45
0.00.324.954 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.327.298 I load_tensors: offloading 4 repeating layers to GPU
0.00.327.299 I load_tensors: offloading output layer to GPU
0.00.327.299 I load_tensors: offloaded 5/5 layers to GPU
0.00.327.326 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.327.327 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.327.629 I llama_init_from_model: n_seq_max     = 1
0.00.327.630 I llama_init_from_model: n_ctx         = 8192
0.00.327.630 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.327.630 I llama_init_from_model: n_batch       = 2048
0.00.327.630 I llama_init_from_model: n_ubatch      = 2048
0.00.327.630 I llama_init_from_model: flash_attn    = 0
0.00.327.630 I llama_init_from_model: freq_base     = 10000.0
0.00.327.631 I llama_init_from_model: freq_scale    = 1
0.00.327.631 I ggml_metal_init: allocating
0.00.327.635 I ggml_metal_init: found device: Apple M4
0.00.327.638 I ggml_metal_init: picking default device: Apple M4
0.00.328.564 I ggml_metal_init: using embedded metal library
0.00.331.460 I ggml_metal_init: GPU name:   Apple M4
0.00.331.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.331.462 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.331.462 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.331.463 I ggml_metal_init: simdgroup reduction   = true
0.00.331.463 I ggml_metal_init: simdgroup matrix mul. = true
0.00.331.463 I ggml_metal_init: has residency sets    = true
0.00.331.463 I ggml_metal_init: has bfloat            = true
0.00.331.463 I ggml_metal_init: use bfloat            = true
0.00.331.464 I ggml_metal_init: hasUnifiedMemory      = true
0.00.331.464 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.341.109 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.344.219 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.344.221 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.344.244 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.351.173 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.351.175 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.351.175 I llama_init_from_model: graph nodes  = 154
0.00.351.176 I llama_init_from_model: graph splits = 2
0.00.351.177 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.351.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.358.536 I 
0.00.358.568 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.358.850 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.358.851 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.358.866 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.358.866 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.358.876 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.358.876 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.359.374 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.363.063 I llama_perf_context_print:        load time =     335.76 ms
0.00.363.064 I llama_perf_context_print: prompt eval time =       3.68 ms /    62 tokens (    0.06 ms per token, 16843.25 tokens per second)
0.00.363.065 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.363.066 I llama_perf_context_print:       total time =       4.53 ms /    63 tokens
0.00.363.334 I ggml_metal_free: deallocating

real	0m1.091s
user	0m0.332s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.107 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.265 I main: llama backend init
0.00.000.271 I main: load the model and apply lora adapter, if any
0.00.073.180 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.085.437 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.085.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.085.454 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.085.455 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.085.456 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.085.456 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.085.457 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.085.458 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.085.459 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.085.459 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.085.460 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.085.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.085.461 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.085.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.085.465 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.085.466 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.085.466 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.092.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.094.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.101.368 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.101.374 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.101.374 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.101.375 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.101.375 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.101.376 I llama_model_loader: - type  f32:  194 tensors
0.00.101.377 I llama_model_loader: - type  f16:   98 tensors
0.00.101.378 I print_info: file format = GGUF V3 (latest)
0.00.101.379 I print_info: file type   = all F32 (guessed)
0.00.101.381 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.110.630 I load: special tokens cache size = 25
0.00.116.453 I load: token to piece cache size = 0.2984 MB
0.00.116.461 I print_info: arch             = gptneox
0.00.116.461 I print_info: vocab_only       = 0
0.00.116.461 I print_info: n_ctx_train      = 2048
0.00.116.462 I print_info: n_embd           = 2048
0.00.116.466 I print_info: n_layer          = 24
0.00.116.472 I print_info: n_head           = 16
0.00.116.472 I print_info: n_head_kv        = 16
0.00.116.472 I print_info: n_rot            = 32
0.00.116.473 I print_info: n_swa            = 0
0.00.116.473 I print_info: n_embd_head_k    = 128
0.00.116.473 I print_info: n_embd_head_v    = 128
0.00.116.473 I print_info: n_gqa            = 1
0.00.116.474 I print_info: n_embd_k_gqa     = 2048
0.00.116.475 I print_info: n_embd_v_gqa     = 2048
0.00.116.475 I print_info: f_norm_eps       = 1.0e-05
0.00.116.476 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.116.476 I print_info: f_clamp_kqv      = 0.0e+00
0.00.116.476 I print_info: f_max_alibi_bias = 0.0e+00
0.00.116.476 I print_info: f_logit_scale    = 0.0e+00
0.00.116.477 I print_info: n_ff             = 8192
0.00.116.477 I print_info: n_expert         = 0
0.00.116.477 I print_info: n_expert_used    = 0
0.00.116.477 I print_info: causal attn      = 1
0.00.116.477 I print_info: pooling type     = 0
0.00.116.477 I print_info: rope type        = 2
0.00.116.478 I print_info: rope scaling     = linear
0.00.116.478 I print_info: freq_base_train  = 10000.0
0.00.116.478 I print_info: freq_scale_train = 1
0.00.116.479 I print_info: n_ctx_orig_yarn  = 2048
0.00.116.479 I print_info: rope_finetuned   = unknown
0.00.116.481 I print_info: ssm_d_conv       = 0
0.00.116.481 I print_info: ssm_d_inner      = 0
0.00.116.481 I print_info: ssm_d_state      = 0
0.00.116.481 I print_info: ssm_dt_rank      = 0
0.00.116.481 I print_info: ssm_dt_b_c_rms   = 0
0.00.116.482 I print_info: model type       = 1.4B
0.00.116.482 I print_info: model params     = 1.41 B
0.00.116.482 I print_info: general.name     = 1.4B
0.00.116.482 I print_info: vocab type       = BPE
0.00.116.483 I print_info: n_vocab          = 50304
0.00.116.483 I print_info: n_merges         = 50009
0.00.116.483 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.116.483 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.116.483 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.116.484 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.116.484 I print_info: LF token         = 187 ''
0.00.116.484 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.116.484 I print_info: max token length = 1024
0.00.116.485 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.199.776 I load_tensors: offloading 24 repeating layers to GPU
0.00.199.779 I load_tensors: offloading output layer to GPU
0.00.199.779 I load_tensors: offloaded 25/25 layers to GPU
0.00.199.806 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.199.807 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.200.448 I llama_init_from_model: n_seq_max     = 1
0.00.200.449 I llama_init_from_model: n_ctx         = 2048
0.00.200.449 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.200.449 I llama_init_from_model: n_batch       = 2048
0.00.200.450 I llama_init_from_model: n_ubatch      = 512
0.00.200.450 I llama_init_from_model: flash_attn    = 0
0.00.200.451 I llama_init_from_model: freq_base     = 10000.0
0.00.200.451 I llama_init_from_model: freq_scale    = 1
0.00.200.451 I ggml_metal_init: allocating
0.00.200.535 I ggml_metal_init: found device: Apple M4
0.00.200.541 I ggml_metal_init: picking default device: Apple M4
0.00.201.270 I ggml_metal_init: using embedded metal library
0.00.242.209 I ggml_metal_init: GPU name:   Apple M4
0.00.242.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.242.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.242.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.242.214 I ggml_metal_init: simdgroup reduction   = true
0.00.242.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.242.214 I ggml_metal_init: has residency sets    = true
0.00.242.214 I ggml_metal_init: has bfloat            = true
0.00.242.214 I ggml_metal_init: use bfloat            = true
0.00.242.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.242.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.335.327 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.363.928 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.363.935 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.363.979 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.368.082 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.368.084 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.368.084 I llama_init_from_model: graph nodes  = 967
0.00.368.084 I llama_init_from_model: graph splits = 2
0.00.368.092 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.368.212 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.368.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.859 I main: llama threadpool init, n_threads = 4
0.00.434.904 I 
0.00.434.935 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.936 I 
0.00.435.114 I sampler seed: 1234
0.00.435.118 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.435.142 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.435.144 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.435.144 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.285.405 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.02.285.406 I llama_perf_context_print:        load time =     360.78 ms
0.02.285.408 I llama_perf_context_print: prompt eval time =      53.47 ms /     7 tokens (    7.64 ms per token,   130.92 tokens per second)
0.02.285.409 I llama_perf_context_print:        eval time =    1793.93 ms /    63 runs   (   28.48 ms per token,    35.12 tokens per second)
0.02.285.409 I llama_perf_context_print:       total time =    1851.44 ms /    70 tokens
0.02.285.700 I ggml_metal_free: deallocating

real	0m2.604s
user	0m0.121s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.539 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.002 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.663 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.673 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.674 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.674 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.675 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.678 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.679 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.680 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.683 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.684 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.802 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.049.758 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.759 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.759 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.760 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.761 I llama_model_loader: - type  f32:  194 tensors
0.00.049.761 I llama_model_loader: - type  f16:   98 tensors
0.00.049.762 I print_info: file format = GGUF V3 (latest)
0.00.049.762 I print_info: file type   = all F32 (guessed)
0.00.049.764 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.061.156 I load: special tokens cache size = 25
0.00.068.846 I load: token to piece cache size = 0.2984 MB
0.00.068.850 I print_info: arch             = gptneox
0.00.068.850 I print_info: vocab_only       = 0
0.00.068.850 I print_info: n_ctx_train      = 2048
0.00.068.851 I print_info: n_embd           = 2048
0.00.068.851 I print_info: n_layer          = 24
0.00.068.854 I print_info: n_head           = 16
0.00.068.855 I print_info: n_head_kv        = 16
0.00.068.855 I print_info: n_rot            = 32
0.00.068.855 I print_info: n_swa            = 0
0.00.068.855 I print_info: n_embd_head_k    = 128
0.00.068.855 I print_info: n_embd_head_v    = 128
0.00.068.856 I print_info: n_gqa            = 1
0.00.068.857 I print_info: n_embd_k_gqa     = 2048
0.00.068.857 I print_info: n_embd_v_gqa     = 2048
0.00.068.858 I print_info: f_norm_eps       = 1.0e-05
0.00.068.860 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.860 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.860 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.860 I print_info: f_logit_scale    = 0.0e+00
0.00.068.861 I print_info: n_ff             = 8192
0.00.068.861 I print_info: n_expert         = 0
0.00.068.861 I print_info: n_expert_used    = 0
0.00.068.861 I print_info: causal attn      = 1
0.00.068.862 I print_info: pooling type     = 0
0.00.068.862 I print_info: rope type        = 2
0.00.068.862 I print_info: rope scaling     = linear
0.00.068.862 I print_info: freq_base_train  = 10000.0
0.00.068.863 I print_info: freq_scale_train = 1
0.00.068.863 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.863 I print_info: rope_finetuned   = unknown
0.00.068.863 I print_info: ssm_d_conv       = 0
0.00.068.863 I print_info: ssm_d_inner      = 0
0.00.068.863 I print_info: ssm_d_state      = 0
0.00.068.864 I print_info: ssm_dt_rank      = 0
0.00.068.864 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.864 I print_info: model type       = 1.4B
0.00.068.864 I print_info: model params     = 1.41 B
0.00.068.864 I print_info: general.name     = 1.4B
0.00.068.865 I print_info: vocab type       = BPE
0.00.068.865 I print_info: n_vocab          = 50304
0.00.068.865 I print_info: n_merges         = 50009
0.00.068.866 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.866 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.866 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.866 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.866 I print_info: LF token         = 187 ''
0.00.068.866 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.867 I print_info: max token length = 1024
0.00.068.867 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.405.565 I load_tensors: offloading 24 repeating layers to GPU
0.01.405.573 I load_tensors: offloading output layer to GPU
0.01.405.574 I load_tensors: offloaded 25/25 layers to GPU
0.01.405.599 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.405.599 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.406.533 I llama_init_from_model: n_seq_max     = 1
0.01.406.534 I llama_init_from_model: n_ctx         = 128
0.01.406.534 I llama_init_from_model: n_ctx_per_seq = 128
0.01.406.534 I llama_init_from_model: n_batch       = 128
0.01.406.535 I llama_init_from_model: n_ubatch      = 128
0.01.406.535 I llama_init_from_model: flash_attn    = 0
0.01.406.536 I llama_init_from_model: freq_base     = 10000.0
0.01.406.536 I llama_init_from_model: freq_scale    = 1
0.01.406.536 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.406.537 I ggml_metal_init: allocating
0.01.406.570 I ggml_metal_init: found device: Apple M4
0.01.406.575 I ggml_metal_init: picking default device: Apple M4
0.01.407.620 I ggml_metal_init: using embedded metal library
0.01.411.596 I ggml_metal_init: GPU name:   Apple M4
0.01.411.598 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.411.598 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.411.599 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.411.599 I ggml_metal_init: simdgroup reduction   = true
0.01.411.599 I ggml_metal_init: simdgroup matrix mul. = true
0.01.411.599 I ggml_metal_init: has residency sets    = true
0.01.411.599 I ggml_metal_init: has bfloat            = true
0.01.411.599 I ggml_metal_init: use bfloat            = true
0.01.411.600 I ggml_metal_init: hasUnifiedMemory      = true
0.01.411.601 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.422.385 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.424.073 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.424.076 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.424.108 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.425.729 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.425.731 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.425.731 I llama_init_from_model: graph nodes  = 967
0.01.425.731 I llama_init_from_model: graph splits = 2
0.01.425.733 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.425.733 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.460.395 I 
0.01.460.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.460.440 I perplexity: tokenizing the input ..
0.01.465.375 I perplexity: tokenization took 4.933 ms
0.01.465.380 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.584.572 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.586.638 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.586.674 I llama_perf_context_print:        load time =    1440.38 ms
0.01.586.675 I llama_perf_context_print: prompt eval time =     118.88 ms /   128 tokens (    0.93 ms per token,  1076.73 tokens per second)
0.01.586.675 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.586.676 I llama_perf_context_print:       total time =     126.28 ms /   129 tokens
0.01.587.087 I ggml_metal_free: deallocating

real	0m1.785s
user	0m0.096s
sys	0m0.249s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.019.968 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.343 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.038.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.358 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.359 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.359 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.359 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.361 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.361 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.362 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.362 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.363 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.365 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.365 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.321 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.047.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.047.896 I llama_model_loader: - type  f32:  194 tensors
0.00.047.896 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.897 I print_info: file format = GGUF V3 (latest)
0.00.047.897 I print_info: file type   = Q8_0
0.00.047.898 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.057.518 I load: special tokens cache size = 25
0.00.065.376 I load: token to piece cache size = 0.2984 MB
0.00.065.379 I print_info: arch             = gptneox
0.00.065.379 I print_info: vocab_only       = 0
0.00.065.380 I print_info: n_ctx_train      = 2048
0.00.065.380 I print_info: n_embd           = 2048
0.00.065.380 I print_info: n_layer          = 24
0.00.065.384 I print_info: n_head           = 16
0.00.065.385 I print_info: n_head_kv        = 16
0.00.065.386 I print_info: n_rot            = 32
0.00.065.386 I print_info: n_swa            = 0
0.00.065.386 I print_info: n_embd_head_k    = 128
0.00.065.386 I print_info: n_embd_head_v    = 128
0.00.065.387 I print_info: n_gqa            = 1
0.00.065.388 I print_info: n_embd_k_gqa     = 2048
0.00.065.388 I print_info: n_embd_v_gqa     = 2048
0.00.065.389 I print_info: f_norm_eps       = 1.0e-05
0.00.065.390 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.390 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.390 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.390 I print_info: f_logit_scale    = 0.0e+00
0.00.065.391 I print_info: n_ff             = 8192
0.00.065.391 I print_info: n_expert         = 0
0.00.065.391 I print_info: n_expert_used    = 0
0.00.065.392 I print_info: causal attn      = 1
0.00.065.392 I print_info: pooling type     = 0
0.00.065.392 I print_info: rope type        = 2
0.00.065.392 I print_info: rope scaling     = linear
0.00.065.394 I print_info: freq_base_train  = 10000.0
0.00.065.395 I print_info: freq_scale_train = 1
0.00.065.397 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.397 I print_info: rope_finetuned   = unknown
0.00.065.397 I print_info: ssm_d_conv       = 0
0.00.065.397 I print_info: ssm_d_inner      = 0
0.00.065.397 I print_info: ssm_d_state      = 0
0.00.065.397 I print_info: ssm_dt_rank      = 0
0.00.065.398 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.398 I print_info: model type       = 1.4B
0.00.065.398 I print_info: model params     = 1.41 B
0.00.065.398 I print_info: general.name     = 1.4B
0.00.065.399 I print_info: vocab type       = BPE
0.00.065.399 I print_info: n_vocab          = 50304
0.00.065.399 I print_info: n_merges         = 50009
0.00.065.400 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.400 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.400 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.400 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.405 I print_info: LF token         = 187 ''
0.00.065.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.405 I print_info: max token length = 1024
0.00.065.406 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.251.901 I load_tensors: offloading 24 repeating layers to GPU
0.01.251.905 I load_tensors: offloading output layer to GPU
0.01.251.906 I load_tensors: offloaded 25/25 layers to GPU
0.01.251.930 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.251.931 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.253.350 I llama_init_from_model: n_seq_max     = 1
0.01.253.352 I llama_init_from_model: n_ctx         = 2048
0.01.253.352 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.253.353 I llama_init_from_model: n_batch       = 2048
0.01.253.353 I llama_init_from_model: n_ubatch      = 512
0.01.253.354 I llama_init_from_model: flash_attn    = 0
0.01.253.355 I llama_init_from_model: freq_base     = 10000.0
0.01.253.355 I llama_init_from_model: freq_scale    = 1
0.01.253.356 I ggml_metal_init: allocating
0.01.253.365 I ggml_metal_init: found device: Apple M4
0.01.253.375 I ggml_metal_init: picking default device: Apple M4
0.01.254.774 I ggml_metal_init: using embedded metal library
0.01.260.374 I ggml_metal_init: GPU name:   Apple M4
0.01.260.376 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.260.377 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.260.378 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.260.378 I ggml_metal_init: simdgroup reduction   = true
0.01.260.378 I ggml_metal_init: simdgroup matrix mul. = true
0.01.260.379 I ggml_metal_init: has residency sets    = true
0.01.260.379 I ggml_metal_init: has bfloat            = true
0.01.260.379 I ggml_metal_init: use bfloat            = true
0.01.260.380 I ggml_metal_init: hasUnifiedMemory      = true
0.01.260.383 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.277.679 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.338.267 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.338.274 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.338.309 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.343.016 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.343.018 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.343.018 I llama_init_from_model: graph nodes  = 967
0.01.343.018 I llama_init_from_model: graph splits = 2
0.01.343.029 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.343.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.343.160 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.400.792 I main: llama threadpool init, n_threads = 4
0.01.400.836 I 
0.01.400.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.400.860 I 
0.01.401.035 I sampler seed: 1234
0.01.401.041 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.401.061 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.401.061 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.401.062 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.484.710 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52514.79 tokens per second)
0.02.484.710 I llama_perf_context_print:        load time =    1380.10 ms
0.02.484.711 I llama_perf_context_print: prompt eval time =      48.59 ms /     7 tokens (    6.94 ms per token,   144.06 tokens per second)
0.02.484.712 I llama_perf_context_print:        eval time =    1032.08 ms /    63 runs   (   16.38 ms per token,    61.04 tokens per second)
0.02.484.712 I llama_perf_context_print:       total time =    1084.63 ms /    70 tokens
0.02.484.954 I ggml_metal_free: deallocating

real	0m2.506s
user	0m0.114s
sys	0m0.288s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.265 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.951 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.957 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.959 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.962 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.962 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.964 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.964 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.965 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.965 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.968 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.968 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.968 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.900 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.808 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.810 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.811 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.812 I llama_model_loader: - type  f32:  194 tensors
0.00.025.812 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.813 I print_info: file format = GGUF V3 (latest)
0.00.025.813 I print_info: file type   = Q8_0
0.00.025.815 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.563 I load: special tokens cache size = 25
0.00.040.827 I load: token to piece cache size = 0.2984 MB
0.00.040.831 I print_info: arch             = gptneox
0.00.040.831 I print_info: vocab_only       = 0
0.00.040.831 I print_info: n_ctx_train      = 2048
0.00.040.831 I print_info: n_embd           = 2048
0.00.040.831 I print_info: n_layer          = 24
0.00.040.835 I print_info: n_head           = 16
0.00.040.836 I print_info: n_head_kv        = 16
0.00.040.836 I print_info: n_rot            = 32
0.00.040.836 I print_info: n_swa            = 0
0.00.040.838 I print_info: n_embd_head_k    = 128
0.00.040.838 I print_info: n_embd_head_v    = 128
0.00.040.839 I print_info: n_gqa            = 1
0.00.040.839 I print_info: n_embd_k_gqa     = 2048
0.00.040.840 I print_info: n_embd_v_gqa     = 2048
0.00.040.841 I print_info: f_norm_eps       = 1.0e-05
0.00.040.841 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.842 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.842 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.842 I print_info: f_logit_scale    = 0.0e+00
0.00.040.843 I print_info: n_ff             = 8192
0.00.040.843 I print_info: n_expert         = 0
0.00.040.843 I print_info: n_expert_used    = 0
0.00.040.843 I print_info: causal attn      = 1
0.00.040.843 I print_info: pooling type     = 0
0.00.040.843 I print_info: rope type        = 2
0.00.040.845 I print_info: rope scaling     = linear
0.00.040.845 I print_info: freq_base_train  = 10000.0
0.00.040.846 I print_info: freq_scale_train = 1
0.00.040.846 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.846 I print_info: rope_finetuned   = unknown
0.00.040.846 I print_info: ssm_d_conv       = 0
0.00.040.846 I print_info: ssm_d_inner      = 0
0.00.040.846 I print_info: ssm_d_state      = 0
0.00.040.847 I print_info: ssm_dt_rank      = 0
0.00.040.847 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.847 I print_info: model type       = 1.4B
0.00.040.847 I print_info: model params     = 1.41 B
0.00.040.847 I print_info: general.name     = 1.4B
0.00.040.848 I print_info: vocab type       = BPE
0.00.040.848 I print_info: n_vocab          = 50304
0.00.040.849 I print_info: n_merges         = 50009
0.00.040.849 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.850 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.850 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.850 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.850 I print_info: LF token         = 187 ''
0.00.040.850 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.850 I print_info: max token length = 1024
0.00.040.851 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.931.333 I load_tensors: offloading 24 repeating layers to GPU
0.00.931.340 I load_tensors: offloading output layer to GPU
0.00.931.341 I load_tensors: offloaded 25/25 layers to GPU
0.00.931.370 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.931.373 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.932.291 I llama_init_from_model: n_seq_max     = 1
0.00.932.292 I llama_init_from_model: n_ctx         = 128
0.00.932.293 I llama_init_from_model: n_ctx_per_seq = 128
0.00.932.293 I llama_init_from_model: n_batch       = 128
0.00.932.294 I llama_init_from_model: n_ubatch      = 128
0.00.932.294 I llama_init_from_model: flash_attn    = 0
0.00.932.295 I llama_init_from_model: freq_base     = 10000.0
0.00.932.296 I llama_init_from_model: freq_scale    = 1
0.00.932.297 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.932.298 I ggml_metal_init: allocating
0.00.932.365 I ggml_metal_init: found device: Apple M4
0.00.932.375 I ggml_metal_init: picking default device: Apple M4
0.00.933.738 I ggml_metal_init: using embedded metal library
0.00.939.085 I ggml_metal_init: GPU name:   Apple M4
0.00.939.088 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.939.089 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.939.090 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.939.090 I ggml_metal_init: simdgroup reduction   = true
0.00.939.090 I ggml_metal_init: simdgroup matrix mul. = true
0.00.939.091 I ggml_metal_init: has residency sets    = true
0.00.939.091 I ggml_metal_init: has bfloat            = true
0.00.939.091 I ggml_metal_init: use bfloat            = true
0.00.939.092 I ggml_metal_init: hasUnifiedMemory      = true
0.00.939.093 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.954.211 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.957.498 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.957.501 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.957.540 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.960.756 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.960.757 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.960.758 I llama_init_from_model: graph nodes  = 967
0.00.960.758 I llama_init_from_model: graph splits = 2
0.00.960.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.960.761 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.986.938 I 
0.00.987.008 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.987.016 I perplexity: tokenizing the input ..
0.00.993.870 I perplexity: tokenization took 6.853 ms
0.00.993.878 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.117.283 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.118.629 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.118.660 I llama_perf_context_print:        load time =     977.09 ms
0.01.118.661 I llama_perf_context_print: prompt eval time =     123.18 ms /   128 tokens (    0.96 ms per token,  1039.16 tokens per second)
0.01.118.662 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.118.662 I llama_perf_context_print:       total time =     131.73 ms /   129 tokens
0.01.119.075 I ggml_metal_free: deallocating

real	0m1.135s
user	0m0.076s
sys	0m0.181s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.012.490 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.351 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.028.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.358 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.359 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.359 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.361 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.362 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.363 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.363 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.363 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.364 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.366 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.373 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.621 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.995 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.997 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.037.998 I llama_model_loader: - type  f32:  194 tensors
0.00.037.999 I llama_model_loader: - type q4_0:   97 tensors
0.00.037.999 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.000 I print_info: file format = GGUF V3 (latest)
0.00.038.000 I print_info: file type   = Q4_0
0.00.038.001 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.693 I load: special tokens cache size = 25
0.00.055.456 I load: token to piece cache size = 0.2984 MB
0.00.055.460 I print_info: arch             = gptneox
0.00.055.460 I print_info: vocab_only       = 0
0.00.055.461 I print_info: n_ctx_train      = 2048
0.00.055.461 I print_info: n_embd           = 2048
0.00.055.461 I print_info: n_layer          = 24
0.00.055.466 I print_info: n_head           = 16
0.00.055.467 I print_info: n_head_kv        = 16
0.00.055.467 I print_info: n_rot            = 32
0.00.055.467 I print_info: n_swa            = 0
0.00.055.467 I print_info: n_embd_head_k    = 128
0.00.055.467 I print_info: n_embd_head_v    = 128
0.00.055.468 I print_info: n_gqa            = 1
0.00.055.469 I print_info: n_embd_k_gqa     = 2048
0.00.055.470 I print_info: n_embd_v_gqa     = 2048
0.00.055.470 I print_info: f_norm_eps       = 1.0e-05
0.00.055.471 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.471 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.471 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.471 I print_info: f_logit_scale    = 0.0e+00
0.00.055.472 I print_info: n_ff             = 8192
0.00.055.472 I print_info: n_expert         = 0
0.00.055.474 I print_info: n_expert_used    = 0
0.00.055.474 I print_info: causal attn      = 1
0.00.055.476 I print_info: pooling type     = 0
0.00.055.476 I print_info: rope type        = 2
0.00.055.477 I print_info: rope scaling     = linear
0.00.055.477 I print_info: freq_base_train  = 10000.0
0.00.055.477 I print_info: freq_scale_train = 1
0.00.055.477 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.478 I print_info: rope_finetuned   = unknown
0.00.055.478 I print_info: ssm_d_conv       = 0
0.00.055.478 I print_info: ssm_d_inner      = 0
0.00.055.478 I print_info: ssm_d_state      = 0
0.00.055.478 I print_info: ssm_dt_rank      = 0
0.00.055.478 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.479 I print_info: model type       = 1.4B
0.00.055.479 I print_info: model params     = 1.41 B
0.00.055.479 I print_info: general.name     = 1.4B
0.00.055.480 I print_info: vocab type       = BPE
0.00.055.480 I print_info: n_vocab          = 50304
0.00.055.480 I print_info: n_merges         = 50009
0.00.055.481 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.481 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.481 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.481 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.481 I print_info: LF token         = 187 ''
0.00.055.482 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.483 I print_info: max token length = 1024
0.00.055.485 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.863 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.875 I load_tensors: offloading output layer to GPU
0.00.604.876 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.912 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.604.914 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.606.518 I llama_init_from_model: n_seq_max     = 1
0.00.606.521 I llama_init_from_model: n_ctx         = 2048
0.00.606.522 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.606.522 I llama_init_from_model: n_batch       = 2048
0.00.606.523 I llama_init_from_model: n_ubatch      = 512
0.00.606.523 I llama_init_from_model: flash_attn    = 0
0.00.606.525 I llama_init_from_model: freq_base     = 10000.0
0.00.606.526 I llama_init_from_model: freq_scale    = 1
0.00.606.528 I ggml_metal_init: allocating
0.00.606.607 I ggml_metal_init: found device: Apple M4
0.00.606.620 I ggml_metal_init: picking default device: Apple M4
0.00.608.488 I ggml_metal_init: using embedded metal library
0.00.614.885 I ggml_metal_init: GPU name:   Apple M4
0.00.614.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.891 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.892 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.892 I ggml_metal_init: simdgroup reduction   = true
0.00.614.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.893 I ggml_metal_init: has residency sets    = true
0.00.614.893 I ggml_metal_init: has bfloat            = true
0.00.614.894 I ggml_metal_init: use bfloat            = true
0.00.614.895 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.896 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.236 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.245 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.692.252 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.692.287 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.515 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.517 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.518 I llama_init_from_model: graph nodes  = 967
0.00.696.518 I llama_init_from_model: graph splits = 2
0.00.696.524 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.652 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.652 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.764 I main: llama threadpool init, n_threads = 4
0.00.753.821 I 
0.00.753.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.850 I 
0.00.754.028 I sampler seed: 1234
0.00.754.032 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.069 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.071 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.074 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.431.432 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48630.14 tokens per second)
0.01.431.432 I llama_perf_context_print:        load time =     740.54 ms
0.01.431.433 I llama_perf_context_print: prompt eval time =      44.99 ms /     7 tokens (    6.43 ms per token,   155.59 tokens per second)
0.01.431.434 I llama_perf_context_print:        eval time =     629.39 ms /    63 runs   (    9.99 ms per token,   100.10 tokens per second)
0.01.431.434 I llama_perf_context_print:       total time =     678.40 ms /    70 tokens
0.01.431.685 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.117s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.262 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.736 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.004 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.009 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.012 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.017 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.017 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.018 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.019 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.022 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.022 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.022 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.025 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.025 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.025 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.814 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.958 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.741 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.742 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.743 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.743 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.744 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.744 I llama_model_loader: - type  f32:  194 tensors
0.00.025.744 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.745 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.745 I print_info: file format = GGUF V3 (latest)
0.00.025.746 I print_info: file type   = Q4_0
0.00.025.747 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.001 I load: special tokens cache size = 25
0.00.040.118 I load: token to piece cache size = 0.2984 MB
0.00.040.123 I print_info: arch             = gptneox
0.00.040.123 I print_info: vocab_only       = 0
0.00.040.123 I print_info: n_ctx_train      = 2048
0.00.040.123 I print_info: n_embd           = 2048
0.00.040.123 I print_info: n_layer          = 24
0.00.040.128 I print_info: n_head           = 16
0.00.040.129 I print_info: n_head_kv        = 16
0.00.040.129 I print_info: n_rot            = 32
0.00.040.129 I print_info: n_swa            = 0
0.00.040.129 I print_info: n_embd_head_k    = 128
0.00.040.131 I print_info: n_embd_head_v    = 128
0.00.040.132 I print_info: n_gqa            = 1
0.00.040.132 I print_info: n_embd_k_gqa     = 2048
0.00.040.133 I print_info: n_embd_v_gqa     = 2048
0.00.040.133 I print_info: f_norm_eps       = 1.0e-05
0.00.040.134 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.134 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.134 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.134 I print_info: f_logit_scale    = 0.0e+00
0.00.040.135 I print_info: n_ff             = 8192
0.00.040.135 I print_info: n_expert         = 0
0.00.040.135 I print_info: n_expert_used    = 0
0.00.040.135 I print_info: causal attn      = 1
0.00.040.135 I print_info: pooling type     = 0
0.00.040.137 I print_info: rope type        = 2
0.00.040.137 I print_info: rope scaling     = linear
0.00.040.137 I print_info: freq_base_train  = 10000.0
0.00.040.137 I print_info: freq_scale_train = 1
0.00.040.138 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.138 I print_info: rope_finetuned   = unknown
0.00.040.138 I print_info: ssm_d_conv       = 0
0.00.040.138 I print_info: ssm_d_inner      = 0
0.00.040.138 I print_info: ssm_d_state      = 0
0.00.040.138 I print_info: ssm_dt_rank      = 0
0.00.040.138 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.139 I print_info: model type       = 1.4B
0.00.040.140 I print_info: model params     = 1.41 B
0.00.040.140 I print_info: general.name     = 1.4B
0.00.040.141 I print_info: vocab type       = BPE
0.00.040.141 I print_info: n_vocab          = 50304
0.00.040.142 I print_info: n_merges         = 50009
0.00.040.142 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.142 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.142 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.143 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.144 I print_info: LF token         = 187 ''
0.00.040.144 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.144 I print_info: max token length = 1024
0.00.040.144 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.554.958 I load_tensors: offloading 24 repeating layers to GPU
0.00.554.975 I load_tensors: offloading output layer to GPU
0.00.554.976 I load_tensors: offloaded 25/25 layers to GPU
0.00.555.011 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.555.012 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.556.766 I llama_init_from_model: n_seq_max     = 1
0.00.556.769 I llama_init_from_model: n_ctx         = 128
0.00.556.770 I llama_init_from_model: n_ctx_per_seq = 128
0.00.556.770 I llama_init_from_model: n_batch       = 128
0.00.556.770 I llama_init_from_model: n_ubatch      = 128
0.00.556.771 I llama_init_from_model: flash_attn    = 0
0.00.556.773 I llama_init_from_model: freq_base     = 10000.0
0.00.556.773 I llama_init_from_model: freq_scale    = 1
0.00.556.774 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.556.776 I ggml_metal_init: allocating
0.00.556.887 I ggml_metal_init: found device: Apple M4
0.00.556.902 I ggml_metal_init: picking default device: Apple M4
0.00.558.738 I ggml_metal_init: using embedded metal library
0.00.564.528 I ggml_metal_init: GPU name:   Apple M4
0.00.564.537 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.564.538 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.564.538 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.564.539 I ggml_metal_init: simdgroup reduction   = true
0.00.564.539 I ggml_metal_init: simdgroup matrix mul. = true
0.00.564.540 I ggml_metal_init: has residency sets    = true
0.00.564.540 I ggml_metal_init: has bfloat            = true
0.00.564.540 I ggml_metal_init: use bfloat            = true
0.00.564.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.564.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.583.719 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.587.297 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.587.301 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.587.346 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.590.827 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.590.829 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.590.829 I llama_init_from_model: graph nodes  = 967
0.00.590.830 I llama_init_from_model: graph splits = 2
0.00.590.833 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.590.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.890 I 
0.00.615.979 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.993 I perplexity: tokenizing the input ..
0.00.623.107 I perplexity: tokenization took 7.111 ms
0.00.623.111 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.758.344 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.759.678 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.759.702 I llama_perf_context_print:        load time =     606.13 ms
0.00.759.703 I llama_perf_context_print: prompt eval time =     134.83 ms /   128 tokens (    1.05 ms per token,   949.32 tokens per second)
0.00.759.703 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.759.705 I llama_perf_context_print:       total time =     143.83 ms /   129 tokens
0.00.760.053 I ggml_metal_free: deallocating

real	0m0.776s
user	0m0.079s
sys	0m0.121s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.800 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.119 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.123 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.125 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.129 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.129 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.908 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.908 I llama_model_loader: - type  f32:  194 tensors
0.00.036.909 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.909 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.909 I print_info: file format = GGUF V3 (latest)
0.00.036.910 I print_info: file type   = Q4_1
0.00.036.911 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.567 I load: special tokens cache size = 25
0.00.052.476 I load: token to piece cache size = 0.2984 MB
0.00.052.479 I print_info: arch             = gptneox
0.00.052.479 I print_info: vocab_only       = 0
0.00.052.480 I print_info: n_ctx_train      = 2048
0.00.052.480 I print_info: n_embd           = 2048
0.00.052.480 I print_info: n_layer          = 24
0.00.052.483 I print_info: n_head           = 16
0.00.052.485 I print_info: n_head_kv        = 16
0.00.052.486 I print_info: n_rot            = 32
0.00.052.486 I print_info: n_swa            = 0
0.00.052.486 I print_info: n_embd_head_k    = 128
0.00.052.486 I print_info: n_embd_head_v    = 128
0.00.052.487 I print_info: n_gqa            = 1
0.00.052.487 I print_info: n_embd_k_gqa     = 2048
0.00.052.488 I print_info: n_embd_v_gqa     = 2048
0.00.052.489 I print_info: f_norm_eps       = 1.0e-05
0.00.052.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.489 I print_info: f_logit_scale    = 0.0e+00
0.00.052.490 I print_info: n_ff             = 8192
0.00.052.490 I print_info: n_expert         = 0
0.00.052.490 I print_info: n_expert_used    = 0
0.00.052.491 I print_info: causal attn      = 1
0.00.052.491 I print_info: pooling type     = 0
0.00.052.492 I print_info: rope type        = 2
0.00.052.494 I print_info: rope scaling     = linear
0.00.052.494 I print_info: freq_base_train  = 10000.0
0.00.052.494 I print_info: freq_scale_train = 1
0.00.052.494 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.495 I print_info: rope_finetuned   = unknown
0.00.052.495 I print_info: ssm_d_conv       = 0
0.00.052.495 I print_info: ssm_d_inner      = 0
0.00.052.495 I print_info: ssm_d_state      = 0
0.00.052.495 I print_info: ssm_dt_rank      = 0
0.00.052.495 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.499 I print_info: model type       = 1.4B
0.00.052.500 I print_info: model params     = 1.41 B
0.00.052.500 I print_info: general.name     = 1.4B
0.00.052.500 I print_info: vocab type       = BPE
0.00.052.500 I print_info: n_vocab          = 50304
0.00.052.500 I print_info: n_merges         = 50009
0.00.052.501 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.501 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.501 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.501 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.502 I print_info: LF token         = 187 ''
0.00.052.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.502 I print_info: max token length = 1024
0.00.052.502 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.768.252 I load_tensors: offloading 24 repeating layers to GPU
0.00.768.269 I load_tensors: offloading output layer to GPU
0.00.768.270 I load_tensors: offloaded 25/25 layers to GPU
0.00.768.303 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.768.310 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.769.918 I llama_init_from_model: n_seq_max     = 1
0.00.769.920 I llama_init_from_model: n_ctx         = 2048
0.00.769.921 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.769.921 I llama_init_from_model: n_batch       = 2048
0.00.769.922 I llama_init_from_model: n_ubatch      = 512
0.00.769.922 I llama_init_from_model: flash_attn    = 0
0.00.769.925 I llama_init_from_model: freq_base     = 10000.0
0.00.769.925 I llama_init_from_model: freq_scale    = 1
0.00.769.927 I ggml_metal_init: allocating
0.00.769.991 I ggml_metal_init: found device: Apple M4
0.00.770.004 I ggml_metal_init: picking default device: Apple M4
0.00.771.866 I ggml_metal_init: using embedded metal library
0.00.777.629 I ggml_metal_init: GPU name:   Apple M4
0.00.777.634 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.777.635 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.777.636 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.777.636 I ggml_metal_init: simdgroup reduction   = true
0.00.777.637 I ggml_metal_init: simdgroup matrix mul. = true
0.00.777.637 I ggml_metal_init: has residency sets    = true
0.00.777.637 I ggml_metal_init: has bfloat            = true
0.00.777.638 I ggml_metal_init: use bfloat            = true
0.00.777.639 I ggml_metal_init: hasUnifiedMemory      = true
0.00.777.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.797.630 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.850.824 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.850.831 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.850.865 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.854.961 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.854.963 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.854.963 I llama_init_from_model: graph nodes  = 967
0.00.854.964 I llama_init_from_model: graph splits = 2
0.00.854.970 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.855.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.855.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.906.723 I main: llama threadpool init, n_threads = 4
0.00.906.764 I 
0.00.906.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.906.784 I 
0.00.906.905 I sampler seed: 1234
0.00.906.911 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.906.951 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.906.954 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.906.954 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.644.012 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.644.013 I llama_perf_context_print:        load time =     897.21 ms
0.01.644.014 I llama_perf_context_print: prompt eval time =      49.27 ms /     7 tokens (    7.04 ms per token,   142.08 tokens per second)
0.01.644.015 I llama_perf_context_print:        eval time =     684.96 ms /    63 runs   (   10.87 ms per token,    91.98 tokens per second)
0.01.644.015 I llama_perf_context_print:       total time =     738.00 ms /    70 tokens
0.01.644.250 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.112s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.768 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.774 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.783 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.784 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.785 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.787 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.789 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.789 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.590 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.591 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.591 I llama_model_loader: - type  f32:  194 tensors
0.00.026.592 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.592 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.593 I print_info: file format = GGUF V3 (latest)
0.00.026.593 I print_info: file type   = Q4_1
0.00.026.594 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.064 I load: special tokens cache size = 25
0.00.041.098 I load: token to piece cache size = 0.2984 MB
0.00.041.101 I print_info: arch             = gptneox
0.00.041.101 I print_info: vocab_only       = 0
0.00.041.102 I print_info: n_ctx_train      = 2048
0.00.041.102 I print_info: n_embd           = 2048
0.00.041.102 I print_info: n_layer          = 24
0.00.041.107 I print_info: n_head           = 16
0.00.041.108 I print_info: n_head_kv        = 16
0.00.041.108 I print_info: n_rot            = 32
0.00.041.108 I print_info: n_swa            = 0
0.00.041.108 I print_info: n_embd_head_k    = 128
0.00.041.108 I print_info: n_embd_head_v    = 128
0.00.041.109 I print_info: n_gqa            = 1
0.00.041.110 I print_info: n_embd_k_gqa     = 2048
0.00.041.111 I print_info: n_embd_v_gqa     = 2048
0.00.041.111 I print_info: f_norm_eps       = 1.0e-05
0.00.041.112 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.112 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.112 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.112 I print_info: f_logit_scale    = 0.0e+00
0.00.041.113 I print_info: n_ff             = 8192
0.00.041.113 I print_info: n_expert         = 0
0.00.041.113 I print_info: n_expert_used    = 0
0.00.041.113 I print_info: causal attn      = 1
0.00.041.113 I print_info: pooling type     = 0
0.00.041.113 I print_info: rope type        = 2
0.00.041.115 I print_info: rope scaling     = linear
0.00.041.115 I print_info: freq_base_train  = 10000.0
0.00.041.115 I print_info: freq_scale_train = 1
0.00.041.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.116 I print_info: rope_finetuned   = unknown
0.00.041.116 I print_info: ssm_d_conv       = 0
0.00.041.116 I print_info: ssm_d_inner      = 0
0.00.041.116 I print_info: ssm_d_state      = 0
0.00.041.116 I print_info: ssm_dt_rank      = 0
0.00.041.116 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.117 I print_info: model type       = 1.4B
0.00.041.117 I print_info: model params     = 1.41 B
0.00.041.117 I print_info: general.name     = 1.4B
0.00.041.119 I print_info: vocab type       = BPE
0.00.041.119 I print_info: n_vocab          = 50304
0.00.041.119 I print_info: n_merges         = 50009
0.00.041.119 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.119 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.120 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.120 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.120 I print_info: LF token         = 187 ''
0.00.041.121 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.127 I print_info: max token length = 1024
0.00.041.129 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.573.243 I load_tensors: offloading 24 repeating layers to GPU
0.00.573.259 I load_tensors: offloading output layer to GPU
0.00.573.260 I load_tensors: offloaded 25/25 layers to GPU
0.00.573.295 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.573.297 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.574.959 I llama_init_from_model: n_seq_max     = 1
0.00.574.962 I llama_init_from_model: n_ctx         = 128
0.00.574.963 I llama_init_from_model: n_ctx_per_seq = 128
0.00.574.963 I llama_init_from_model: n_batch       = 128
0.00.574.963 I llama_init_from_model: n_ubatch      = 128
0.00.574.964 I llama_init_from_model: flash_attn    = 0
0.00.574.966 I llama_init_from_model: freq_base     = 10000.0
0.00.574.967 I llama_init_from_model: freq_scale    = 1
0.00.574.967 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.574.970 I ggml_metal_init: allocating
0.00.575.053 I ggml_metal_init: found device: Apple M4
0.00.575.067 I ggml_metal_init: picking default device: Apple M4
0.00.576.902 I ggml_metal_init: using embedded metal library
0.00.583.548 I ggml_metal_init: GPU name:   Apple M4
0.00.583.558 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.583.559 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.583.560 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.583.561 I ggml_metal_init: simdgroup reduction   = true
0.00.583.561 I ggml_metal_init: simdgroup matrix mul. = true
0.00.583.562 I ggml_metal_init: has residency sets    = true
0.00.583.562 I ggml_metal_init: has bfloat            = true
0.00.583.562 I ggml_metal_init: use bfloat            = true
0.00.583.563 I ggml_metal_init: hasUnifiedMemory      = true
0.00.583.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.602.611 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.606.214 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.606.221 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.606.268 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.609.352 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.609.354 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.609.355 I llama_init_from_model: graph nodes  = 967
0.00.609.355 I llama_init_from_model: graph splits = 2
0.00.609.358 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.609.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.655 I 
0.00.636.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.745 I perplexity: tokenizing the input ..
0.00.644.033 I perplexity: tokenization took 7.287 ms
0.00.644.046 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.599 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.781.021 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.781.043 I llama_perf_context_print:        load time =     627.75 ms
0.00.781.045 I llama_perf_context_print: prompt eval time =     135.15 ms /   128 tokens (    1.06 ms per token,   947.07 tokens per second)
0.00.781.046 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.046 I llama_perf_context_print:       total time =     144.39 ms /   129 tokens
0.00.781.424 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.081s
sys	0m0.119s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.652 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.657 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.663 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.664 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.664 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.664 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.664 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.667 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.667 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.668 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.669 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.671 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.671 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.561 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.281 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.283 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.283 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.283 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.284 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.284 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.285 I llama_model_loader: - type  f32:  194 tensors
0.00.026.285 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.285 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.286 I print_info: file format = GGUF V3 (latest)
0.00.026.286 I print_info: file type   = Q5_0
0.00.026.287 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.566 I load: special tokens cache size = 25
0.00.040.523 I load: token to piece cache size = 0.2984 MB
0.00.040.526 I print_info: arch             = gptneox
0.00.040.526 I print_info: vocab_only       = 0
0.00.040.527 I print_info: n_ctx_train      = 2048
0.00.040.527 I print_info: n_embd           = 2048
0.00.040.527 I print_info: n_layer          = 24
0.00.040.530 I print_info: n_head           = 16
0.00.040.530 I print_info: n_head_kv        = 16
0.00.040.530 I print_info: n_rot            = 32
0.00.040.531 I print_info: n_swa            = 0
0.00.040.531 I print_info: n_embd_head_k    = 128
0.00.040.531 I print_info: n_embd_head_v    = 128
0.00.040.533 I print_info: n_gqa            = 1
0.00.040.534 I print_info: n_embd_k_gqa     = 2048
0.00.040.535 I print_info: n_embd_v_gqa     = 2048
0.00.040.535 I print_info: f_norm_eps       = 1.0e-05
0.00.040.536 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.540 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.540 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.541 I print_info: f_logit_scale    = 0.0e+00
0.00.040.543 I print_info: n_ff             = 8192
0.00.040.543 I print_info: n_expert         = 0
0.00.040.543 I print_info: n_expert_used    = 0
0.00.040.543 I print_info: causal attn      = 1
0.00.040.543 I print_info: pooling type     = 0
0.00.040.544 I print_info: rope type        = 2
0.00.040.544 I print_info: rope scaling     = linear
0.00.040.544 I print_info: freq_base_train  = 10000.0
0.00.040.544 I print_info: freq_scale_train = 1
0.00.040.546 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.546 I print_info: rope_finetuned   = unknown
0.00.040.546 I print_info: ssm_d_conv       = 0
0.00.040.546 I print_info: ssm_d_inner      = 0
0.00.040.546 I print_info: ssm_d_state      = 0
0.00.040.547 I print_info: ssm_dt_rank      = 0
0.00.040.547 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.547 I print_info: model type       = 1.4B
0.00.040.547 I print_info: model params     = 1.41 B
0.00.040.548 I print_info: general.name     = 1.4B
0.00.040.548 I print_info: vocab type       = BPE
0.00.040.548 I print_info: n_vocab          = 50304
0.00.040.548 I print_info: n_merges         = 50009
0.00.040.549 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.549 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.549 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.549 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.549 I print_info: LF token         = 187 ''
0.00.040.550 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.550 I print_info: max token length = 1024
0.00.040.550 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.568.310 I load_tensors: offloading 24 repeating layers to GPU
0.00.568.325 I load_tensors: offloading output layer to GPU
0.00.568.326 I load_tensors: offloaded 25/25 layers to GPU
0.00.568.361 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.568.362 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.569.882 I llama_init_from_model: n_seq_max     = 1
0.00.569.885 I llama_init_from_model: n_ctx         = 2048
0.00.569.886 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.569.887 I llama_init_from_model: n_batch       = 2048
0.00.569.887 I llama_init_from_model: n_ubatch      = 512
0.00.569.887 I llama_init_from_model: flash_attn    = 0
0.00.569.889 I llama_init_from_model: freq_base     = 10000.0
0.00.569.890 I llama_init_from_model: freq_scale    = 1
0.00.569.892 I ggml_metal_init: allocating
0.00.569.965 I ggml_metal_init: found device: Apple M4
0.00.569.979 I ggml_metal_init: picking default device: Apple M4
0.00.571.888 I ggml_metal_init: using embedded metal library
0.00.578.616 I ggml_metal_init: GPU name:   Apple M4
0.00.578.620 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.578.621 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.578.621 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.578.622 I ggml_metal_init: simdgroup reduction   = true
0.00.578.622 I ggml_metal_init: simdgroup matrix mul. = true
0.00.578.622 I ggml_metal_init: has residency sets    = true
0.00.578.623 I ggml_metal_init: has bfloat            = true
0.00.578.623 I ggml_metal_init: use bfloat            = true
0.00.578.624 I ggml_metal_init: hasUnifiedMemory      = true
0.00.578.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.596.904 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.655.167 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.655.174 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.655.208 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.660.470 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.660.472 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.660.472 I llama_init_from_model: graph nodes  = 967
0.00.660.472 I llama_init_from_model: graph splits = 2
0.00.660.480 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.660.611 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.660.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.888 I main: llama threadpool init, n_threads = 4
0.00.715.935 I 
0.00.715.958 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.959 I 
0.00.716.126 I sampler seed: 1234
0.00.716.130 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.716.141 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.716.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.716.141 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.509.925 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47747.14 tokens per second)
0.01.509.926 I llama_perf_context_print:        load time =     705.32 ms
0.01.509.926 I llama_perf_context_print: prompt eval time =      43.07 ms /     7 tokens (    6.15 ms per token,   162.51 tokens per second)
0.01.509.927 I llama_perf_context_print:        eval time =     748.00 ms /    63 runs   (   11.87 ms per token,    84.22 tokens per second)
0.01.509.927 I llama_perf_context_print:       total time =     794.75 ms /    70 tokens
0.01.510.216 I ggml_metal_free: deallocating

real	0m1.528s
user	0m0.111s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.690 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.696 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.700 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.700 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.701 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.701 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.702 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.702 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.703 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.703 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.704 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.704 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.704 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.706 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.707 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.707 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.640 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.457 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.458 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.459 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.459 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.459 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.460 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.460 I llama_model_loader: - type  f32:  194 tensors
0.00.025.461 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.461 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.462 I print_info: file format = GGUF V3 (latest)
0.00.025.462 I print_info: file type   = Q5_0
0.00.025.464 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.697 I load: special tokens cache size = 25
0.00.040.090 I load: token to piece cache size = 0.2984 MB
0.00.040.095 I print_info: arch             = gptneox
0.00.040.095 I print_info: vocab_only       = 0
0.00.040.096 I print_info: n_ctx_train      = 2048
0.00.040.096 I print_info: n_embd           = 2048
0.00.040.096 I print_info: n_layer          = 24
0.00.040.100 I print_info: n_head           = 16
0.00.040.101 I print_info: n_head_kv        = 16
0.00.040.101 I print_info: n_rot            = 32
0.00.040.101 I print_info: n_swa            = 0
0.00.040.102 I print_info: n_embd_head_k    = 128
0.00.040.102 I print_info: n_embd_head_v    = 128
0.00.040.102 I print_info: n_gqa            = 1
0.00.040.103 I print_info: n_embd_k_gqa     = 2048
0.00.040.106 I print_info: n_embd_v_gqa     = 2048
0.00.040.107 I print_info: f_norm_eps       = 1.0e-05
0.00.040.107 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.107 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.108 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.108 I print_info: f_logit_scale    = 0.0e+00
0.00.040.110 I print_info: n_ff             = 8192
0.00.040.111 I print_info: n_expert         = 0
0.00.040.111 I print_info: n_expert_used    = 0
0.00.040.111 I print_info: causal attn      = 1
0.00.040.112 I print_info: pooling type     = 0
0.00.040.112 I print_info: rope type        = 2
0.00.040.112 I print_info: rope scaling     = linear
0.00.040.112 I print_info: freq_base_train  = 10000.0
0.00.040.113 I print_info: freq_scale_train = 1
0.00.040.113 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.113 I print_info: rope_finetuned   = unknown
0.00.040.113 I print_info: ssm_d_conv       = 0
0.00.040.114 I print_info: ssm_d_inner      = 0
0.00.040.114 I print_info: ssm_d_state      = 0
0.00.040.114 I print_info: ssm_dt_rank      = 0
0.00.040.114 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.138 I print_info: model type       = 1.4B
0.00.040.140 I print_info: model params     = 1.41 B
0.00.040.140 I print_info: general.name     = 1.4B
0.00.040.141 I print_info: vocab type       = BPE
0.00.040.141 I print_info: n_vocab          = 50304
0.00.040.141 I print_info: n_merges         = 50009
0.00.040.143 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.143 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.143 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.143 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.144 I print_info: LF token         = 187 ''
0.00.040.146 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.146 I print_info: max token length = 1024
0.00.040.146 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.499 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.516 I load_tensors: offloading output layer to GPU
0.00.599.517 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.552 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.599.554 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.601.112 I llama_init_from_model: n_seq_max     = 1
0.00.601.115 I llama_init_from_model: n_ctx         = 128
0.00.601.116 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.116 I llama_init_from_model: n_batch       = 128
0.00.601.117 I llama_init_from_model: n_ubatch      = 128
0.00.601.117 I llama_init_from_model: flash_attn    = 0
0.00.601.119 I llama_init_from_model: freq_base     = 10000.0
0.00.601.119 I llama_init_from_model: freq_scale    = 1
0.00.601.120 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.122 I ggml_metal_init: allocating
0.00.601.205 I ggml_metal_init: found device: Apple M4
0.00.601.219 I ggml_metal_init: picking default device: Apple M4
0.00.603.041 I ggml_metal_init: using embedded metal library
0.00.609.549 I ggml_metal_init: GPU name:   Apple M4
0.00.609.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.555 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.557 I ggml_metal_init: simdgroup reduction   = true
0.00.609.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.557 I ggml_metal_init: has residency sets    = true
0.00.609.557 I ggml_metal_init: has bfloat            = true
0.00.609.558 I ggml_metal_init: use bfloat            = true
0.00.609.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.894 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.441 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.445 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.487 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.873 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.874 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.875 I llama_init_from_model: graph nodes  = 967
0.00.633.875 I llama_init_from_model: graph splits = 2
0.00.633.878 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.878 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.146 I 
0.00.663.228 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.235 I perplexity: tokenizing the input ..
0.00.670.441 I perplexity: tokenization took 7.203 ms
0.00.670.449 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.144 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.807.483 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.807.509 I llama_perf_context_print:        load time =     653.25 ms
0.00.807.510 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.75 tokens per second)
0.00.807.511 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.511 I llama_perf_context_print:       total time =     144.37 ms /   129 tokens
0.00.807.885 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.080s
sys	0m0.145s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.330 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.138 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.143 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.145 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.146 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.146 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.146 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.147 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.148 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.148 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.148 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.149 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.150 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.153 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.153 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.154 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.976 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.150 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.947 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.949 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.949 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.949 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.949 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.950 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.950 I llama_model_loader: - type  f32:  194 tensors
0.00.024.951 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.951 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.952 I print_info: file format = GGUF V3 (latest)
0.00.024.952 I print_info: file type   = Q5_1
0.00.024.953 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.126 I load: special tokens cache size = 25
0.00.039.385 I load: token to piece cache size = 0.2984 MB
0.00.039.388 I print_info: arch             = gptneox
0.00.039.388 I print_info: vocab_only       = 0
0.00.039.388 I print_info: n_ctx_train      = 2048
0.00.039.389 I print_info: n_embd           = 2048
0.00.039.389 I print_info: n_layer          = 24
0.00.039.392 I print_info: n_head           = 16
0.00.039.392 I print_info: n_head_kv        = 16
0.00.039.393 I print_info: n_rot            = 32
0.00.039.395 I print_info: n_swa            = 0
0.00.039.396 I print_info: n_embd_head_k    = 128
0.00.039.396 I print_info: n_embd_head_v    = 128
0.00.039.396 I print_info: n_gqa            = 1
0.00.039.397 I print_info: n_embd_k_gqa     = 2048
0.00.039.398 I print_info: n_embd_v_gqa     = 2048
0.00.039.399 I print_info: f_norm_eps       = 1.0e-05
0.00.039.399 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.399 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.399 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.399 I print_info: f_logit_scale    = 0.0e+00
0.00.039.400 I print_info: n_ff             = 8192
0.00.039.400 I print_info: n_expert         = 0
0.00.039.400 I print_info: n_expert_used    = 0
0.00.039.400 I print_info: causal attn      = 1
0.00.039.401 I print_info: pooling type     = 0
0.00.039.402 I print_info: rope type        = 2
0.00.039.403 I print_info: rope scaling     = linear
0.00.039.404 I print_info: freq_base_train  = 10000.0
0.00.039.404 I print_info: freq_scale_train = 1
0.00.039.404 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.404 I print_info: rope_finetuned   = unknown
0.00.039.405 I print_info: ssm_d_conv       = 0
0.00.039.405 I print_info: ssm_d_inner      = 0
0.00.039.405 I print_info: ssm_d_state      = 0
0.00.039.405 I print_info: ssm_dt_rank      = 0
0.00.039.405 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.405 I print_info: model type       = 1.4B
0.00.039.406 I print_info: model params     = 1.41 B
0.00.039.406 I print_info: general.name     = 1.4B
0.00.039.407 I print_info: vocab type       = BPE
0.00.039.407 I print_info: n_vocab          = 50304
0.00.039.407 I print_info: n_merges         = 50009
0.00.039.408 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.408 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.408 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.408 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.408 I print_info: LF token         = 187 ''
0.00.039.409 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.409 I print_info: max token length = 1024
0.00.039.409 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.662.035 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.050 I load_tensors: offloading output layer to GPU
0.00.662.051 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.092 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.662.093 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.663.731 I llama_init_from_model: n_seq_max     = 1
0.00.663.734 I llama_init_from_model: n_ctx         = 2048
0.00.663.735 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.663.735 I llama_init_from_model: n_batch       = 2048
0.00.663.736 I llama_init_from_model: n_ubatch      = 512
0.00.663.736 I llama_init_from_model: flash_attn    = 0
0.00.663.737 I llama_init_from_model: freq_base     = 10000.0
0.00.663.738 I llama_init_from_model: freq_scale    = 1
0.00.663.739 I ggml_metal_init: allocating
0.00.663.750 I ggml_metal_init: found device: Apple M4
0.00.663.761 I ggml_metal_init: picking default device: Apple M4
0.00.665.310 I ggml_metal_init: using embedded metal library
0.00.671.646 I ggml_metal_init: GPU name:   Apple M4
0.00.671.650 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.651 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.651 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.652 I ggml_metal_init: simdgroup reduction   = true
0.00.671.652 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.652 I ggml_metal_init: has residency sets    = true
0.00.671.652 I ggml_metal_init: has bfloat            = true
0.00.671.653 I ggml_metal_init: use bfloat            = true
0.00.671.654 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.655 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.741 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.178 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.744.185 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.220 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.749.567 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.749.570 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.749.570 I llama_init_from_model: graph nodes  = 967
0.00.749.570 I llama_init_from_model: graph splits = 2
0.00.749.576 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.749.705 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.706 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.459 I main: llama threadpool init, n_threads = 4
0.00.806.523 I 
0.00.806.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.552 I 
0.00.806.709 I sampler seed: 1234
0.00.806.713 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.738 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.740 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.740 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.637.623 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52321.30 tokens per second)
0.01.637.624 I llama_perf_context_print:        load time =     797.40 ms
0.01.637.626 I llama_perf_context_print: prompt eval time =      42.20 ms /     7 tokens (    6.03 ms per token,   165.90 tokens per second)
0.01.637.627 I llama_perf_context_print:        eval time =     785.78 ms /    63 runs   (   12.47 ms per token,    80.18 tokens per second)
0.01.637.629 I llama_perf_context_print:       total time =     831.89 ms /    70 tokens
0.01.637.895 I ggml_metal_free: deallocating

real	0m1.655s
user	0m0.110s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.959 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.651 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.658 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.664 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.665 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.665 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.666 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.666 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.668 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.669 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.669 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.669 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.670 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.672 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.672 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.455 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.619 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.407 I llama_model_loader: - type  f32:  194 tensors
0.00.025.407 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.407 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.408 I print_info: file format = GGUF V3 (latest)
0.00.025.409 I print_info: file type   = Q5_1
0.00.025.411 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.202 I load: special tokens cache size = 25
0.00.040.390 I load: token to piece cache size = 0.2984 MB
0.00.040.394 I print_info: arch             = gptneox
0.00.040.395 I print_info: vocab_only       = 0
0.00.040.395 I print_info: n_ctx_train      = 2048
0.00.040.395 I print_info: n_embd           = 2048
0.00.040.395 I print_info: n_layer          = 24
0.00.040.400 I print_info: n_head           = 16
0.00.040.400 I print_info: n_head_kv        = 16
0.00.040.404 I print_info: n_rot            = 32
0.00.040.404 I print_info: n_swa            = 0
0.00.040.404 I print_info: n_embd_head_k    = 128
0.00.040.404 I print_info: n_embd_head_v    = 128
0.00.040.405 I print_info: n_gqa            = 1
0.00.040.406 I print_info: n_embd_k_gqa     = 2048
0.00.040.407 I print_info: n_embd_v_gqa     = 2048
0.00.040.407 I print_info: f_norm_eps       = 1.0e-05
0.00.040.408 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.408 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.408 I print_info: f_logit_scale    = 0.0e+00
0.00.040.409 I print_info: n_ff             = 8192
0.00.040.409 I print_info: n_expert         = 0
0.00.040.410 I print_info: n_expert_used    = 0
0.00.040.410 I print_info: causal attn      = 1
0.00.040.410 I print_info: pooling type     = 0
0.00.040.410 I print_info: rope type        = 2
0.00.040.411 I print_info: rope scaling     = linear
0.00.040.411 I print_info: freq_base_train  = 10000.0
0.00.040.412 I print_info: freq_scale_train = 1
0.00.040.413 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.413 I print_info: rope_finetuned   = unknown
0.00.040.414 I print_info: ssm_d_conv       = 0
0.00.040.414 I print_info: ssm_d_inner      = 0
0.00.040.414 I print_info: ssm_d_state      = 0
0.00.040.414 I print_info: ssm_dt_rank      = 0
0.00.040.414 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.414 I print_info: model type       = 1.4B
0.00.040.415 I print_info: model params     = 1.41 B
0.00.040.415 I print_info: general.name     = 1.4B
0.00.040.415 I print_info: vocab type       = BPE
0.00.040.415 I print_info: n_vocab          = 50304
0.00.040.416 I print_info: n_merges         = 50009
0.00.040.416 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.417 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.417 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.417 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.417 I print_info: LF token         = 187 ''
0.00.040.421 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.422 I print_info: max token length = 1024
0.00.040.422 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.656.928 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.937 I load_tensors: offloading output layer to GPU
0.00.656.938 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.967 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.656.970 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.658.515 I llama_init_from_model: n_seq_max     = 1
0.00.658.517 I llama_init_from_model: n_ctx         = 128
0.00.658.517 I llama_init_from_model: n_ctx_per_seq = 128
0.00.658.518 I llama_init_from_model: n_batch       = 128
0.00.658.518 I llama_init_from_model: n_ubatch      = 128
0.00.658.518 I llama_init_from_model: flash_attn    = 0
0.00.658.519 I llama_init_from_model: freq_base     = 10000.0
0.00.658.520 I llama_init_from_model: freq_scale    = 1
0.00.658.521 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.658.523 I ggml_metal_init: allocating
0.00.658.578 I ggml_metal_init: found device: Apple M4
0.00.658.592 I ggml_metal_init: picking default device: Apple M4
0.00.660.038 I ggml_metal_init: using embedded metal library
0.00.665.998 I ggml_metal_init: GPU name:   Apple M4
0.00.666.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.003 I ggml_metal_init: simdgroup reduction   = true
0.00.666.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.004 I ggml_metal_init: has residency sets    = true
0.00.666.004 I ggml_metal_init: has bfloat            = true
0.00.666.005 I ggml_metal_init: use bfloat            = true
0.00.666.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.008 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.550 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.686.004 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.686.011 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.686.065 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.232 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.689.234 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.689.234 I llama_init_from_model: graph nodes  = 967
0.00.689.234 I llama_init_from_model: graph splits = 2
0.00.689.237 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.689.237 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.722 I 
0.00.719.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.812 I perplexity: tokenizing the input ..
0.00.726.678 I perplexity: tokenization took 6.863 ms
0.00.726.683 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.867.544 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.868.888 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.868.916 I llama_perf_context_print:        load time =     710.75 ms
0.00.868.918 I llama_perf_context_print: prompt eval time =     140.62 ms /   128 tokens (    1.10 ms per token,   910.26 tokens per second)
0.00.868.920 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.868.920 I llama_perf_context_print:       total time =     149.20 ms /   129 tokens
0.00.869.310 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.078s
sys	0m0.138s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.180 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.898 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.904 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.905 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.910 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.910 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.910 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.911 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.912 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.912 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.912 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.913 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.914 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.918 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.918 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.918 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.835 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.512 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.512 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.513 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.513 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.514 I llama_model_loader: - type  f32:  194 tensors
0.00.025.514 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.514 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.515 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.515 I print_info: file format = GGUF V3 (latest)
0.00.025.516 I print_info: file type   = Q2_K - Medium
0.00.025.517 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.442 I load: special tokens cache size = 25
0.00.039.348 I load: token to piece cache size = 0.2984 MB
0.00.039.351 I print_info: arch             = gptneox
0.00.039.352 I print_info: vocab_only       = 0
0.00.039.352 I print_info: n_ctx_train      = 2048
0.00.039.352 I print_info: n_embd           = 2048
0.00.039.352 I print_info: n_layer          = 24
0.00.039.355 I print_info: n_head           = 16
0.00.039.356 I print_info: n_head_kv        = 16
0.00.039.356 I print_info: n_rot            = 32
0.00.039.356 I print_info: n_swa            = 0
0.00.039.357 I print_info: n_embd_head_k    = 128
0.00.039.357 I print_info: n_embd_head_v    = 128
0.00.039.358 I print_info: n_gqa            = 1
0.00.039.359 I print_info: n_embd_k_gqa     = 2048
0.00.039.360 I print_info: n_embd_v_gqa     = 2048
0.00.039.360 I print_info: f_norm_eps       = 1.0e-05
0.00.039.360 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.361 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.361 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.363 I print_info: f_logit_scale    = 0.0e+00
0.00.039.363 I print_info: n_ff             = 8192
0.00.039.363 I print_info: n_expert         = 0
0.00.039.364 I print_info: n_expert_used    = 0
0.00.039.364 I print_info: causal attn      = 1
0.00.039.365 I print_info: pooling type     = 0
0.00.039.367 I print_info: rope type        = 2
0.00.039.367 I print_info: rope scaling     = linear
0.00.039.368 I print_info: freq_base_train  = 10000.0
0.00.039.368 I print_info: freq_scale_train = 1
0.00.039.368 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.368 I print_info: rope_finetuned   = unknown
0.00.039.368 I print_info: ssm_d_conv       = 0
0.00.039.369 I print_info: ssm_d_inner      = 0
0.00.039.369 I print_info: ssm_d_state      = 0
0.00.039.369 I print_info: ssm_dt_rank      = 0
0.00.039.369 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.369 I print_info: model type       = 1.4B
0.00.039.370 I print_info: model params     = 1.41 B
0.00.039.370 I print_info: general.name     = 1.4B
0.00.039.374 I print_info: vocab type       = BPE
0.00.039.374 I print_info: n_vocab          = 50304
0.00.039.374 I print_info: n_merges         = 50009
0.00.039.374 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.375 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.375 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.375 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.376 I print_info: LF token         = 187 ''
0.00.039.378 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.378 I print_info: max token length = 1024
0.00.039.379 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.373.922 I load_tensors: offloading 24 repeating layers to GPU
0.00.373.936 I load_tensors: offloading output layer to GPU
0.00.373.936 I load_tensors: offloaded 25/25 layers to GPU
0.00.373.969 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.373.975 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.375.355 I llama_init_from_model: n_seq_max     = 1
0.00.375.357 I llama_init_from_model: n_ctx         = 2048
0.00.375.358 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.375.358 I llama_init_from_model: n_batch       = 2048
0.00.375.359 I llama_init_from_model: n_ubatch      = 512
0.00.375.359 I llama_init_from_model: flash_attn    = 0
0.00.375.361 I llama_init_from_model: freq_base     = 10000.0
0.00.375.362 I llama_init_from_model: freq_scale    = 1
0.00.375.366 I ggml_metal_init: allocating
0.00.375.447 I ggml_metal_init: found device: Apple M4
0.00.375.460 I ggml_metal_init: picking default device: Apple M4
0.00.377.397 I ggml_metal_init: using embedded metal library
0.00.383.394 I ggml_metal_init: GPU name:   Apple M4
0.00.383.407 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.383.408 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.383.409 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.383.409 I ggml_metal_init: simdgroup reduction   = true
0.00.383.410 I ggml_metal_init: simdgroup matrix mul. = true
0.00.383.410 I ggml_metal_init: has residency sets    = true
0.00.383.411 I ggml_metal_init: has bfloat            = true
0.00.383.411 I ggml_metal_init: use bfloat            = true
0.00.383.415 I ggml_metal_init: hasUnifiedMemory      = true
0.00.383.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.405.488 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.460.714 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.460.723 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.460.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.465.214 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.465.216 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.465.216 I llama_init_from_model: graph nodes  = 967
0.00.465.217 I llama_init_from_model: graph splits = 2
0.00.465.223 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.465.343 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.465.344 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.526.143 I main: llama threadpool init, n_threads = 4
0.00.526.185 I 
0.00.526.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.526.208 I 
0.00.526.353 I sampler seed: 1234
0.00.526.358 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.526.397 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.526.401 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.526.401 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.209.562 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.01.209.563 I llama_perf_context_print:        load time =     515.25 ms
0.01.209.564 I llama_perf_context_print: prompt eval time =      44.47 ms /     7 tokens (    6.35 ms per token,   157.41 tokens per second)
0.01.209.564 I llama_perf_context_print:        eval time =     635.92 ms /    63 runs   (   10.09 ms per token,    99.07 tokens per second)
0.01.209.566 I llama_perf_context_print:       total time =     684.13 ms /    70 tokens
0.01.209.769 I ggml_metal_free: deallocating

real	0m1.228s
user	0m0.112s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.388 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.209 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.210 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.211 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.214 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.214 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.214 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.215 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.220 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.221 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.957 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.036 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.785 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.788 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.789 I llama_model_loader: - type  f32:  194 tensors
0.00.024.789 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.789 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.790 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.790 I print_info: file format = GGUF V3 (latest)
0.00.024.791 I print_info: file type   = Q2_K - Medium
0.00.024.792 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.821 I load: special tokens cache size = 25
0.00.038.872 I load: token to piece cache size = 0.2984 MB
0.00.038.876 I print_info: arch             = gptneox
0.00.038.877 I print_info: vocab_only       = 0
0.00.038.877 I print_info: n_ctx_train      = 2048
0.00.038.877 I print_info: n_embd           = 2048
0.00.038.877 I print_info: n_layer          = 24
0.00.038.882 I print_info: n_head           = 16
0.00.038.882 I print_info: n_head_kv        = 16
0.00.038.882 I print_info: n_rot            = 32
0.00.038.883 I print_info: n_swa            = 0
0.00.038.883 I print_info: n_embd_head_k    = 128
0.00.038.883 I print_info: n_embd_head_v    = 128
0.00.038.884 I print_info: n_gqa            = 1
0.00.038.884 I print_info: n_embd_k_gqa     = 2048
0.00.038.885 I print_info: n_embd_v_gqa     = 2048
0.00.038.886 I print_info: f_norm_eps       = 1.0e-05
0.00.038.886 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.886 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.887 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.887 I print_info: f_logit_scale    = 0.0e+00
0.00.038.887 I print_info: n_ff             = 8192
0.00.038.887 I print_info: n_expert         = 0
0.00.038.888 I print_info: n_expert_used    = 0
0.00.038.888 I print_info: causal attn      = 1
0.00.038.888 I print_info: pooling type     = 0
0.00.038.888 I print_info: rope type        = 2
0.00.038.888 I print_info: rope scaling     = linear
0.00.038.888 I print_info: freq_base_train  = 10000.0
0.00.038.889 I print_info: freq_scale_train = 1
0.00.038.889 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.889 I print_info: rope_finetuned   = unknown
0.00.038.889 I print_info: ssm_d_conv       = 0
0.00.038.889 I print_info: ssm_d_inner      = 0
0.00.038.892 I print_info: ssm_d_state      = 0
0.00.038.892 I print_info: ssm_dt_rank      = 0
0.00.038.893 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.893 I print_info: model type       = 1.4B
0.00.038.893 I print_info: model params     = 1.41 B
0.00.038.893 I print_info: general.name     = 1.4B
0.00.038.894 I print_info: vocab type       = BPE
0.00.038.894 I print_info: n_vocab          = 50304
0.00.038.895 I print_info: n_merges         = 50009
0.00.038.896 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.896 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.896 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.896 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.896 I print_info: LF token         = 187 ''
0.00.038.897 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.897 I print_info: max token length = 1024
0.00.038.897 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.361.277 I load_tensors: offloading 24 repeating layers to GPU
0.00.361.293 I load_tensors: offloading output layer to GPU
0.00.361.294 I load_tensors: offloaded 25/25 layers to GPU
0.00.361.332 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.361.333 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.363.174 I llama_init_from_model: n_seq_max     = 1
0.00.363.176 I llama_init_from_model: n_ctx         = 128
0.00.363.177 I llama_init_from_model: n_ctx_per_seq = 128
0.00.363.177 I llama_init_from_model: n_batch       = 128
0.00.363.178 I llama_init_from_model: n_ubatch      = 128
0.00.363.178 I llama_init_from_model: flash_attn    = 0
0.00.363.181 I llama_init_from_model: freq_base     = 10000.0
0.00.363.181 I llama_init_from_model: freq_scale    = 1
0.00.363.182 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.363.184 I ggml_metal_init: allocating
0.00.363.276 I ggml_metal_init: found device: Apple M4
0.00.363.289 I ggml_metal_init: picking default device: Apple M4
0.00.365.302 I ggml_metal_init: using embedded metal library
0.00.370.846 I ggml_metal_init: GPU name:   Apple M4
0.00.370.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.370.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.370.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.370.870 I ggml_metal_init: simdgroup reduction   = true
0.00.370.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.370.870 I ggml_metal_init: has residency sets    = true
0.00.370.871 I ggml_metal_init: has bfloat            = true
0.00.370.871 I ggml_metal_init: use bfloat            = true
0.00.370.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.370.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.424 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.397.120 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.397.127 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.397.183 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.400.536 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.400.538 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.400.538 I llama_init_from_model: graph nodes  = 967
0.00.400.539 I llama_init_from_model: graph splits = 2
0.00.400.542 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.400.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.433.268 I 
0.00.433.353 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.433.358 I perplexity: tokenizing the input ..
0.00.440.031 I perplexity: tokenization took 6.67 ms
0.00.440.036 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.585.187 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.586.697 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.586.720 I llama_perf_context_print:        load time =     423.87 ms
0.00.586.721 I llama_perf_context_print: prompt eval time =     144.26 ms /   128 tokens (    1.13 ms per token,   887.27 tokens per second)
0.00.586.721 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.586.722 I llama_perf_context_print:       total time =     153.46 ms /   129 tokens
0.00.587.067 I ggml_metal_free: deallocating

real	0m0.602s
user	0m0.081s
sys	0m0.089s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.940 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.431 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.433 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.433 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.434 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.435 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.436 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.437 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.442 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.339 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.319 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.320 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.321 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.321 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.321 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.322 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.322 I llama_model_loader: - type  f32:  194 tensors
0.00.025.322 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.323 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.323 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.323 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.324 I print_info: file format = GGUF V3 (latest)
0.00.025.324 I print_info: file type   = Q3_K - Medium
0.00.025.325 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.568 I load: special tokens cache size = 25
0.00.039.302 I load: token to piece cache size = 0.2984 MB
0.00.039.305 I print_info: arch             = gptneox
0.00.039.305 I print_info: vocab_only       = 0
0.00.039.306 I print_info: n_ctx_train      = 2048
0.00.039.306 I print_info: n_embd           = 2048
0.00.039.306 I print_info: n_layer          = 24
0.00.039.310 I print_info: n_head           = 16
0.00.039.310 I print_info: n_head_kv        = 16
0.00.039.311 I print_info: n_rot            = 32
0.00.039.311 I print_info: n_swa            = 0
0.00.039.311 I print_info: n_embd_head_k    = 128
0.00.039.314 I print_info: n_embd_head_v    = 128
0.00.039.314 I print_info: n_gqa            = 1
0.00.039.315 I print_info: n_embd_k_gqa     = 2048
0.00.039.316 I print_info: n_embd_v_gqa     = 2048
0.00.039.316 I print_info: f_norm_eps       = 1.0e-05
0.00.039.318 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.318 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.318 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.319 I print_info: f_logit_scale    = 0.0e+00
0.00.039.319 I print_info: n_ff             = 8192
0.00.039.319 I print_info: n_expert         = 0
0.00.039.320 I print_info: n_expert_used    = 0
0.00.039.321 I print_info: causal attn      = 1
0.00.039.322 I print_info: pooling type     = 0
0.00.039.322 I print_info: rope type        = 2
0.00.039.322 I print_info: rope scaling     = linear
0.00.039.322 I print_info: freq_base_train  = 10000.0
0.00.039.323 I print_info: freq_scale_train = 1
0.00.039.323 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.323 I print_info: rope_finetuned   = unknown
0.00.039.327 I print_info: ssm_d_conv       = 0
0.00.039.327 I print_info: ssm_d_inner      = 0
0.00.039.328 I print_info: ssm_d_state      = 0
0.00.039.328 I print_info: ssm_dt_rank      = 0
0.00.039.328 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.328 I print_info: model type       = 1.4B
0.00.039.329 I print_info: model params     = 1.41 B
0.00.039.329 I print_info: general.name     = 1.4B
0.00.039.329 I print_info: vocab type       = BPE
0.00.039.329 I print_info: n_vocab          = 50304
0.00.039.329 I print_info: n_merges         = 50009
0.00.039.330 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.330 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.330 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.330 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.330 I print_info: LF token         = 187 ''
0.00.039.330 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.331 I print_info: max token length = 1024
0.00.039.331 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.440.653 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.665 I load_tensors: offloading output layer to GPU
0.00.440.666 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.699 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.701 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.137 I llama_init_from_model: n_seq_max     = 1
0.00.442.140 I llama_init_from_model: n_ctx         = 2048
0.00.442.141 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.442.141 I llama_init_from_model: n_batch       = 2048
0.00.442.141 I llama_init_from_model: n_ubatch      = 512
0.00.442.142 I llama_init_from_model: flash_attn    = 0
0.00.442.144 I llama_init_from_model: freq_base     = 10000.0
0.00.442.145 I llama_init_from_model: freq_scale    = 1
0.00.442.147 I ggml_metal_init: allocating
0.00.442.216 I ggml_metal_init: found device: Apple M4
0.00.442.230 I ggml_metal_init: picking default device: Apple M4
0.00.444.153 I ggml_metal_init: using embedded metal library
0.00.449.778 I ggml_metal_init: GPU name:   Apple M4
0.00.449.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.449.784 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.449.785 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.449.786 I ggml_metal_init: simdgroup reduction   = true
0.00.449.786 I ggml_metal_init: simdgroup matrix mul. = true
0.00.449.786 I ggml_metal_init: has residency sets    = true
0.00.449.787 I ggml_metal_init: has bfloat            = true
0.00.449.787 I ggml_metal_init: use bfloat            = true
0.00.449.788 I ggml_metal_init: hasUnifiedMemory      = true
0.00.449.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.470.667 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.532.028 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.532.036 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.532.070 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.536.422 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.536.424 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.536.424 I llama_init_from_model: graph nodes  = 967
0.00.536.424 I llama_init_from_model: graph splits = 2
0.00.536.430 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.536.560 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.536.561 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.595.967 I main: llama threadpool init, n_threads = 4
0.00.596.007 I 
0.00.596.030 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.030 I 
0.00.596.172 I sampler seed: 1234
0.00.596.176 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.596.219 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.596.221 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.596.222 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.349.448 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48398.09 tokens per second)
0.01.349.449 I llama_perf_context_print:        load time =     586.31 ms
0.01.349.450 I llama_perf_context_print: prompt eval time =      49.74 ms /     7 tokens (    7.11 ms per token,   140.73 tokens per second)
0.01.349.451 I llama_perf_context_print:        eval time =     700.88 ms /    63 runs   (   11.13 ms per token,    89.89 tokens per second)
0.01.349.451 I llama_perf_context_print:       total time =     754.20 ms /    70 tokens
0.01.349.697 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.114s
sys	0m0.191s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.952 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.974 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.977 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.978 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.978 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.979 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.980 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.981 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.983 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.983 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.983 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.756 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.897 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.674 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.676 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.678 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.678 I llama_model_loader: - type  f32:  194 tensors
0.00.024.679 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.681 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.681 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.681 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.682 I print_info: file format = GGUF V3 (latest)
0.00.024.682 I print_info: file type   = Q3_K - Medium
0.00.024.684 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.785 I load: special tokens cache size = 25
0.00.038.976 I load: token to piece cache size = 0.2984 MB
0.00.038.980 I print_info: arch             = gptneox
0.00.038.981 I print_info: vocab_only       = 0
0.00.038.981 I print_info: n_ctx_train      = 2048
0.00.038.981 I print_info: n_embd           = 2048
0.00.038.981 I print_info: n_layer          = 24
0.00.038.986 I print_info: n_head           = 16
0.00.038.986 I print_info: n_head_kv        = 16
0.00.038.986 I print_info: n_rot            = 32
0.00.038.987 I print_info: n_swa            = 0
0.00.038.987 I print_info: n_embd_head_k    = 128
0.00.038.987 I print_info: n_embd_head_v    = 128
0.00.038.988 I print_info: n_gqa            = 1
0.00.038.988 I print_info: n_embd_k_gqa     = 2048
0.00.038.989 I print_info: n_embd_v_gqa     = 2048
0.00.038.991 I print_info: f_norm_eps       = 1.0e-05
0.00.038.992 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.992 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.992 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.992 I print_info: f_logit_scale    = 0.0e+00
0.00.038.993 I print_info: n_ff             = 8192
0.00.038.993 I print_info: n_expert         = 0
0.00.038.993 I print_info: n_expert_used    = 0
0.00.038.993 I print_info: causal attn      = 1
0.00.038.993 I print_info: pooling type     = 0
0.00.038.993 I print_info: rope type        = 2
0.00.038.994 I print_info: rope scaling     = linear
0.00.038.994 I print_info: freq_base_train  = 10000.0
0.00.038.994 I print_info: freq_scale_train = 1
0.00.038.995 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.995 I print_info: rope_finetuned   = unknown
0.00.038.995 I print_info: ssm_d_conv       = 0
0.00.038.997 I print_info: ssm_d_inner      = 0
0.00.038.997 I print_info: ssm_d_state      = 0
0.00.038.997 I print_info: ssm_dt_rank      = 0
0.00.038.998 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.998 I print_info: model type       = 1.4B
0.00.038.998 I print_info: model params     = 1.41 B
0.00.038.998 I print_info: general.name     = 1.4B
0.00.038.999 I print_info: vocab type       = BPE
0.00.038.999 I print_info: n_vocab          = 50304
0.00.039.000 I print_info: n_merges         = 50009
0.00.039.001 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: LF token         = 187 ''
0.00.039.002 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.002 I print_info: max token length = 1024
0.00.039.002 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.472.645 I load_tensors: offloading 24 repeating layers to GPU
0.00.472.662 I load_tensors: offloading output layer to GPU
0.00.472.663 I load_tensors: offloaded 25/25 layers to GPU
0.00.472.699 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.472.701 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.474.120 I llama_init_from_model: n_seq_max     = 1
0.00.474.124 I llama_init_from_model: n_ctx         = 128
0.00.474.124 I llama_init_from_model: n_ctx_per_seq = 128
0.00.474.125 I llama_init_from_model: n_batch       = 128
0.00.474.125 I llama_init_from_model: n_ubatch      = 128
0.00.474.126 I llama_init_from_model: flash_attn    = 0
0.00.474.127 I llama_init_from_model: freq_base     = 10000.0
0.00.474.128 I llama_init_from_model: freq_scale    = 1
0.00.474.128 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.474.131 I ggml_metal_init: allocating
0.00.474.219 I ggml_metal_init: found device: Apple M4
0.00.474.235 I ggml_metal_init: picking default device: Apple M4
0.00.476.197 I ggml_metal_init: using embedded metal library
0.00.481.868 I ggml_metal_init: GPU name:   Apple M4
0.00.481.885 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.481.885 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.481.886 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.481.886 I ggml_metal_init: simdgroup reduction   = true
0.00.481.887 I ggml_metal_init: simdgroup matrix mul. = true
0.00.481.887 I ggml_metal_init: has residency sets    = true
0.00.481.887 I ggml_metal_init: has bfloat            = true
0.00.481.888 I ggml_metal_init: use bfloat            = true
0.00.481.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.481.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.503.476 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.507.154 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.507.159 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.507.212 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.510.444 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.510.446 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.510.446 I llama_init_from_model: graph nodes  = 967
0.00.510.447 I llama_init_from_model: graph splits = 2
0.00.510.450 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.510.450 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.536.733 I 
0.00.536.815 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.536.822 I perplexity: tokenizing the input ..
0.00.543.854 I perplexity: tokenization took 7.028 ms
0.00.543.862 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.676.095 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.677.391 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.677.415 I llama_perf_context_print:        load time =     527.77 ms
0.00.677.416 I llama_perf_context_print: prompt eval time =     131.36 ms /   128 tokens (    1.03 ms per token,   974.41 tokens per second)
0.00.677.417 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.677.417 I llama_perf_context_print:       total time =     140.69 ms /   129 tokens
0.00.677.801 I ggml_metal_free: deallocating

real	0m0.691s
user	0m0.081s
sys	0m0.133s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.008.795 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.032 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.028.037 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.039 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.040 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.041 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.041 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.042 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.043 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.043 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.044 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.044 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.045 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.904 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.036.895 I llama_model_loader: - type  f32:  194 tensors
0.00.036.896 I llama_model_loader: - type q4_K:   61 tensors
0.00.036.896 I llama_model_loader: - type q5_K:   24 tensors
0.00.036.896 I llama_model_loader: - type q6_K:   13 tensors
0.00.036.896 I print_info: file format = GGUF V3 (latest)
0.00.036.897 I print_info: file type   = Q4_K - Medium
0.00.036.897 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.499 I load: special tokens cache size = 25
0.00.052.386 I load: token to piece cache size = 0.2984 MB
0.00.052.389 I print_info: arch             = gptneox
0.00.052.390 I print_info: vocab_only       = 0
0.00.052.390 I print_info: n_ctx_train      = 2048
0.00.052.390 I print_info: n_embd           = 2048
0.00.052.390 I print_info: n_layer          = 24
0.00.052.393 I print_info: n_head           = 16
0.00.052.394 I print_info: n_head_kv        = 16
0.00.052.394 I print_info: n_rot            = 32
0.00.052.394 I print_info: n_swa            = 0
0.00.052.394 I print_info: n_embd_head_k    = 128
0.00.052.397 I print_info: n_embd_head_v    = 128
0.00.052.397 I print_info: n_gqa            = 1
0.00.052.398 I print_info: n_embd_k_gqa     = 2048
0.00.052.399 I print_info: n_embd_v_gqa     = 2048
0.00.052.399 I print_info: f_norm_eps       = 1.0e-05
0.00.052.401 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.402 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.403 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.403 I print_info: f_logit_scale    = 0.0e+00
0.00.052.404 I print_info: n_ff             = 8192
0.00.052.404 I print_info: n_expert         = 0
0.00.052.404 I print_info: n_expert_used    = 0
0.00.052.404 I print_info: causal attn      = 1
0.00.052.404 I print_info: pooling type     = 0
0.00.052.404 I print_info: rope type        = 2
0.00.052.405 I print_info: rope scaling     = linear
0.00.052.405 I print_info: freq_base_train  = 10000.0
0.00.052.405 I print_info: freq_scale_train = 1
0.00.052.405 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.405 I print_info: rope_finetuned   = unknown
0.00.052.406 I print_info: ssm_d_conv       = 0
0.00.052.406 I print_info: ssm_d_inner      = 0
0.00.052.406 I print_info: ssm_d_state      = 0
0.00.052.406 I print_info: ssm_dt_rank      = 0
0.00.052.406 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.406 I print_info: model type       = 1.4B
0.00.052.407 I print_info: model params     = 1.41 B
0.00.052.407 I print_info: general.name     = 1.4B
0.00.052.411 I print_info: vocab type       = BPE
0.00.052.411 I print_info: n_vocab          = 50304
0.00.052.411 I print_info: n_merges         = 50009
0.00.052.411 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.411 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.412 I print_info: LF token         = 187 ''
0.00.052.412 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.413 I print_info: max token length = 1024
0.00.052.413 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.005.023 I load_tensors: offloading 24 repeating layers to GPU
0.01.005.028 I load_tensors: offloading output layer to GPU
0.01.005.028 I load_tensors: offloaded 25/25 layers to GPU
0.01.005.044 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.01.005.047 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.01.005.938 I llama_init_from_model: n_seq_max     = 1
0.01.005.941 I llama_init_from_model: n_ctx         = 2048
0.01.005.942 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.005.942 I llama_init_from_model: n_batch       = 2048
0.01.005.943 I llama_init_from_model: n_ubatch      = 512
0.01.005.943 I llama_init_from_model: flash_attn    = 0
0.01.005.944 I llama_init_from_model: freq_base     = 10000.0
0.01.005.945 I llama_init_from_model: freq_scale    = 1
0.01.005.946 I ggml_metal_init: allocating
0.01.005.980 I ggml_metal_init: found device: Apple M4
0.01.005.993 I ggml_metal_init: picking default device: Apple M4
0.01.007.106 I ggml_metal_init: using embedded metal library
0.01.011.232 I ggml_metal_init: GPU name:   Apple M4
0.01.011.237 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.011.238 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.011.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.011.239 I ggml_metal_init: simdgroup reduction   = true
0.01.011.239 I ggml_metal_init: simdgroup matrix mul. = true
0.01.011.240 I ggml_metal_init: has residency sets    = true
0.01.011.240 I ggml_metal_init: has bfloat            = true
0.01.011.240 I ggml_metal_init: use bfloat            = true
0.01.011.242 I ggml_metal_init: hasUnifiedMemory      = true
0.01.011.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.029.577 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.060.684 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.060.691 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.060.726 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.065.179 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.065.181 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.065.182 I llama_init_from_model: graph nodes  = 967
0.01.065.182 I llama_init_from_model: graph splits = 2
0.01.065.187 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.065.315 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.065.316 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.121.478 I main: llama threadpool init, n_threads = 4
0.01.121.518 I 
0.01.121.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.121.540 I 
0.01.121.675 I sampler seed: 1234
0.01.121.680 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.121.691 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.121.691 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.121.691 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.883.546 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48931.77 tokens per second)
0.01.883.546 I llama_perf_context_print:        load time =    1111.98 ms
0.01.883.547 I llama_perf_context_print: prompt eval time =      47.32 ms /     7 tokens (    6.76 ms per token,   147.93 tokens per second)
0.01.883.548 I llama_perf_context_print:        eval time =     711.51 ms /    63 runs   (   11.29 ms per token,    88.54 tokens per second)
0.01.883.548 I llama_perf_context_print:       total time =     762.77 ms /    70 tokens
0.01.883.776 I ggml_metal_free: deallocating

real	0m1.901s
user	0m0.106s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.635 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.660 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.662 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.662 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.663 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.663 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.663 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.664 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.667 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.668 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.669 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.671 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.671 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.337 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.338 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.338 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.339 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.339 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.340 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.340 I llama_model_loader: - type  f32:  194 tensors
0.00.024.340 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.341 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.341 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.341 I print_info: file format = GGUF V3 (latest)
0.00.024.342 I print_info: file type   = Q4_K - Medium
0.00.024.343 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.490 I load: special tokens cache size = 25
0.00.038.756 I load: token to piece cache size = 0.2984 MB
0.00.038.760 I print_info: arch             = gptneox
0.00.038.760 I print_info: vocab_only       = 0
0.00.038.760 I print_info: n_ctx_train      = 2048
0.00.038.760 I print_info: n_embd           = 2048
0.00.038.761 I print_info: n_layer          = 24
0.00.038.764 I print_info: n_head           = 16
0.00.038.765 I print_info: n_head_kv        = 16
0.00.038.765 I print_info: n_rot            = 32
0.00.038.766 I print_info: n_swa            = 0
0.00.038.766 I print_info: n_embd_head_k    = 128
0.00.038.766 I print_info: n_embd_head_v    = 128
0.00.038.767 I print_info: n_gqa            = 1
0.00.038.767 I print_info: n_embd_k_gqa     = 2048
0.00.038.768 I print_info: n_embd_v_gqa     = 2048
0.00.038.769 I print_info: f_norm_eps       = 1.0e-05
0.00.038.769 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.769 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.769 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.770 I print_info: f_logit_scale    = 0.0e+00
0.00.038.770 I print_info: n_ff             = 8192
0.00.038.770 I print_info: n_expert         = 0
0.00.038.771 I print_info: n_expert_used    = 0
0.00.038.771 I print_info: causal attn      = 1
0.00.038.771 I print_info: pooling type     = 0
0.00.038.771 I print_info: rope type        = 2
0.00.038.771 I print_info: rope scaling     = linear
0.00.038.771 I print_info: freq_base_train  = 10000.0
0.00.038.772 I print_info: freq_scale_train = 1
0.00.038.772 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.772 I print_info: rope_finetuned   = unknown
0.00.038.772 I print_info: ssm_d_conv       = 0
0.00.038.772 I print_info: ssm_d_inner      = 0
0.00.038.772 I print_info: ssm_d_state      = 0
0.00.038.772 I print_info: ssm_dt_rank      = 0
0.00.038.773 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.773 I print_info: model type       = 1.4B
0.00.038.773 I print_info: model params     = 1.41 B
0.00.038.773 I print_info: general.name     = 1.4B
0.00.038.774 I print_info: vocab type       = BPE
0.00.038.774 I print_info: n_vocab          = 50304
0.00.038.774 I print_info: n_merges         = 50009
0.00.038.774 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.775 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.775 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.775 I print_info: LF token         = 187 ''
0.00.038.775 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.775 I print_info: max token length = 1024
0.00.038.779 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.512.049 I load_tensors: offloading 24 repeating layers to GPU
0.00.512.062 I load_tensors: offloading output layer to GPU
0.00.512.063 I load_tensors: offloaded 25/25 layers to GPU
0.00.512.096 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.512.098 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.513.787 I llama_init_from_model: n_seq_max     = 1
0.00.513.789 I llama_init_from_model: n_ctx         = 128
0.00.513.790 I llama_init_from_model: n_ctx_per_seq = 128
0.00.513.790 I llama_init_from_model: n_batch       = 128
0.00.513.791 I llama_init_from_model: n_ubatch      = 128
0.00.513.791 I llama_init_from_model: flash_attn    = 0
0.00.513.794 I llama_init_from_model: freq_base     = 10000.0
0.00.513.794 I llama_init_from_model: freq_scale    = 1
0.00.513.795 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.513.797 I ggml_metal_init: allocating
0.00.513.893 I ggml_metal_init: found device: Apple M4
0.00.513.907 I ggml_metal_init: picking default device: Apple M4
0.00.515.797 I ggml_metal_init: using embedded metal library
0.00.522.610 I ggml_metal_init: GPU name:   Apple M4
0.00.522.619 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.522.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.522.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.522.621 I ggml_metal_init: simdgroup reduction   = true
0.00.522.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.522.621 I ggml_metal_init: has residency sets    = true
0.00.522.622 I ggml_metal_init: has bfloat            = true
0.00.522.622 I ggml_metal_init: use bfloat            = true
0.00.522.623 I ggml_metal_init: hasUnifiedMemory      = true
0.00.522.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.541.078 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.544.667 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.544.671 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.544.738 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.547.869 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.547.870 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.547.871 I llama_init_from_model: graph nodes  = 967
0.00.547.871 I llama_init_from_model: graph splits = 2
0.00.547.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.547.874 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.690 I 
0.00.575.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.575.780 I perplexity: tokenizing the input ..
0.00.582.991 I perplexity: tokenization took 7.209 ms
0.00.582.997 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.728.351 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.729.689 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.729.712 I llama_perf_context_print:        load time =     567.04 ms
0.00.729.713 I llama_perf_context_print: prompt eval time =     144.95 ms /   128 tokens (    1.13 ms per token,   883.04 tokens per second)
0.00.729.714 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.729.714 I llama_perf_context_print:       total time =     154.03 ms /   129 tokens
0.00.730.089 I ggml_metal_free: deallocating

real	0m0.744s
user	0m0.080s
sys	0m0.121s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.010.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.394 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.399 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.401 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.402 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.402 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.403 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.403 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.403 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.404 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.404 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.404 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.405 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.407 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.407 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.099 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.100 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.101 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.101 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.101 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.102 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.102 I llama_model_loader: - type  f32:  194 tensors
0.00.027.103 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.103 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.103 I print_info: file format = GGUF V3 (latest)
0.00.027.104 I print_info: file type   = Q5_K - Medium
0.00.027.105 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.708 I load: special tokens cache size = 25
0.00.041.589 I load: token to piece cache size = 0.2984 MB
0.00.041.592 I print_info: arch             = gptneox
0.00.041.592 I print_info: vocab_only       = 0
0.00.041.592 I print_info: n_ctx_train      = 2048
0.00.041.593 I print_info: n_embd           = 2048
0.00.041.593 I print_info: n_layer          = 24
0.00.041.596 I print_info: n_head           = 16
0.00.041.596 I print_info: n_head_kv        = 16
0.00.041.597 I print_info: n_rot            = 32
0.00.041.597 I print_info: n_swa            = 0
0.00.041.597 I print_info: n_embd_head_k    = 128
0.00.041.597 I print_info: n_embd_head_v    = 128
0.00.041.598 I print_info: n_gqa            = 1
0.00.041.599 I print_info: n_embd_k_gqa     = 2048
0.00.041.599 I print_info: n_embd_v_gqa     = 2048
0.00.041.600 I print_info: f_norm_eps       = 1.0e-05
0.00.041.600 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.602 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.602 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.602 I print_info: f_logit_scale    = 0.0e+00
0.00.041.603 I print_info: n_ff             = 8192
0.00.041.603 I print_info: n_expert         = 0
0.00.041.604 I print_info: n_expert_used    = 0
0.00.041.604 I print_info: causal attn      = 1
0.00.041.604 I print_info: pooling type     = 0
0.00.041.604 I print_info: rope type        = 2
0.00.041.606 I print_info: rope scaling     = linear
0.00.041.606 I print_info: freq_base_train  = 10000.0
0.00.041.607 I print_info: freq_scale_train = 1
0.00.041.607 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.607 I print_info: rope_finetuned   = unknown
0.00.041.607 I print_info: ssm_d_conv       = 0
0.00.041.607 I print_info: ssm_d_inner      = 0
0.00.041.607 I print_info: ssm_d_state      = 0
0.00.041.608 I print_info: ssm_dt_rank      = 0
0.00.041.608 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.608 I print_info: model type       = 1.4B
0.00.041.608 I print_info: model params     = 1.41 B
0.00.041.609 I print_info: general.name     = 1.4B
0.00.041.609 I print_info: vocab type       = BPE
0.00.041.609 I print_info: n_vocab          = 50304
0.00.041.609 I print_info: n_merges         = 50009
0.00.041.614 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.614 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.614 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.614 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.614 I print_info: LF token         = 187 ''
0.00.041.615 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.615 I print_info: max token length = 1024
0.00.041.615 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.586.734 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.736 I load_tensors: offloading output layer to GPU
0.00.586.737 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.764 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.586.767 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.588.286 I llama_init_from_model: n_seq_max     = 1
0.00.588.288 I llama_init_from_model: n_ctx         = 2048
0.00.588.289 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.588.289 I llama_init_from_model: n_batch       = 2048
0.00.588.289 I llama_init_from_model: n_ubatch      = 512
0.00.588.290 I llama_init_from_model: flash_attn    = 0
0.00.588.291 I llama_init_from_model: freq_base     = 10000.0
0.00.588.292 I llama_init_from_model: freq_scale    = 1
0.00.588.293 I ggml_metal_init: allocating
0.00.588.351 I ggml_metal_init: found device: Apple M4
0.00.588.364 I ggml_metal_init: picking default device: Apple M4
0.00.589.924 I ggml_metal_init: using embedded metal library
0.00.596.030 I ggml_metal_init: GPU name:   Apple M4
0.00.596.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.036 I ggml_metal_init: simdgroup reduction   = true
0.00.596.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.036 I ggml_metal_init: has residency sets    = true
0.00.596.036 I ggml_metal_init: has bfloat            = true
0.00.596.036 I ggml_metal_init: use bfloat            = true
0.00.596.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.893 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.925 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.664.932 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.664.967 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.670.121 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.670.123 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.670.124 I llama_init_from_model: graph nodes  = 967
0.00.670.124 I llama_init_from_model: graph splits = 2
0.00.670.129 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.670.251 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.670.251 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.733.985 I main: llama threadpool init, n_threads = 4
0.00.734.026 I 
0.00.734.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.051 I 
0.00.734.218 I sampler seed: 1234
0.00.734.222 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.240 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.240 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.240 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.585.149 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.585.150 I llama_perf_context_print:        load time =     722.36 ms
0.01.585.150 I llama_perf_context_print: prompt eval time =      52.64 ms /     7 tokens (    7.52 ms per token,   132.99 tokens per second)
0.01.585.151 I llama_perf_context_print:        eval time =     795.41 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.585.151 I llama_perf_context_print:       total time =     851.93 ms /    70 tokens
0.01.585.393 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.109s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.859 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.874 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.880 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.886 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.887 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.889 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.889 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.890 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.890 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.890 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.891 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.892 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.893 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.893 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.627 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.475 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.475 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.476 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.476 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.476 I llama_model_loader: - type  f32:  194 tensors
0.00.025.477 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.477 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.478 I print_info: file format = GGUF V3 (latest)
0.00.025.478 I print_info: file type   = Q5_K - Medium
0.00.025.479 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.484 I load: special tokens cache size = 25
0.00.039.621 I load: token to piece cache size = 0.2984 MB
0.00.039.626 I print_info: arch             = gptneox
0.00.039.626 I print_info: vocab_only       = 0
0.00.039.626 I print_info: n_ctx_train      = 2048
0.00.039.626 I print_info: n_embd           = 2048
0.00.039.627 I print_info: n_layer          = 24
0.00.039.630 I print_info: n_head           = 16
0.00.039.631 I print_info: n_head_kv        = 16
0.00.039.631 I print_info: n_rot            = 32
0.00.039.631 I print_info: n_swa            = 0
0.00.039.632 I print_info: n_embd_head_k    = 128
0.00.039.632 I print_info: n_embd_head_v    = 128
0.00.039.634 I print_info: n_gqa            = 1
0.00.039.635 I print_info: n_embd_k_gqa     = 2048
0.00.039.635 I print_info: n_embd_v_gqa     = 2048
0.00.039.636 I print_info: f_norm_eps       = 1.0e-05
0.00.039.636 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.636 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.636 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.637 I print_info: f_logit_scale    = 0.0e+00
0.00.039.639 I print_info: n_ff             = 8192
0.00.039.640 I print_info: n_expert         = 0
0.00.039.640 I print_info: n_expert_used    = 0
0.00.039.640 I print_info: causal attn      = 1
0.00.039.640 I print_info: pooling type     = 0
0.00.039.640 I print_info: rope type        = 2
0.00.039.640 I print_info: rope scaling     = linear
0.00.039.641 I print_info: freq_base_train  = 10000.0
0.00.039.641 I print_info: freq_scale_train = 1
0.00.039.641 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.641 I print_info: rope_finetuned   = unknown
0.00.039.641 I print_info: ssm_d_conv       = 0
0.00.039.642 I print_info: ssm_d_inner      = 0
0.00.039.643 I print_info: ssm_d_state      = 0
0.00.039.643 I print_info: ssm_dt_rank      = 0
0.00.039.643 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.643 I print_info: model type       = 1.4B
0.00.039.643 I print_info: model params     = 1.41 B
0.00.039.643 I print_info: general.name     = 1.4B
0.00.039.645 I print_info: vocab type       = BPE
0.00.039.645 I print_info: n_vocab          = 50304
0.00.039.645 I print_info: n_merges         = 50009
0.00.039.645 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.646 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.646 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.646 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.646 I print_info: LF token         = 187 ''
0.00.039.646 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.647 I print_info: max token length = 1024
0.00.039.647 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.582.381 I load_tensors: offloading 24 repeating layers to GPU
0.00.582.397 I load_tensors: offloading output layer to GPU
0.00.582.398 I load_tensors: offloaded 25/25 layers to GPU
0.00.582.430 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.582.432 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.584.160 I llama_init_from_model: n_seq_max     = 1
0.00.584.163 I llama_init_from_model: n_ctx         = 128
0.00.584.163 I llama_init_from_model: n_ctx_per_seq = 128
0.00.584.164 I llama_init_from_model: n_batch       = 128
0.00.584.164 I llama_init_from_model: n_ubatch      = 128
0.00.584.164 I llama_init_from_model: flash_attn    = 0
0.00.584.166 I llama_init_from_model: freq_base     = 10000.0
0.00.584.167 I llama_init_from_model: freq_scale    = 1
0.00.584.167 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.584.171 I ggml_metal_init: allocating
0.00.584.249 I ggml_metal_init: found device: Apple M4
0.00.584.263 I ggml_metal_init: picking default device: Apple M4
0.00.586.130 I ggml_metal_init: using embedded metal library
0.00.593.544 I ggml_metal_init: GPU name:   Apple M4
0.00.593.549 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.551 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.551 I ggml_metal_init: simdgroup reduction   = true
0.00.593.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.552 I ggml_metal_init: has residency sets    = true
0.00.593.552 I ggml_metal_init: has bfloat            = true
0.00.593.552 I ggml_metal_init: use bfloat            = true
0.00.593.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.557 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.058 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.777 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.615.793 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.862 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.024 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.619.026 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.619.026 I llama_init_from_model: graph nodes  = 967
0.00.619.027 I llama_init_from_model: graph splits = 2
0.00.619.030 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.117 I 
0.00.651.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.208 I perplexity: tokenizing the input ..
0.00.658.504 I perplexity: tokenization took 7.293 ms
0.00.658.523 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.603 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.943 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.963 I llama_perf_context_print:        load time =     641.25 ms
0.00.796.963 I llama_perf_context_print: prompt eval time =     136.23 ms /   128 tokens (    1.06 ms per token,   939.61 tokens per second)
0.00.796.964 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.964 I llama_perf_context_print:       total time =     145.85 ms /   129 tokens
0.00.797.295 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.081s
sys	0m0.131s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.806 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.049 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.050 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.050 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.050 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.053 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.053 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.054 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.054 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.054 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.055 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.057 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.058 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.874 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.769 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.770 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.770 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.770 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.771 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.771 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.771 I llama_model_loader: - type  f32:  194 tensors
0.00.025.772 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.773 I print_info: file format = GGUF V3 (latest)
0.00.025.773 I print_info: file type   = Q6_K
0.00.025.774 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.024 I load: special tokens cache size = 25
0.00.040.061 I load: token to piece cache size = 0.2984 MB
0.00.040.063 I print_info: arch             = gptneox
0.00.040.064 I print_info: vocab_only       = 0
0.00.040.064 I print_info: n_ctx_train      = 2048
0.00.040.064 I print_info: n_embd           = 2048
0.00.040.064 I print_info: n_layer          = 24
0.00.040.067 I print_info: n_head           = 16
0.00.040.067 I print_info: n_head_kv        = 16
0.00.040.068 I print_info: n_rot            = 32
0.00.040.068 I print_info: n_swa            = 0
0.00.040.068 I print_info: n_embd_head_k    = 128
0.00.040.068 I print_info: n_embd_head_v    = 128
0.00.040.069 I print_info: n_gqa            = 1
0.00.040.070 I print_info: n_embd_k_gqa     = 2048
0.00.040.070 I print_info: n_embd_v_gqa     = 2048
0.00.040.071 I print_info: f_norm_eps       = 1.0e-05
0.00.040.071 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.072 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.072 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.072 I print_info: f_logit_scale    = 0.0e+00
0.00.040.073 I print_info: n_ff             = 8192
0.00.040.073 I print_info: n_expert         = 0
0.00.040.073 I print_info: n_expert_used    = 0
0.00.040.073 I print_info: causal attn      = 1
0.00.040.073 I print_info: pooling type     = 0
0.00.040.073 I print_info: rope type        = 2
0.00.040.074 I print_info: rope scaling     = linear
0.00.040.075 I print_info: freq_base_train  = 10000.0
0.00.040.075 I print_info: freq_scale_train = 1
0.00.040.075 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.075 I print_info: rope_finetuned   = unknown
0.00.040.075 I print_info: ssm_d_conv       = 0
0.00.040.076 I print_info: ssm_d_inner      = 0
0.00.040.076 I print_info: ssm_d_state      = 0
0.00.040.076 I print_info: ssm_dt_rank      = 0
0.00.040.076 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.076 I print_info: model type       = 1.4B
0.00.040.077 I print_info: model params     = 1.41 B
0.00.040.077 I print_info: general.name     = 1.4B
0.00.040.077 I print_info: vocab type       = BPE
0.00.040.078 I print_info: n_vocab          = 50304
0.00.040.078 I print_info: n_merges         = 50009
0.00.040.078 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.079 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.079 I print_info: LF token         = 187 ''
0.00.040.079 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.079 I print_info: max token length = 1024
0.00.040.080 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.443 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.447 I load_tensors: offloading output layer to GPU
0.00.638.449 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.472 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.638.474 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.639.879 I llama_init_from_model: n_seq_max     = 1
0.00.639.881 I llama_init_from_model: n_ctx         = 2048
0.00.639.882 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.882 I llama_init_from_model: n_batch       = 2048
0.00.639.882 I llama_init_from_model: n_ubatch      = 512
0.00.639.883 I llama_init_from_model: flash_attn    = 0
0.00.639.884 I llama_init_from_model: freq_base     = 10000.0
0.00.639.884 I llama_init_from_model: freq_scale    = 1
0.00.639.885 I ggml_metal_init: allocating
0.00.639.902 I ggml_metal_init: found device: Apple M4
0.00.639.910 I ggml_metal_init: picking default device: Apple M4
0.00.641.428 I ggml_metal_init: using embedded metal library
0.00.647.430 I ggml_metal_init: GPU name:   Apple M4
0.00.647.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.434 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.435 I ggml_metal_init: simdgroup reduction   = true
0.00.647.435 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.435 I ggml_metal_init: has residency sets    = true
0.00.647.435 I ggml_metal_init: has bfloat            = true
0.00.647.436 I ggml_metal_init: use bfloat            = true
0.00.647.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.141 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.223 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.717.230 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.717.265 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.721.622 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.721.625 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.721.625 I llama_init_from_model: graph nodes  = 967
0.00.721.625 I llama_init_from_model: graph splits = 2
0.00.721.631 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.721.755 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.721.756 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.705 I main: llama threadpool init, n_threads = 4
0.00.786.748 I 
0.00.786.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.771 I 
0.00.786.921 I sampler seed: 1234
0.00.786.926 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.946 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.946 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.946 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.673.850 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.01.673.851 I llama_perf_context_print:        load time =     777.15 ms
0.01.673.852 I llama_perf_context_print: prompt eval time =      57.56 ms /     7 tokens (    8.22 ms per token,   121.61 tokens per second)
0.01.673.852 I llama_perf_context_print:        eval time =     826.36 ms /    63 runs   (   13.12 ms per token,    76.24 tokens per second)
0.01.673.853 I llama_perf_context_print:       total time =     887.89 ms /    70 tokens
0.01.674.138 I ggml_metal_free: deallocating

real	0m1.690s
user	0m0.108s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4791 (84d5f4bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.769 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.686 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.698 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.699 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.701 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.701 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.702 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.702 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.705 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.430 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.499 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.212 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.215 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.216 I llama_model_loader: - type  f32:  194 tensors
0.00.024.217 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.217 I print_info: file format = GGUF V3 (latest)
0.00.024.218 I print_info: file type   = Q6_K
0.00.024.219 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.311 I load: special tokens cache size = 25
0.00.038.318 I load: token to piece cache size = 0.2984 MB
0.00.038.322 I print_info: arch             = gptneox
0.00.038.322 I print_info: vocab_only       = 0
0.00.038.323 I print_info: n_ctx_train      = 2048
0.00.038.323 I print_info: n_embd           = 2048
0.00.038.323 I print_info: n_layer          = 24
0.00.038.327 I print_info: n_head           = 16
0.00.038.327 I print_info: n_head_kv        = 16
0.00.038.328 I print_info: n_rot            = 32
0.00.038.328 I print_info: n_swa            = 0
0.00.038.328 I print_info: n_embd_head_k    = 128
0.00.038.328 I print_info: n_embd_head_v    = 128
0.00.038.329 I print_info: n_gqa            = 1
0.00.038.330 I print_info: n_embd_k_gqa     = 2048
0.00.038.330 I print_info: n_embd_v_gqa     = 2048
0.00.038.331 I print_info: f_norm_eps       = 1.0e-05
0.00.038.331 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.331 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.332 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.332 I print_info: f_logit_scale    = 0.0e+00
0.00.038.333 I print_info: n_ff             = 8192
0.00.038.333 I print_info: n_expert         = 0
0.00.038.333 I print_info: n_expert_used    = 0
0.00.038.333 I print_info: causal attn      = 1
0.00.038.333 I print_info: pooling type     = 0
0.00.038.333 I print_info: rope type        = 2
0.00.038.333 I print_info: rope scaling     = linear
0.00.038.334 I print_info: freq_base_train  = 10000.0
0.00.038.334 I print_info: freq_scale_train = 1
0.00.038.334 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.336 I print_info: rope_finetuned   = unknown
0.00.038.336 I print_info: ssm_d_conv       = 0
0.00.038.336 I print_info: ssm_d_inner      = 0
0.00.038.336 I print_info: ssm_d_state      = 0
0.00.038.336 I print_info: ssm_dt_rank      = 0
0.00.038.336 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.337 I print_info: model type       = 1.4B
0.00.038.337 I print_info: model params     = 1.41 B
0.00.038.337 I print_info: general.name     = 1.4B
0.00.038.338 I print_info: vocab type       = BPE
0.00.038.338 I print_info: n_vocab          = 50304
0.00.038.340 I print_info: n_merges         = 50009
0.00.038.340 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.341 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.341 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.341 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.341 I print_info: LF token         = 187 ''
0.00.038.342 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.342 I print_info: max token length = 1024
0.00.038.342 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.167 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.173 I load_tensors: offloading output layer to GPU
0.00.634.174 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.200 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.634.202 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.635.608 I llama_init_from_model: n_seq_max     = 1
0.00.635.610 I llama_init_from_model: n_ctx         = 128
0.00.635.611 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.611 I llama_init_from_model: n_batch       = 128
0.00.635.612 I llama_init_from_model: n_ubatch      = 128
0.00.635.612 I llama_init_from_model: flash_attn    = 0
0.00.635.614 I llama_init_from_model: freq_base     = 10000.0
0.00.635.615 I llama_init_from_model: freq_scale    = 1
0.00.635.615 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.617 I ggml_metal_init: allocating
0.00.635.677 I ggml_metal_init: found device: Apple M4
0.00.635.688 I ggml_metal_init: picking default device: Apple M4
0.00.637.140 I ggml_metal_init: using embedded metal library
0.00.642.819 I ggml_metal_init: GPU name:   Apple M4
0.00.642.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.824 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.825 I ggml_metal_init: simdgroup reduction   = true
0.00.642.825 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.825 I ggml_metal_init: has residency sets    = true
0.00.642.825 I ggml_metal_init: has bfloat            = true
0.00.642.826 I ggml_metal_init: use bfloat            = true
0.00.642.827 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.319 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.792 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.662.796 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.662.837 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.057 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.666.059 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.666.059 I llama_init_from_model: graph nodes  = 967
0.00.666.060 I llama_init_from_model: graph splits = 2
0.00.666.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.666.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.591 I 
0.00.703.674 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.682 I perplexity: tokenizing the input ..
0.00.710.718 I perplexity: tokenization took 7.032 ms
0.00.710.726 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.841.694 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.843.035 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.843.056 I llama_perf_context_print:        load time =     694.81 ms
0.00.843.057 I llama_perf_context_print: prompt eval time =     130.48 ms /   128 tokens (    1.02 ms per token,   980.98 tokens per second)
0.00.843.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.843.058 I llama_perf_context_print:       total time =     139.47 ms /   129 tokens
0.00.843.387 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.076s
sys	0m0.150s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4791 (84d5f4bc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120b061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120b06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120b06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120b07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120b075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120b07e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120b08370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120b088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120b08d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120b09200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120b096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120b09960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120b0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120b0ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120b0b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120b0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120b0c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120b0c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120b0cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120b0d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120b0e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120b0e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120b0eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120b0f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120b0fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120b10120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120b10730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120b113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120b118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120b11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120b12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120b12300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120b12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120b130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120b13390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120b13830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120b13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120b14170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120b14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120b14ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120b14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120b153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120b15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120b15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120b15ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120b16600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120b16c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120b17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120b17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120b18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120b18760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120b18d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120b19380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120b19990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120b1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120b1a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120b1aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120b1ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120b1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120b1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120b1be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120b1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120b1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120b1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120b1d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120b1d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120b1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120b1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120b1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120b1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120b1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120b1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120b1f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120b1fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120b20060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120b205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120b20b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120b21050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120b215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120b21af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120b22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120b22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120b22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120b23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120b23580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120b23ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120b24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120b24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120b24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120b25010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120b25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120b25ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120b26000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120b26550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120b26aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120b26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120b27540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120b17220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120b279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120b28160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120b286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120b28c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120b29150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120b296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120b29bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120b2a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120b2a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120b2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120b2b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120b2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120b2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120b2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120b2c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120b2cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120b2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120b2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120b2d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120b2dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120b2e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120b2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120b2eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120b2f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120b2f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120b2f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120b2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120b30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120b30730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120b30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120b31070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120b31510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120b319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120b31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120b322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120b32790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120b32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120b330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120b33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120b33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120b33eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120b34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120b347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120b34c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120b35130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120b355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120b35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120b35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120b363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120b36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120b36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120b37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120b37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120b37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120b37f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120b38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120b388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120b38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120b391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120b39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120b39b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120b39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120b3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120b3a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120b3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120b3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120b3b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120b3bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120b3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120b3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120b3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120b3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120b3d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120b3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120b3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120b3e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120b3e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120b3e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120b3ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120b3f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120b3f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120b3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120b400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120b40590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120b40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120b40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120b41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120b41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120b41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120b42150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120b425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120b42a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120b42f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120b433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120b43870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120b43dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120b44310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120b44860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120b44db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120b45070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120b45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120b45c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120b462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120b46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120b46f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120b471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120b47800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120b47e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120b48600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120b48aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120b48f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120b493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120b49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120b4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120b4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120b4ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120b4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120b4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120b4bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120b4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120b4c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120b4cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120b4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120b4d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120b4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120b4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120b4e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120b4eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120b4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120b4f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120b4fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120b50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120b505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120b50b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120b51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120b515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120b51b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120b52060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120b525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120b52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120b53050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120b535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120b53af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120b54040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120b54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120b54ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120b55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120b55580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120b55ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120b56020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120b56570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120b56ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120b57010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120b57560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120b57ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120b58000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120b58550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120b58aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120b58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120b59540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120b59a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120b59fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120b5a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120b5aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120b5afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120b5b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120b5ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120b5bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120b5c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120b5c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120b5ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120b5d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120b5d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120b5dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120b5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120b5e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120b5ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120b5eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120b5f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120b5f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120b5fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120b60130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120b605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120b60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x120b60f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x120b613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x120b61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x120b61cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x120b62190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x120b62630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x120b62ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x120b62f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x120b63410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x120b638b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120b63e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120b64520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120b64c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120b65360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120b65a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120b65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120b66530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120b667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120b66e00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.682.246 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.682.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x103607ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x103607f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x103608390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x103608800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x103608c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1036090e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x103609550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1036099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x103609e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10360a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10360a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10360ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10360b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10360c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10360c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10360d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10360d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10360de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10360e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10360ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10360f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10360fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1036102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x103610a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x103611120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1036113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1036116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x103611b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x103611f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1036123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x103612860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x103612d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x103613200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1036134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x103613930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x103613da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x103614210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x103614680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x103614af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x103614f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1036153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x103615840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x103615cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x103616120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x103616590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x103616a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x103616e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1036172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x103617750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x103617bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x103618030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1036184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x103618910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x103618d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1036191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x103619660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x103619bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10361a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10361a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10361a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10361ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10361b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10361b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10361bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10361bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10361c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10361c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10361cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10361d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10361d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10361da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10361def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10361e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10361e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10361ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10361f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10361f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10361f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10361fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x103620270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1036206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x103620b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x103620fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x103621430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1036218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x103621d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x103622180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1036225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x103622a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x103622ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x103623340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1036237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x103623c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x103624090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x103624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x103624970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x103624de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x103625250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1036256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x103625b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x103625fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x103626410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x103626880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x103626cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x103627160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1036275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x103627a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x103627eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x103628320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x103628790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x103628c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x103629070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1036294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x103629950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x103629dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10362a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10362a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10362ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10362af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10362b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10362b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10362bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10362c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10362c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10362ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10362ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10362d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10362d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10362dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10362e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10362e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10362e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10362eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10362f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10362f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10362faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10362ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1036303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x103630840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x103630cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x103631120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x103631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x103631a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x103631e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1036322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x103632750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x103632bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x103633030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1036334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x103633910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x103633d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1036341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x103634660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x103634ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x103634f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1036353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x103635820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x103635c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x103636100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x103636570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1036369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x103636e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1036372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x103637730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x103637ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x103638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x103638c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x103638f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1036391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x103639630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x103639aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x103639f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10363a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10363a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10363ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10363b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10363b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10363b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10363be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10363c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10363c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10363cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10363cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10363d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10363d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10363dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10363e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10363e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10363ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10363eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10363f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10363f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10363fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1036400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x103640520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x103640990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x103640e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x103641270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1036416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x103641b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x103641fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x103642430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x103642990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x103642ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x103643310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x103643780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x103643bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x103644060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x103644580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x103644a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x103645600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1036458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x103645e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x103646440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x103646a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x103646fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x103647580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x103647b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x103648100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1036486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x103648c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x103649240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x103649800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x103649dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10364a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10364a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10364af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10364b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10364ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10364c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10364c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10364cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10364d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10364d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10364dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10364e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10364e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10364ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10364f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10364f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10364ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x103650540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x103650b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1036510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x103651680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x103651c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x103652200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1036527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x103652d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x103653340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x103653900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x103653ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x103654480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x103654a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x103655000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1036555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x103655b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x103656140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x103656700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x103656cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x103657280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x103657840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x103657e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1036583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x103658980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x103658f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x103659500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x103659ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x103659fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10365a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10365a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10365aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10365b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10365b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10365bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10365c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10365c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10365ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10365d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10365d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10365dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10365e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10365e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10365eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10365efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10365f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10365f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10365fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1036603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1036608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x103660dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1036612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1036617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1036621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1036628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x103663010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x103663730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1036639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1036641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1036644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x103664ab0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120c052d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120c05740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120c05bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120c06020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120c06490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120c06900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120c06d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120c071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120c07650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120c07b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120c07fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120c08650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120c09170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120c09920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120c0a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120c0a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120c0af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120c0b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120c0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120c0c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120c0cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120c0d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120c0dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120c0e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120c0e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120c0ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120c0eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120c0f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120c0f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120c0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120c10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120c10590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120c10a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120c10cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120c11130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120c115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120c11a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120c11e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120c122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120c12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120c12bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120c13040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120c134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120c13920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120c13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120c14200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120c14670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120c14ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120c14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120c153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120c15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120c15ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120c16110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120c16580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120c169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120c16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120c173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120c178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120c17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120c181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120c18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120c18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120c18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120c19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120c197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120c19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120c1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120c1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120c1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120c1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120c1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120c1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120c1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120c1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120c1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120c1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120c1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120c1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120c1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120c1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120c1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120c1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120c1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120c1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120c1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120c1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120c1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120c1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120c20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120c206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120c20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120c20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120c21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120c21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120c21d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120c22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120c225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120c22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120c232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120c237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120c23d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120c24330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120c248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120c24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120c25440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120c259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120c25fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120c26550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120c26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120c270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120c27660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120c27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120c281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120c28770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120c28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120c29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120c29670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120c29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120c2a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120c2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120c2aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120c2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120c2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120c2b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120c2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120c2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120c2c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120c2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120c2d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120c2d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120c2dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120c2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120c2e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120c2eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120c2f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120c2f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120c2fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120c2ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120c30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120c30970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120c30e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120c31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120c31870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120c31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120c32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120c32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120c32c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120c33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120c33670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120c33b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120c34070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120c34570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120c34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120c34f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120c35470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120c35970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120c35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120c36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120c36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120c36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120c37270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120c37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120c37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120c38170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120c38670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120c38b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120c39070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120c39570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120c39a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120c39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120c3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120c3a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120c3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120c3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120c3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120c3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120c3c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120c3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120c3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120c3d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120c3d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120c3db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120c3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120c3e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120c3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120c3ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120c3f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120c3f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120c3fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120c40370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120c40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120c40d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120c41270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120c41770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120c41d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120c422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120c42880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120c42e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120c43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120c43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120c44060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120c44850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120c44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120c44fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120c455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120c45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120c463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120c46860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120c46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120c471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120c47950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120c47ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120c483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120c48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120c48e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120c493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120c49930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120c49e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120c4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120c4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120c4ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120c4b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120c4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120c4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120c4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120c4c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120c4ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120c4d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120c4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120c4de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120c4e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120c4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120c4ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120c4f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120c4f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120c4fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120c50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120c508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120c50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120c51360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120c518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120c51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120c52350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120c528a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120c52df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120c53340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120c53890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120c53de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120c54330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120c54880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120c54dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120c55320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120c55870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120c55dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120c56310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120c56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120c56db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120c57300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120c57850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120c57da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120c582f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120c58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120c58d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120c592e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120c59830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120c59d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120c5a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120c5a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120c5ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120c5b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120c5b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120c5b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120c5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120c5c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120c5c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120c5cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120c5d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120c5d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120c5da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120c5def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120c5e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120c5e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x120c5ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x120c5f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x120c5f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x120c5fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x120c5ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x120c603f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x120c60890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x120c60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x120c611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x120c61670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120c61bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120c622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120c62a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120c63120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120c63840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120c63b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120c642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120c645b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120c64bc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.729s
user	0m0.277s
sys	0m0.324s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4791 (84d5f4bc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142807f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1428086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142808c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142809220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1428097d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142809d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14280a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14280a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14280ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14280b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14280b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14280bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14280c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14280d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14280d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14280df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14280e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14280edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14280f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14280fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1428103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142810b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142811220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1428121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1428124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142812ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142813720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142813c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142813f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1428143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142814680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142814f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142815450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142815710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142816050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1428164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142816990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142816e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1428172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142817770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142817c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1428180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142818370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142818980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142818f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1428198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142819ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14281a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14281aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14281b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14281b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14281bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14281c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14281c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14281ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14281d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14281d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14281df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14281e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14281e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14281eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14281efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14281f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14281f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14281fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142820220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1428206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142820b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142821000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1428214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142821e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1428223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142822930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142822e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1428233d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142823920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142823e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1428243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142824910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142824e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1428253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142825900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142825e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1428263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1428268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142826e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142827390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1428278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142827e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142828380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1428288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142828e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142829370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1428298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1428195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142829d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14282a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14282aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14282af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14282b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14282ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14282bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14282c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14282ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14282cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14282d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14282da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14282df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14282e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14282e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14282ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14282f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14282f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14282fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142830110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1428305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142830a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142830ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142831390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142831830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142831cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142832170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142832610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142832ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142832f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1428333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142833890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142833d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1428341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142834670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142834b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142834fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142835450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1428358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142835d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142836230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1428366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142836b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142837010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1428374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142837950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142837df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142838290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142838730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142838bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142839070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142839510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1428399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142839e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14283a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14283a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14283ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14283b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14283b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14283ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14283beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14283c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14283c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14283cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14283d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14283d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14283da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14283df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14283e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14283e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14283ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14283f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14283f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14283fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14283ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142840410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1428408b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142840d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1428411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142841690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142841b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142841fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142842470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142842910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142842db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142843250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1428436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142843b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142844030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1428444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142844970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142844e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1428452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142845750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142845bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142846140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142846690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142846be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142847130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1428473f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142847a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142848010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142848620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142848e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1428492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142849570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142849b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14284a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14284a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14284ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14284b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14284b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14284bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14284c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14284c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14284cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14284d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14284d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14284def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14284e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14284e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14284eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14284f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14284f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14284fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142850420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142850970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142850ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142851410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142851960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142851eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142852400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142852950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142852ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1428533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142853940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142853e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1428543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142854930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142854e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1428553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142855920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142855e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1428563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142856910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142856e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1428573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142857900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142857e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1428583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1428588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142858e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142859390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1428598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142859e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14285a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14285a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14285ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14285b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14285b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14285be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14285c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14285c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14285ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14285d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14285d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14285ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14285e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14285e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14285ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14285f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14285f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14285fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14285ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142860450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1428608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142860d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142861230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1428616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142861b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142862010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1428624b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142862950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142862df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x142863290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x142863730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x142863bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x142864070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x142864510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1428649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x142864e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1428652f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x142865790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x142865c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142866180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1428668a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142866fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1428676e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142867e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1428680c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1428688b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142868b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142869180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.404 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142847cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142849830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142868e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1428476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1428482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14281b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14281ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14281d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142849e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142812760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142819250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142819b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142818c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14281b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14281a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142811760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142829ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142868380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142814940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142814c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14284a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1428488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142812d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142813030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1428132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1428695e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1428698a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142869b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142869e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14286a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14286a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14286a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14286a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14286abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14286aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14286b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14286b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14286b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14286b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14286bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14286bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14286c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14286c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14286c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14286ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14286cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14286cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14286d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14286d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14286d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14286daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14286dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14286e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14286e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14286e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14286e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14286eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14286ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14286f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14286f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14286f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14286f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14286fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14286fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142870120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1428703e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1428706a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142870960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142870c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142870ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1428711a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142871460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142871720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1428719e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142871ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142871f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142872220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1428724e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1428727a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142872a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142872d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142872fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1428732a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142873560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142873820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142873ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142873da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142874060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142874320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1428745e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1428748a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142874b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142874e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1428750e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1428753a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142875660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142875920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142875be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142875ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142876160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142876420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1428766e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1428769a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142876c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142876f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1428771e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1428774a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142877760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142877a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142877ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142877fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142878260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142878520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1428787e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142878aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142878d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142879020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1428792e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1428795a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142879860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142879b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142879de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14287a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14287a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14287a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14287a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14287aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14287ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14287b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14287b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14287b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14287b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14287bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14287bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14287c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14287c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14287c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14287c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14287cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14287cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14287d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14287d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14287d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14287da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14287dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14287dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14287e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14287e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14287e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14287eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14287eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14287f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14287f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14287f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14287f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14287fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14287fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1428800e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1428803a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142880660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142880920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142880be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142880ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142881160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142881420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1428816e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1428819a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142881c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142881f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1428821e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1428824a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142882760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142882a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142882ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142882fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142883260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142883520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1428837e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142883aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142883d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142884020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1428842e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1428845a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142884860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142884b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142884de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1428850a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142885360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142885620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1428858e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142885ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142885e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142886120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1428863e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1428866a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142886960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142886c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142886ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1428871a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142887460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142887720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1428879e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x142887ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142887f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142888220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1428884e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1428887a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142888a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142888d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142888fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1428895b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142889870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142889b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142889df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14288a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14288a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14288a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14288ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14288b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14288b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14288be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14288c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14288c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14288cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14288d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14288d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14288dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14288e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14288e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14288edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14288f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14288f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14288fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142890310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142890860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142890db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142891300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142891850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142891da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1428922f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142892840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142892d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1428932e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142893830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142893d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1428942d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142894820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142894d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1428952c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142895810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142895d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1428962b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142896800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142896d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1428972a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1428977f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142897d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142898290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1428987e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142898d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142899280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1428997d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142899d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14289a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14289a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14289ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14289b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14289b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14289b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14289baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14289bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14289c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14289c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14289cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14289d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14289d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14289d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14289de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14289e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14289e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14289eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14289efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14289f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14289f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14289fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1428a01a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1428a0610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1428a0a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1428a0ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1428a1360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1428a17d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1428a1c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1428a20b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1428a2b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1428a3230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1428a3950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1428a4070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1428a4330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1428a4b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1428a4de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1428a53f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14170a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14170a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14170ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14170b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14170b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14170ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14170bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14170c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14170c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14170cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14170d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14170d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14170e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14170e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14170f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14170f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141710010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141710730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141710e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141711620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141711d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141712460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141712b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1417132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1417139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141713c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141713f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1417143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141714820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141714c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141715190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1417156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141715b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141715dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141716240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1417166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141716c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141717110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141717610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141717b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141718010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141718510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141718a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141718f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141719410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141719880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141719cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14171a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14171a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14171aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14171aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14171b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14171b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14171bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14171c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14171c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14171cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14171cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14171d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14171dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14171e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14171e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14171eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14171f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14171f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14171f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14171fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1417202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141720740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141720be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141721080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141721520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1417219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141721f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141722460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1417229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141722f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141723450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1417239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141723ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141724440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141724990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141724ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141725430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141725980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141725ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141726420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141726970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141726ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141727960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141728400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141728ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1417293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141729940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14172a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14172a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14172ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14172b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14172b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14172be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14172c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14172c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14172ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14172d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14172d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14172de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14172e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14172e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14172ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14172f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14172f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14172fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1417300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141730560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141730a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141730ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141731340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1417317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141731c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141732120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1417325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141732a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141732f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1417333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141733840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141733ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141734180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141734620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141734ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141734f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141735400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1417358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141735d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1417361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141736680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141736b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141736fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141737460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141737900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141738240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1417386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141738b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141739020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1417394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141739960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141739e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14173a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14173a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14173abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14173b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14173b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14173b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14173be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14173c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14173c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14173cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14173d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14173d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14173da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14173dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14173e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14173e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14173eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14173f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14173f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14173fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14173ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1417403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141740860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141740d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1417411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141741640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141741ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141741f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141742420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1417428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141742d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141743200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1417436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141744480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141744920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141744dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141745260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141745700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141745ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141746040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141746590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141746ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141747030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141747580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141747840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141747e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141748460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141748a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141749260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141749700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1417499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141749fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14174a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14174add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14174b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14174b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14174bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14174c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14174c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14174ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14174d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14174d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14174ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14174e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14174e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14174ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14174f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14174f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14174fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141750320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141750870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141750dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141751310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141751860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141751db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141752300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141752850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141752da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1417532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141753840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141753d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1417542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141754830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141754d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1417552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141755820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141755d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1417562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141756810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141756d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1417572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141757800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141757d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1417582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1417587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141758d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141759290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1417597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141759d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14175a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14175a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14175ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14175b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14175b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14175bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14175c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14175c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14175cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14175d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14175d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14175dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14175e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14175e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14175ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14175f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14175f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14175fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14175ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141760400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1417608a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141760d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1417611e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141761680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141761b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141761fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141762460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141762900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141762da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141763240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1417636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x141763b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x141764020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1417644c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x141764960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x141764e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1417652a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x141765740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x141765be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x141766080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1417665d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141766cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141767410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141767b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141768250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141768510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141768d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141768fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1417695d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.958s
user	0m0.231s
sys	0m0.190s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.41 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.84 sec*proc (2 tests)

Total Test time (real) =   1.86 sec
        1.88 real         0.52 user         0.23 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.08 sys
```
