Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.659s
user	0m0.905s
sys	0m1.302s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target build_info
[  4%] Built target sha256
[  4%] Built target xxhash
[  4%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Built target llava
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Built target test-c
[ 34%] Built target llama-simple-chat
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Built target common
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-log
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-chat
[ 48%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-arg-parser
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-backend-ops
[ 63%] Built target test-chat-template
[ 63%] Built target test-autorelease
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-gguf
[ 63%] Built target test-arg-parser
[ 63%] Built target test-barrier
[ 63%] Built target test-quantize-fns
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-rope
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-infill
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Built target llama-imatrix
[ 76%] Built target llama-bench
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-cli
[ 81%] Generating loading.html.hpp
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-parallel
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Built target llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-speculative
[ 91%] Built target llama-run
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-tokenize
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.308s
user	0m6.500s
sys	0m10.438s

main: quantize time =  5797.00 ms
main:    total time =  5797.00 ms

main: quantize time =  3491.28 ms
main:    total time =  3491.28 ms

main: quantize time =  3320.36 ms
main:    total time =  3320.36 ms

main: quantize time =  3622.01 ms
main:    total time =  3622.01 ms

main: quantize time =  2768.65 ms
main:    total time =  2768.65 ms

main: quantize time =  5680.32 ms
main:    total time =  5680.32 ms

main: quantize time =  5988.27 ms
main:    total time =  5988.27 ms

main: quantize time =  6991.69 ms
main:    total time =  6991.69 ms

main: quantize time =  6103.68 ms
main:    total time =  6103.68 ms

main: quantize time =  5142.39 ms
main:    total time =  5142.39 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.214 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.374 I main: llama backend init
0.00.000.381 I main: load the model and apply lora adapter, if any
0.00.049.321 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.061.677 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.061.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.061.696 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.061.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.061.698 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.061.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.061.699 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.061.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.061.702 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.061.702 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.061.703 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.061.704 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.061.704 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.061.705 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.061.710 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.061.710 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.061.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.068.529 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.672 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.137 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.079.145 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.149 I llama_model_loader: - type  f32:  194 tensors
0.00.079.149 I llama_model_loader: - type  f16:   98 tensors
0.00.079.160 I print_info: file format = GGUF V3 (latest)
0.00.079.162 I print_info: file type   = all F32 (guessed)
0.00.079.164 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.417 I load: special tokens cache size = 25
0.00.105.969 I load: token to piece cache size = 0.2984 MB
0.00.105.973 I print_info: arch             = gptneox
0.00.105.973 I print_info: vocab_only       = 0
0.00.105.973 I print_info: n_ctx_train      = 2048
0.00.105.973 I print_info: n_embd           = 2048
0.00.105.974 I print_info: n_layer          = 24
0.00.105.979 I print_info: n_head           = 16
0.00.105.980 I print_info: n_head_kv        = 16
0.00.105.980 I print_info: n_rot            = 32
0.00.105.980 I print_info: n_swa            = 0
0.00.105.981 I print_info: n_embd_head_k    = 128
0.00.105.981 I print_info: n_embd_head_v    = 128
0.00.105.982 I print_info: n_gqa            = 1
0.00.105.983 I print_info: n_embd_k_gqa     = 2048
0.00.105.983 I print_info: n_embd_v_gqa     = 2048
0.00.105.984 I print_info: f_norm_eps       = 1.0e-05
0.00.105.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.985 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.985 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.986 I print_info: f_logit_scale    = 0.0e+00
0.00.105.986 I print_info: n_ff             = 8192
0.00.105.987 I print_info: n_expert         = 0
0.00.105.987 I print_info: n_expert_used    = 0
0.00.105.987 I print_info: causal attn      = 1
0.00.105.987 I print_info: pooling type     = 0
0.00.105.988 I print_info: rope type        = 2
0.00.105.988 I print_info: rope scaling     = linear
0.00.105.989 I print_info: freq_base_train  = 10000.0
0.00.105.989 I print_info: freq_scale_train = 1
0.00.105.989 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.990 I print_info: rope_finetuned   = unknown
0.00.105.990 I print_info: ssm_d_conv       = 0
0.00.105.990 I print_info: ssm_d_inner      = 0
0.00.105.990 I print_info: ssm_d_state      = 0
0.00.105.990 I print_info: ssm_dt_rank      = 0
0.00.105.990 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.991 I print_info: model type       = 1.4B
0.00.105.994 I print_info: model params     = 1.41 B
0.00.105.994 I print_info: general.name     = 1.4B
0.00.105.995 I print_info: vocab type       = BPE
0.00.105.995 I print_info: n_vocab          = 50304
0.00.105.995 I print_info: n_merges         = 50009
0.00.105.996 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.996 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.996 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.996 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.997 I print_info: LF token         = 187 'Ċ'
0.00.105.997 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.997 I print_info: max token length = 1024
0.00.105.997 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.157.770 I load_tensors: offloading 24 repeating layers to GPU
0.00.157.774 I load_tensors: offloading output layer to GPU
0.00.157.774 I load_tensors: offloaded 25/25 layers to GPU
0.00.157.800 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.157.801 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.158.196 I llama_init_from_model: n_seq_max     = 1
0.00.158.197 I llama_init_from_model: n_ctx         = 2048
0.00.158.197 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.158.197 I llama_init_from_model: n_batch       = 2048
0.00.158.198 I llama_init_from_model: n_ubatch      = 512
0.00.158.198 I llama_init_from_model: flash_attn    = 0
0.00.158.199 I llama_init_from_model: freq_base     = 10000.0
0.00.158.199 I llama_init_from_model: freq_scale    = 1
0.00.158.200 I ggml_metal_init: allocating
0.00.158.233 I ggml_metal_init: found device: Apple M4
0.00.158.240 I ggml_metal_init: picking default device: Apple M4
0.00.158.922 I ggml_metal_init: using embedded metal library
0.00.168.240 I ggml_metal_init: GPU name:   Apple M4
0.00.168.243 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.168.243 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.168.244 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.168.244 I ggml_metal_init: simdgroup reduction   = true
0.00.168.244 I ggml_metal_init: simdgroup matrix mul. = true
0.00.168.244 I ggml_metal_init: has residency sets    = true
0.00.168.244 I ggml_metal_init: has bfloat            = true
0.00.168.245 I ggml_metal_init: use bfloat            = true
0.00.168.245 I ggml_metal_init: hasUnifiedMemory      = true
0.00.168.246 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.196.111 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.223.885 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.223.891 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.223.913 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.227.463 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.227.465 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.227.466 I llama_init_from_model: graph nodes  = 967
0.00.227.466 I llama_init_from_model: graph splits = 2
0.00.227.469 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.227.598 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.227.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.294.972 I main: llama threadpool init, n_threads = 4
0.00.295.015 I 
0.00.295.046 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.295.047 I 
0.00.295.090 I sampler seed: 1234
0.00.295.094 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.295.119 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.295.120 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.295.120 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.135.105 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.02.135.106 I llama_perf_context_print:        load time =     244.76 ms
0.02.135.107 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.48 tokens per second)
0.02.135.108 I llama_perf_context_print:        eval time =    1793.51 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.135.112 I llama_perf_context_print:       total time =    1841.01 ms /    70 tokens
0.02.135.354 I ggml_metal_free: deallocating

real	0m2.474s
user	0m0.135s
sys	0m0.143s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.066 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.105 I main: llama backend init
0.00.000.106 I main: load the model and apply lora adapter, if any
0.00.009.914 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.818 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.823 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.827 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.829 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.831 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.832 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.833 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.834 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.783 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.807 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.719 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.721 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.721 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.721 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.722 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.722 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.723 I llama_model_loader: - type  f32:  194 tensors
0.00.036.723 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.724 I print_info: file format = GGUF V3 (latest)
0.00.036.724 I print_info: file type   = Q8_0
0.00.036.725 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.399 I load: special tokens cache size = 25
0.00.051.434 I load: token to piece cache size = 0.2984 MB
0.00.051.439 I print_info: arch             = gptneox
0.00.051.439 I print_info: vocab_only       = 0
0.00.051.439 I print_info: n_ctx_train      = 2048
0.00.051.439 I print_info: n_embd           = 2048
0.00.051.442 I print_info: n_layer          = 24
0.00.051.447 I print_info: n_head           = 16
0.00.051.448 I print_info: n_head_kv        = 16
0.00.051.448 I print_info: n_rot            = 32
0.00.051.448 I print_info: n_swa            = 0
0.00.051.449 I print_info: n_embd_head_k    = 128
0.00.051.449 I print_info: n_embd_head_v    = 128
0.00.051.449 I print_info: n_gqa            = 1
0.00.051.450 I print_info: n_embd_k_gqa     = 2048
0.00.051.451 I print_info: n_embd_v_gqa     = 2048
0.00.051.451 I print_info: f_norm_eps       = 1.0e-05
0.00.051.452 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.453 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.453 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.453 I print_info: f_logit_scale    = 0.0e+00
0.00.051.455 I print_info: n_ff             = 8192
0.00.051.455 I print_info: n_expert         = 0
0.00.051.455 I print_info: n_expert_used    = 0
0.00.051.456 I print_info: causal attn      = 1
0.00.051.456 I print_info: pooling type     = 0
0.00.051.456 I print_info: rope type        = 2
0.00.051.456 I print_info: rope scaling     = linear
0.00.051.457 I print_info: freq_base_train  = 10000.0
0.00.051.457 I print_info: freq_scale_train = 1
0.00.051.457 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.458 I print_info: rope_finetuned   = unknown
0.00.051.458 I print_info: ssm_d_conv       = 0
0.00.051.458 I print_info: ssm_d_inner      = 0
0.00.051.458 I print_info: ssm_d_state      = 0
0.00.051.458 I print_info: ssm_dt_rank      = 0
0.00.051.458 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.459 I print_info: model type       = 1.4B
0.00.051.459 I print_info: model params     = 1.41 B
0.00.051.459 I print_info: general.name     = 1.4B
0.00.051.460 I print_info: vocab type       = BPE
0.00.051.460 I print_info: n_vocab          = 50304
0.00.051.461 I print_info: n_merges         = 50009
0.00.051.461 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.461 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.461 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.461 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.461 I print_info: LF token         = 187 'Ċ'
0.00.051.462 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.462 I print_info: max token length = 1024
0.00.051.462 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.991.055 I load_tensors: offloading 24 repeating layers to GPU
0.00.991.060 I load_tensors: offloading output layer to GPU
0.00.991.061 I load_tensors: offloaded 25/25 layers to GPU
0.00.991.085 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.991.087 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.992.120 I llama_init_from_model: n_seq_max     = 1
0.00.992.122 I llama_init_from_model: n_ctx         = 2048
0.00.992.123 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.992.123 I llama_init_from_model: n_batch       = 2048
0.00.992.123 I llama_init_from_model: n_ubatch      = 512
0.00.992.124 I llama_init_from_model: flash_attn    = 0
0.00.992.124 I llama_init_from_model: freq_base     = 10000.0
0.00.992.125 I llama_init_from_model: freq_scale    = 1
0.00.992.126 I ggml_metal_init: allocating
0.00.992.140 I ggml_metal_init: found device: Apple M4
0.00.992.152 I ggml_metal_init: picking default device: Apple M4
0.00.993.273 I ggml_metal_init: using embedded metal library
0.00.998.406 I ggml_metal_init: GPU name:   Apple M4
0.00.998.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.998.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.998.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.998.411 I ggml_metal_init: simdgroup reduction   = true
0.00.998.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.998.412 I ggml_metal_init: has residency sets    = true
0.00.998.412 I ggml_metal_init: has bfloat            = true
0.00.998.412 I ggml_metal_init: use bfloat            = true
0.00.998.413 I ggml_metal_init: hasUnifiedMemory      = true
0.00.998.414 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.014.601 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.061.107 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.061.116 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.061.142 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.065.398 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.065.400 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.065.400 I llama_init_from_model: graph nodes  = 967
0.01.065.401 I llama_init_from_model: graph splits = 2
0.01.065.406 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.065.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.065.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.120.635 I main: llama threadpool init, n_threads = 4
0.01.120.678 I 
0.01.120.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.120.700 I 
0.01.120.856 I sampler seed: 1234
0.01.120.860 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.120.908 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.120.911 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.120.911 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.218.655 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.02.218.656 I llama_perf_context_print:        load time =    1109.99 ms
0.02.218.657 I llama_perf_context_print: prompt eval time =      48.40 ms /     7 tokens (    6.91 ms per token,   144.63 tokens per second)
0.02.218.658 I llama_perf_context_print:        eval time =    1046.55 ms /    63 runs   (   16.61 ms per token,    60.20 tokens per second)
0.02.218.658 I llama_perf_context_print:       total time =    1098.74 ms /    70 tokens
0.02.218.883 I ggml_metal_free: deallocating

real	0m2.237s
user	0m0.111s
sys	0m0.262s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.011.408 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.855 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.863 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.864 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.864 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.865 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.866 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.866 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.867 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.867 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.868 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.868 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.871 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.780 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.592 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.593 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.594 I llama_model_loader: - type  f32:  194 tensors
0.00.027.595 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.595 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.596 I print_info: file format = GGUF V3 (latest)
0.00.027.596 I print_info: file type   = Q4_0
0.00.027.597 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.755 I load: special tokens cache size = 25
0.00.041.923 I load: token to piece cache size = 0.2984 MB
0.00.041.927 I print_info: arch             = gptneox
0.00.041.927 I print_info: vocab_only       = 0
0.00.041.927 I print_info: n_ctx_train      = 2048
0.00.041.927 I print_info: n_embd           = 2048
0.00.041.928 I print_info: n_layer          = 24
0.00.041.930 I print_info: n_head           = 16
0.00.041.931 I print_info: n_head_kv        = 16
0.00.041.931 I print_info: n_rot            = 32
0.00.041.932 I print_info: n_swa            = 0
0.00.041.932 I print_info: n_embd_head_k    = 128
0.00.041.933 I print_info: n_embd_head_v    = 128
0.00.041.934 I print_info: n_gqa            = 1
0.00.041.935 I print_info: n_embd_k_gqa     = 2048
0.00.041.936 I print_info: n_embd_v_gqa     = 2048
0.00.041.936 I print_info: f_norm_eps       = 1.0e-05
0.00.041.937 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.937 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.937 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.937 I print_info: f_logit_scale    = 0.0e+00
0.00.041.938 I print_info: n_ff             = 8192
0.00.041.938 I print_info: n_expert         = 0
0.00.041.938 I print_info: n_expert_used    = 0
0.00.041.938 I print_info: causal attn      = 1
0.00.041.938 I print_info: pooling type     = 0
0.00.041.939 I print_info: rope type        = 2
0.00.041.941 I print_info: rope scaling     = linear
0.00.041.942 I print_info: freq_base_train  = 10000.0
0.00.041.942 I print_info: freq_scale_train = 1
0.00.041.942 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.942 I print_info: rope_finetuned   = unknown
0.00.041.942 I print_info: ssm_d_conv       = 0
0.00.041.942 I print_info: ssm_d_inner      = 0
0.00.041.942 I print_info: ssm_d_state      = 0
0.00.041.943 I print_info: ssm_dt_rank      = 0
0.00.041.943 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.943 I print_info: model type       = 1.4B
0.00.041.943 I print_info: model params     = 1.41 B
0.00.041.943 I print_info: general.name     = 1.4B
0.00.041.944 I print_info: vocab type       = BPE
0.00.041.944 I print_info: n_vocab          = 50304
0.00.041.944 I print_info: n_merges         = 50009
0.00.041.945 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.945 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.945 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.945 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.945 I print_info: LF token         = 187 'Ċ'
0.00.041.946 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.946 I print_info: max token length = 1024
0.00.041.946 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.562.686 I load_tensors: offloading 24 repeating layers to GPU
0.00.562.703 I load_tensors: offloading output layer to GPU
0.00.562.704 I load_tensors: offloaded 25/25 layers to GPU
0.00.562.736 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.562.737 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.564.455 I llama_init_from_model: n_seq_max     = 1
0.00.564.458 I llama_init_from_model: n_ctx         = 2048
0.00.564.459 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.564.460 I llama_init_from_model: n_batch       = 2048
0.00.564.460 I llama_init_from_model: n_ubatch      = 512
0.00.564.461 I llama_init_from_model: flash_attn    = 0
0.00.564.463 I llama_init_from_model: freq_base     = 10000.0
0.00.564.464 I llama_init_from_model: freq_scale    = 1
0.00.564.466 I ggml_metal_init: allocating
0.00.564.542 I ggml_metal_init: found device: Apple M4
0.00.564.555 I ggml_metal_init: picking default device: Apple M4
0.00.566.375 I ggml_metal_init: using embedded metal library
0.00.572.799 I ggml_metal_init: GPU name:   Apple M4
0.00.572.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.572.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.572.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.572.806 I ggml_metal_init: simdgroup reduction   = true
0.00.572.806 I ggml_metal_init: simdgroup matrix mul. = true
0.00.572.806 I ggml_metal_init: has residency sets    = true
0.00.572.807 I ggml_metal_init: has bfloat            = true
0.00.572.807 I ggml_metal_init: use bfloat            = true
0.00.572.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.572.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.591.272 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.653.129 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.653.135 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.653.166 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.658.316 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.658.318 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.658.319 I llama_init_from_model: graph nodes  = 967
0.00.658.319 I llama_init_from_model: graph splits = 2
0.00.658.327 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.658.454 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.658.454 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.897 I main: llama threadpool init, n_threads = 4
0.00.712.941 I 
0.00.712.965 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.965 I 
0.00.713.139 I sampler seed: 1234
0.00.713.144 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.187 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.190 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.191 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.398.149 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.398.149 I llama_perf_context_print:        load time =     700.73 ms
0.01.398.150 I llama_perf_context_print: prompt eval time =      49.29 ms /     7 tokens (    7.04 ms per token,   142.01 tokens per second)
0.01.398.151 I llama_perf_context_print:        eval time =     632.81 ms /    63 runs   (   10.04 ms per token,    99.56 tokens per second)
0.01.398.152 I llama_perf_context_print:       total time =     686.01 ms /    70 tokens
0.01.398.432 I ggml_metal_free: deallocating

real	0m1.419s
user	0m0.110s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.006 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.798 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.810 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.810 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.811 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.812 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.813 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.815 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.817 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.817 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.731 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.478 I llama_model_loader: - type  f32:  194 tensors
0.00.026.478 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.479 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.479 I print_info: file format = GGUF V3 (latest)
0.00.026.480 I print_info: file type   = Q4_1
0.00.026.480 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.423 I load: special tokens cache size = 25
0.00.040.301 I load: token to piece cache size = 0.2984 MB
0.00.040.303 I print_info: arch             = gptneox
0.00.040.303 I print_info: vocab_only       = 0
0.00.040.304 I print_info: n_ctx_train      = 2048
0.00.040.304 I print_info: n_embd           = 2048
0.00.040.304 I print_info: n_layer          = 24
0.00.040.307 I print_info: n_head           = 16
0.00.040.307 I print_info: n_head_kv        = 16
0.00.040.308 I print_info: n_rot            = 32
0.00.040.308 I print_info: n_swa            = 0
0.00.040.308 I print_info: n_embd_head_k    = 128
0.00.040.308 I print_info: n_embd_head_v    = 128
0.00.040.309 I print_info: n_gqa            = 1
0.00.040.310 I print_info: n_embd_k_gqa     = 2048
0.00.040.310 I print_info: n_embd_v_gqa     = 2048
0.00.040.311 I print_info: f_norm_eps       = 1.0e-05
0.00.040.311 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.311 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.311 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.312 I print_info: f_logit_scale    = 0.0e+00
0.00.040.312 I print_info: n_ff             = 8192
0.00.040.312 I print_info: n_expert         = 0
0.00.040.312 I print_info: n_expert_used    = 0
0.00.040.313 I print_info: causal attn      = 1
0.00.040.313 I print_info: pooling type     = 0
0.00.040.313 I print_info: rope type        = 2
0.00.040.313 I print_info: rope scaling     = linear
0.00.040.313 I print_info: freq_base_train  = 10000.0
0.00.040.314 I print_info: freq_scale_train = 1
0.00.040.314 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.314 I print_info: rope_finetuned   = unknown
0.00.040.314 I print_info: ssm_d_conv       = 0
0.00.040.316 I print_info: ssm_d_inner      = 0
0.00.040.316 I print_info: ssm_d_state      = 0
0.00.040.317 I print_info: ssm_dt_rank      = 0
0.00.040.317 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.317 I print_info: model type       = 1.4B
0.00.040.317 I print_info: model params     = 1.41 B
0.00.040.318 I print_info: general.name     = 1.4B
0.00.040.318 I print_info: vocab type       = BPE
0.00.040.318 I print_info: n_vocab          = 50304
0.00.040.318 I print_info: n_merges         = 50009
0.00.040.319 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.320 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.320 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.320 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.321 I print_info: LF token         = 187 'Ċ'
0.00.040.321 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.321 I print_info: max token length = 1024
0.00.040.322 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.612.620 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.635 I load_tensors: offloading output layer to GPU
0.00.612.636 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.670 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.612.671 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.614.096 I llama_init_from_model: n_seq_max     = 1
0.00.614.099 I llama_init_from_model: n_ctx         = 2048
0.00.614.099 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.614.100 I llama_init_from_model: n_batch       = 2048
0.00.614.100 I llama_init_from_model: n_ubatch      = 512
0.00.614.101 I llama_init_from_model: flash_attn    = 0
0.00.614.103 I llama_init_from_model: freq_base     = 10000.0
0.00.614.104 I llama_init_from_model: freq_scale    = 1
0.00.614.106 I ggml_metal_init: allocating
0.00.614.186 I ggml_metal_init: found device: Apple M4
0.00.614.201 I ggml_metal_init: picking default device: Apple M4
0.00.616.096 I ggml_metal_init: using embedded metal library
0.00.622.689 I ggml_metal_init: GPU name:   Apple M4
0.00.622.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.695 I ggml_metal_init: simdgroup reduction   = true
0.00.622.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.695 I ggml_metal_init: has residency sets    = true
0.00.622.696 I ggml_metal_init: has bfloat            = true
0.00.622.696 I ggml_metal_init: use bfloat            = true
0.00.622.697 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.699 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.231 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.234 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.693.241 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.693.265 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.697.499 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.697.501 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.697.502 I llama_init_from_model: graph nodes  = 967
0.00.697.502 I llama_init_from_model: graph splits = 2
0.00.697.509 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.697.643 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.697.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.584 I main: llama threadpool init, n_threads = 4
0.00.750.630 I 
0.00.750.652 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.652 I 
0.00.750.806 I sampler seed: 1234
0.00.750.810 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.830 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.831 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.831 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.467.061 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.467.062 I llama_perf_context_print:        load time =     739.86 ms
0.01.467.062 I llama_perf_context_print: prompt eval time =      39.31 ms /     7 tokens (    5.62 ms per token,   178.07 tokens per second)
0.01.467.063 I llama_perf_context_print:        eval time =     674.03 ms /    63 runs   (   10.70 ms per token,    93.47 tokens per second)
0.01.467.064 I llama_perf_context_print:       total time =     717.19 ms /    70 tokens
0.01.467.286 I ggml_metal_free: deallocating

real	0m1.484s
user	0m0.109s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.012.075 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.955 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.968 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.969 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.971 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.973 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.973 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.617 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.620 I llama_model_loader: - type  f32:  194 tensors
0.00.028.620 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.620 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.620 I print_info: file format = GGUF V3 (latest)
0.00.028.621 I print_info: file type   = Q5_0
0.00.028.621 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.505 I load: special tokens cache size = 25
0.00.042.561 I load: token to piece cache size = 0.2984 MB
0.00.042.564 I print_info: arch             = gptneox
0.00.042.564 I print_info: vocab_only       = 0
0.00.042.565 I print_info: n_ctx_train      = 2048
0.00.042.565 I print_info: n_embd           = 2048
0.00.042.565 I print_info: n_layer          = 24
0.00.042.568 I print_info: n_head           = 16
0.00.042.569 I print_info: n_head_kv        = 16
0.00.042.570 I print_info: n_rot            = 32
0.00.042.570 I print_info: n_swa            = 0
0.00.042.570 I print_info: n_embd_head_k    = 128
0.00.042.570 I print_info: n_embd_head_v    = 128
0.00.042.571 I print_info: n_gqa            = 1
0.00.042.572 I print_info: n_embd_k_gqa     = 2048
0.00.042.572 I print_info: n_embd_v_gqa     = 2048
0.00.042.573 I print_info: f_norm_eps       = 1.0e-05
0.00.042.573 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.573 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.573 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.573 I print_info: f_logit_scale    = 0.0e+00
0.00.042.574 I print_info: n_ff             = 8192
0.00.042.574 I print_info: n_expert         = 0
0.00.042.574 I print_info: n_expert_used    = 0
0.00.042.574 I print_info: causal attn      = 1
0.00.042.575 I print_info: pooling type     = 0
0.00.042.576 I print_info: rope type        = 2
0.00.042.578 I print_info: rope scaling     = linear
0.00.042.578 I print_info: freq_base_train  = 10000.0
0.00.042.578 I print_info: freq_scale_train = 1
0.00.042.579 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.579 I print_info: rope_finetuned   = unknown
0.00.042.579 I print_info: ssm_d_conv       = 0
0.00.042.579 I print_info: ssm_d_inner      = 0
0.00.042.580 I print_info: ssm_d_state      = 0
0.00.042.580 I print_info: ssm_dt_rank      = 0
0.00.042.581 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.581 I print_info: model type       = 1.4B
0.00.042.581 I print_info: model params     = 1.41 B
0.00.042.581 I print_info: general.name     = 1.4B
0.00.042.582 I print_info: vocab type       = BPE
0.00.042.582 I print_info: n_vocab          = 50304
0.00.042.582 I print_info: n_merges         = 50009
0.00.042.582 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.587 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.588 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.588 I print_info: LF token         = 187 'Ċ'
0.00.042.588 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.589 I print_info: max token length = 1024
0.00.042.589 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.708.214 I load_tensors: offloading 24 repeating layers to GPU
0.00.708.225 I load_tensors: offloading output layer to GPU
0.00.708.226 I load_tensors: offloaded 25/25 layers to GPU
0.00.708.258 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.708.260 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.709.943 I llama_init_from_model: n_seq_max     = 1
0.00.709.945 I llama_init_from_model: n_ctx         = 2048
0.00.709.946 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.709.946 I llama_init_from_model: n_batch       = 2048
0.00.709.947 I llama_init_from_model: n_ubatch      = 512
0.00.709.947 I llama_init_from_model: flash_attn    = 0
0.00.709.948 I llama_init_from_model: freq_base     = 10000.0
0.00.709.949 I llama_init_from_model: freq_scale    = 1
0.00.709.950 I ggml_metal_init: allocating
0.00.709.958 I ggml_metal_init: found device: Apple M4
0.00.709.966 I ggml_metal_init: picking default device: Apple M4
0.00.711.404 I ggml_metal_init: using embedded metal library
0.00.717.539 I ggml_metal_init: GPU name:   Apple M4
0.00.717.543 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.544 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.545 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.545 I ggml_metal_init: simdgroup reduction   = true
0.00.717.545 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.546 I ggml_metal_init: has residency sets    = true
0.00.717.546 I ggml_metal_init: has bfloat            = true
0.00.717.546 I ggml_metal_init: use bfloat            = true
0.00.717.547 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.548 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.735.132 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.792.286 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.792.292 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.792.316 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.797.196 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.797.199 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.797.199 I llama_init_from_model: graph nodes  = 967
0.00.797.199 I llama_init_from_model: graph splits = 2
0.00.797.205 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.797.328 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.797.329 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.858.126 I main: llama threadpool init, n_threads = 4
0.00.858.168 I 
0.00.858.191 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.858.191 I 
0.00.858.346 I sampler seed: 1234
0.00.858.350 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.370 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.370 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.370 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.645.702 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.645.703 I llama_perf_context_print:        load time =     845.33 ms
0.01.645.704 I llama_perf_context_print: prompt eval time =      51.12 ms /     7 tokens (    7.30 ms per token,   136.93 tokens per second)
0.01.645.704 I llama_perf_context_print:        eval time =     733.24 ms /    63 runs   (   11.64 ms per token,    85.92 tokens per second)
0.01.645.705 I llama_perf_context_print:       total time =     788.29 ms /    70 tokens
0.01.645.935 I ggml_metal_free: deallocating

real	0m1.665s
user	0m0.109s
sys	0m0.229s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.239 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.241 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.241 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.242 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.242 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.243 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.244 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.244 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.244 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.246 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.248 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.251 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.346 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.262 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.263 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.263 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.263 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.264 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.264 I llama_model_loader: - type  f32:  194 tensors
0.00.026.264 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.265 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.265 I print_info: file format = GGUF V3 (latest)
0.00.026.266 I print_info: file type   = Q5_1
0.00.026.267 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.124 I load: special tokens cache size = 25
0.00.039.848 I load: token to piece cache size = 0.2984 MB
0.00.039.851 I print_info: arch             = gptneox
0.00.039.851 I print_info: vocab_only       = 0
0.00.039.852 I print_info: n_ctx_train      = 2048
0.00.039.852 I print_info: n_embd           = 2048
0.00.039.852 I print_info: n_layer          = 24
0.00.039.855 I print_info: n_head           = 16
0.00.039.856 I print_info: n_head_kv        = 16
0.00.039.856 I print_info: n_rot            = 32
0.00.039.856 I print_info: n_swa            = 0
0.00.039.856 I print_info: n_embd_head_k    = 128
0.00.039.857 I print_info: n_embd_head_v    = 128
0.00.039.858 I print_info: n_gqa            = 1
0.00.039.859 I print_info: n_embd_k_gqa     = 2048
0.00.039.859 I print_info: n_embd_v_gqa     = 2048
0.00.039.860 I print_info: f_norm_eps       = 1.0e-05
0.00.039.860 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.861 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.861 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.861 I print_info: f_logit_scale    = 0.0e+00
0.00.039.862 I print_info: n_ff             = 8192
0.00.039.862 I print_info: n_expert         = 0
0.00.039.862 I print_info: n_expert_used    = 0
0.00.039.862 I print_info: causal attn      = 1
0.00.039.864 I print_info: pooling type     = 0
0.00.039.865 I print_info: rope type        = 2
0.00.039.866 I print_info: rope scaling     = linear
0.00.039.866 I print_info: freq_base_train  = 10000.0
0.00.039.866 I print_info: freq_scale_train = 1
0.00.039.867 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.867 I print_info: rope_finetuned   = unknown
0.00.039.867 I print_info: ssm_d_conv       = 0
0.00.039.867 I print_info: ssm_d_inner      = 0
0.00.039.867 I print_info: ssm_d_state      = 0
0.00.039.868 I print_info: ssm_dt_rank      = 0
0.00.039.868 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.868 I print_info: model type       = 1.4B
0.00.039.868 I print_info: model params     = 1.41 B
0.00.039.869 I print_info: general.name     = 1.4B
0.00.039.869 I print_info: vocab type       = BPE
0.00.039.869 I print_info: n_vocab          = 50304
0.00.039.870 I print_info: n_merges         = 50009
0.00.039.871 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.871 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: LF token         = 187 'Ċ'
0.00.039.872 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: max token length = 1024
0.00.039.873 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.704.048 I load_tensors: offloading 24 repeating layers to GPU
0.00.704.060 I load_tensors: offloading output layer to GPU
0.00.704.061 I load_tensors: offloaded 25/25 layers to GPU
0.00.704.097 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.704.100 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.705.763 I llama_init_from_model: n_seq_max     = 1
0.00.705.766 I llama_init_from_model: n_ctx         = 2048
0.00.705.766 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.705.766 I llama_init_from_model: n_batch       = 2048
0.00.705.767 I llama_init_from_model: n_ubatch      = 512
0.00.705.767 I llama_init_from_model: flash_attn    = 0
0.00.705.768 I llama_init_from_model: freq_base     = 10000.0
0.00.705.769 I llama_init_from_model: freq_scale    = 1
0.00.705.770 I ggml_metal_init: allocating
0.00.705.783 I ggml_metal_init: found device: Apple M4
0.00.705.796 I ggml_metal_init: picking default device: Apple M4
0.00.707.276 I ggml_metal_init: using embedded metal library
0.00.713.585 I ggml_metal_init: GPU name:   Apple M4
0.00.713.589 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.713.590 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.713.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.713.591 I ggml_metal_init: simdgroup reduction   = true
0.00.713.591 I ggml_metal_init: simdgroup matrix mul. = true
0.00.713.592 I ggml_metal_init: has residency sets    = true
0.00.713.592 I ggml_metal_init: has bfloat            = true
0.00.713.592 I ggml_metal_init: use bfloat            = true
0.00.713.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.713.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.730.863 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.784.353 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.784.360 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.784.381 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.788.692 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.788.695 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.788.695 I llama_init_from_model: graph nodes  = 967
0.00.788.695 I llama_init_from_model: graph splits = 2
0.00.788.703 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.788.833 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.788.834 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.805 I main: llama threadpool init, n_threads = 4
0.00.847.852 I 
0.00.847.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.875 I 
0.00.848.051 I sampler seed: 1234
0.00.848.055 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.848.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.848.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.848.074 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.685.419 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.685.421 I llama_perf_context_print:        load time =     838.24 ms
0.01.685.422 I llama_perf_context_print: prompt eval time =      50.46 ms /     7 tokens (    7.21 ms per token,   138.72 tokens per second)
0.01.685.422 I llama_perf_context_print:        eval time =     784.00 ms /    63 runs   (   12.44 ms per token,    80.36 tokens per second)
0.01.685.423 I llama_perf_context_print:       total time =     838.34 ms /    70 tokens
0.01.685.653 I ggml_metal_free: deallocating

real	0m1.704s
user	0m0.109s
sys	0m0.231s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.987 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.682 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.687 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.689 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.690 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.690 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.690 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.691 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.691 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.692 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.692 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.693 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.693 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.693 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.694 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.695 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.695 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.696 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.313 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.314 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.314 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.314 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.315 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.315 I llama_model_loader: - type  f32:  194 tensors
0.00.025.316 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.316 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.316 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.317 I print_info: file format = GGUF V3 (latest)
0.00.025.317 I print_info: file type   = Q2_K - Medium
0.00.025.318 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.208 I load: special tokens cache size = 25
0.00.038.901 I load: token to piece cache size = 0.2984 MB
0.00.038.904 I print_info: arch             = gptneox
0.00.038.904 I print_info: vocab_only       = 0
0.00.038.904 I print_info: n_ctx_train      = 2048
0.00.038.904 I print_info: n_embd           = 2048
0.00.038.905 I print_info: n_layer          = 24
0.00.038.907 I print_info: n_head           = 16
0.00.038.908 I print_info: n_head_kv        = 16
0.00.038.908 I print_info: n_rot            = 32
0.00.038.909 I print_info: n_swa            = 0
0.00.038.909 I print_info: n_embd_head_k    = 128
0.00.038.909 I print_info: n_embd_head_v    = 128
0.00.038.910 I print_info: n_gqa            = 1
0.00.038.912 I print_info: n_embd_k_gqa     = 2048
0.00.038.913 I print_info: n_embd_v_gqa     = 2048
0.00.038.913 I print_info: f_norm_eps       = 1.0e-05
0.00.038.913 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.914 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.914 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.914 I print_info: f_logit_scale    = 0.0e+00
0.00.038.915 I print_info: n_ff             = 8192
0.00.038.915 I print_info: n_expert         = 0
0.00.038.915 I print_info: n_expert_used    = 0
0.00.038.915 I print_info: causal attn      = 1
0.00.038.915 I print_info: pooling type     = 0
0.00.038.915 I print_info: rope type        = 2
0.00.038.916 I print_info: rope scaling     = linear
0.00.038.916 I print_info: freq_base_train  = 10000.0
0.00.038.916 I print_info: freq_scale_train = 1
0.00.038.917 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.918 I print_info: rope_finetuned   = unknown
0.00.038.918 I print_info: ssm_d_conv       = 0
0.00.038.918 I print_info: ssm_d_inner      = 0
0.00.038.918 I print_info: ssm_d_state      = 0
0.00.038.918 I print_info: ssm_dt_rank      = 0
0.00.038.918 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.919 I print_info: model type       = 1.4B
0.00.038.919 I print_info: model params     = 1.41 B
0.00.038.919 I print_info: general.name     = 1.4B
0.00.038.920 I print_info: vocab type       = BPE
0.00.038.920 I print_info: n_vocab          = 50304
0.00.038.920 I print_info: n_merges         = 50009
0.00.038.921 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.921 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.921 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.921 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.921 I print_info: LF token         = 187 'Ċ'
0.00.038.922 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.922 I print_info: max token length = 1024
0.00.038.922 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.417.621 I load_tensors: offloading 24 repeating layers to GPU
0.00.417.632 I load_tensors: offloading output layer to GPU
0.00.417.633 I load_tensors: offloaded 25/25 layers to GPU
0.00.417.662 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.417.663 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.419.257 I llama_init_from_model: n_seq_max     = 1
0.00.419.261 I llama_init_from_model: n_ctx         = 2048
0.00.419.262 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.419.262 I llama_init_from_model: n_batch       = 2048
0.00.419.263 I llama_init_from_model: n_ubatch      = 512
0.00.419.263 I llama_init_from_model: flash_attn    = 0
0.00.419.265 I llama_init_from_model: freq_base     = 10000.0
0.00.419.265 I llama_init_from_model: freq_scale    = 1
0.00.419.267 I ggml_metal_init: allocating
0.00.419.334 I ggml_metal_init: found device: Apple M4
0.00.419.347 I ggml_metal_init: picking default device: Apple M4
0.00.421.079 I ggml_metal_init: using embedded metal library
0.00.426.790 I ggml_metal_init: GPU name:   Apple M4
0.00.426.803 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.426.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.426.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.426.806 I ggml_metal_init: simdgroup reduction   = true
0.00.426.806 I ggml_metal_init: simdgroup matrix mul. = true
0.00.426.806 I ggml_metal_init: has residency sets    = true
0.00.426.806 I ggml_metal_init: has bfloat            = true
0.00.426.807 I ggml_metal_init: use bfloat            = true
0.00.426.811 I ggml_metal_init: hasUnifiedMemory      = true
0.00.426.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.449.210 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.511.346 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.511.353 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.511.380 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.515.360 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.515.362 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.515.362 I llama_init_from_model: graph nodes  = 967
0.00.515.362 I llama_init_from_model: graph splits = 2
0.00.515.368 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.515.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.515.497 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.574.920 I main: llama threadpool init, n_threads = 4
0.00.574.965 I 
0.00.574.985 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.985 I 
0.00.575.160 I sampler seed: 1234
0.00.575.165 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.575.217 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.575.219 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.575.219 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.253.729 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.01.253.730 I llama_perf_context_print:        load time =     564.21 ms
0.01.253.731 I llama_perf_context_print: prompt eval time =      42.96 ms /     7 tokens (    6.14 ms per token,   162.93 tokens per second)
0.01.253.731 I llama_perf_context_print:        eval time =     632.66 ms /    63 runs   (   10.04 ms per token,    99.58 tokens per second)
0.01.253.732 I llama_perf_context_print:       total time =     679.53 ms /    70 tokens
0.01.253.962 I ggml_metal_free: deallocating

real	0m1.273s
user	0m0.112s
sys	0m0.188s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.715 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.532 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.539 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.540 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.540 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.542 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.542 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.543 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.543 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.545 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.361 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.362 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.362 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.363 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.363 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.363 I llama_model_loader: - type  f32:  194 tensors
0.00.025.364 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.364 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.364 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.364 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.365 I print_info: file format = GGUF V3 (latest)
0.00.025.365 I print_info: file type   = Q3_K - Medium
0.00.025.371 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.272 I load: special tokens cache size = 25
0.00.039.197 I load: token to piece cache size = 0.2984 MB
0.00.039.200 I print_info: arch             = gptneox
0.00.039.200 I print_info: vocab_only       = 0
0.00.039.200 I print_info: n_ctx_train      = 2048
0.00.039.201 I print_info: n_embd           = 2048
0.00.039.201 I print_info: n_layer          = 24
0.00.039.203 I print_info: n_head           = 16
0.00.039.204 I print_info: n_head_kv        = 16
0.00.039.204 I print_info: n_rot            = 32
0.00.039.205 I print_info: n_swa            = 0
0.00.039.205 I print_info: n_embd_head_k    = 128
0.00.039.205 I print_info: n_embd_head_v    = 128
0.00.039.206 I print_info: n_gqa            = 1
0.00.039.206 I print_info: n_embd_k_gqa     = 2048
0.00.039.207 I print_info: n_embd_v_gqa     = 2048
0.00.039.208 I print_info: f_norm_eps       = 1.0e-05
0.00.039.208 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.208 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.208 I print_info: f_logit_scale    = 0.0e+00
0.00.039.209 I print_info: n_ff             = 8192
0.00.039.209 I print_info: n_expert         = 0
0.00.039.209 I print_info: n_expert_used    = 0
0.00.039.211 I print_info: causal attn      = 1
0.00.039.213 I print_info: pooling type     = 0
0.00.039.213 I print_info: rope type        = 2
0.00.039.213 I print_info: rope scaling     = linear
0.00.039.213 I print_info: freq_base_train  = 10000.0
0.00.039.214 I print_info: freq_scale_train = 1
0.00.039.214 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.214 I print_info: rope_finetuned   = unknown
0.00.039.214 I print_info: ssm_d_conv       = 0
0.00.039.215 I print_info: ssm_d_inner      = 0
0.00.039.215 I print_info: ssm_d_state      = 0
0.00.039.215 I print_info: ssm_dt_rank      = 0
0.00.039.215 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.217 I print_info: model type       = 1.4B
0.00.039.217 I print_info: model params     = 1.41 B
0.00.039.217 I print_info: general.name     = 1.4B
0.00.039.218 I print_info: vocab type       = BPE
0.00.039.218 I print_info: n_vocab          = 50304
0.00.039.218 I print_info: n_merges         = 50009
0.00.039.218 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.220 I print_info: LF token         = 187 'Ċ'
0.00.039.220 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.220 I print_info: max token length = 1024
0.00.039.221 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.436.113 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.131 I load_tensors: offloading output layer to GPU
0.00.436.132 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.165 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.166 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.775 I llama_init_from_model: n_seq_max     = 1
0.00.437.780 I llama_init_from_model: n_ctx         = 2048
0.00.437.780 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.437.781 I llama_init_from_model: n_batch       = 2048
0.00.437.782 I llama_init_from_model: n_ubatch      = 512
0.00.437.782 I llama_init_from_model: flash_attn    = 0
0.00.437.784 I llama_init_from_model: freq_base     = 10000.0
0.00.437.784 I llama_init_from_model: freq_scale    = 1
0.00.437.787 I ggml_metal_init: allocating
0.00.437.860 I ggml_metal_init: found device: Apple M4
0.00.437.873 I ggml_metal_init: picking default device: Apple M4
0.00.439.705 I ggml_metal_init: using embedded metal library
0.00.445.610 I ggml_metal_init: GPU name:   Apple M4
0.00.445.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.619 I ggml_metal_init: simdgroup reduction   = true
0.00.445.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.619 I ggml_metal_init: has residency sets    = true
0.00.445.620 I ggml_metal_init: has bfloat            = true
0.00.445.620 I ggml_metal_init: use bfloat            = true
0.00.445.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.631 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.610 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.525.688 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.525.696 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.525.726 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.531.150 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.531.153 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.531.153 I llama_init_from_model: graph nodes  = 967
0.00.531.153 I llama_init_from_model: graph splits = 2
0.00.531.159 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.531.287 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.531.288 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.209 I main: llama threadpool init, n_threads = 4
0.00.581.255 I 
0.00.581.275 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.275 I 
0.00.581.384 I sampler seed: 1234
0.00.581.388 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.581.422 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.581.425 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.581.426 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.330.805 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47875.93 tokens per second)
0.01.330.806 I llama_perf_context_print:        load time =     571.77 ms
0.01.330.807 I llama_perf_context_print: prompt eval time =      47.08 ms /     7 tokens (    6.73 ms per token,   148.68 tokens per second)
0.01.330.808 I llama_perf_context_print:        eval time =     699.77 ms /    63 runs   (   11.11 ms per token,    90.03 tokens per second)
0.01.330.808 I llama_perf_context_print:       total time =     750.32 ms /    70 tokens
0.01.331.044 I ggml_metal_free: deallocating

real	0m1.347s
user	0m0.112s
sys	0m0.175s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.584 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.023.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.597 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.598 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.601 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.601 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.604 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.605 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.605 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.611 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.454 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.455 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.032.456 I llama_model_loader: - type  f32:  194 tensors
0.00.032.456 I llama_model_loader: - type q4_K:   61 tensors
0.00.032.457 I llama_model_loader: - type q5_K:   24 tensors
0.00.032.457 I llama_model_loader: - type q6_K:   13 tensors
0.00.032.458 I print_info: file format = GGUF V3 (latest)
0.00.032.458 I print_info: file type   = Q4_K - Medium
0.00.032.459 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.040.616 I load: special tokens cache size = 25
0.00.046.760 I load: token to piece cache size = 0.2984 MB
0.00.046.764 I print_info: arch             = gptneox
0.00.046.764 I print_info: vocab_only       = 0
0.00.046.764 I print_info: n_ctx_train      = 2048
0.00.046.764 I print_info: n_embd           = 2048
0.00.046.765 I print_info: n_layer          = 24
0.00.046.768 I print_info: n_head           = 16
0.00.046.769 I print_info: n_head_kv        = 16
0.00.046.769 I print_info: n_rot            = 32
0.00.046.769 I print_info: n_swa            = 0
0.00.046.770 I print_info: n_embd_head_k    = 128
0.00.046.770 I print_info: n_embd_head_v    = 128
0.00.046.770 I print_info: n_gqa            = 1
0.00.046.771 I print_info: n_embd_k_gqa     = 2048
0.00.046.772 I print_info: n_embd_v_gqa     = 2048
0.00.046.772 I print_info: f_norm_eps       = 1.0e-05
0.00.046.773 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.777 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.778 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.778 I print_info: f_logit_scale    = 0.0e+00
0.00.046.778 I print_info: n_ff             = 8192
0.00.046.783 I print_info: n_expert         = 0
0.00.046.783 I print_info: n_expert_used    = 0
0.00.046.783 I print_info: causal attn      = 1
0.00.046.783 I print_info: pooling type     = 0
0.00.046.783 I print_info: rope type        = 2
0.00.046.784 I print_info: rope scaling     = linear
0.00.046.784 I print_info: freq_base_train  = 10000.0
0.00.046.784 I print_info: freq_scale_train = 1
0.00.046.786 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.786 I print_info: rope_finetuned   = unknown
0.00.046.786 I print_info: ssm_d_conv       = 0
0.00.046.786 I print_info: ssm_d_inner      = 0
0.00.046.786 I print_info: ssm_d_state      = 0
0.00.046.786 I print_info: ssm_dt_rank      = 0
0.00.046.787 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.787 I print_info: model type       = 1.4B
0.00.046.787 I print_info: model params     = 1.41 B
0.00.046.787 I print_info: general.name     = 1.4B
0.00.046.790 I print_info: vocab type       = BPE
0.00.046.791 I print_info: n_vocab          = 50304
0.00.046.791 I print_info: n_merges         = 50009
0.00.046.791 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.791 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.791 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.791 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.793 I print_info: LF token         = 187 'Ċ'
0.00.046.793 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.793 I print_info: max token length = 1024
0.00.046.794 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.547.617 I load_tensors: offloading 24 repeating layers to GPU
0.00.547.633 I load_tensors: offloading output layer to GPU
0.00.547.634 I load_tensors: offloaded 25/25 layers to GPU
0.00.547.666 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.547.674 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.549.068 I llama_init_from_model: n_seq_max     = 1
0.00.549.071 I llama_init_from_model: n_ctx         = 2048
0.00.549.071 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.549.072 I llama_init_from_model: n_batch       = 2048
0.00.549.072 I llama_init_from_model: n_ubatch      = 512
0.00.549.073 I llama_init_from_model: flash_attn    = 0
0.00.549.075 I llama_init_from_model: freq_base     = 10000.0
0.00.549.075 I llama_init_from_model: freq_scale    = 1
0.00.549.077 I ggml_metal_init: allocating
0.00.549.155 I ggml_metal_init: found device: Apple M4
0.00.549.167 I ggml_metal_init: picking default device: Apple M4
0.00.551.077 I ggml_metal_init: using embedded metal library
0.00.557.772 I ggml_metal_init: GPU name:   Apple M4
0.00.557.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.557.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.557.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.557.779 I ggml_metal_init: simdgroup reduction   = true
0.00.557.779 I ggml_metal_init: simdgroup matrix mul. = true
0.00.557.779 I ggml_metal_init: has residency sets    = true
0.00.557.779 I ggml_metal_init: has bfloat            = true
0.00.557.779 I ggml_metal_init: use bfloat            = true
0.00.557.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.557.782 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.575.473 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.725 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.625.734 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.625.767 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.630.419 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.630.421 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.630.421 I llama_init_from_model: graph nodes  = 967
0.00.630.422 I llama_init_from_model: graph splits = 2
0.00.630.428 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.630.556 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.630.557 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.442 I main: llama threadpool init, n_threads = 4
0.00.687.485 I 
0.00.687.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.506 I 
0.00.687.679 I sampler seed: 1234
0.00.687.684 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.704 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.705 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.705 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.464.744 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.464.744 I llama_perf_context_print:        load time =     677.82 ms
0.01.464.745 I llama_perf_context_print: prompt eval time =      57.59 ms /     7 tokens (    8.23 ms per token,   121.55 tokens per second)
0.01.464.746 I llama_perf_context_print:        eval time =     716.53 ms /    63 runs   (   11.37 ms per token,    87.92 tokens per second)
0.01.464.747 I llama_perf_context_print:       total time =     778.02 ms /    70 tokens
0.01.464.972 I ggml_metal_free: deallocating

real	0m1.482s
user	0m0.110s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.015.820 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.031.489 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.491 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.492 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.492 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.495 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.495 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.495 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.496 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.496 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.496 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.498 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.498 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.790 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.153 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.428 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.428 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.429 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.041.430 I llama_model_loader: - type  f32:  194 tensors
0.00.041.430 I llama_model_loader: - type q5_K:   61 tensors
0.00.041.430 I llama_model_loader: - type q6_K:   37 tensors
0.00.041.431 I print_info: file format = GGUF V3 (latest)
0.00.041.431 I print_info: file type   = Q5_K - Medium
0.00.041.432 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.051.096 I load: special tokens cache size = 25
0.00.058.879 I load: token to piece cache size = 0.2984 MB
0.00.058.883 I print_info: arch             = gptneox
0.00.058.883 I print_info: vocab_only       = 0
0.00.058.883 I print_info: n_ctx_train      = 2048
0.00.058.883 I print_info: n_embd           = 2048
0.00.058.884 I print_info: n_layer          = 24
0.00.058.887 I print_info: n_head           = 16
0.00.058.888 I print_info: n_head_kv        = 16
0.00.058.888 I print_info: n_rot            = 32
0.00.058.889 I print_info: n_swa            = 0
0.00.058.889 I print_info: n_embd_head_k    = 128
0.00.058.889 I print_info: n_embd_head_v    = 128
0.00.058.890 I print_info: n_gqa            = 1
0.00.058.891 I print_info: n_embd_k_gqa     = 2048
0.00.058.894 I print_info: n_embd_v_gqa     = 2048
0.00.058.894 I print_info: f_norm_eps       = 1.0e-05
0.00.058.900 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.901 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.903 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.904 I print_info: f_logit_scale    = 0.0e+00
0.00.058.905 I print_info: n_ff             = 8192
0.00.058.905 I print_info: n_expert         = 0
0.00.058.905 I print_info: n_expert_used    = 0
0.00.058.906 I print_info: causal attn      = 1
0.00.058.906 I print_info: pooling type     = 0
0.00.058.907 I print_info: rope type        = 2
0.00.058.907 I print_info: rope scaling     = linear
0.00.058.907 I print_info: freq_base_train  = 10000.0
0.00.058.908 I print_info: freq_scale_train = 1
0.00.058.908 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.908 I print_info: rope_finetuned   = unknown
0.00.058.908 I print_info: ssm_d_conv       = 0
0.00.058.909 I print_info: ssm_d_inner      = 0
0.00.058.910 I print_info: ssm_d_state      = 0
0.00.058.910 I print_info: ssm_dt_rank      = 0
0.00.058.910 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.911 I print_info: model type       = 1.4B
0.00.058.911 I print_info: model params     = 1.41 B
0.00.058.911 I print_info: general.name     = 1.4B
0.00.058.912 I print_info: vocab type       = BPE
0.00.058.912 I print_info: n_vocab          = 50304
0.00.058.912 I print_info: n_merges         = 50009
0.00.058.912 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.913 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.913 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.914 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.915 I print_info: LF token         = 187 'Ċ'
0.00.058.915 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.915 I print_info: max token length = 1024
0.00.058.916 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.305 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.312 I load_tensors: offloading output layer to GPU
0.00.638.314 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.346 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.638.348 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.640.050 I llama_init_from_model: n_seq_max     = 1
0.00.640.052 I llama_init_from_model: n_ctx         = 2048
0.00.640.053 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.640.053 I llama_init_from_model: n_batch       = 2048
0.00.640.054 I llama_init_from_model: n_ubatch      = 512
0.00.640.054 I llama_init_from_model: flash_attn    = 0
0.00.640.056 I llama_init_from_model: freq_base     = 10000.0
0.00.640.057 I llama_init_from_model: freq_scale    = 1
0.00.640.065 I ggml_metal_init: allocating
0.00.640.133 I ggml_metal_init: found device: Apple M4
0.00.640.147 I ggml_metal_init: picking default device: Apple M4
0.00.641.779 I ggml_metal_init: using embedded metal library
0.00.648.073 I ggml_metal_init: GPU name:   Apple M4
0.00.648.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.079 I ggml_metal_init: simdgroup reduction   = true
0.00.648.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.080 I ggml_metal_init: has residency sets    = true
0.00.648.080 I ggml_metal_init: has bfloat            = true
0.00.648.080 I ggml_metal_init: use bfloat            = true
0.00.648.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.140 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.719.613 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.719.620 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.719.643 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.723.920 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.723.922 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.723.922 I llama_init_from_model: graph nodes  = 967
0.00.723.923 I llama_init_from_model: graph splits = 2
0.00.723.928 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.724.057 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.058 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.056 I main: llama threadpool init, n_threads = 4
0.00.786.101 I 
0.00.786.123 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.124 I 
0.00.786.281 I sampler seed: 1234
0.00.786.285 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.296 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.297 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.297 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.638.834 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.01.638.834 I llama_perf_context_print:        load time =     769.43 ms
0.01.638.835 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.77 tokens per second)
0.01.638.836 I llama_perf_context_print:        eval time =     798.05 ms /    63 runs   (   12.67 ms per token,    78.94 tokens per second)
0.01.638.836 I llama_perf_context_print:       total time =     853.58 ms /    70 tokens
0.01.639.105 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.114s
sys	0m0.224s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.737 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.154 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.161 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.162 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.165 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.166 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.170 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.170 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.913 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.614 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.616 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.617 I llama_model_loader: - type  f32:  194 tensors
0.00.025.617 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.617 I print_info: file format = GGUF V3 (latest)
0.00.025.618 I print_info: file type   = Q6_K
0.00.025.619 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.531 I load: special tokens cache size = 25
0.00.039.260 I load: token to piece cache size = 0.2984 MB
0.00.039.263 I print_info: arch             = gptneox
0.00.039.263 I print_info: vocab_only       = 0
0.00.039.263 I print_info: n_ctx_train      = 2048
0.00.039.263 I print_info: n_embd           = 2048
0.00.039.264 I print_info: n_layer          = 24
0.00.039.266 I print_info: n_head           = 16
0.00.039.267 I print_info: n_head_kv        = 16
0.00.039.267 I print_info: n_rot            = 32
0.00.039.267 I print_info: n_swa            = 0
0.00.039.267 I print_info: n_embd_head_k    = 128
0.00.039.267 I print_info: n_embd_head_v    = 128
0.00.039.268 I print_info: n_gqa            = 1
0.00.039.269 I print_info: n_embd_k_gqa     = 2048
0.00.039.270 I print_info: n_embd_v_gqa     = 2048
0.00.039.271 I print_info: f_norm_eps       = 1.0e-05
0.00.039.271 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.271 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.271 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.272 I print_info: f_logit_scale    = 0.0e+00
0.00.039.272 I print_info: n_ff             = 8192
0.00.039.272 I print_info: n_expert         = 0
0.00.039.273 I print_info: n_expert_used    = 0
0.00.039.273 I print_info: causal attn      = 1
0.00.039.273 I print_info: pooling type     = 0
0.00.039.273 I print_info: rope type        = 2
0.00.039.273 I print_info: rope scaling     = linear
0.00.039.274 I print_info: freq_base_train  = 10000.0
0.00.039.274 I print_info: freq_scale_train = 1
0.00.039.274 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.275 I print_info: rope_finetuned   = unknown
0.00.039.275 I print_info: ssm_d_conv       = 0
0.00.039.275 I print_info: ssm_d_inner      = 0
0.00.039.275 I print_info: ssm_d_state      = 0
0.00.039.275 I print_info: ssm_dt_rank      = 0
0.00.039.275 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.276 I print_info: model type       = 1.4B
0.00.039.276 I print_info: model params     = 1.41 B
0.00.039.276 I print_info: general.name     = 1.4B
0.00.039.276 I print_info: vocab type       = BPE
0.00.039.277 I print_info: n_vocab          = 50304
0.00.039.277 I print_info: n_merges         = 50009
0.00.039.277 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: LF token         = 187 'Ċ'
0.00.039.279 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.280 I print_info: max token length = 1024
0.00.039.280 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.316 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.319 I load_tensors: offloading output layer to GPU
0.00.641.320 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.344 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.641.347 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.642.972 I llama_init_from_model: n_seq_max     = 1
0.00.642.974 I llama_init_from_model: n_ctx         = 2048
0.00.642.974 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.642.975 I llama_init_from_model: n_batch       = 2048
0.00.642.976 I llama_init_from_model: n_ubatch      = 512
0.00.642.976 I llama_init_from_model: flash_attn    = 0
0.00.642.977 I llama_init_from_model: freq_base     = 10000.0
0.00.642.977 I llama_init_from_model: freq_scale    = 1
0.00.642.979 I ggml_metal_init: allocating
0.00.643.026 I ggml_metal_init: found device: Apple M4
0.00.643.037 I ggml_metal_init: picking default device: Apple M4
0.00.644.546 I ggml_metal_init: using embedded metal library
0.00.650.334 I ggml_metal_init: GPU name:   Apple M4
0.00.650.338 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.340 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.340 I ggml_metal_init: simdgroup reduction   = true
0.00.650.340 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.341 I ggml_metal_init: has residency sets    = true
0.00.650.341 I ggml_metal_init: has bfloat            = true
0.00.650.341 I ggml_metal_init: use bfloat            = true
0.00.650.342 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.343 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.759 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.297 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.723.304 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.723.329 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.234 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.728.237 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.728.237 I llama_init_from_model: graph nodes  = 967
0.00.728.237 I llama_init_from_model: graph splits = 2
0.00.728.243 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.728.371 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.897 I main: llama threadpool init, n_threads = 4
0.00.795.941 I 
0.00.795.962 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.963 I 
0.00.796.139 I sampler seed: 1234
0.00.796.143 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.154 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.156 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.156 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.677.920 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.677.920 I llama_perf_context_print:        load time =     786.42 ms
0.01.677.921 I llama_perf_context_print: prompt eval time =      54.03 ms /     7 tokens (    7.72 ms per token,   129.56 tokens per second)
0.01.677.922 I llama_perf_context_print:        eval time =     824.93 ms /    63 runs   (   13.09 ms per token,    76.37 tokens per second)
0.01.677.922 I llama_perf_context_print:       total time =     882.77 ms /    70 tokens
0.01.678.204 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.108s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.543 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.355 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.639 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.645 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.647 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.648 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.654 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.654 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.655 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.658 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.659 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.661 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.667 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.671 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.671 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.949 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.801 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.251 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.253 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.254 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.255 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.256 I llama_model_loader: - type  f32:  194 tensors
0.00.056.256 I llama_model_loader: - type  f16:   98 tensors
0.00.056.257 I print_info: file format = GGUF V3 (latest)
0.00.056.258 I print_info: file type   = all F32 (guessed)
0.00.056.259 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.601 I load: special tokens cache size = 25
0.00.077.516 I load: token to piece cache size = 0.2984 MB
0.00.077.519 I print_info: arch             = gptneox
0.00.077.519 I print_info: vocab_only       = 0
0.00.077.519 I print_info: n_ctx_train      = 2048
0.00.077.519 I print_info: n_embd           = 2048
0.00.077.520 I print_info: n_layer          = 24
0.00.077.523 I print_info: n_head           = 16
0.00.077.524 I print_info: n_head_kv        = 16
0.00.077.526 I print_info: n_rot            = 32
0.00.077.526 I print_info: n_swa            = 0
0.00.077.526 I print_info: n_embd_head_k    = 128
0.00.077.526 I print_info: n_embd_head_v    = 128
0.00.077.527 I print_info: n_gqa            = 1
0.00.077.528 I print_info: n_embd_k_gqa     = 2048
0.00.077.528 I print_info: n_embd_v_gqa     = 2048
0.00.077.529 I print_info: f_norm_eps       = 1.0e-05
0.00.077.529 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.529 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.530 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.530 I print_info: f_logit_scale    = 0.0e+00
0.00.077.530 I print_info: n_ff             = 8192
0.00.077.531 I print_info: n_expert         = 0
0.00.077.531 I print_info: n_expert_used    = 0
0.00.077.531 I print_info: causal attn      = 1
0.00.077.531 I print_info: pooling type     = 0
0.00.077.535 I print_info: rope type        = 2
0.00.077.535 I print_info: rope scaling     = linear
0.00.077.536 I print_info: freq_base_train  = 10000.0
0.00.077.536 I print_info: freq_scale_train = 1
0.00.077.536 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.536 I print_info: rope_finetuned   = unknown
0.00.077.537 I print_info: ssm_d_conv       = 0
0.00.077.537 I print_info: ssm_d_inner      = 0
0.00.077.537 I print_info: ssm_d_state      = 0
0.00.077.537 I print_info: ssm_dt_rank      = 0
0.00.077.537 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.538 I print_info: model type       = 1.4B
0.00.077.538 I print_info: model params     = 1.41 B
0.00.077.538 I print_info: general.name     = 1.4B
0.00.077.539 I print_info: vocab type       = BPE
0.00.077.539 I print_info: n_vocab          = 50304
0.00.077.540 I print_info: n_merges         = 50009
0.00.077.540 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.540 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.540 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.541 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.541 I print_info: LF token         = 187 'Ċ'
0.00.077.541 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.542 I print_info: max token length = 1024
0.00.077.543 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.367.552 I load_tensors: offloading 24 repeating layers to GPU
0.01.367.556 I load_tensors: offloading output layer to GPU
0.01.367.556 I load_tensors: offloaded 25/25 layers to GPU
0.01.367.583 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.367.585 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.368.243 I llama_init_from_model: n_seq_max     = 1
0.01.368.244 I llama_init_from_model: n_ctx         = 128
0.01.368.244 I llama_init_from_model: n_ctx_per_seq = 128
0.01.368.244 I llama_init_from_model: n_batch       = 128
0.01.368.244 I llama_init_from_model: n_ubatch      = 128
0.01.368.244 I llama_init_from_model: flash_attn    = 0
0.01.368.245 I llama_init_from_model: freq_base     = 10000.0
0.01.368.245 I llama_init_from_model: freq_scale    = 1
0.01.368.246 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.368.246 I ggml_metal_init: allocating
0.01.368.292 I ggml_metal_init: found device: Apple M4
0.01.368.297 I ggml_metal_init: picking default device: Apple M4
0.01.369.345 I ggml_metal_init: using embedded metal library
0.01.373.014 I ggml_metal_init: GPU name:   Apple M4
0.01.373.017 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.373.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.373.018 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.373.018 I ggml_metal_init: simdgroup reduction   = true
0.01.373.019 I ggml_metal_init: simdgroup matrix mul. = true
0.01.373.019 I ggml_metal_init: has residency sets    = true
0.01.373.019 I ggml_metal_init: has bfloat            = true
0.01.373.019 I ggml_metal_init: use bfloat            = true
0.01.373.020 I ggml_metal_init: hasUnifiedMemory      = true
0.01.373.020 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.383.572 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.385.266 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.385.268 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.385.281 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.386.929 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.386.930 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.386.930 I llama_init_from_model: graph nodes  = 967
0.01.386.931 I llama_init_from_model: graph splits = 2
0.01.386.932 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.386.932 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.421.569 I 
0.01.421.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.421.615 I perplexity: tokenizing the input ..
0.01.426.681 I perplexity: tokenization took 5.063 ms
0.01.426.685 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.556.819 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.558.265 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.558.296 I llama_perf_context_print:        load time =    1397.20 ms
0.01.558.298 I llama_perf_context_print: prompt eval time =     129.82 ms /   128 tokens (    1.01 ms per token,   985.98 tokens per second)
0.01.558.298 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.558.299 I llama_perf_context_print:       total time =     136.73 ms /   129 tokens
0.01.558.659 I ggml_metal_free: deallocating

real	0m1.765s
user	0m0.098s
sys	0m0.244s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.298 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.139 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.307 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.315 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.316 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.317 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.317 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.318 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.318 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.319 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.278 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.399 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.400 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.401 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.401 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.401 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.402 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.402 I llama_model_loader: - type  f32:  194 tensors
0.00.026.402 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.403 I print_info: file format = GGUF V3 (latest)
0.00.026.404 I print_info: file type   = Q8_0
0.00.026.405 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.845 I load: special tokens cache size = 25
0.00.041.035 I load: token to piece cache size = 0.2984 MB
0.00.041.040 I print_info: arch             = gptneox
0.00.041.040 I print_info: vocab_only       = 0
0.00.041.040 I print_info: n_ctx_train      = 2048
0.00.041.040 I print_info: n_embd           = 2048
0.00.041.040 I print_info: n_layer          = 24
0.00.041.045 I print_info: n_head           = 16
0.00.041.046 I print_info: n_head_kv        = 16
0.00.041.046 I print_info: n_rot            = 32
0.00.041.046 I print_info: n_swa            = 0
0.00.041.049 I print_info: n_embd_head_k    = 128
0.00.041.050 I print_info: n_embd_head_v    = 128
0.00.041.050 I print_info: n_gqa            = 1
0.00.041.052 I print_info: n_embd_k_gqa     = 2048
0.00.041.052 I print_info: n_embd_v_gqa     = 2048
0.00.041.053 I print_info: f_norm_eps       = 1.0e-05
0.00.041.053 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.053 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.054 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.054 I print_info: f_logit_scale    = 0.0e+00
0.00.041.054 I print_info: n_ff             = 8192
0.00.041.055 I print_info: n_expert         = 0
0.00.041.055 I print_info: n_expert_used    = 0
0.00.041.055 I print_info: causal attn      = 1
0.00.041.056 I print_info: pooling type     = 0
0.00.041.056 I print_info: rope type        = 2
0.00.041.056 I print_info: rope scaling     = linear
0.00.041.056 I print_info: freq_base_train  = 10000.0
0.00.041.057 I print_info: freq_scale_train = 1
0.00.041.057 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.057 I print_info: rope_finetuned   = unknown
0.00.041.057 I print_info: ssm_d_conv       = 0
0.00.041.057 I print_info: ssm_d_inner      = 0
0.00.041.058 I print_info: ssm_d_state      = 0
0.00.041.058 I print_info: ssm_dt_rank      = 0
0.00.041.058 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.059 I print_info: model type       = 1.4B
0.00.041.059 I print_info: model params     = 1.41 B
0.00.041.059 I print_info: general.name     = 1.4B
0.00.041.059 I print_info: vocab type       = BPE
0.00.041.060 I print_info: n_vocab          = 50304
0.00.041.060 I print_info: n_merges         = 50009
0.00.041.060 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.060 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.060 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.060 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.061 I print_info: LF token         = 187 'Ċ'
0.00.041.061 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.061 I print_info: max token length = 1024
0.00.041.062 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.825.297 I load_tensors: offloading 24 repeating layers to GPU
0.00.825.301 I load_tensors: offloading output layer to GPU
0.00.825.301 I load_tensors: offloaded 25/25 layers to GPU
0.00.825.326 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.825.328 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.826.314 I llama_init_from_model: n_seq_max     = 1
0.00.826.316 I llama_init_from_model: n_ctx         = 128
0.00.826.316 I llama_init_from_model: n_ctx_per_seq = 128
0.00.826.317 I llama_init_from_model: n_batch       = 128
0.00.826.317 I llama_init_from_model: n_ubatch      = 128
0.00.826.317 I llama_init_from_model: flash_attn    = 0
0.00.826.318 I llama_init_from_model: freq_base     = 10000.0
0.00.826.319 I llama_init_from_model: freq_scale    = 1
0.00.826.319 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.826.321 I ggml_metal_init: allocating
0.00.826.356 I ggml_metal_init: found device: Apple M4
0.00.826.366 I ggml_metal_init: picking default device: Apple M4
0.00.827.744 I ggml_metal_init: using embedded metal library
0.00.832.931 I ggml_metal_init: GPU name:   Apple M4
0.00.832.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.832.935 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.832.936 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.832.937 I ggml_metal_init: simdgroup reduction   = true
0.00.832.937 I ggml_metal_init: simdgroup matrix mul. = true
0.00.832.937 I ggml_metal_init: has residency sets    = true
0.00.832.937 I ggml_metal_init: has bfloat            = true
0.00.832.938 I ggml_metal_init: use bfloat            = true
0.00.832.938 I ggml_metal_init: hasUnifiedMemory      = true
0.00.832.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.848.040 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.851.375 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.851.378 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.851.422 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.854.556 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.854.558 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.854.558 I llama_init_from_model: graph nodes  = 967
0.00.854.559 I llama_init_from_model: graph splits = 2
0.00.854.561 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.854.561 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.881.086 I 
0.00.881.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.881.168 I perplexity: tokenizing the input ..
0.00.888.059 I perplexity: tokenization took 6.89 ms
0.00.888.063 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.011.645 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.012.933 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.012.958 I llama_perf_context_print:        load time =     870.94 ms
0.01.012.958 I llama_perf_context_print: prompt eval time =     123.35 ms /   128 tokens (    0.96 ms per token,  1037.71 tokens per second)
0.01.012.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.012.959 I llama_perf_context_print:       total time =     131.88 ms /   129 tokens
0.01.013.344 I ggml_metal_free: deallocating

real	0m1.030s
user	0m0.077s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.279 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.797 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.194 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.200 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.209 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.210 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.210 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.210 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.212 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.214 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.214 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.216 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.092 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.131 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.979 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.981 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.981 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.982 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.982 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.982 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.983 I llama_model_loader: - type  f32:  194 tensors
0.00.025.983 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.983 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.984 I print_info: file format = GGUF V3 (latest)
0.00.025.985 I print_info: file type   = Q4_0
0.00.025.986 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.693 I load: special tokens cache size = 25
0.00.040.807 I load: token to piece cache size = 0.2984 MB
0.00.040.811 I print_info: arch             = gptneox
0.00.040.811 I print_info: vocab_only       = 0
0.00.040.812 I print_info: n_ctx_train      = 2048
0.00.040.812 I print_info: n_embd           = 2048
0.00.040.812 I print_info: n_layer          = 24
0.00.040.817 I print_info: n_head           = 16
0.00.040.817 I print_info: n_head_kv        = 16
0.00.040.818 I print_info: n_rot            = 32
0.00.040.818 I print_info: n_swa            = 0
0.00.040.818 I print_info: n_embd_head_k    = 128
0.00.040.818 I print_info: n_embd_head_v    = 128
0.00.040.819 I print_info: n_gqa            = 1
0.00.040.820 I print_info: n_embd_k_gqa     = 2048
0.00.040.823 I print_info: n_embd_v_gqa     = 2048
0.00.040.823 I print_info: f_norm_eps       = 1.0e-05
0.00.040.823 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.824 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.824 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.824 I print_info: f_logit_scale    = 0.0e+00
0.00.040.825 I print_info: n_ff             = 8192
0.00.040.828 I print_info: n_expert         = 0
0.00.040.829 I print_info: n_expert_used    = 0
0.00.040.829 I print_info: causal attn      = 1
0.00.040.830 I print_info: pooling type     = 0
0.00.040.830 I print_info: rope type        = 2
0.00.040.830 I print_info: rope scaling     = linear
0.00.040.830 I print_info: freq_base_train  = 10000.0
0.00.040.831 I print_info: freq_scale_train = 1
0.00.040.831 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.831 I print_info: rope_finetuned   = unknown
0.00.040.831 I print_info: ssm_d_conv       = 0
0.00.040.833 I print_info: ssm_d_inner      = 0
0.00.040.833 I print_info: ssm_d_state      = 0
0.00.040.833 I print_info: ssm_dt_rank      = 0
0.00.040.833 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.833 I print_info: model type       = 1.4B
0.00.040.834 I print_info: model params     = 1.41 B
0.00.040.834 I print_info: general.name     = 1.4B
0.00.040.834 I print_info: vocab type       = BPE
0.00.040.834 I print_info: n_vocab          = 50304
0.00.040.835 I print_info: n_merges         = 50009
0.00.040.835 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.835 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.835 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.835 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.836 I print_info: LF token         = 187 'Ċ'
0.00.040.836 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.836 I print_info: max token length = 1024
0.00.040.836 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.555.289 I load_tensors: offloading 24 repeating layers to GPU
0.00.555.303 I load_tensors: offloading output layer to GPU
0.00.555.304 I load_tensors: offloaded 25/25 layers to GPU
0.00.555.339 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.555.341 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.557.029 I llama_init_from_model: n_seq_max     = 1
0.00.557.032 I llama_init_from_model: n_ctx         = 128
0.00.557.032 I llama_init_from_model: n_ctx_per_seq = 128
0.00.557.033 I llama_init_from_model: n_batch       = 128
0.00.557.033 I llama_init_from_model: n_ubatch      = 128
0.00.557.034 I llama_init_from_model: flash_attn    = 0
0.00.557.037 I llama_init_from_model: freq_base     = 10000.0
0.00.557.037 I llama_init_from_model: freq_scale    = 1
0.00.557.038 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.557.040 I ggml_metal_init: allocating
0.00.557.163 I ggml_metal_init: found device: Apple M4
0.00.557.177 I ggml_metal_init: picking default device: Apple M4
0.00.559.116 I ggml_metal_init: using embedded metal library
0.00.564.527 I ggml_metal_init: GPU name:   Apple M4
0.00.564.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.564.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.564.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.564.536 I ggml_metal_init: simdgroup reduction   = true
0.00.564.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.564.537 I ggml_metal_init: has residency sets    = true
0.00.564.537 I ggml_metal_init: has bfloat            = true
0.00.564.537 I ggml_metal_init: use bfloat            = true
0.00.564.539 I ggml_metal_init: hasUnifiedMemory      = true
0.00.564.542 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.584.467 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.588.102 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.588.114 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.588.147 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.591.532 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.591.534 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.591.534 I llama_init_from_model: graph nodes  = 967
0.00.591.535 I llama_init_from_model: graph splits = 2
0.00.591.538 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.591.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.754 I 
0.00.615.822 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.829 I perplexity: tokenizing the input ..
0.00.622.251 I perplexity: tokenization took 6.42 ms
0.00.622.257 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.754.224 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.755.571 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.755.595 I llama_perf_context_print:        load time =     605.95 ms
0.00.755.596 I llama_perf_context_print: prompt eval time =     131.56 ms /   128 tokens (    1.03 ms per token,   972.92 tokens per second)
0.00.755.597 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.755.597 I llama_perf_context_print:       total time =     139.85 ms /   129 tokens
0.00.755.970 I ggml_metal_free: deallocating

real	0m0.772s
user	0m0.080s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.737 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.130 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.136 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.138 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.139 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.143 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.144 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.144 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.097 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.065 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.066 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.067 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.067 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.067 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.068 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.068 I llama_model_loader: - type  f32:  194 tensors
0.00.025.069 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.069 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.070 I print_info: file format = GGUF V3 (latest)
0.00.025.070 I print_info: file type   = Q4_1
0.00.025.071 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.721 I load: special tokens cache size = 25
0.00.039.794 I load: token to piece cache size = 0.2984 MB
0.00.039.799 I print_info: arch             = gptneox
0.00.039.799 I print_info: vocab_only       = 0
0.00.039.799 I print_info: n_ctx_train      = 2048
0.00.039.799 I print_info: n_embd           = 2048
0.00.039.799 I print_info: n_layer          = 24
0.00.039.804 I print_info: n_head           = 16
0.00.039.805 I print_info: n_head_kv        = 16
0.00.039.805 I print_info: n_rot            = 32
0.00.039.805 I print_info: n_swa            = 0
0.00.039.805 I print_info: n_embd_head_k    = 128
0.00.039.805 I print_info: n_embd_head_v    = 128
0.00.039.806 I print_info: n_gqa            = 1
0.00.039.807 I print_info: n_embd_k_gqa     = 2048
0.00.039.808 I print_info: n_embd_v_gqa     = 2048
0.00.039.808 I print_info: f_norm_eps       = 1.0e-05
0.00.039.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.809 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.809 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.809 I print_info: f_logit_scale    = 0.0e+00
0.00.039.810 I print_info: n_ff             = 8192
0.00.039.810 I print_info: n_expert         = 0
0.00.039.810 I print_info: n_expert_used    = 0
0.00.039.810 I print_info: causal attn      = 1
0.00.039.810 I print_info: pooling type     = 0
0.00.039.810 I print_info: rope type        = 2
0.00.039.811 I print_info: rope scaling     = linear
0.00.039.811 I print_info: freq_base_train  = 10000.0
0.00.039.811 I print_info: freq_scale_train = 1
0.00.039.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.812 I print_info: rope_finetuned   = unknown
0.00.039.812 I print_info: ssm_d_conv       = 0
0.00.039.812 I print_info: ssm_d_inner      = 0
0.00.039.812 I print_info: ssm_d_state      = 0
0.00.039.812 I print_info: ssm_dt_rank      = 0
0.00.039.813 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.813 I print_info: model type       = 1.4B
0.00.039.813 I print_info: model params     = 1.41 B
0.00.039.813 I print_info: general.name     = 1.4B
0.00.039.814 I print_info: vocab type       = BPE
0.00.039.814 I print_info: n_vocab          = 50304
0.00.039.814 I print_info: n_merges         = 50009
0.00.039.815 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.815 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.815 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.815 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.815 I print_info: LF token         = 187 'Ċ'
0.00.039.816 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: max token length = 1024
0.00.039.816 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.143 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.157 I load_tensors: offloading output layer to GPU
0.00.617.158 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.185 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.617.187 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.618.763 I llama_init_from_model: n_seq_max     = 1
0.00.618.769 I llama_init_from_model: n_ctx         = 128
0.00.618.770 I llama_init_from_model: n_ctx_per_seq = 128
0.00.618.770 I llama_init_from_model: n_batch       = 128
0.00.618.771 I llama_init_from_model: n_ubatch      = 128
0.00.618.771 I llama_init_from_model: flash_attn    = 0
0.00.618.772 I llama_init_from_model: freq_base     = 10000.0
0.00.618.773 I llama_init_from_model: freq_scale    = 1
0.00.618.773 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.618.778 I ggml_metal_init: allocating
0.00.618.827 I ggml_metal_init: found device: Apple M4
0.00.618.842 I ggml_metal_init: picking default device: Apple M4
0.00.620.465 I ggml_metal_init: using embedded metal library
0.00.626.314 I ggml_metal_init: GPU name:   Apple M4
0.00.626.320 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.321 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.322 I ggml_metal_init: simdgroup reduction   = true
0.00.626.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.323 I ggml_metal_init: has residency sets    = true
0.00.626.323 I ggml_metal_init: has bfloat            = true
0.00.626.323 I ggml_metal_init: use bfloat            = true
0.00.626.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.523 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.649.167 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.649.171 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.649.197 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.652.683 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.652.685 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.652.685 I llama_init_from_model: graph nodes  = 967
0.00.652.685 I llama_init_from_model: graph splits = 2
0.00.652.688 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.652.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.043 I 
0.00.678.112 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.120 I perplexity: tokenizing the input ..
0.00.684.020 I perplexity: tokenization took 5.9 ms
0.00.684.024 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.753 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.810.166 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.810.195 I llama_perf_context_print:        load time =     669.30 ms
0.00.810.196 I llama_perf_context_print: prompt eval time =     124.48 ms /   128 tokens (    0.97 ms per token,  1028.25 tokens per second)
0.00.810.196 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.198 I llama_perf_context_print:       total time =     132.15 ms /   129 tokens
0.00.810.575 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.078s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.866 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.933 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.935 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.936 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.937 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.938 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.939 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.939 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.939 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.940 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.940 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.943 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.943 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.944 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.876 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.780 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.782 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.782 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.782 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.783 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.783 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.784 I llama_model_loader: - type  f32:  194 tensors
0.00.025.784 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.784 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.785 I print_info: file format = GGUF V3 (latest)
0.00.025.786 I print_info: file type   = Q5_0
0.00.025.787 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.257 I load: special tokens cache size = 25
0.00.040.175 I load: token to piece cache size = 0.2984 MB
0.00.040.179 I print_info: arch             = gptneox
0.00.040.180 I print_info: vocab_only       = 0
0.00.040.180 I print_info: n_ctx_train      = 2048
0.00.040.180 I print_info: n_embd           = 2048
0.00.040.180 I print_info: n_layer          = 24
0.00.040.185 I print_info: n_head           = 16
0.00.040.186 I print_info: n_head_kv        = 16
0.00.040.186 I print_info: n_rot            = 32
0.00.040.186 I print_info: n_swa            = 0
0.00.040.186 I print_info: n_embd_head_k    = 128
0.00.040.186 I print_info: n_embd_head_v    = 128
0.00.040.187 I print_info: n_gqa            = 1
0.00.040.188 I print_info: n_embd_k_gqa     = 2048
0.00.040.191 I print_info: n_embd_v_gqa     = 2048
0.00.040.192 I print_info: f_norm_eps       = 1.0e-05
0.00.040.194 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.194 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.194 I print_info: f_logit_scale    = 0.0e+00
0.00.040.195 I print_info: n_ff             = 8192
0.00.040.195 I print_info: n_expert         = 0
0.00.040.195 I print_info: n_expert_used    = 0
0.00.040.195 I print_info: causal attn      = 1
0.00.040.195 I print_info: pooling type     = 0
0.00.040.195 I print_info: rope type        = 2
0.00.040.196 I print_info: rope scaling     = linear
0.00.040.196 I print_info: freq_base_train  = 10000.0
0.00.040.197 I print_info: freq_scale_train = 1
0.00.040.197 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.197 I print_info: rope_finetuned   = unknown
0.00.040.197 I print_info: ssm_d_conv       = 0
0.00.040.197 I print_info: ssm_d_inner      = 0
0.00.040.198 I print_info: ssm_d_state      = 0
0.00.040.198 I print_info: ssm_dt_rank      = 0
0.00.040.198 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.198 I print_info: model type       = 1.4B
0.00.040.199 I print_info: model params     = 1.41 B
0.00.040.199 I print_info: general.name     = 1.4B
0.00.040.200 I print_info: vocab type       = BPE
0.00.040.200 I print_info: n_vocab          = 50304
0.00.040.200 I print_info: n_merges         = 50009
0.00.040.200 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.201 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.201 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.201 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.201 I print_info: LF token         = 187 'Ċ'
0.00.040.202 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.202 I print_info: max token length = 1024
0.00.040.202 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.683.962 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.977 I load_tensors: offloading output layer to GPU
0.00.683.977 I load_tensors: offloaded 25/25 layers to GPU
0.00.684.008 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.684.009 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.685.535 I llama_init_from_model: n_seq_max     = 1
0.00.685.538 I llama_init_from_model: n_ctx         = 128
0.00.685.539 I llama_init_from_model: n_ctx_per_seq = 128
0.00.685.539 I llama_init_from_model: n_batch       = 128
0.00.685.539 I llama_init_from_model: n_ubatch      = 128
0.00.685.540 I llama_init_from_model: flash_attn    = 0
0.00.685.542 I llama_init_from_model: freq_base     = 10000.0
0.00.685.543 I llama_init_from_model: freq_scale    = 1
0.00.685.543 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.685.545 I ggml_metal_init: allocating
0.00.685.625 I ggml_metal_init: found device: Apple M4
0.00.685.639 I ggml_metal_init: picking default device: Apple M4
0.00.687.519 I ggml_metal_init: using embedded metal library
0.00.693.946 I ggml_metal_init: GPU name:   Apple M4
0.00.693.951 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.693.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.693.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.693.953 I ggml_metal_init: simdgroup reduction   = true
0.00.693.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.693.954 I ggml_metal_init: has residency sets    = true
0.00.693.954 I ggml_metal_init: has bfloat            = true
0.00.693.954 I ggml_metal_init: use bfloat            = true
0.00.693.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.693.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.711.276 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.714.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.714.757 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.714.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.922 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.717.924 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.717.924 I llama_init_from_model: graph nodes  = 967
0.00.717.925 I llama_init_from_model: graph splits = 2
0.00.717.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.717.929 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.972 I 
0.00.748.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.061 I perplexity: tokenizing the input ..
0.00.754.390 I perplexity: tokenization took 6.327 ms
0.00.754.396 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.895.890 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.897.253 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.897.280 I llama_perf_context_print:        load time =     738.10 ms
0.00.897.281 I llama_perf_context_print: prompt eval time =     141.20 ms /   128 tokens (    1.10 ms per token,   906.50 tokens per second)
0.00.897.282 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.897.282 I llama_perf_context_print:       total time =     149.31 ms /   129 tokens
0.00.897.673 I ggml_metal_free: deallocating

real	0m0.914s
user	0m0.078s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.420 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.428 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.433 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.435 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.435 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.436 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.436 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.440 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.315 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.225 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.226 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.227 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.227 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.227 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.228 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.228 I llama_model_loader: - type  f32:  194 tensors
0.00.025.229 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.229 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.229 I print_info: file format = GGUF V3 (latest)
0.00.025.230 I print_info: file type   = Q5_1
0.00.025.231 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.441 I load: special tokens cache size = 25
0.00.039.465 I load: token to piece cache size = 0.2984 MB
0.00.039.468 I print_info: arch             = gptneox
0.00.039.468 I print_info: vocab_only       = 0
0.00.039.468 I print_info: n_ctx_train      = 2048
0.00.039.468 I print_info: n_embd           = 2048
0.00.039.469 I print_info: n_layer          = 24
0.00.039.472 I print_info: n_head           = 16
0.00.039.473 I print_info: n_head_kv        = 16
0.00.039.473 I print_info: n_rot            = 32
0.00.039.473 I print_info: n_swa            = 0
0.00.039.474 I print_info: n_embd_head_k    = 128
0.00.039.474 I print_info: n_embd_head_v    = 128
0.00.039.475 I print_info: n_gqa            = 1
0.00.039.475 I print_info: n_embd_k_gqa     = 2048
0.00.039.476 I print_info: n_embd_v_gqa     = 2048
0.00.039.476 I print_info: f_norm_eps       = 1.0e-05
0.00.039.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.477 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.477 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.477 I print_info: f_logit_scale    = 0.0e+00
0.00.039.478 I print_info: n_ff             = 8192
0.00.039.478 I print_info: n_expert         = 0
0.00.039.478 I print_info: n_expert_used    = 0
0.00.039.478 I print_info: causal attn      = 1
0.00.039.478 I print_info: pooling type     = 0
0.00.039.479 I print_info: rope type        = 2
0.00.039.480 I print_info: rope scaling     = linear
0.00.039.480 I print_info: freq_base_train  = 10000.0
0.00.039.480 I print_info: freq_scale_train = 1
0.00.039.481 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.481 I print_info: rope_finetuned   = unknown
0.00.039.481 I print_info: ssm_d_conv       = 0
0.00.039.481 I print_info: ssm_d_inner      = 0
0.00.039.481 I print_info: ssm_d_state      = 0
0.00.039.481 I print_info: ssm_dt_rank      = 0
0.00.039.483 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.484 I print_info: model params     = 1.41 B
0.00.039.484 I print_info: general.name     = 1.4B
0.00.039.485 I print_info: vocab type       = BPE
0.00.039.485 I print_info: n_vocab          = 50304
0.00.039.485 I print_info: n_merges         = 50009
0.00.039.485 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.486 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: LF token         = 187 'Ċ'
0.00.039.490 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.490 I print_info: max token length = 1024
0.00.039.490 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.677.688 I load_tensors: offloading 24 repeating layers to GPU
0.00.677.701 I load_tensors: offloading output layer to GPU
0.00.677.702 I load_tensors: offloaded 25/25 layers to GPU
0.00.677.735 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.677.736 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.679.251 I llama_init_from_model: n_seq_max     = 1
0.00.679.254 I llama_init_from_model: n_ctx         = 128
0.00.679.255 I llama_init_from_model: n_ctx_per_seq = 128
0.00.679.255 I llama_init_from_model: n_batch       = 128
0.00.679.255 I llama_init_from_model: n_ubatch      = 128
0.00.679.256 I llama_init_from_model: flash_attn    = 0
0.00.679.257 I llama_init_from_model: freq_base     = 10000.0
0.00.679.258 I llama_init_from_model: freq_scale    = 1
0.00.679.258 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.679.259 I ggml_metal_init: allocating
0.00.679.279 I ggml_metal_init: found device: Apple M4
0.00.679.292 I ggml_metal_init: picking default device: Apple M4
0.00.680.767 I ggml_metal_init: using embedded metal library
0.00.687.023 I ggml_metal_init: GPU name:   Apple M4
0.00.687.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.687.028 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.687.029 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.687.030 I ggml_metal_init: simdgroup reduction   = true
0.00.687.030 I ggml_metal_init: simdgroup matrix mul. = true
0.00.687.030 I ggml_metal_init: has residency sets    = true
0.00.687.031 I ggml_metal_init: has bfloat            = true
0.00.687.031 I ggml_metal_init: use bfloat            = true
0.00.687.032 I ggml_metal_init: hasUnifiedMemory      = true
0.00.687.033 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.704.219 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.723 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.707.732 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.707.768 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.710.741 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.710.743 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.710.743 I llama_init_from_model: graph nodes  = 967
0.00.710.744 I llama_init_from_model: graph splits = 2
0.00.710.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.710.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.688 I 
0.00.744.774 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.782 I perplexity: tokenizing the input ..
0.00.752.079 I perplexity: tokenization took 7.293 ms
0.00.752.086 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.899.116 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.900.622 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.900.642 I llama_perf_context_print:        load time =     735.76 ms
0.00.900.643 I llama_perf_context_print: prompt eval time =     146.10 ms /   128 tokens (    1.14 ms per token,   876.14 tokens per second)
0.00.900.644 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.900.644 I llama_perf_context_print:       total time =     155.96 ms /   129 tokens
0.00.901.022 I ggml_metal_free: deallocating

real	0m0.915s
user	0m0.079s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.017 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.876 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.882 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.884 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.884 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.885 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.885 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.885 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.886 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.886 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.887 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.887 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.888 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.889 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.894 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.895 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.722 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.749 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.563 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.565 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.565 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.566 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.566 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.567 I llama_model_loader: - type  f32:  194 tensors
0.00.025.567 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.567 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.568 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.568 I print_info: file format = GGUF V3 (latest)
0.00.025.569 I print_info: file type   = Q2_K - Medium
0.00.025.570 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.369 I load: special tokens cache size = 25
0.00.039.430 I load: token to piece cache size = 0.2984 MB
0.00.039.432 I print_info: arch             = gptneox
0.00.039.433 I print_info: vocab_only       = 0
0.00.039.433 I print_info: n_ctx_train      = 2048
0.00.039.433 I print_info: n_embd           = 2048
0.00.039.433 I print_info: n_layer          = 24
0.00.039.437 I print_info: n_head           = 16
0.00.039.438 I print_info: n_head_kv        = 16
0.00.039.438 I print_info: n_rot            = 32
0.00.039.438 I print_info: n_swa            = 0
0.00.039.439 I print_info: n_embd_head_k    = 128
0.00.039.439 I print_info: n_embd_head_v    = 128
0.00.039.440 I print_info: n_gqa            = 1
0.00.039.440 I print_info: n_embd_k_gqa     = 2048
0.00.039.441 I print_info: n_embd_v_gqa     = 2048
0.00.039.441 I print_info: f_norm_eps       = 1.0e-05
0.00.039.442 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.442 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.442 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.442 I print_info: f_logit_scale    = 0.0e+00
0.00.039.443 I print_info: n_ff             = 8192
0.00.039.443 I print_info: n_expert         = 0
0.00.039.443 I print_info: n_expert_used    = 0
0.00.039.444 I print_info: causal attn      = 1
0.00.039.444 I print_info: pooling type     = 0
0.00.039.444 I print_info: rope type        = 2
0.00.039.444 I print_info: rope scaling     = linear
0.00.039.445 I print_info: freq_base_train  = 10000.0
0.00.039.445 I print_info: freq_scale_train = 1
0.00.039.445 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.445 I print_info: rope_finetuned   = unknown
0.00.039.447 I print_info: ssm_d_conv       = 0
0.00.039.447 I print_info: ssm_d_inner      = 0
0.00.039.447 I print_info: ssm_d_state      = 0
0.00.039.447 I print_info: ssm_dt_rank      = 0
0.00.039.447 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.448 I print_info: model type       = 1.4B
0.00.039.448 I print_info: model params     = 1.41 B
0.00.039.448 I print_info: general.name     = 1.4B
0.00.039.449 I print_info: vocab type       = BPE
0.00.039.449 I print_info: n_vocab          = 50304
0.00.039.449 I print_info: n_merges         = 50009
0.00.039.449 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.450 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.450 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.450 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.450 I print_info: LF token         = 187 'Ċ'
0.00.039.451 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.451 I print_info: max token length = 1024
0.00.039.451 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.420.235 I load_tensors: offloading 24 repeating layers to GPU
0.00.420.247 I load_tensors: offloading output layer to GPU
0.00.420.247 I load_tensors: offloaded 25/25 layers to GPU
0.00.420.279 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.420.280 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.421.908 I llama_init_from_model: n_seq_max     = 1
0.00.421.913 I llama_init_from_model: n_ctx         = 128
0.00.421.914 I llama_init_from_model: n_ctx_per_seq = 128
0.00.421.914 I llama_init_from_model: n_batch       = 128
0.00.421.915 I llama_init_from_model: n_ubatch      = 128
0.00.421.915 I llama_init_from_model: flash_attn    = 0
0.00.421.917 I llama_init_from_model: freq_base     = 10000.0
0.00.421.918 I llama_init_from_model: freq_scale    = 1
0.00.421.918 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.421.923 I ggml_metal_init: allocating
0.00.421.981 I ggml_metal_init: found device: Apple M4
0.00.421.998 I ggml_metal_init: picking default device: Apple M4
0.00.424.325 I ggml_metal_init: using embedded metal library
0.00.430.394 I ggml_metal_init: GPU name:   Apple M4
0.00.430.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.430.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.430.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.430.417 I ggml_metal_init: simdgroup reduction   = true
0.00.430.418 I ggml_metal_init: simdgroup matrix mul. = true
0.00.430.418 I ggml_metal_init: has residency sets    = true
0.00.430.418 I ggml_metal_init: has bfloat            = true
0.00.430.418 I ggml_metal_init: use bfloat            = true
0.00.430.420 I ggml_metal_init: hasUnifiedMemory      = true
0.00.430.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.452.576 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.456.425 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.456.432 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.456.467 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.459.919 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.459.921 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.459.921 I llama_init_from_model: graph nodes  = 967
0.00.459.922 I llama_init_from_model: graph splits = 2
0.00.459.925 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.459.925 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.064 I 
0.00.490.139 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.145 I perplexity: tokenizing the input ..
0.00.496.537 I perplexity: tokenization took 6.391 ms
0.00.496.541 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.627.766 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.629.186 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.629.209 I llama_perf_context_print:        load time =     480.04 ms
0.00.629.210 I llama_perf_context_print: prompt eval time =     130.99 ms /   128 tokens (    1.02 ms per token,   977.20 tokens per second)
0.00.629.211 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.629.212 I llama_perf_context_print:       total time =     139.15 ms /   129 tokens
0.00.629.575 I ggml_metal_free: deallocating

real	0m0.644s
user	0m0.080s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.389 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.658 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.666 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.666 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.675 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.676 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.678 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.680 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.680 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.680 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.540 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.616 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.586 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.587 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.588 I llama_model_loader: - type  f32:  194 tensors
0.00.024.588 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.588 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.588 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.589 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.589 I print_info: file format = GGUF V3 (latest)
0.00.024.590 I print_info: file type   = Q3_K - Medium
0.00.024.591 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.912 I load: special tokens cache size = 25
0.00.038.940 I load: token to piece cache size = 0.2984 MB
0.00.038.943 I print_info: arch             = gptneox
0.00.038.943 I print_info: vocab_only       = 0
0.00.038.944 I print_info: n_ctx_train      = 2048
0.00.038.944 I print_info: n_embd           = 2048
0.00.038.944 I print_info: n_layer          = 24
0.00.038.947 I print_info: n_head           = 16
0.00.038.951 I print_info: n_head_kv        = 16
0.00.038.952 I print_info: n_rot            = 32
0.00.038.952 I print_info: n_swa            = 0
0.00.038.952 I print_info: n_embd_head_k    = 128
0.00.038.953 I print_info: n_embd_head_v    = 128
0.00.038.953 I print_info: n_gqa            = 1
0.00.038.954 I print_info: n_embd_k_gqa     = 2048
0.00.038.954 I print_info: n_embd_v_gqa     = 2048
0.00.038.955 I print_info: f_norm_eps       = 1.0e-05
0.00.038.955 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.955 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.956 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.956 I print_info: f_logit_scale    = 0.0e+00
0.00.038.957 I print_info: n_ff             = 8192
0.00.038.957 I print_info: n_expert         = 0
0.00.038.957 I print_info: n_expert_used    = 0
0.00.038.957 I print_info: causal attn      = 1
0.00.038.957 I print_info: pooling type     = 0
0.00.038.957 I print_info: rope type        = 2
0.00.038.958 I print_info: rope scaling     = linear
0.00.038.958 I print_info: freq_base_train  = 10000.0
0.00.038.958 I print_info: freq_scale_train = 1
0.00.038.958 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.960 I print_info: rope_finetuned   = unknown
0.00.038.960 I print_info: ssm_d_conv       = 0
0.00.038.960 I print_info: ssm_d_inner      = 0
0.00.038.960 I print_info: ssm_d_state      = 0
0.00.038.960 I print_info: ssm_dt_rank      = 0
0.00.038.960 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.960 I print_info: model type       = 1.4B
0.00.038.961 I print_info: model params     = 1.41 B
0.00.038.961 I print_info: general.name     = 1.4B
0.00.038.961 I print_info: vocab type       = BPE
0.00.038.962 I print_info: n_vocab          = 50304
0.00.038.962 I print_info: n_merges         = 50009
0.00.038.962 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.962 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.962 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.962 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.963 I print_info: LF token         = 187 'Ċ'
0.00.038.963 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.963 I print_info: max token length = 1024
0.00.038.963 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.416.770 I load_tensors: offloading 24 repeating layers to GPU
0.00.416.778 I load_tensors: offloading output layer to GPU
0.00.416.778 I load_tensors: offloaded 25/25 layers to GPU
0.00.416.800 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.416.801 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.417.568 I llama_init_from_model: n_seq_max     = 1
0.00.417.571 I llama_init_from_model: n_ctx         = 128
0.00.417.572 I llama_init_from_model: n_ctx_per_seq = 128
0.00.417.572 I llama_init_from_model: n_batch       = 128
0.00.417.572 I llama_init_from_model: n_ubatch      = 128
0.00.417.573 I llama_init_from_model: flash_attn    = 0
0.00.417.574 I llama_init_from_model: freq_base     = 10000.0
0.00.417.574 I llama_init_from_model: freq_scale    = 1
0.00.417.575 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.417.576 I ggml_metal_init: allocating
0.00.417.643 I ggml_metal_init: found device: Apple M4
0.00.417.655 I ggml_metal_init: picking default device: Apple M4
0.00.418.730 I ggml_metal_init: using embedded metal library
0.00.422.947 I ggml_metal_init: GPU name:   Apple M4
0.00.422.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.422.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.422.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.422.955 I ggml_metal_init: simdgroup reduction   = true
0.00.422.955 I ggml_metal_init: simdgroup matrix mul. = true
0.00.422.956 I ggml_metal_init: has residency sets    = true
0.00.422.956 I ggml_metal_init: has bfloat            = true
0.00.422.956 I ggml_metal_init: use bfloat            = true
0.00.422.957 I ggml_metal_init: hasUnifiedMemory      = true
0.00.422.960 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.438.312 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.439.898 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.439.900 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.439.916 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.441.444 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.441.445 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.441.446 I llama_init_from_model: graph nodes  = 967
0.00.441.446 I llama_init_from_model: graph splits = 2
0.00.441.447 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.441.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.467.817 I 
0.00.467.846 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.467.848 I perplexity: tokenizing the input ..
0.00.471.650 I perplexity: tokenization took 3.801 ms
0.00.471.654 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.605.500 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.606.907 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.606.928 I llama_perf_context_print:        load time =     459.42 ms
0.00.606.929 I llama_perf_context_print: prompt eval time =     133.62 ms /   128 tokens (    1.04 ms per token,   957.90 tokens per second)
0.00.606.930 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.606.930 I llama_perf_context_print:       total time =     139.11 ms /   129 tokens
0.00.607.239 I ggml_metal_free: deallocating

real	0m0.623s
user	0m0.070s
sys	0m0.076s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.823 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.247 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.253 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.260 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.261 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.261 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.264 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.264 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.265 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.267 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.396 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.351 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.352 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.353 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.353 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.354 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.354 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.355 I llama_model_loader: - type  f32:  194 tensors
0.00.025.355 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.355 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.356 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.356 I print_info: file format = GGUF V3 (latest)
0.00.025.361 I print_info: file type   = Q4_K - Medium
0.00.025.362 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.015 I load: special tokens cache size = 25
0.00.040.128 I load: token to piece cache size = 0.2984 MB
0.00.040.132 I print_info: arch             = gptneox
0.00.040.132 I print_info: vocab_only       = 0
0.00.040.132 I print_info: n_ctx_train      = 2048
0.00.040.132 I print_info: n_embd           = 2048
0.00.040.133 I print_info: n_layer          = 24
0.00.040.137 I print_info: n_head           = 16
0.00.040.137 I print_info: n_head_kv        = 16
0.00.040.138 I print_info: n_rot            = 32
0.00.040.138 I print_info: n_swa            = 0
0.00.040.138 I print_info: n_embd_head_k    = 128
0.00.040.138 I print_info: n_embd_head_v    = 128
0.00.040.138 I print_info: n_gqa            = 1
0.00.040.139 I print_info: n_embd_k_gqa     = 2048
0.00.040.143 I print_info: n_embd_v_gqa     = 2048
0.00.040.144 I print_info: f_norm_eps       = 1.0e-05
0.00.040.144 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.144 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.145 I print_info: f_logit_scale    = 0.0e+00
0.00.040.145 I print_info: n_ff             = 8192
0.00.040.145 I print_info: n_expert         = 0
0.00.040.145 I print_info: n_expert_used    = 0
0.00.040.146 I print_info: causal attn      = 1
0.00.040.146 I print_info: pooling type     = 0
0.00.040.146 I print_info: rope type        = 2
0.00.040.146 I print_info: rope scaling     = linear
0.00.040.147 I print_info: freq_base_train  = 10000.0
0.00.040.147 I print_info: freq_scale_train = 1
0.00.040.147 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.147 I print_info: rope_finetuned   = unknown
0.00.040.147 I print_info: ssm_d_conv       = 0
0.00.040.147 I print_info: ssm_d_inner      = 0
0.00.040.147 I print_info: ssm_d_state      = 0
0.00.040.148 I print_info: ssm_dt_rank      = 0
0.00.040.148 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.148 I print_info: model type       = 1.4B
0.00.040.148 I print_info: model params     = 1.41 B
0.00.040.148 I print_info: general.name     = 1.4B
0.00.040.149 I print_info: vocab type       = BPE
0.00.040.149 I print_info: n_vocab          = 50304
0.00.040.149 I print_info: n_merges         = 50009
0.00.040.149 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.150 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.150 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.151 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.151 I print_info: LF token         = 187 'Ċ'
0.00.040.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.152 I print_info: max token length = 1024
0.00.040.152 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.555.957 I load_tensors: offloading 24 repeating layers to GPU
0.00.555.970 I load_tensors: offloading output layer to GPU
0.00.555.970 I load_tensors: offloaded 25/25 layers to GPU
0.00.556.004 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.556.006 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.557.033 I llama_init_from_model: n_seq_max     = 1
0.00.557.038 I llama_init_from_model: n_ctx         = 128
0.00.557.039 I llama_init_from_model: n_ctx_per_seq = 128
0.00.557.039 I llama_init_from_model: n_batch       = 128
0.00.557.040 I llama_init_from_model: n_ubatch      = 128
0.00.557.040 I llama_init_from_model: flash_attn    = 0
0.00.557.041 I llama_init_from_model: freq_base     = 10000.0
0.00.557.041 I llama_init_from_model: freq_scale    = 1
0.00.557.042 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.557.044 I ggml_metal_init: allocating
0.00.557.083 I ggml_metal_init: found device: Apple M4
0.00.557.094 I ggml_metal_init: picking default device: Apple M4
0.00.558.342 I ggml_metal_init: using embedded metal library
0.00.563.207 I ggml_metal_init: GPU name:   Apple M4
0.00.563.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.563.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.563.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.563.215 I ggml_metal_init: simdgroup reduction   = true
0.00.563.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.563.221 I ggml_metal_init: has residency sets    = true
0.00.563.221 I ggml_metal_init: has bfloat            = true
0.00.563.222 I ggml_metal_init: use bfloat            = true
0.00.563.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.563.226 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.579.501 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.582.378 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.582.381 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.582.403 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.585.104 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.585.106 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.585.107 I llama_init_from_model: graph nodes  = 967
0.00.585.107 I llama_init_from_model: graph splits = 2
0.00.585.109 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.585.110 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.865 I 
0.00.614.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.920 I perplexity: tokenizing the input ..
0.00.619.338 I perplexity: tokenization took 4.416 ms
0.00.619.342 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.984 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.762.392 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.762.417 I llama_perf_context_print:        load time =     606.04 ms
0.00.762.417 I llama_perf_context_print: prompt eval time =     141.33 ms /   128 tokens (    1.10 ms per token,   905.68 tokens per second)
0.00.762.418 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.418 I llama_perf_context_print:       total time =     147.55 ms /   129 tokens
0.00.762.823 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.073s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.138 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.140 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.141 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.142 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.142 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.143 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.143 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.144 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.149 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.087 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.090 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.091 I llama_model_loader: - type  f32:  194 tensors
0.00.027.091 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.092 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.092 I print_info: file format = GGUF V3 (latest)
0.00.027.093 I print_info: file type   = Q5_K - Medium
0.00.027.094 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.700 I load: special tokens cache size = 25
0.00.041.847 I load: token to piece cache size = 0.2984 MB
0.00.041.852 I print_info: arch             = gptneox
0.00.041.852 I print_info: vocab_only       = 0
0.00.041.852 I print_info: n_ctx_train      = 2048
0.00.041.852 I print_info: n_embd           = 2048
0.00.041.853 I print_info: n_layer          = 24
0.00.041.858 I print_info: n_head           = 16
0.00.041.858 I print_info: n_head_kv        = 16
0.00.041.859 I print_info: n_rot            = 32
0.00.041.859 I print_info: n_swa            = 0
0.00.041.859 I print_info: n_embd_head_k    = 128
0.00.041.860 I print_info: n_embd_head_v    = 128
0.00.041.861 I print_info: n_gqa            = 1
0.00.041.862 I print_info: n_embd_k_gqa     = 2048
0.00.041.862 I print_info: n_embd_v_gqa     = 2048
0.00.041.863 I print_info: f_norm_eps       = 1.0e-05
0.00.041.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.864 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.865 I print_info: f_logit_scale    = 0.0e+00
0.00.041.866 I print_info: n_ff             = 8192
0.00.041.866 I print_info: n_expert         = 0
0.00.041.866 I print_info: n_expert_used    = 0
0.00.041.866 I print_info: causal attn      = 1
0.00.041.867 I print_info: pooling type     = 0
0.00.041.867 I print_info: rope type        = 2
0.00.041.867 I print_info: rope scaling     = linear
0.00.041.867 I print_info: freq_base_train  = 10000.0
0.00.041.868 I print_info: freq_scale_train = 1
0.00.041.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.868 I print_info: rope_finetuned   = unknown
0.00.041.868 I print_info: ssm_d_conv       = 0
0.00.041.868 I print_info: ssm_d_inner      = 0
0.00.041.869 I print_info: ssm_d_state      = 0
0.00.041.869 I print_info: ssm_dt_rank      = 0
0.00.041.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.869 I print_info: model type       = 1.4B
0.00.041.869 I print_info: model params     = 1.41 B
0.00.041.870 I print_info: general.name     = 1.4B
0.00.041.870 I print_info: vocab type       = BPE
0.00.041.870 I print_info: n_vocab          = 50304
0.00.041.871 I print_info: n_merges         = 50009
0.00.041.873 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.873 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.873 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.873 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.873 I print_info: LF token         = 187 'Ċ'
0.00.041.873 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.874 I print_info: max token length = 1024
0.00.041.874 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.612.288 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.295 I load_tensors: offloading output layer to GPU
0.00.612.296 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.327 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.612.329 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.613.893 I llama_init_from_model: n_seq_max     = 1
0.00.613.895 I llama_init_from_model: n_ctx         = 128
0.00.613.896 I llama_init_from_model: n_ctx_per_seq = 128
0.00.613.896 I llama_init_from_model: n_batch       = 128
0.00.613.897 I llama_init_from_model: n_ubatch      = 128
0.00.613.897 I llama_init_from_model: flash_attn    = 0
0.00.613.898 I llama_init_from_model: freq_base     = 10000.0
0.00.613.899 I llama_init_from_model: freq_scale    = 1
0.00.613.899 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.613.901 I ggml_metal_init: allocating
0.00.613.974 I ggml_metal_init: found device: Apple M4
0.00.613.990 I ggml_metal_init: picking default device: Apple M4
0.00.615.631 I ggml_metal_init: using embedded metal library
0.00.621.586 I ggml_metal_init: GPU name:   Apple M4
0.00.621.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.592 I ggml_metal_init: simdgroup reduction   = true
0.00.621.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.593 I ggml_metal_init: has residency sets    = true
0.00.621.593 I ggml_metal_init: has bfloat            = true
0.00.621.593 I ggml_metal_init: use bfloat            = true
0.00.621.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.627 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.033 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.641.036 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.641.063 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.644.415 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.644.417 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.644.418 I llama_init_from_model: graph nodes  = 967
0.00.644.418 I llama_init_from_model: graph splits = 2
0.00.644.420 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.644.421 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.121 I 
0.00.677.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.216 I perplexity: tokenizing the input ..
0.00.684.072 I perplexity: tokenization took 6.853 ms
0.00.684.081 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.882 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.827.232 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.827.254 I llama_perf_context_print:        load time =     666.12 ms
0.00.827.255 I llama_perf_context_print: prompt eval time =     140.94 ms /   128 tokens (    1.10 ms per token,   908.19 tokens per second)
0.00.827.255 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.256 I llama_perf_context_print:       total time =     150.14 ms /   129 tokens
0.00.827.632 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.079s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.042 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.046 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.052 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.058 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.061 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.062 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.063 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.064 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.065 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.065 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.959 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.890 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.891 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.892 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.892 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.892 I llama_model_loader: - type  f32:  194 tensors
0.00.024.893 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.894 I print_info: file format = GGUF V3 (latest)
0.00.024.894 I print_info: file type   = Q6_K
0.00.024.895 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.782 I load: special tokens cache size = 25
0.00.038.724 I load: token to piece cache size = 0.2984 MB
0.00.038.727 I print_info: arch             = gptneox
0.00.038.727 I print_info: vocab_only       = 0
0.00.038.727 I print_info: n_ctx_train      = 2048
0.00.038.727 I print_info: n_embd           = 2048
0.00.038.728 I print_info: n_layer          = 24
0.00.038.731 I print_info: n_head           = 16
0.00.038.732 I print_info: n_head_kv        = 16
0.00.038.732 I print_info: n_rot            = 32
0.00.038.732 I print_info: n_swa            = 0
0.00.038.732 I print_info: n_embd_head_k    = 128
0.00.038.732 I print_info: n_embd_head_v    = 128
0.00.038.733 I print_info: n_gqa            = 1
0.00.038.734 I print_info: n_embd_k_gqa     = 2048
0.00.038.735 I print_info: n_embd_v_gqa     = 2048
0.00.038.735 I print_info: f_norm_eps       = 1.0e-05
0.00.038.738 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.738 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.738 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.738 I print_info: f_logit_scale    = 0.0e+00
0.00.038.739 I print_info: n_ff             = 8192
0.00.038.739 I print_info: n_expert         = 0
0.00.038.739 I print_info: n_expert_used    = 0
0.00.038.739 I print_info: causal attn      = 1
0.00.038.739 I print_info: pooling type     = 0
0.00.038.740 I print_info: rope type        = 2
0.00.038.740 I print_info: rope scaling     = linear
0.00.038.740 I print_info: freq_base_train  = 10000.0
0.00.038.741 I print_info: freq_scale_train = 1
0.00.038.741 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.741 I print_info: rope_finetuned   = unknown
0.00.038.741 I print_info: ssm_d_conv       = 0
0.00.038.741 I print_info: ssm_d_inner      = 0
0.00.038.741 I print_info: ssm_d_state      = 0
0.00.038.741 I print_info: ssm_dt_rank      = 0
0.00.038.742 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.742 I print_info: model type       = 1.4B
0.00.038.742 I print_info: model params     = 1.41 B
0.00.038.742 I print_info: general.name     = 1.4B
0.00.038.743 I print_info: vocab type       = BPE
0.00.038.743 I print_info: n_vocab          = 50304
0.00.038.743 I print_info: n_merges         = 50009
0.00.038.744 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.744 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.744 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.744 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.744 I print_info: LF token         = 187 'Ċ'
0.00.038.745 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: max token length = 1024
0.00.038.745 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.578.567 I load_tensors: offloading 24 repeating layers to GPU
0.00.578.571 I load_tensors: offloading output layer to GPU
0.00.578.572 I load_tensors: offloaded 25/25 layers to GPU
0.00.578.597 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.578.599 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.580.084 I llama_init_from_model: n_seq_max     = 1
0.00.580.086 I llama_init_from_model: n_ctx         = 128
0.00.580.087 I llama_init_from_model: n_ctx_per_seq = 128
0.00.580.087 I llama_init_from_model: n_batch       = 128
0.00.580.087 I llama_init_from_model: n_ubatch      = 128
0.00.580.088 I llama_init_from_model: flash_attn    = 0
0.00.580.089 I llama_init_from_model: freq_base     = 10000.0
0.00.580.089 I llama_init_from_model: freq_scale    = 1
0.00.580.090 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.580.092 I ggml_metal_init: allocating
0.00.580.155 I ggml_metal_init: found device: Apple M4
0.00.580.167 I ggml_metal_init: picking default device: Apple M4
0.00.581.633 I ggml_metal_init: using embedded metal library
0.00.587.299 I ggml_metal_init: GPU name:   Apple M4
0.00.587.303 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.587.303 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.587.304 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.587.304 I ggml_metal_init: simdgroup reduction   = true
0.00.587.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.587.305 I ggml_metal_init: has residency sets    = true
0.00.587.305 I ggml_metal_init: has bfloat            = true
0.00.587.305 I ggml_metal_init: use bfloat            = true
0.00.587.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.587.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.603.642 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.607.310 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.607.313 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.607.369 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.610.577 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.610.578 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.610.579 I llama_init_from_model: graph nodes  = 967
0.00.610.579 I llama_init_from_model: graph splits = 2
0.00.610.582 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.610.582 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.996 I 
0.00.649.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.082 I perplexity: tokenizing the input ..
0.00.656.337 I perplexity: tokenization took 7.253 ms
0.00.656.354 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.771 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.799.191 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.799.217 I llama_perf_context_print:        load time =     639.94 ms
0.00.799.218 I llama_perf_context_print: prompt eval time =     140.40 ms /   128 tokens (    1.10 ms per token,   911.69 tokens per second)
0.00.799.219 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.219 I llama_perf_context_print:       total time =     150.23 ms /   129 tokens
0.00.799.574 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.077s
sys	0m0.143s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.270 I build: 4665 (2d219b38) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.217 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.953 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.974 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.988 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.989 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.989 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.992 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.994 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.995 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.995 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.996 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.997 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.006 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.006 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.007 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.069 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.238 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.241 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.242 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.242 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.243 I llama_model_loader: - type  f32:  194 tensors
0.00.055.244 I llama_model_loader: - type  f16:   98 tensors
0.00.055.245 I print_info: file format = GGUF V3 (latest)
0.00.055.246 I print_info: file type   = all F32 (guessed)
0.00.055.249 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.934 I load: special tokens cache size = 25
0.00.077.264 I load: token to piece cache size = 0.2984 MB
0.00.077.267 I print_info: arch             = gptneox
0.00.077.268 I print_info: vocab_only       = 0
0.00.077.268 I print_info: n_ctx_train      = 2048
0.00.077.268 I print_info: n_embd           = 2048
0.00.077.268 I print_info: n_layer          = 24
0.00.077.272 I print_info: n_head           = 16
0.00.077.272 I print_info: n_head_kv        = 16
0.00.077.273 I print_info: n_rot            = 32
0.00.077.273 I print_info: n_swa            = 0
0.00.077.273 I print_info: n_embd_head_k    = 128
0.00.077.273 I print_info: n_embd_head_v    = 128
0.00.077.276 I print_info: n_gqa            = 1
0.00.077.277 I print_info: n_embd_k_gqa     = 2048
0.00.077.278 I print_info: n_embd_v_gqa     = 2048
0.00.077.278 I print_info: f_norm_eps       = 1.0e-05
0.00.077.279 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.279 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.279 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.280 I print_info: f_logit_scale    = 0.0e+00
0.00.077.280 I print_info: n_ff             = 8192
0.00.077.281 I print_info: n_expert         = 0
0.00.077.281 I print_info: n_expert_used    = 0
0.00.077.281 I print_info: causal attn      = 1
0.00.077.281 I print_info: pooling type     = 0
0.00.077.281 I print_info: rope type        = 2
0.00.077.281 I print_info: rope scaling     = linear
0.00.077.282 I print_info: freq_base_train  = 10000.0
0.00.077.283 I print_info: freq_scale_train = 1
0.00.077.283 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.283 I print_info: rope_finetuned   = unknown
0.00.077.283 I print_info: ssm_d_conv       = 0
0.00.077.283 I print_info: ssm_d_inner      = 0
0.00.077.284 I print_info: ssm_d_state      = 0
0.00.077.284 I print_info: ssm_dt_rank      = 0
0.00.077.284 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.285 I print_info: model type       = 1.4B
0.00.077.286 I print_info: model params     = 1.41 B
0.00.077.286 I print_info: general.name     = 1.4B
0.00.077.287 I print_info: vocab type       = BPE
0.00.077.287 I print_info: n_vocab          = 50304
0.00.077.287 I print_info: n_merges         = 50009
0.00.077.287 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.287 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.288 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.288 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.288 I print_info: LF token         = 187 'Ċ'
0.00.077.288 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.290 I print_info: max token length = 1024
0.00.077.290 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.300.139 I load_tensors: offloading 24 repeating layers to GPU
0.01.300.143 I load_tensors: offloading output layer to GPU
0.01.300.144 I load_tensors: offloaded 25/25 layers to GPU
0.01.300.172 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.300.174 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.301.466 I llama_init_from_model: n_seq_max     = 1
0.01.301.467 I llama_init_from_model: n_ctx         = 128
0.01.301.467 I llama_init_from_model: n_ctx_per_seq = 128
0.01.301.468 I llama_init_from_model: n_batch       = 128
0.01.301.468 I llama_init_from_model: n_ubatch      = 128
0.01.301.468 I llama_init_from_model: flash_attn    = 0
0.01.301.469 I llama_init_from_model: freq_base     = 10000.0
0.01.301.469 I llama_init_from_model: freq_scale    = 1
0.01.301.470 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.301.470 I ggml_metal_init: allocating
0.01.301.567 I ggml_metal_init: found device: Apple M4
0.01.301.577 I ggml_metal_init: picking default device: Apple M4
0.01.302.750 I ggml_metal_init: using embedded metal library
0.01.306.442 I ggml_metal_init: GPU name:   Apple M4
0.01.306.445 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.306.446 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.306.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.306.447 I ggml_metal_init: simdgroup reduction   = true
0.01.306.447 I ggml_metal_init: simdgroup matrix mul. = true
0.01.306.447 I ggml_metal_init: has residency sets    = true
0.01.306.447 I ggml_metal_init: has bfloat            = true
0.01.306.447 I ggml_metal_init: use bfloat            = true
0.01.306.448 I ggml_metal_init: hasUnifiedMemory      = true
0.01.306.449 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.316.965 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.318.613 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.318.615 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.318.643 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.320.202 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.320.203 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.320.204 I llama_init_from_model: graph nodes  = 967
0.01.320.204 I llama_init_from_model: graph splits = 2
0.01.320.205 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.320.206 I 
0.01.320.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.320.246 I compute_imatrix: tokenizing the input ..
0.01.324.305 I compute_imatrix: tokenization took 4.058 ms
0.01.324.307 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.590.446 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.592.836 I llama_perf_context_print:        load time =    1567.22 ms
0.01.592.837 I llama_perf_context_print: prompt eval time =     264.37 ms /   128 tokens (    2.07 ms per token,   484.18 tokens per second)
0.01.592.837 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.592.838 I llama_perf_context_print:       total time =    1569.61 ms /   129 tokens
0.01.593.287 I ggml_metal_free: deallocating

real	0m1.791s
user	0m0.127s
sys	0m0.239s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4665 (2d219b38)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121406120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121406790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121406c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121409a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121409ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12140a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12140a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12140ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12140b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12140b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12140be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12140c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12140ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12140d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12140de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12140e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12140ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12140f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12140faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121410270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121410990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1214110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1214117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121412070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121412790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121412a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121413060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121413cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121414210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1214144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121414970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121414c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1214154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121415a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121415cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121416160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121416600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121416aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121416f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1214173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121417880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121417d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1214181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121418660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121418920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121418f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121419540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121419e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12141a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12141aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12141b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12141b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12141bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12141c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12141cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12141cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12141d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12141d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12141dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12141e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12141e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12141ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12141f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12141f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12141f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12141fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121420330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1214207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121420c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121421110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1214215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121421a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121421ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121422440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121422990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121422ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121423430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121423980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121423ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121424420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121424970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121424ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121425410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121425960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121425eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121426400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121426950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121426ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1214273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121427940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121427e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1214283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121428930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121428e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1214293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121429920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121429e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121419b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12142a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12142aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12142afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12142b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12142ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12142bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12142c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12142ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12142cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12142d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12142da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12142dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12142e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12142ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12142efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12142f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12142f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12142fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121430220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1214306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121430b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121431000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1214314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121431940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121431de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121432280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121432720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121432bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121433060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121433500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1214339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121433e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1214342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121434780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121434c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1214350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121435560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121435a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121435ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121436340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1214367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121436c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121437120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1214375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121437a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121437f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1214383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121438840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121438ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121439180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121439620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121439ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121439f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12143a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12143a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12143ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12143b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12143b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12143bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12143bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12143c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12143c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12143cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12143d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12143d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12143db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12143e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12143e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12143e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12143ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12143f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12143f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12143fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121440080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121440520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1214409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121440e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121441300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1214417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121441c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1214420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121442580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121442a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121442ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121443360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121443800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121443ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121444140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1214445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121444a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121444f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1214453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121445860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121445d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1214461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1214466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121446c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121447190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1214476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1214479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121447fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1214485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121448bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1214493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121449860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121449b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12144a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12144a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12144af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12144b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12144b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12144bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12144c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12144ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12144cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12144d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12144da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12144df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12144e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12144e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12144ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12144f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12144f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12144ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121450480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1214509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121450f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121451470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1214519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121451f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121452460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1214529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121452f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121453450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1214539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121453ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121454440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121454990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121454ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121455430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121455980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121455ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121456420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121456970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121456ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121457410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121457960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121457eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121458400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121458950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121458ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1214593f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121459940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121459e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12145a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12145a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12145ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12145b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12145b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12145be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12145c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12145c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12145ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12145d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12145d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12145de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12145e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12145e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12145ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12145f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12145f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12145fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1214600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121460560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121460a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121460ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121461340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1214617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121461c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121462120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1214625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121462a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121462f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1214633a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1214638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121464010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121464730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121464e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121465570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121465830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121466020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1214662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1214668f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.684.498 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.501 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121309e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12130a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12130a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12130ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12130b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12130b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12130b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12130bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12130c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12130c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12130caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12130d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12130dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12130e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12130ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12130f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12130fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1213101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1213108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121311090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1213117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121311ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1213125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121312d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121313430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1213136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1213139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121313e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121314290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121314700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121314c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121315110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121315580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121315840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121315cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121316120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121316680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121316b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121317080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121317580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121317a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121317f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121318480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121318980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121318e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1213192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121319760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121319bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12131a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12131a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12131a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12131ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12131b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12131b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12131bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12131c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12131c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12131ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12131d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12131d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12131dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12131e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12131e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12131ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12131ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12131f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12131f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12131fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1213201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121320650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121320af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121320f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121321430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121321980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121321ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121322420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121322970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121322ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121323410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121323960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121323eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121324400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121324950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121324ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1213253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121325940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121325e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1213263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121326930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121326e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1213273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121327920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121327e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1213283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121328910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121328e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1213293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121329900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121329e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12132a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12132a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12132ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12132b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12132b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12132be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12132c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12132c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12132ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12132d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12132d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12132de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12132e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12132e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12132ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12132f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12132f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12132fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12132ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121330470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121330910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121330db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121331250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1213316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121331b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121332030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1213324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121332970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121332e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1213332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121333750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121333bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121334090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121334530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1213349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121334e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121335310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1213357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121335c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1213360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121336590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121336a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121336ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121337370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121337810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121337cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121338150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1213385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121338a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121338f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1213393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121339870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121339d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12133a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12133a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12133aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12133af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12133b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12133b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12133bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12133c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12133c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12133cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12133cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12133d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12133d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12133ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12133e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12133e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12133ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12133f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12133f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12133f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12133fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1213402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121340770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121340c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1213410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121341550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1213419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121341e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121342330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1213427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121342c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121343110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1213435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121343a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121343ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121344390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121344830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121344cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121345170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121345610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121345ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121346000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121346550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121346aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121346ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1213472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1213478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121347ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1213484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121348cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121349170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121349430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121349a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12134a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12134a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12134ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12134b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12134b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12134bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12134c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12134c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12134cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12134d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12134d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12134ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12134e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12134e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12134eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12134f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12134f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12134fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1213502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121350830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121350d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1213512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121351820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121351d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1213522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121352810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121352d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1213532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121353800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121353d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1213542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1213547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121354d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121355290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1213557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121355d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121356280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1213567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121356d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121357270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1213577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121357d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121358260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1213587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121358d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121359250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1213597a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121359cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12135a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12135a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12135ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12135b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12135b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12135bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12135c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12135c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12135ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12135d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12135d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12135dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12135e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12135e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12135ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12135f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12135f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12135f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12135fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121360310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1213607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121360c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1213610f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121361590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121361a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121361ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121362370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121362810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121362cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121363200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121363920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121364040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121364760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121364e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121365140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121365930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121365bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121366200 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121365eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121347b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121347570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121348190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12131ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121349d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12130d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1213099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12131d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121365400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12131bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12134a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12130cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121366970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121366fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121367260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121367520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1213677e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121367aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121367d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121368020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1213682e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1213685a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121368860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121368b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121368de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1213690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121369360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121369620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1213698e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121369ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121369e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12136a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12136a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12136a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12136a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12136ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12136aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12136b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12136b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12136b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12136b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12136bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12136bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12136c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12136c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12136c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12136ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12136cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12136cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12136d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12136d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12136d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12136dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12136dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12136e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12136e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12136e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12136e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12136eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12136ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12136f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12136f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12136f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12136f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12136fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12136fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121370160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121370420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1213706e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1213709a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121370c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121370f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1213711e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1213714a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121371760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121371a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121371ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121371fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121372260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121372520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1213727e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121372aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121372d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121373020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1213732e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1213735a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121373860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121373b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121373de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1213740a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121374360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121374620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1213748e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121374ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121374e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121375120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1213753e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1213756a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121375960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121375c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121375ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1213761a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121376460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121376720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1213769e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121376ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121376f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121377220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1213774e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1213777a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121377a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121377d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121377fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1213782a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121378560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121378820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121378ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121378da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121379060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121379320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1213795e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1213798a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121379b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121379e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12137a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12137a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12137a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12137a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12137abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12137aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12137b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12137b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12137b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12137b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12137bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12137bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12137c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12137c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12137c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12137ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12137cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12137cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12137d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12137d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12137d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12137daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12137dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12137e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12137e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12137e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12137e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12137eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12137ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12137f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12137f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12137f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12137f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12137fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12137fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121380120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1213803e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1213806a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121380960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121380c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121380ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1213811a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121381460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121381720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1213819e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121381ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121381f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121382220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1213824e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1213827a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121382a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121382d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121382fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1213832a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121383560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121383820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121383ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121383da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121384060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121384320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1213845e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1213848a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121384b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121384e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1213850e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1213853a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121385660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121385920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121385be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121385ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121386160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121386420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1213866e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1213869a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121386c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121386f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1213871e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1213874a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121387760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121387a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121387ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121387fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121388260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121388520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1213887e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121388db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121389070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121389330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1213895f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1213898b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121389b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121389e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12138a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12138a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12138a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12138a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12138abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12138aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12138b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12138b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12138b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12138b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12138bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12138bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12138c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12138c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12138c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12138ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12138ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12138cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12138d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12138d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12138d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12138dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12138dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12138e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12138e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12138e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12138e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12138eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12138edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12138f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12138f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12138f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12138f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12138fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12138fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121390130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1213903f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1213906b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121390970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121390ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121391410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121391960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121391eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121392400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121392950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121392ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1213933f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121393940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121393e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1213943e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1213946a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121394960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121394e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121395360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121395860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121395d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121396260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121396760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121396c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121397160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121397660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121397b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121398060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121398560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121398a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121398f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121399970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12139a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12139a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12139aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12139b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12139b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12139bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12139c250 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.743s
user	0m0.278s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4665 (2d219b38)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126f0b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126f0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126f0bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126f0c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126f0c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126f0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126f0d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126f0d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126f0df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126f0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126f0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126f0ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126f0f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126f10160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126f10970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126f11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126f117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126f11ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126f125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126f12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126f134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126f13c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126f14320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126f14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126f152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126f155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126f15bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126f16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126f16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126f17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126f17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126f18010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126f18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126f18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126f18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126f19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126f195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126f19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126f19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126f1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126f1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126f1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126f1b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126f1b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126f1ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126f1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126f1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126f1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126f1d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126f1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126f1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126f1e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126f1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126f1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126f1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126f1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126f20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126f20810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126f21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126f212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126f21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126f21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126f220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126f22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126f229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126f22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126f23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126f237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126f23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126f24100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126f245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126f24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126f24f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126f254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126f25a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126f25f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126f264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126f26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126f26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126f274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126f27a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126f27f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126f284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126f28a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126f28f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126f294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126f299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126f29f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126f2a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126f2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126f2af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126f2b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126f2b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126f2bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126f2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126f2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126f1c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126f2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126f2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126f2db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126f2e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126f2e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126f2eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126f2f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126f2f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126f2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126f30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126f305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126f30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126f31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126f315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126f31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126f31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126f32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126f328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126f32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126f33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126f336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126f33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126f33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126f34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126f34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126f34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126f35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126f35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126f35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126f36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126f364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126f36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126f36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126f372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126f37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126f380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126f38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126f389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126f38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126f39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126f397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126f39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126f3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126f3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126f3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126f3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126f3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126f3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126f3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126f3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126f3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126f3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126f3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126f3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126f3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126f3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126f3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126f3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126f3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126f3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126f3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126f3f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126f3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126f40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126f406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126f41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126f414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126f41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126f41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126f42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126f42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126f42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126f43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126f43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126f439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126f43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126f442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126f44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126f44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126f450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126f45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126f45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126f45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126f46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126f467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126f46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126f47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126f475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126f47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126f47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126f483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126f48850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126f48cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126f49240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126f49790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126f49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126f4a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126f4a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126f4ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126f4b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126f4b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126f4bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126f4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126f4c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126f4cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126f4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126f4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126f4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126f4e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126f4f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126f4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126f4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126f50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126f50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126f50aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126f50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126f51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126f51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126f51fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126f52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126f52a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126f52fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126f53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126f53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126f53fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126f54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126f54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126f54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126f55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126f55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126f55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126f564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126f56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126f574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126f57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126f57f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126f584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126f58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126f58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126f594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126f59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126f59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126f5a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126f5aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126f5af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126f5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126f5b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126f5bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126f5c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126f5c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126f5cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126f5d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126f5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126f5df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126f5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126f5e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126f5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126f5f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126f5f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126f5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126f60450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126f609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126f60ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126f61440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126f61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126f61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126f622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126f62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126f62c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126f630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126f63550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126f639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126f63e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126f64330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126f647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126f64c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126f65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126f655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126f65a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126f65ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126f66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126f66b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126f67280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126f679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126f680c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126f68380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126f68b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126f68e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126f69440 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.109.753 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.758 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1280053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1280069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1280072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1280090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12800a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12800a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12800ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12800b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12800bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12800c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12800cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12800d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12800d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12800e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12800e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12800e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12800eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12800ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12800f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12800f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12800fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1280101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1280111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1280123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1280130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1280139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1280142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1280158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1280161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1280170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1280186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12801a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12801a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12801aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12801aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12801b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12801b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12801bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12801c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12801c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12801c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12801cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12801d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12801d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12801db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12801df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12801e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12801e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12801ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12801f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12801f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12801fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12801fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1280214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1280233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1280245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1280252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1280264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1280283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1280299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12802a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12802a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12802abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12802b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12802b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12802b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12802bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12802c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12802c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12802cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12802cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12802d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12802d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12802dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12802e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12802e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12802e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12802ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12802f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12802f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12802fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1280308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1280311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1280327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1280330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1280339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128035c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128035ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128036190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128036600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128036a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128036ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128037350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1280377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128037c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1280380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128038510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128038980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128038df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128039260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1280396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128039b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128039fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12803a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12803a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12803ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12803b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12803b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12803ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12803bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12803c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12803c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12803cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12803d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12803d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12803d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12803ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12803e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12803e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12803eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12803ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12803f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12803f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12803fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1280402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128040750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128041030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128041550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128041a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1280425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128042890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128042e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1280439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128044550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1280450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128045690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128046210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1280467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128046d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128047910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128047ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128048490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128048a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1280495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128049b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12804a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12804a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12804acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12804b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12804b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12804be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12804c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12804c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12804cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12804d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12804dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12804e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12804e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12804ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12804f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12804f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12804fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128050310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1280508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128050e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128051450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128051a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128051fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128052590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128053110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1280536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128054250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128054810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128054dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128055390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128055950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128055f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1280564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128056a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128056f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128057490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128057990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128057e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128058390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128058890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128058d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128059290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128059c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12805a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12805a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12805ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12805b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12805b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12805bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12805c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12805cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12805d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12805d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12805dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12805e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12805e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e0a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e0ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e0b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e0bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e0bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e0c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e0c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e0cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e0e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e0f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e11c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e12ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e138f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e14010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e14590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e15cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e16160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e16d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e17260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e17760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e17c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e18660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e19560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e19a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e19ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e1a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e1a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e1ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e1b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e1b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e1b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e1bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e1c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e1c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e1ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e1dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e1e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e1e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e1f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e1fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e1ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e20450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e20d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e21230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e21b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e23000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e23550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e23aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e26520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e26a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e26fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e27510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e27a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e28500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e28a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e29a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e29f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e2a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e2aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e2af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e2b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e2ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e2bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e2c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e2ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e2d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e2e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e2e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e2f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e2fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e31e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e33550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e33e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e34330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e35110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e35a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e35ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e36830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e37610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e37ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e37f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e38890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e38d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e39670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e39b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e39fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e3a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e3b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e3bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e3c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e3c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e3d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e3d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e3dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e3e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e3ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e3f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e3f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e3fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e40570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e40a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e40eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e41350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e42130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e42f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e43850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e44190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e44630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e44ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e44f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e45410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e46690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e46be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e47e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e4b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e4b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e4c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e4d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e4def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e4e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e4fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e50420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e50970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e51960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e52400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e57900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e57e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e59390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e59e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e5a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e5ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e5be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e5c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e5c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e5ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e5d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e5d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e5ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e5e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e5f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e5f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e5fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e60110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e60a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e60ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e61390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e61830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e61cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e62170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e62610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e62ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126e62f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126e633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126e63890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126e63de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126e64500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126e64c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126e65340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126e65a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126e65d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126e66510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126e667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126e66de0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.963s
user	0m0.238s
sys	0m0.191s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
