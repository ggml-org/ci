Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.551s
user	0m0.890s
sys	0m1.234s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Built target test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-barrier
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Built target test-rope
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-batched
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-bench
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-create
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-cli
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-passkey
[ 85%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-perplexity
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-tts
[ 92%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.013s
user	0m6.291s
sys	0m9.620s

main: quantize time =  5345.35 ms
main:    total time =  5345.35 ms

main: quantize time =  2052.19 ms
main:    total time =  2052.19 ms

main: quantize time =  3362.97 ms
main:    total time =  3362.97 ms

main: quantize time =  2168.08 ms
main:    total time =  2168.08 ms

main: quantize time =  2775.25 ms
main:    total time =  2775.25 ms

main: quantize time =  5397.20 ms
main:    total time =  5397.20 ms

main: quantize time =  5662.99 ms
main:    total time =  5662.99 ms

main: quantize time =  7003.28 ms
main:    total time =  7003.28 ms

main: quantize time =  6118.80 ms
main:    total time =  6118.80 ms

main: quantize time =  4730.82 ms
main:    total time =  4730.82 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.190 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.442 I main: llama backend init
0.00.000.457 I main: load the model and apply lora adapter, if any
0.00.069.594 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.082.761 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.082.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.082.790 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.082.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.082.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.082.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.082.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.082.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.082.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.082.797 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.082.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.082.798 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.082.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.082.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.082.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.082.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.082.805 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.089.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.092.121 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.100.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.100.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.100.733 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.100.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.100.734 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.100.735 I llama_model_loader: - type  f32:  194 tensors
0.00.100.736 I llama_model_loader: - type  f16:   98 tensors
0.00.100.742 I print_info: file format = GGUF V3 (latest)
0.00.100.744 I print_info: file type   = all F32 (guessed)
0.00.100.746 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.134.822 I load: special tokens cache size = 25
0.00.142.368 I load: token to piece cache size = 0.2984 MB
0.00.142.371 I print_info: arch             = gptneox
0.00.142.372 I print_info: vocab_only       = 0
0.00.142.372 I print_info: n_ctx_train      = 2048
0.00.142.372 I print_info: n_embd           = 2048
0.00.142.372 I print_info: n_layer          = 24
0.00.142.376 I print_info: n_head           = 16
0.00.142.377 I print_info: n_head_kv        = 16
0.00.142.377 I print_info: n_rot            = 32
0.00.142.377 I print_info: n_swa            = 0
0.00.142.377 I print_info: n_embd_head_k    = 128
0.00.142.377 I print_info: n_embd_head_v    = 128
0.00.142.378 I print_info: n_gqa            = 1
0.00.142.379 I print_info: n_embd_k_gqa     = 2048
0.00.142.380 I print_info: n_embd_v_gqa     = 2048
0.00.142.380 I print_info: f_norm_eps       = 1.0e-05
0.00.142.380 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.142.381 I print_info: f_clamp_kqv      = 0.0e+00
0.00.142.381 I print_info: f_max_alibi_bias = 0.0e+00
0.00.142.381 I print_info: f_logit_scale    = 0.0e+00
0.00.142.382 I print_info: n_ff             = 8192
0.00.142.382 I print_info: n_expert         = 0
0.00.142.382 I print_info: n_expert_used    = 0
0.00.142.382 I print_info: causal attn      = 1
0.00.142.382 I print_info: pooling type     = 0
0.00.142.382 I print_info: rope type        = 2
0.00.142.383 I print_info: rope scaling     = linear
0.00.142.383 I print_info: freq_base_train  = 10000.0
0.00.142.383 I print_info: freq_scale_train = 1
0.00.142.384 I print_info: n_ctx_orig_yarn  = 2048
0.00.142.384 I print_info: rope_finetuned   = unknown
0.00.142.384 I print_info: ssm_d_conv       = 0
0.00.142.384 I print_info: ssm_d_inner      = 0
0.00.142.385 I print_info: ssm_d_state      = 0
0.00.142.385 I print_info: ssm_dt_rank      = 0
0.00.142.385 I print_info: ssm_dt_b_c_rms   = 0
0.00.142.385 I print_info: model type       = 1.4B
0.00.142.385 I print_info: model params     = 1.41 B
0.00.142.386 I print_info: general.name     = 1.4B
0.00.142.386 I print_info: vocab type       = BPE
0.00.142.386 I print_info: n_vocab          = 50304
0.00.142.387 I print_info: n_merges         = 50009
0.00.142.387 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.142.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.142.388 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.142.389 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.142.390 I print_info: LF token         = 128 'Ä'
0.00.142.390 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.142.390 I print_info: max token length = 1024
0.00.145.181 I load_tensors: offloading 24 repeating layers to GPU
0.00.145.181 I load_tensors: offloading output layer to GPU
0.00.145.181 I load_tensors: offloaded 25/25 layers to GPU
0.00.145.200 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.145.201 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.145.531 I llama_init_from_model: n_seq_max     = 1
0.00.145.533 I llama_init_from_model: n_ctx         = 2048
0.00.145.533 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.145.533 I llama_init_from_model: n_batch       = 2048
0.00.145.533 I llama_init_from_model: n_ubatch      = 512
0.00.145.533 I llama_init_from_model: flash_attn    = 0
0.00.145.534 I llama_init_from_model: freq_base     = 10000.0
0.00.145.534 I llama_init_from_model: freq_scale    = 1
0.00.145.535 I ggml_metal_init: allocating
0.00.145.538 I ggml_metal_init: found device: Apple M4
0.00.145.540 I ggml_metal_init: picking default device: Apple M4
0.00.146.274 I ggml_metal_init: using embedded metal library
0.00.167.253 I ggml_metal_init: GPU name:   Apple M4
0.00.167.255 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.167.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.167.256 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.167.256 I ggml_metal_init: simdgroup reduction   = true
0.00.167.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.167.257 I ggml_metal_init: has bfloat            = true
0.00.167.257 I ggml_metal_init: use bfloat            = true
0.00.167.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.167.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.247.080 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.266.776 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.266.781 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.266.805 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.267.782 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.267.783 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.267.784 I llama_init_from_model: graph nodes  = 967
0.00.267.784 I llama_init_from_model: graph splits = 2
0.00.267.787 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.267.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.267.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.348.887 I main: llama threadpool init, n_threads = 4
0.00.348.925 I 
0.00.348.958 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.348.959 I 
0.00.349.026 I sampler seed: 1234
0.00.349.030 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.349.055 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.349.056 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.349.056 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.174.133 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.02.174.133 I llama_perf_context_print:        load time =     278.25 ms
0.02.174.135 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.38 tokens per second)
0.02.174.136 I llama_perf_context_print:        eval time =    1778.62 ms /    63 runs   (   28.23 ms per token,    35.42 tokens per second)
0.02.174.136 I llama_perf_context_print:       total time =    1826.28 ms /    70 tokens
0.02.174.352 I ggml_metal_free: deallocating

real	0m2.526s
user	0m0.154s
sys	0m0.111s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.897 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.905 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.914 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.915 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.916 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.917 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.917 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.917 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.918 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.918 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.918 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.920 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.921 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.106 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.244 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.295 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.296 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.297 I llama_model_loader: - type  f32:  194 tensors
0.00.037.297 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.298 I print_info: file format = GGUF V3 (latest)
0.00.037.299 I print_info: file type   = Q8_0
0.00.037.301 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.483 I load: special tokens cache size = 25
0.00.064.622 I load: token to piece cache size = 0.2984 MB
0.00.064.626 I print_info: arch             = gptneox
0.00.064.627 I print_info: vocab_only       = 0
0.00.064.627 I print_info: n_ctx_train      = 2048
0.00.064.627 I print_info: n_embd           = 2048
0.00.064.627 I print_info: n_layer          = 24
0.00.064.633 I print_info: n_head           = 16
0.00.064.633 I print_info: n_head_kv        = 16
0.00.064.634 I print_info: n_rot            = 32
0.00.064.634 I print_info: n_swa            = 0
0.00.064.634 I print_info: n_embd_head_k    = 128
0.00.064.634 I print_info: n_embd_head_v    = 128
0.00.064.635 I print_info: n_gqa            = 1
0.00.064.635 I print_info: n_embd_k_gqa     = 2048
0.00.064.636 I print_info: n_embd_v_gqa     = 2048
0.00.064.637 I print_info: f_norm_eps       = 1.0e-05
0.00.064.638 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.638 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.638 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.638 I print_info: f_logit_scale    = 0.0e+00
0.00.064.639 I print_info: n_ff             = 8192
0.00.064.639 I print_info: n_expert         = 0
0.00.064.639 I print_info: n_expert_used    = 0
0.00.064.639 I print_info: causal attn      = 1
0.00.064.640 I print_info: pooling type     = 0
0.00.064.640 I print_info: rope type        = 2
0.00.064.640 I print_info: rope scaling     = linear
0.00.064.641 I print_info: freq_base_train  = 10000.0
0.00.064.641 I print_info: freq_scale_train = 1
0.00.064.641 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.642 I print_info: rope_finetuned   = unknown
0.00.064.642 I print_info: ssm_d_conv       = 0
0.00.064.642 I print_info: ssm_d_inner      = 0
0.00.064.642 I print_info: ssm_d_state      = 0
0.00.064.642 I print_info: ssm_dt_rank      = 0
0.00.064.642 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.643 I print_info: model type       = 1.4B
0.00.064.647 I print_info: model params     = 1.41 B
0.00.064.648 I print_info: general.name     = 1.4B
0.00.064.648 I print_info: vocab type       = BPE
0.00.064.648 I print_info: n_vocab          = 50304
0.00.064.648 I print_info: n_merges         = 50009
0.00.064.649 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.649 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.649 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.649 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.650 I print_info: LF token         = 128 'Ä'
0.00.064.650 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.650 I print_info: max token length = 1024
0.00.067.074 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.074 I load_tensors: offloading output layer to GPU
0.00.067.074 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.086 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.087 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.412 I llama_init_from_model: n_seq_max     = 1
0.00.067.413 I llama_init_from_model: n_ctx         = 2048
0.00.067.413 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.413 I llama_init_from_model: n_batch       = 2048
0.00.067.413 I llama_init_from_model: n_ubatch      = 512
0.00.067.413 I llama_init_from_model: flash_attn    = 0
0.00.067.414 I llama_init_from_model: freq_base     = 10000.0
0.00.067.414 I llama_init_from_model: freq_scale    = 1
0.00.067.415 I ggml_metal_init: allocating
0.00.067.418 I ggml_metal_init: found device: Apple M4
0.00.067.420 I ggml_metal_init: picking default device: Apple M4
0.00.068.165 I ggml_metal_init: using embedded metal library
0.00.070.756 I ggml_metal_init: GPU name:   Apple M4
0.00.070.757 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.757 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.758 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.758 I ggml_metal_init: simdgroup reduction   = true
0.00.070.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.759 I ggml_metal_init: has bfloat            = true
0.00.070.759 I ggml_metal_init: use bfloat            = true
0.00.070.759 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.284 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.097 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.107.111 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.150 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.289 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.108.292 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.108.292 I llama_init_from_model: graph nodes  = 967
0.00.108.292 I llama_init_from_model: graph splits = 2
0.00.108.297 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.108.425 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.177.523 I main: llama threadpool init, n_threads = 4
0.01.177.554 I 
0.01.177.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.177.576 I 
0.01.177.808 I sampler seed: 1234
0.01.177.812 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.177.833 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.177.833 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.177.833 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.266.927 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61365.60 tokens per second)
0.02.266.928 I llama_perf_context_print:        load time =    1166.82 ms
0.02.266.928 I llama_perf_context_print: prompt eval time =      46.67 ms /     7 tokens (    6.67 ms per token,   150.00 tokens per second)
0.02.266.929 I llama_perf_context_print:        eval time =    1039.56 ms /    63 runs   (   16.50 ms per token,    60.60 tokens per second)
0.02.266.930 I llama_perf_context_print:       total time =    1090.26 ms /    70 tokens
0.02.267.149 I ggml_metal_free: deallocating

real	0m2.286s
user	0m0.115s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.021.046 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.025 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.030 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.032 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.033 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.033 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.034 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.034 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.037 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.037 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.037 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.038 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.038 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.042 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.044 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.044 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.045 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.429 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.621 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.622 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.623 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.048.625 I llama_model_loader: - type  f32:  194 tensors
0.00.048.625 I llama_model_loader: - type q4_0:   97 tensors
0.00.048.625 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.626 I print_info: file format = GGUF V3 (latest)
0.00.048.627 I print_info: file type   = Q4_0
0.00.048.628 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.077.182 I load: special tokens cache size = 25
0.00.088.871 I load: token to piece cache size = 0.2984 MB
0.00.088.876 I print_info: arch             = gptneox
0.00.088.877 I print_info: vocab_only       = 0
0.00.088.877 I print_info: n_ctx_train      = 2048
0.00.088.877 I print_info: n_embd           = 2048
0.00.088.877 I print_info: n_layer          = 24
0.00.088.882 I print_info: n_head           = 16
0.00.088.883 I print_info: n_head_kv        = 16
0.00.088.884 I print_info: n_rot            = 32
0.00.088.884 I print_info: n_swa            = 0
0.00.088.884 I print_info: n_embd_head_k    = 128
0.00.088.884 I print_info: n_embd_head_v    = 128
0.00.088.888 I print_info: n_gqa            = 1
0.00.088.889 I print_info: n_embd_k_gqa     = 2048
0.00.088.890 I print_info: n_embd_v_gqa     = 2048
0.00.088.891 I print_info: f_norm_eps       = 1.0e-05
0.00.088.893 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.893 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.893 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.893 I print_info: f_logit_scale    = 0.0e+00
0.00.088.895 I print_info: n_ff             = 8192
0.00.088.895 I print_info: n_expert         = 0
0.00.088.895 I print_info: n_expert_used    = 0
0.00.088.895 I print_info: causal attn      = 1
0.00.088.895 I print_info: pooling type     = 0
0.00.088.895 I print_info: rope type        = 2
0.00.088.896 I print_info: rope scaling     = linear
0.00.088.897 I print_info: freq_base_train  = 10000.0
0.00.088.897 I print_info: freq_scale_train = 1
0.00.088.897 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.898 I print_info: rope_finetuned   = unknown
0.00.088.900 I print_info: ssm_d_conv       = 0
0.00.088.900 I print_info: ssm_d_inner      = 0
0.00.088.900 I print_info: ssm_d_state      = 0
0.00.088.900 I print_info: ssm_dt_rank      = 0
0.00.088.900 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.901 I print_info: model type       = 1.4B
0.00.088.901 I print_info: model params     = 1.41 B
0.00.088.901 I print_info: general.name     = 1.4B
0.00.088.902 I print_info: vocab type       = BPE
0.00.088.902 I print_info: n_vocab          = 50304
0.00.088.903 I print_info: n_merges         = 50009
0.00.088.903 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.903 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.903 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.904 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.907 I print_info: LF token         = 128 'Ä'
0.00.088.907 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.907 I print_info: max token length = 1024
0.00.091.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.893 I load_tensors: offloading output layer to GPU
0.00.091.894 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.906 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.091.907 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.092.374 I llama_init_from_model: n_seq_max     = 1
0.00.092.376 I llama_init_from_model: n_ctx         = 2048
0.00.092.376 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.092.376 I llama_init_from_model: n_batch       = 2048
0.00.092.377 I llama_init_from_model: n_ubatch      = 512
0.00.092.377 I llama_init_from_model: flash_attn    = 0
0.00.092.377 I llama_init_from_model: freq_base     = 10000.0
0.00.092.378 I llama_init_from_model: freq_scale    = 1
0.00.092.378 I ggml_metal_init: allocating
0.00.092.383 I ggml_metal_init: found device: Apple M4
0.00.092.386 I ggml_metal_init: picking default device: Apple M4
0.00.093.403 I ggml_metal_init: using embedded metal library
0.00.097.297 I ggml_metal_init: GPU name:   Apple M4
0.00.097.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.300 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.301 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.301 I ggml_metal_init: simdgroup reduction   = true
0.00.097.301 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.301 I ggml_metal_init: has bfloat            = true
0.00.097.301 I ggml_metal_init: use bfloat            = true
0.00.097.302 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.305 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.291 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.133.950 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.133.963 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.133.996 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.135.110 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.135.111 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.135.112 I llama_init_from_model: graph nodes  = 967
0.00.135.112 I llama_init_from_model: graph splits = 2
0.00.135.116 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.135.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.135.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.115 I main: llama threadpool init, n_threads = 4
0.00.796.166 I 
0.00.796.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.201 I 
0.00.796.511 I sampler seed: 1234
0.00.796.516 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.542 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.545 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.545 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.480.114 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.480.115 I llama_perf_context_print:        load time =     773.84 ms
0.01.480.116 I llama_perf_context_print: prompt eval time =      47.38 ms /     7 tokens (    6.77 ms per token,   147.75 tokens per second)
0.01.480.116 I llama_perf_context_print:        eval time =     633.15 ms /    63 runs   (   10.05 ms per token,    99.50 tokens per second)
0.01.480.117 I llama_perf_context_print:       total time =     685.23 ms /    70 tokens
0.01.480.388 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.141s
sys	0m0.179s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.183 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.187 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.189 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.192 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.192 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.193 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.193 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.193 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.194 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.197 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.931 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.989 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.723 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.725 I llama_model_loader: - type  f32:  194 tensors
0.00.025.725 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.726 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.726 I print_info: file format = GGUF V3 (latest)
0.00.025.726 I print_info: file type   = Q4_1
0.00.025.727 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.242 I load: special tokens cache size = 25
0.00.050.327 I load: token to piece cache size = 0.2984 MB
0.00.050.330 I print_info: arch             = gptneox
0.00.050.330 I print_info: vocab_only       = 0
0.00.050.330 I print_info: n_ctx_train      = 2048
0.00.050.330 I print_info: n_embd           = 2048
0.00.050.331 I print_info: n_layer          = 24
0.00.050.333 I print_info: n_head           = 16
0.00.050.334 I print_info: n_head_kv        = 16
0.00.050.334 I print_info: n_rot            = 32
0.00.050.334 I print_info: n_swa            = 0
0.00.050.334 I print_info: n_embd_head_k    = 128
0.00.050.334 I print_info: n_embd_head_v    = 128
0.00.050.335 I print_info: n_gqa            = 1
0.00.050.336 I print_info: n_embd_k_gqa     = 2048
0.00.050.337 I print_info: n_embd_v_gqa     = 2048
0.00.050.337 I print_info: f_norm_eps       = 1.0e-05
0.00.050.338 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.338 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.338 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.338 I print_info: f_logit_scale    = 0.0e+00
0.00.050.339 I print_info: n_ff             = 8192
0.00.050.339 I print_info: n_expert         = 0
0.00.050.339 I print_info: n_expert_used    = 0
0.00.050.339 I print_info: causal attn      = 1
0.00.050.339 I print_info: pooling type     = 0
0.00.050.342 I print_info: rope type        = 2
0.00.050.345 I print_info: rope scaling     = linear
0.00.050.345 I print_info: freq_base_train  = 10000.0
0.00.050.345 I print_info: freq_scale_train = 1
0.00.050.345 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.346 I print_info: rope_finetuned   = unknown
0.00.050.346 I print_info: ssm_d_conv       = 0
0.00.050.346 I print_info: ssm_d_inner      = 0
0.00.050.346 I print_info: ssm_d_state      = 0
0.00.050.346 I print_info: ssm_dt_rank      = 0
0.00.050.346 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.346 I print_info: model type       = 1.4B
0.00.050.347 I print_info: model params     = 1.41 B
0.00.050.347 I print_info: general.name     = 1.4B
0.00.050.348 I print_info: vocab type       = BPE
0.00.050.348 I print_info: n_vocab          = 50304
0.00.050.348 I print_info: n_merges         = 50009
0.00.050.348 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.348 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.348 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.349 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.349 I print_info: LF token         = 128 'Ä'
0.00.050.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.350 I print_info: max token length = 1024
0.00.052.330 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.330 I load_tensors: offloading output layer to GPU
0.00.052.330 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.341 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.342 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.620 I llama_init_from_model: n_seq_max     = 1
0.00.052.621 I llama_init_from_model: n_ctx         = 2048
0.00.052.621 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.621 I llama_init_from_model: n_batch       = 2048
0.00.052.622 I llama_init_from_model: n_ubatch      = 512
0.00.052.622 I llama_init_from_model: flash_attn    = 0
0.00.052.622 I llama_init_from_model: freq_base     = 10000.0
0.00.052.622 I llama_init_from_model: freq_scale    = 1
0.00.052.623 I ggml_metal_init: allocating
0.00.052.626 I ggml_metal_init: found device: Apple M4
0.00.052.628 I ggml_metal_init: picking default device: Apple M4
0.00.053.250 I ggml_metal_init: using embedded metal library
0.00.055.587 I ggml_metal_init: GPU name:   Apple M4
0.00.055.589 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.589 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.590 I ggml_metal_init: simdgroup reduction   = true
0.00.055.590 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.590 I ggml_metal_init: has bfloat            = true
0.00.055.590 I ggml_metal_init: use bfloat            = true
0.00.055.591 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.241 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.306 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.316 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.340 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.306 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.308 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.308 I llama_init_from_model: graph nodes  = 967
0.00.085.308 I llama_init_from_model: graph splits = 2
0.00.085.311 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.427 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.764 I main: llama threadpool init, n_threads = 4
0.00.671.800 I 
0.00.671.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.845 I 
0.00.672.070 I sampler seed: 1234
0.00.672.074 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.672.085 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.672.086 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.672.086 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.400.791 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60322.85 tokens per second)
0.01.400.791 I llama_perf_context_print:        load time =     661.97 ms
0.01.400.792 I llama_perf_context_print: prompt eval time =      45.61 ms /     7 tokens (    6.52 ms per token,   153.49 tokens per second)
0.01.400.793 I llama_perf_context_print:        eval time =     680.08 ms /    63 runs   (   10.79 ms per token,    92.64 tokens per second)
0.01.400.793 I llama_perf_context_print:       total time =     729.97 ms /    70 tokens
0.01.400.985 I ggml_metal_free: deallocating

real	0m1.419s
user	0m0.109s
sys	0m0.136s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.829 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.070 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.079 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.080 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.080 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.081 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.081 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.082 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.082 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.085 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.086 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.978 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.020 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.793 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.794 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.795 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.795 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.796 I llama_model_loader: - type  f32:  194 tensors
0.00.026.796 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.797 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.797 I print_info: file format = GGUF V3 (latest)
0.00.026.798 I print_info: file type   = Q5_0
0.00.026.798 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.308 I load: special tokens cache size = 25
0.00.051.384 I load: token to piece cache size = 0.2984 MB
0.00.051.387 I print_info: arch             = gptneox
0.00.051.387 I print_info: vocab_only       = 0
0.00.051.387 I print_info: n_ctx_train      = 2048
0.00.051.387 I print_info: n_embd           = 2048
0.00.051.388 I print_info: n_layer          = 24
0.00.051.390 I print_info: n_head           = 16
0.00.051.391 I print_info: n_head_kv        = 16
0.00.051.391 I print_info: n_rot            = 32
0.00.051.391 I print_info: n_swa            = 0
0.00.051.392 I print_info: n_embd_head_k    = 128
0.00.051.392 I print_info: n_embd_head_v    = 128
0.00.051.392 I print_info: n_gqa            = 1
0.00.051.393 I print_info: n_embd_k_gqa     = 2048
0.00.051.394 I print_info: n_embd_v_gqa     = 2048
0.00.051.395 I print_info: f_norm_eps       = 1.0e-05
0.00.051.395 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.395 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.395 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.395 I print_info: f_logit_scale    = 0.0e+00
0.00.051.396 I print_info: n_ff             = 8192
0.00.051.396 I print_info: n_expert         = 0
0.00.051.396 I print_info: n_expert_used    = 0
0.00.051.397 I print_info: causal attn      = 1
0.00.051.397 I print_info: pooling type     = 0
0.00.051.398 I print_info: rope type        = 2
0.00.051.400 I print_info: rope scaling     = linear
0.00.051.401 I print_info: freq_base_train  = 10000.0
0.00.051.401 I print_info: freq_scale_train = 1
0.00.051.401 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.402 I print_info: rope_finetuned   = unknown
0.00.051.402 I print_info: ssm_d_conv       = 0
0.00.051.402 I print_info: ssm_d_inner      = 0
0.00.051.402 I print_info: ssm_d_state      = 0
0.00.051.402 I print_info: ssm_dt_rank      = 0
0.00.051.402 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.402 I print_info: model type       = 1.4B
0.00.051.403 I print_info: model params     = 1.41 B
0.00.051.403 I print_info: general.name     = 1.4B
0.00.051.403 I print_info: vocab type       = BPE
0.00.051.404 I print_info: n_vocab          = 50304
0.00.051.404 I print_info: n_merges         = 50009
0.00.051.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.404 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.405 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.405 I print_info: LF token         = 128 'Ä'
0.00.051.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.411 I print_info: max token length = 1024
0.00.053.164 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.165 I load_tensors: offloading output layer to GPU
0.00.053.165 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.170 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.171 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.443 I llama_init_from_model: n_seq_max     = 1
0.00.053.444 I llama_init_from_model: n_ctx         = 2048
0.00.053.444 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.444 I llama_init_from_model: n_batch       = 2048
0.00.053.444 I llama_init_from_model: n_ubatch      = 512
0.00.053.444 I llama_init_from_model: flash_attn    = 0
0.00.053.445 I llama_init_from_model: freq_base     = 10000.0
0.00.053.445 I llama_init_from_model: freq_scale    = 1
0.00.053.446 I ggml_metal_init: allocating
0.00.053.449 I ggml_metal_init: found device: Apple M4
0.00.053.450 I ggml_metal_init: picking default device: Apple M4
0.00.054.031 I ggml_metal_init: using embedded metal library
0.00.056.360 I ggml_metal_init: GPU name:   Apple M4
0.00.056.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.362 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.362 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.363 I ggml_metal_init: simdgroup reduction   = true
0.00.056.363 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.363 I ggml_metal_init: has bfloat            = true
0.00.056.363 I ggml_metal_init: use bfloat            = true
0.00.056.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.917 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.290 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.301 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.324 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.324 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.325 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.325 I llama_init_from_model: graph nodes  = 967
0.00.087.325 I llama_init_from_model: graph splits = 2
0.00.087.328 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.458 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.661 I main: llama threadpool init, n_threads = 4
0.00.697.698 I 
0.00.697.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.722 I 
0.00.697.942 I sampler seed: 1234
0.00.697.946 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.697.957 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.697.957 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.697.957 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.492.184 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.492.185 I llama_perf_context_print:        load time =     686.95 ms
0.01.492.186 I llama_perf_context_print: prompt eval time =      47.02 ms /     7 tokens (    6.72 ms per token,   148.89 tokens per second)
0.01.492.188 I llama_perf_context_print:        eval time =     744.13 ms /    63 runs   (   11.81 ms per token,    84.66 tokens per second)
0.01.492.188 I llama_perf_context_print:       total time =     795.40 ms /    70 tokens
0.01.492.453 I ggml_metal_free: deallocating

real	0m1.512s
user	0m0.109s
sys	0m0.153s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.456 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.768 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.772 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.773 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.773 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.774 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.774 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.774 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.777 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.777 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.777 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.778 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.779 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.780 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.784 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.785 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.785 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.765 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.920 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.921 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.922 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.922 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.923 I llama_model_loader: - type  f32:  194 tensors
0.00.028.925 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.925 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.927 I print_info: file format = GGUF V3 (latest)
0.00.028.927 I print_info: file type   = Q5_1
0.00.028.928 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.048.145 I load: special tokens cache size = 25
0.00.054.186 I load: token to piece cache size = 0.2984 MB
0.00.054.190 I print_info: arch             = gptneox
0.00.054.191 I print_info: vocab_only       = 0
0.00.054.191 I print_info: n_ctx_train      = 2048
0.00.054.191 I print_info: n_embd           = 2048
0.00.054.191 I print_info: n_layer          = 24
0.00.054.197 I print_info: n_head           = 16
0.00.054.197 I print_info: n_head_kv        = 16
0.00.054.199 I print_info: n_rot            = 32
0.00.054.199 I print_info: n_swa            = 0
0.00.054.199 I print_info: n_embd_head_k    = 128
0.00.054.200 I print_info: n_embd_head_v    = 128
0.00.054.200 I print_info: n_gqa            = 1
0.00.054.201 I print_info: n_embd_k_gqa     = 2048
0.00.054.202 I print_info: n_embd_v_gqa     = 2048
0.00.054.202 I print_info: f_norm_eps       = 1.0e-05
0.00.054.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.203 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.203 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.203 I print_info: f_logit_scale    = 0.0e+00
0.00.054.204 I print_info: n_ff             = 8192
0.00.054.204 I print_info: n_expert         = 0
0.00.054.204 I print_info: n_expert_used    = 0
0.00.054.204 I print_info: causal attn      = 1
0.00.054.205 I print_info: pooling type     = 0
0.00.054.206 I print_info: rope type        = 2
0.00.054.208 I print_info: rope scaling     = linear
0.00.054.208 I print_info: freq_base_train  = 10000.0
0.00.054.208 I print_info: freq_scale_train = 1
0.00.054.209 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.209 I print_info: rope_finetuned   = unknown
0.00.054.209 I print_info: ssm_d_conv       = 0
0.00.054.209 I print_info: ssm_d_inner      = 0
0.00.054.209 I print_info: ssm_d_state      = 0
0.00.054.209 I print_info: ssm_dt_rank      = 0
0.00.054.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.210 I print_info: model type       = 1.4B
0.00.054.210 I print_info: model params     = 1.41 B
0.00.054.210 I print_info: general.name     = 1.4B
0.00.054.211 I print_info: vocab type       = BPE
0.00.054.211 I print_info: n_vocab          = 50304
0.00.054.212 I print_info: n_merges         = 50009
0.00.054.212 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.213 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.213 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.213 I print_info: LF token         = 128 'Ä'
0.00.054.213 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.213 I print_info: max token length = 1024
0.00.056.232 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.232 I load_tensors: offloading output layer to GPU
0.00.056.233 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.244 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.245 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.056.520 I llama_init_from_model: n_seq_max     = 1
0.00.056.520 I llama_init_from_model: n_ctx         = 2048
0.00.056.521 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.521 I llama_init_from_model: n_batch       = 2048
0.00.056.521 I llama_init_from_model: n_ubatch      = 512
0.00.056.521 I llama_init_from_model: flash_attn    = 0
0.00.056.522 I llama_init_from_model: freq_base     = 10000.0
0.00.056.522 I llama_init_from_model: freq_scale    = 1
0.00.056.523 I ggml_metal_init: allocating
0.00.056.526 I ggml_metal_init: found device: Apple M4
0.00.056.528 I ggml_metal_init: picking default device: Apple M4
0.00.057.179 I ggml_metal_init: using embedded metal library
0.00.059.577 I ggml_metal_init: GPU name:   Apple M4
0.00.059.578 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.579 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.579 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.579 I ggml_metal_init: simdgroup reduction   = true
0.00.059.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.580 I ggml_metal_init: has bfloat            = true
0.00.059.580 I ggml_metal_init: use bfloat            = true
0.00.059.580 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.581 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.962 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.763 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.773 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.806 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.769 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.091.771 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.091.771 I llama_init_from_model: graph nodes  = 967
0.00.091.772 I llama_init_from_model: graph splits = 2
0.00.091.774 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.890 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.891 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.204 I main: llama threadpool init, n_threads = 4
0.00.804.242 I 
0.00.804.264 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.264 I 
0.00.804.668 I sampler seed: 1234
0.00.804.679 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.693 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.695 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.695 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.641.238 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.01.641.239 I llama_perf_context_print:        load time =     791.89 ms
0.01.641.240 I llama_perf_context_print: prompt eval time =      42.38 ms /     7 tokens (    6.05 ms per token,   165.18 tokens per second)
0.01.641.241 I llama_perf_context_print:        eval time =     791.14 ms /    63 runs   (   12.56 ms per token,    79.63 tokens per second)
0.01.641.241 I llama_perf_context_print:       total time =     837.90 ms /    70 tokens
0.01.641.443 I ggml_metal_free: deallocating

real	0m1.660s
user	0m0.110s
sys	0m0.150s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.764 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.335 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.340 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.342 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.343 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.343 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.343 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.345 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.346 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.348 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.348 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.349 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.360 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.444 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.397 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.398 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.398 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.399 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.399 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.399 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.400 I llama_model_loader: - type  f32:  194 tensors
0.00.025.400 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.400 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.400 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.401 I print_info: file format = GGUF V3 (latest)
0.00.025.401 I print_info: file type   = Q2_K - Medium
0.00.025.402 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.959 I load: special tokens cache size = 25
0.00.050.020 I load: token to piece cache size = 0.2984 MB
0.00.050.023 I print_info: arch             = gptneox
0.00.050.023 I print_info: vocab_only       = 0
0.00.050.023 I print_info: n_ctx_train      = 2048
0.00.050.023 I print_info: n_embd           = 2048
0.00.050.024 I print_info: n_layer          = 24
0.00.050.026 I print_info: n_head           = 16
0.00.050.027 I print_info: n_head_kv        = 16
0.00.050.027 I print_info: n_rot            = 32
0.00.050.027 I print_info: n_swa            = 0
0.00.050.028 I print_info: n_embd_head_k    = 128
0.00.050.028 I print_info: n_embd_head_v    = 128
0.00.050.028 I print_info: n_gqa            = 1
0.00.050.029 I print_info: n_embd_k_gqa     = 2048
0.00.050.030 I print_info: n_embd_v_gqa     = 2048
0.00.050.031 I print_info: f_norm_eps       = 1.0e-05
0.00.050.031 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.031 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.031 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.031 I print_info: f_logit_scale    = 0.0e+00
0.00.050.032 I print_info: n_ff             = 8192
0.00.050.032 I print_info: n_expert         = 0
0.00.050.033 I print_info: n_expert_used    = 0
0.00.050.033 I print_info: causal attn      = 1
0.00.050.033 I print_info: pooling type     = 0
0.00.050.033 I print_info: rope type        = 2
0.00.050.033 I print_info: rope scaling     = linear
0.00.050.034 I print_info: freq_base_train  = 10000.0
0.00.050.034 I print_info: freq_scale_train = 1
0.00.050.034 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.034 I print_info: rope_finetuned   = unknown
0.00.050.034 I print_info: ssm_d_conv       = 0
0.00.050.036 I print_info: ssm_d_inner      = 0
0.00.050.036 I print_info: ssm_d_state      = 0
0.00.050.036 I print_info: ssm_dt_rank      = 0
0.00.050.037 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.037 I print_info: model type       = 1.4B
0.00.050.037 I print_info: model params     = 1.41 B
0.00.050.038 I print_info: general.name     = 1.4B
0.00.050.038 I print_info: vocab type       = BPE
0.00.050.038 I print_info: n_vocab          = 50304
0.00.050.040 I print_info: n_merges         = 50009
0.00.050.040 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.040 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.040 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.041 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.041 I print_info: LF token         = 128 'Ä'
0.00.050.041 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.042 I print_info: max token length = 1024
0.00.051.895 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.895 I load_tensors: offloading output layer to GPU
0.00.051.896 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.906 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.908 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.180 I llama_init_from_model: n_seq_max     = 1
0.00.052.181 I llama_init_from_model: n_ctx         = 2048
0.00.052.181 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.181 I llama_init_from_model: n_batch       = 2048
0.00.052.182 I llama_init_from_model: n_ubatch      = 512
0.00.052.182 I llama_init_from_model: flash_attn    = 0
0.00.052.182 I llama_init_from_model: freq_base     = 10000.0
0.00.052.182 I llama_init_from_model: freq_scale    = 1
0.00.052.183 I ggml_metal_init: allocating
0.00.052.186 I ggml_metal_init: found device: Apple M4
0.00.052.188 I ggml_metal_init: picking default device: Apple M4
0.00.052.783 I ggml_metal_init: using embedded metal library
0.00.055.101 I ggml_metal_init: GPU name:   Apple M4
0.00.055.102 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.103 I ggml_metal_init: simdgroup reduction   = true
0.00.055.103 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.103 I ggml_metal_init: has bfloat            = true
0.00.055.104 I ggml_metal_init: use bfloat            = true
0.00.055.104 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.105 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.724 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.754 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.762 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.783 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.736 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.737 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.738 I llama_init_from_model: graph nodes  = 967
0.00.084.738 I llama_init_from_model: graph splits = 2
0.00.084.741 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.870 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.480.381 I main: llama threadpool init, n_threads = 4
0.00.480.421 I 
0.00.480.442 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.480.442 I 
0.00.480.665 I sampler seed: 1234
0.00.480.670 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.480.681 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.480.681 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.480.681 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.154.674 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 64840.18 tokens per second)
0.01.154.674 I llama_perf_context_print:        load time =     469.74 ms
0.01.154.678 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.69 tokens per second)
0.01.154.680 I llama_perf_context_print:        eval time =     635.30 ms /    63 runs   (   10.08 ms per token,    99.17 tokens per second)
0.01.154.681 I llama_perf_context_print:       total time =     675.16 ms /    70 tokens
0.01.154.948 I ggml_metal_free: deallocating

real	0m1.173s
user	0m0.109s
sys	0m0.109s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.983 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.379 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.384 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.386 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.387 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.389 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.390 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.391 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.391 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.392 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.393 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.396 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.500 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.620 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.614 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.617 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.617 I llama_model_loader: - type  f32:  194 tensors
0.00.025.618 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.618 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.618 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.618 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.619 I print_info: file format = GGUF V3 (latest)
0.00.025.619 I print_info: file type   = Q3_K - Medium
0.00.025.620 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.142 I load: special tokens cache size = 25
0.00.050.194 I load: token to piece cache size = 0.2984 MB
0.00.050.197 I print_info: arch             = gptneox
0.00.050.197 I print_info: vocab_only       = 0
0.00.050.197 I print_info: n_ctx_train      = 2048
0.00.050.197 I print_info: n_embd           = 2048
0.00.050.198 I print_info: n_layer          = 24
0.00.050.200 I print_info: n_head           = 16
0.00.050.201 I print_info: n_head_kv        = 16
0.00.050.201 I print_info: n_rot            = 32
0.00.050.202 I print_info: n_swa            = 0
0.00.050.202 I print_info: n_embd_head_k    = 128
0.00.050.202 I print_info: n_embd_head_v    = 128
0.00.050.203 I print_info: n_gqa            = 1
0.00.050.203 I print_info: n_embd_k_gqa     = 2048
0.00.050.204 I print_info: n_embd_v_gqa     = 2048
0.00.050.204 I print_info: f_norm_eps       = 1.0e-05
0.00.050.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.207 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.207 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.207 I print_info: f_logit_scale    = 0.0e+00
0.00.050.208 I print_info: n_ff             = 8192
0.00.050.208 I print_info: n_expert         = 0
0.00.050.208 I print_info: n_expert_used    = 0
0.00.050.210 I print_info: causal attn      = 1
0.00.050.211 I print_info: pooling type     = 0
0.00.050.211 I print_info: rope type        = 2
0.00.050.212 I print_info: rope scaling     = linear
0.00.050.212 I print_info: freq_base_train  = 10000.0
0.00.050.212 I print_info: freq_scale_train = 1
0.00.050.214 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.214 I print_info: rope_finetuned   = unknown
0.00.050.214 I print_info: ssm_d_conv       = 0
0.00.050.214 I print_info: ssm_d_inner      = 0
0.00.050.214 I print_info: ssm_d_state      = 0
0.00.050.214 I print_info: ssm_dt_rank      = 0
0.00.050.215 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.215 I print_info: model type       = 1.4B
0.00.050.215 I print_info: model params     = 1.41 B
0.00.050.215 I print_info: general.name     = 1.4B
0.00.050.216 I print_info: vocab type       = BPE
0.00.050.216 I print_info: n_vocab          = 50304
0.00.050.217 I print_info: n_merges         = 50009
0.00.050.218 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.218 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.218 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.218 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.218 I print_info: LF token         = 128 'Ä'
0.00.050.219 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.219 I print_info: max token length = 1024
0.00.052.081 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.082 I load_tensors: offloading output layer to GPU
0.00.052.082 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.092 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.093 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.365 I llama_init_from_model: n_seq_max     = 1
0.00.052.365 I llama_init_from_model: n_ctx         = 2048
0.00.052.366 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.366 I llama_init_from_model: n_batch       = 2048
0.00.052.366 I llama_init_from_model: n_ubatch      = 512
0.00.052.366 I llama_init_from_model: flash_attn    = 0
0.00.052.366 I llama_init_from_model: freq_base     = 10000.0
0.00.052.367 I llama_init_from_model: freq_scale    = 1
0.00.052.367 I ggml_metal_init: allocating
0.00.052.370 I ggml_metal_init: found device: Apple M4
0.00.052.372 I ggml_metal_init: picking default device: Apple M4
0.00.052.957 I ggml_metal_init: using embedded metal library
0.00.055.313 I ggml_metal_init: GPU name:   Apple M4
0.00.055.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.315 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.315 I ggml_metal_init: simdgroup reduction   = true
0.00.055.315 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.315 I ggml_metal_init: has bfloat            = true
0.00.055.316 I ggml_metal_init: use bfloat            = true
0.00.055.316 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.316 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.922 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.029 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.037 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.056 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.016 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.017 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.018 I llama_init_from_model: graph nodes  = 967
0.00.085.018 I llama_init_from_model: graph splits = 2
0.00.085.021 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.535.568 I main: llama threadpool init, n_threads = 4
0.00.535.602 I 
0.00.535.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.535.627 I 
0.00.535.840 I sampler seed: 1234
0.00.535.844 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.535.855 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.535.855 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.535.855 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.275.694 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.275.695 I llama_perf_context_print:        load time =     525.72 ms
0.01.275.696 I llama_perf_context_print: prompt eval time =      43.08 ms /     7 tokens (    6.15 ms per token,   162.49 tokens per second)
0.01.275.696 I llama_perf_context_print:        eval time =     693.66 ms /    63 runs   (   11.01 ms per token,    90.82 tokens per second)
0.01.275.697 I llama_perf_context_print:       total time =     740.99 ms /    70 tokens
0.01.275.953 I ggml_metal_free: deallocating

real	0m1.291s
user	0m0.109s
sys	0m0.124s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.804 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.406 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.411 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.416 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.417 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.438 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.439 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.440 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.440 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.440 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.440 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.441 I llama_model_loader: - type  f32:  194 tensors
0.00.025.441 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.441 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.441 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.442 I print_info: file format = GGUF V3 (latest)
0.00.025.442 I print_info: file type   = Q4_K - Medium
0.00.025.443 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.518 I load: special tokens cache size = 25
0.00.050.549 I load: token to piece cache size = 0.2984 MB
0.00.050.552 I print_info: arch             = gptneox
0.00.050.552 I print_info: vocab_only       = 0
0.00.050.552 I print_info: n_ctx_train      = 2048
0.00.050.552 I print_info: n_embd           = 2048
0.00.050.553 I print_info: n_layer          = 24
0.00.050.555 I print_info: n_head           = 16
0.00.050.556 I print_info: n_head_kv        = 16
0.00.050.557 I print_info: n_rot            = 32
0.00.050.557 I print_info: n_swa            = 0
0.00.050.557 I print_info: n_embd_head_k    = 128
0.00.050.557 I print_info: n_embd_head_v    = 128
0.00.050.558 I print_info: n_gqa            = 1
0.00.050.559 I print_info: n_embd_k_gqa     = 2048
0.00.050.559 I print_info: n_embd_v_gqa     = 2048
0.00.050.560 I print_info: f_norm_eps       = 1.0e-05
0.00.050.560 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.560 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.561 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.561 I print_info: f_logit_scale    = 0.0e+00
0.00.050.561 I print_info: n_ff             = 8192
0.00.050.562 I print_info: n_expert         = 0
0.00.050.562 I print_info: n_expert_used    = 0
0.00.050.562 I print_info: causal attn      = 1
0.00.050.562 I print_info: pooling type     = 0
0.00.050.562 I print_info: rope type        = 2
0.00.050.563 I print_info: rope scaling     = linear
0.00.050.563 I print_info: freq_base_train  = 10000.0
0.00.050.563 I print_info: freq_scale_train = 1
0.00.050.563 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.564 I print_info: rope_finetuned   = unknown
0.00.050.564 I print_info: ssm_d_conv       = 0
0.00.050.564 I print_info: ssm_d_inner      = 0
0.00.050.564 I print_info: ssm_d_state      = 0
0.00.050.564 I print_info: ssm_dt_rank      = 0
0.00.050.565 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.565 I print_info: model type       = 1.4B
0.00.050.568 I print_info: model params     = 1.41 B
0.00.050.568 I print_info: general.name     = 1.4B
0.00.050.568 I print_info: vocab type       = BPE
0.00.050.569 I print_info: n_vocab          = 50304
0.00.050.570 I print_info: n_merges         = 50009
0.00.050.570 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.570 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.570 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.570 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.571 I print_info: LF token         = 128 'Ä'
0.00.050.572 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.572 I print_info: max token length = 1024
0.00.052.506 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.506 I load_tensors: offloading output layer to GPU
0.00.052.507 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.517 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.518 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.795 I llama_init_from_model: n_seq_max     = 1
0.00.052.795 I llama_init_from_model: n_ctx         = 2048
0.00.052.795 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.796 I llama_init_from_model: n_batch       = 2048
0.00.052.796 I llama_init_from_model: n_ubatch      = 512
0.00.052.796 I llama_init_from_model: flash_attn    = 0
0.00.052.796 I llama_init_from_model: freq_base     = 10000.0
0.00.052.797 I llama_init_from_model: freq_scale    = 1
0.00.052.797 I ggml_metal_init: allocating
0.00.052.800 I ggml_metal_init: found device: Apple M4
0.00.052.802 I ggml_metal_init: picking default device: Apple M4
0.00.053.403 I ggml_metal_init: using embedded metal library
0.00.055.693 I ggml_metal_init: GPU name:   Apple M4
0.00.055.694 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.695 I ggml_metal_init: simdgroup reduction   = true
0.00.055.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.696 I ggml_metal_init: has bfloat            = true
0.00.055.696 I ggml_metal_init: use bfloat            = true
0.00.055.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.020 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.518 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.524 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.545 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.609 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.610 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.611 I llama_init_from_model: graph nodes  = 967
0.00.085.611 I llama_init_from_model: graph splits = 2
0.00.085.613 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.509 I main: llama threadpool init, n_threads = 4
0.00.609.546 I 
0.00.609.568 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.568 I 
0.00.609.794 I sampler seed: 1234
0.00.609.801 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.839 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.843 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.844 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.364.221 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.01.364.222 I llama_perf_context_print:        load time =     599.85 ms
0.01.364.222 I llama_perf_context_print: prompt eval time =      49.78 ms /     7 tokens (    7.11 ms per token,   140.62 tokens per second)
0.01.364.223 I llama_perf_context_print:        eval time =     701.53 ms /    63 runs   (   11.14 ms per token,    89.80 tokens per second)
0.01.364.223 I llama_perf_context_print:       total time =     755.57 ms /    70 tokens
0.01.364.456 I ggml_metal_free: deallocating

real	0m1.381s
user	0m0.111s
sys	0m0.138s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.971 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.430 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.439 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.442 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.443 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.444 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.446 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.447 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.447 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.524 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.420 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.421 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.421 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.422 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.422 I llama_model_loader: - type  f32:  194 tensors
0.00.026.423 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.423 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.423 I print_info: file format = GGUF V3 (latest)
0.00.026.424 I print_info: file type   = Q5_K - Medium
0.00.026.425 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.009 I load: special tokens cache size = 25
0.00.051.102 I load: token to piece cache size = 0.2984 MB
0.00.051.104 I print_info: arch             = gptneox
0.00.051.104 I print_info: vocab_only       = 0
0.00.051.105 I print_info: n_ctx_train      = 2048
0.00.051.105 I print_info: n_embd           = 2048
0.00.051.105 I print_info: n_layer          = 24
0.00.051.108 I print_info: n_head           = 16
0.00.051.108 I print_info: n_head_kv        = 16
0.00.051.109 I print_info: n_rot            = 32
0.00.051.111 I print_info: n_swa            = 0
0.00.051.111 I print_info: n_embd_head_k    = 128
0.00.051.111 I print_info: n_embd_head_v    = 128
0.00.051.112 I print_info: n_gqa            = 1
0.00.051.113 I print_info: n_embd_k_gqa     = 2048
0.00.051.113 I print_info: n_embd_v_gqa     = 2048
0.00.051.114 I print_info: f_norm_eps       = 1.0e-05
0.00.051.114 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.115 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.115 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.115 I print_info: f_logit_scale    = 0.0e+00
0.00.051.115 I print_info: n_ff             = 8192
0.00.051.116 I print_info: n_expert         = 0
0.00.051.116 I print_info: n_expert_used    = 0
0.00.051.116 I print_info: causal attn      = 1
0.00.051.116 I print_info: pooling type     = 0
0.00.051.116 I print_info: rope type        = 2
0.00.051.117 I print_info: rope scaling     = linear
0.00.051.122 I print_info: freq_base_train  = 10000.0
0.00.051.125 I print_info: freq_scale_train = 1
0.00.051.125 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.127 I print_info: rope_finetuned   = unknown
0.00.051.127 I print_info: ssm_d_conv       = 0
0.00.051.127 I print_info: ssm_d_inner      = 0
0.00.051.127 I print_info: ssm_d_state      = 0
0.00.051.127 I print_info: ssm_dt_rank      = 0
0.00.051.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.127 I print_info: model type       = 1.4B
0.00.051.128 I print_info: model params     = 1.41 B
0.00.051.128 I print_info: general.name     = 1.4B
0.00.051.128 I print_info: vocab type       = BPE
0.00.051.129 I print_info: n_vocab          = 50304
0.00.051.130 I print_info: n_merges         = 50009
0.00.051.130 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.130 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.130 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.131 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.131 I print_info: LF token         = 128 'Ä'
0.00.051.131 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.131 I print_info: max token length = 1024
0.00.053.063 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.063 I load_tensors: offloading output layer to GPU
0.00.053.064 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.074 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.075 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.360 I llama_init_from_model: n_seq_max     = 1
0.00.053.361 I llama_init_from_model: n_ctx         = 2048
0.00.053.361 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.361 I llama_init_from_model: n_batch       = 2048
0.00.053.361 I llama_init_from_model: n_ubatch      = 512
0.00.053.362 I llama_init_from_model: flash_attn    = 0
0.00.053.362 I llama_init_from_model: freq_base     = 10000.0
0.00.053.362 I llama_init_from_model: freq_scale    = 1
0.00.053.363 I ggml_metal_init: allocating
0.00.053.366 I ggml_metal_init: found device: Apple M4
0.00.053.367 I ggml_metal_init: picking default device: Apple M4
0.00.053.957 I ggml_metal_init: using embedded metal library
0.00.056.291 I ggml_metal_init: GPU name:   Apple M4
0.00.056.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.293 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.293 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.294 I ggml_metal_init: simdgroup reduction   = true
0.00.056.294 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.294 I ggml_metal_init: has bfloat            = true
0.00.056.294 I ggml_metal_init: use bfloat            = true
0.00.056.294 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.823 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.999 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.008 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.028 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.134 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.136 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.136 I llama_init_from_model: graph nodes  = 967
0.00.086.137 I llama_init_from_model: graph splits = 2
0.00.086.140 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.270 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.974 I main: llama threadpool init, n_threads = 4
0.00.702.012 I 
0.00.702.035 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.035 I 
0.00.702.279 I sampler seed: 1234
0.00.702.285 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.305 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.306 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.306 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.543.453 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.543.454 I llama_perf_context_print:        load time =     690.06 ms
0.01.543.455 I llama_perf_context_print: prompt eval time =      51.52 ms /     7 tokens (    7.36 ms per token,   135.88 tokens per second)
0.01.543.455 I llama_perf_context_print:        eval time =     786.62 ms /    63 runs   (   12.49 ms per token,    80.09 tokens per second)
0.01.543.456 I llama_perf_context_print:       total time =     842.43 ms /    70 tokens
0.01.543.703 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.110s
sys	0m0.161s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.765 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.729 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.739 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.740 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.741 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.742 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.742 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.743 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.745 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.747 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.749 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.873 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.769 I llama_model_loader: - type  f32:  194 tensors
0.00.025.769 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.769 I print_info: file format = GGUF V3 (latest)
0.00.025.770 I print_info: file type   = Q6_K
0.00.025.771 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.381 I load: special tokens cache size = 25
0.00.050.390 I load: token to piece cache size = 0.2984 MB
0.00.050.392 I print_info: arch             = gptneox
0.00.050.393 I print_info: vocab_only       = 0
0.00.050.393 I print_info: n_ctx_train      = 2048
0.00.050.393 I print_info: n_embd           = 2048
0.00.050.393 I print_info: n_layer          = 24
0.00.050.396 I print_info: n_head           = 16
0.00.050.397 I print_info: n_head_kv        = 16
0.00.050.397 I print_info: n_rot            = 32
0.00.050.397 I print_info: n_swa            = 0
0.00.050.397 I print_info: n_embd_head_k    = 128
0.00.050.397 I print_info: n_embd_head_v    = 128
0.00.050.400 I print_info: n_gqa            = 1
0.00.050.401 I print_info: n_embd_k_gqa     = 2048
0.00.050.401 I print_info: n_embd_v_gqa     = 2048
0.00.050.407 I print_info: f_norm_eps       = 1.0e-05
0.00.050.408 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.410 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.410 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.410 I print_info: f_logit_scale    = 0.0e+00
0.00.050.414 I print_info: n_ff             = 8192
0.00.050.414 I print_info: n_expert         = 0
0.00.050.415 I print_info: n_expert_used    = 0
0.00.050.415 I print_info: causal attn      = 1
0.00.050.415 I print_info: pooling type     = 0
0.00.050.415 I print_info: rope type        = 2
0.00.050.415 I print_info: rope scaling     = linear
0.00.050.416 I print_info: freq_base_train  = 10000.0
0.00.050.416 I print_info: freq_scale_train = 1
0.00.050.416 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.416 I print_info: rope_finetuned   = unknown
0.00.050.416 I print_info: ssm_d_conv       = 0
0.00.050.416 I print_info: ssm_d_inner      = 0
0.00.050.417 I print_info: ssm_d_state      = 0
0.00.050.417 I print_info: ssm_dt_rank      = 0
0.00.050.417 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.417 I print_info: model type       = 1.4B
0.00.050.417 I print_info: model params     = 1.41 B
0.00.050.418 I print_info: general.name     = 1.4B
0.00.050.418 I print_info: vocab type       = BPE
0.00.050.418 I print_info: n_vocab          = 50304
0.00.050.418 I print_info: n_merges         = 50009
0.00.050.419 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.419 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.419 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.419 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.419 I print_info: LF token         = 128 'Ä'
0.00.050.420 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.420 I print_info: max token length = 1024
0.00.052.225 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.225 I load_tensors: offloading output layer to GPU
0.00.052.226 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.231 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.231 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.613 I llama_init_from_model: n_seq_max     = 1
0.00.052.614 I llama_init_from_model: n_ctx         = 2048
0.00.052.614 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.614 I llama_init_from_model: n_batch       = 2048
0.00.052.614 I llama_init_from_model: n_ubatch      = 512
0.00.052.614 I llama_init_from_model: flash_attn    = 0
0.00.052.615 I llama_init_from_model: freq_base     = 10000.0
0.00.052.615 I llama_init_from_model: freq_scale    = 1
0.00.052.615 I ggml_metal_init: allocating
0.00.052.618 I ggml_metal_init: found device: Apple M4
0.00.052.620 I ggml_metal_init: picking default device: Apple M4
0.00.053.218 I ggml_metal_init: using embedded metal library
0.00.055.561 I ggml_metal_init: GPU name:   Apple M4
0.00.055.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.563 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.563 I ggml_metal_init: simdgroup reduction   = true
0.00.055.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.564 I ggml_metal_init: has bfloat            = true
0.00.055.564 I ggml_metal_init: use bfloat            = true
0.00.055.564 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.146 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.686 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.693 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.713 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.713 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.715 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.715 I llama_init_from_model: graph nodes  = 967
0.00.085.715 I llama_init_from_model: graph splits = 2
0.00.085.718 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.847 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.847 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.907 I main: llama threadpool init, n_threads = 4
0.00.742.943 I 
0.00.742.966 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.967 I 
0.00.743.185 I sampler seed: 1234
0.00.743.190 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.212 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.212 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.212 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.619.945 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.619.946 I llama_perf_context_print:        load time =     733.28 ms
0.01.619.947 I llama_perf_context_print: prompt eval time =      54.41 ms /     7 tokens (    7.77 ms per token,   128.65 tokens per second)
0.01.619.947 I llama_perf_context_print:        eval time =     819.33 ms /    63 runs   (   13.01 ms per token,    76.89 tokens per second)
0.01.619.948 I llama_perf_context_print:       total time =     877.90 ms /    70 tokens
0.01.620.188 I ggml_metal_free: deallocating

real	0m1.637s
user	0m0.109s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.413 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.069 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.542 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.549 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.550 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.556 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.556 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.557 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.558 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.558 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.558 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.559 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.559 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.560 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.561 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.562 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.562 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.340 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.341 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.342 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.343 I llama_model_loader: - type  f32:  194 tensors
0.00.037.343 I llama_model_loader: - type  f16:   98 tensors
0.00.037.344 I print_info: file format = GGUF V3 (latest)
0.00.037.344 I print_info: file type   = all F32 (guessed)
0.00.037.345 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.056.500 I load: special tokens cache size = 25
0.00.062.343 I load: token to piece cache size = 0.2984 MB
0.00.062.346 I print_info: arch             = gptneox
0.00.062.347 I print_info: vocab_only       = 0
0.00.062.347 I print_info: n_ctx_train      = 2048
0.00.062.347 I print_info: n_embd           = 2048
0.00.062.347 I print_info: n_layer          = 24
0.00.062.351 I print_info: n_head           = 16
0.00.062.351 I print_info: n_head_kv        = 16
0.00.062.352 I print_info: n_rot            = 32
0.00.062.352 I print_info: n_swa            = 0
0.00.062.352 I print_info: n_embd_head_k    = 128
0.00.062.352 I print_info: n_embd_head_v    = 128
0.00.062.353 I print_info: n_gqa            = 1
0.00.062.355 I print_info: n_embd_k_gqa     = 2048
0.00.062.356 I print_info: n_embd_v_gqa     = 2048
0.00.062.356 I print_info: f_norm_eps       = 1.0e-05
0.00.062.357 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.357 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.357 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.357 I print_info: f_logit_scale    = 0.0e+00
0.00.062.357 I print_info: n_ff             = 8192
0.00.062.358 I print_info: n_expert         = 0
0.00.062.358 I print_info: n_expert_used    = 0
0.00.062.358 I print_info: causal attn      = 1
0.00.062.358 I print_info: pooling type     = 0
0.00.062.358 I print_info: rope type        = 2
0.00.062.358 I print_info: rope scaling     = linear
0.00.062.359 I print_info: freq_base_train  = 10000.0
0.00.062.359 I print_info: freq_scale_train = 1
0.00.062.359 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.359 I print_info: rope_finetuned   = unknown
0.00.062.359 I print_info: ssm_d_conv       = 0
0.00.062.359 I print_info: ssm_d_inner      = 0
0.00.062.360 I print_info: ssm_d_state      = 0
0.00.062.360 I print_info: ssm_dt_rank      = 0
0.00.062.360 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.360 I print_info: model type       = 1.4B
0.00.062.362 I print_info: model params     = 1.41 B
0.00.062.362 I print_info: general.name     = 1.4B
0.00.062.363 I print_info: vocab type       = BPE
0.00.062.363 I print_info: n_vocab          = 50304
0.00.062.363 I print_info: n_merges         = 50009
0.00.062.363 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.363 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.363 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.364 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.364 I print_info: LF token         = 128 'Ä'
0.00.062.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.364 I print_info: max token length = 1024
0.00.064.795 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.795 I load_tensors: offloading output layer to GPU
0.00.064.795 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.806 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.064.807 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.065.096 I llama_init_from_model: n_seq_max     = 1
0.00.065.097 I llama_init_from_model: n_ctx         = 128
0.00.065.097 I llama_init_from_model: n_ctx_per_seq = 128
0.00.065.097 I llama_init_from_model: n_batch       = 128
0.00.065.097 I llama_init_from_model: n_ubatch      = 128
0.00.065.097 I llama_init_from_model: flash_attn    = 0
0.00.065.098 I llama_init_from_model: freq_base     = 10000.0
0.00.065.098 I llama_init_from_model: freq_scale    = 1
0.00.065.099 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.099 I ggml_metal_init: allocating
0.00.065.102 I ggml_metal_init: found device: Apple M4
0.00.065.104 I ggml_metal_init: picking default device: Apple M4
0.00.065.715 I ggml_metal_init: using embedded metal library
0.00.068.419 I ggml_metal_init: GPU name:   Apple M4
0.00.068.421 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.422 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.422 I ggml_metal_init: simdgroup reduction   = true
0.00.068.422 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.422 I ggml_metal_init: has bfloat            = true
0.00.068.422 I ggml_metal_init: use bfloat            = true
0.00.068.423 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.423 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.709 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.936 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.938 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.953 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.994 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.995 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.996 I llama_init_from_model: graph nodes  = 967
0.00.080.996 I llama_init_from_model: graph splits = 2
0.00.080.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.121.509 I 
0.01.121.577 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.121.607 I perplexity: tokenizing the input ..
0.01.131.701 I perplexity: tokenization took 10.092 ms
0.01.131.724 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.250.898 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.254.055 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.254.071 I llama_perf_context_print:        load time =    1102.43 ms
0.01.254.072 I llama_perf_context_print: prompt eval time =     118.93 ms /   128 tokens (    0.93 ms per token,  1076.30 tokens per second)
0.01.254.073 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.254.074 I llama_perf_context_print:       total time =     132.57 ms /   129 tokens
0.01.254.910 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.103s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.019 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.025 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.032 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.032 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.033 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.033 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.034 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.034 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.035 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.035 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.035 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.036 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.036 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.983 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.005 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.846 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.850 I llama_model_loader: - type  f32:  194 tensors
0.00.027.851 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.851 I print_info: file format = GGUF V3 (latest)
0.00.027.852 I print_info: file type   = Q8_0
0.00.027.853 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.613 I load: special tokens cache size = 25
0.00.053.766 I load: token to piece cache size = 0.2984 MB
0.00.053.770 I print_info: arch             = gptneox
0.00.053.771 I print_info: vocab_only       = 0
0.00.053.771 I print_info: n_ctx_train      = 2048
0.00.053.771 I print_info: n_embd           = 2048
0.00.053.771 I print_info: n_layer          = 24
0.00.053.777 I print_info: n_head           = 16
0.00.053.778 I print_info: n_head_kv        = 16
0.00.053.778 I print_info: n_rot            = 32
0.00.053.778 I print_info: n_swa            = 0
0.00.053.779 I print_info: n_embd_head_k    = 128
0.00.053.779 I print_info: n_embd_head_v    = 128
0.00.053.779 I print_info: n_gqa            = 1
0.00.053.780 I print_info: n_embd_k_gqa     = 2048
0.00.053.782 I print_info: n_embd_v_gqa     = 2048
0.00.053.783 I print_info: f_norm_eps       = 1.0e-05
0.00.053.783 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.784 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.785 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.785 I print_info: f_logit_scale    = 0.0e+00
0.00.053.786 I print_info: n_ff             = 8192
0.00.053.786 I print_info: n_expert         = 0
0.00.053.787 I print_info: n_expert_used    = 0
0.00.053.787 I print_info: causal attn      = 1
0.00.053.787 I print_info: pooling type     = 0
0.00.053.787 I print_info: rope type        = 2
0.00.053.787 I print_info: rope scaling     = linear
0.00.053.787 I print_info: freq_base_train  = 10000.0
0.00.053.788 I print_info: freq_scale_train = 1
0.00.053.788 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.788 I print_info: rope_finetuned   = unknown
0.00.053.788 I print_info: ssm_d_conv       = 0
0.00.053.788 I print_info: ssm_d_inner      = 0
0.00.053.788 I print_info: ssm_d_state      = 0
0.00.053.788 I print_info: ssm_dt_rank      = 0
0.00.053.789 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.789 I print_info: model type       = 1.4B
0.00.053.789 I print_info: model params     = 1.41 B
0.00.053.789 I print_info: general.name     = 1.4B
0.00.053.790 I print_info: vocab type       = BPE
0.00.053.790 I print_info: n_vocab          = 50304
0.00.053.790 I print_info: n_merges         = 50009
0.00.053.791 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.791 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.791 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.791 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.791 I print_info: LF token         = 128 'Ä'
0.00.053.791 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.792 I print_info: max token length = 1024
0.00.055.888 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.888 I load_tensors: offloading output layer to GPU
0.00.055.888 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.899 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.900 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.056.202 I llama_init_from_model: n_seq_max     = 1
0.00.056.203 I llama_init_from_model: n_ctx         = 128
0.00.056.203 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.203 I llama_init_from_model: n_batch       = 128
0.00.056.204 I llama_init_from_model: n_ubatch      = 128
0.00.056.204 I llama_init_from_model: flash_attn    = 0
0.00.056.204 I llama_init_from_model: freq_base     = 10000.0
0.00.056.204 I llama_init_from_model: freq_scale    = 1
0.00.056.205 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.205 I ggml_metal_init: allocating
0.00.056.209 I ggml_metal_init: found device: Apple M4
0.00.056.211 I ggml_metal_init: picking default device: Apple M4
0.00.056.811 I ggml_metal_init: using embedded metal library
0.00.059.234 I ggml_metal_init: GPU name:   Apple M4
0.00.059.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.238 I ggml_metal_init: simdgroup reduction   = true
0.00.059.238 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.238 I ggml_metal_init: has bfloat            = true
0.00.059.238 I ggml_metal_init: use bfloat            = true
0.00.059.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.239 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.959 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.208 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.210 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.225 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.123 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.124 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.125 I llama_init_from_model: graph nodes  = 967
0.00.071.125 I llama_init_from_model: graph splits = 2
0.00.071.126 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.946 I 
0.00.814.985 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.996 I perplexity: tokenizing the input ..
0.00.826.053 I perplexity: tokenization took 11.056 ms
0.00.826.064 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.950.437 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.951.672 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.951.686 I llama_perf_context_print:        load time =     805.10 ms
0.00.951.687 I llama_perf_context_print: prompt eval time =     124.15 ms /   128 tokens (    0.97 ms per token,  1030.99 tokens per second)
0.00.951.688 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.951.688 I llama_perf_context_print:       total time =     136.74 ms /   129 tokens
0.00.952.227 I ggml_metal_free: deallocating

real	0m0.976s
user	0m0.087s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.772 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.003 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.013 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.015 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.015 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.017 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.017 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.017 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.292 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.487 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.834 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.834 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.834 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.835 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.835 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.837 I llama_model_loader: - type  f32:  194 tensors
0.00.038.837 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.837 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.838 I print_info: file format = GGUF V3 (latest)
0.00.038.839 I print_info: file type   = Q4_0
0.00.038.843 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.823 I load: special tokens cache size = 25
0.00.069.943 I load: token to piece cache size = 0.2984 MB
0.00.069.948 I print_info: arch             = gptneox
0.00.069.948 I print_info: vocab_only       = 0
0.00.069.948 I print_info: n_ctx_train      = 2048
0.00.069.948 I print_info: n_embd           = 2048
0.00.069.948 I print_info: n_layer          = 24
0.00.069.952 I print_info: n_head           = 16
0.00.069.955 I print_info: n_head_kv        = 16
0.00.069.955 I print_info: n_rot            = 32
0.00.069.955 I print_info: n_swa            = 0
0.00.069.955 I print_info: n_embd_head_k    = 128
0.00.069.955 I print_info: n_embd_head_v    = 128
0.00.069.956 I print_info: n_gqa            = 1
0.00.069.957 I print_info: n_embd_k_gqa     = 2048
0.00.069.957 I print_info: n_embd_v_gqa     = 2048
0.00.069.958 I print_info: f_norm_eps       = 1.0e-05
0.00.069.958 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.958 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.958 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.958 I print_info: f_logit_scale    = 0.0e+00
0.00.069.959 I print_info: n_ff             = 8192
0.00.069.959 I print_info: n_expert         = 0
0.00.069.959 I print_info: n_expert_used    = 0
0.00.069.959 I print_info: causal attn      = 1
0.00.069.960 I print_info: pooling type     = 0
0.00.069.960 I print_info: rope type        = 2
0.00.069.960 I print_info: rope scaling     = linear
0.00.069.960 I print_info: freq_base_train  = 10000.0
0.00.069.961 I print_info: freq_scale_train = 1
0.00.069.961 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.961 I print_info: rope_finetuned   = unknown
0.00.069.961 I print_info: ssm_d_conv       = 0
0.00.069.961 I print_info: ssm_d_inner      = 0
0.00.069.961 I print_info: ssm_d_state      = 0
0.00.069.961 I print_info: ssm_dt_rank      = 0
0.00.069.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.962 I print_info: model type       = 1.4B
0.00.069.962 I print_info: model params     = 1.41 B
0.00.069.962 I print_info: general.name     = 1.4B
0.00.069.963 I print_info: vocab type       = BPE
0.00.069.963 I print_info: n_vocab          = 50304
0.00.069.963 I print_info: n_merges         = 50009
0.00.069.963 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.963 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.963 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.964 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.964 I print_info: LF token         = 128 'Ä'
0.00.069.964 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.964 I print_info: max token length = 1024
0.00.072.139 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.140 I load_tensors: offloading output layer to GPU
0.00.072.140 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.150 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.072.152 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.072.452 I llama_init_from_model: n_seq_max     = 1
0.00.072.453 I llama_init_from_model: n_ctx         = 128
0.00.072.454 I llama_init_from_model: n_ctx_per_seq = 128
0.00.072.454 I llama_init_from_model: n_batch       = 128
0.00.072.454 I llama_init_from_model: n_ubatch      = 128
0.00.072.454 I llama_init_from_model: flash_attn    = 0
0.00.072.454 I llama_init_from_model: freq_base     = 10000.0
0.00.072.455 I llama_init_from_model: freq_scale    = 1
0.00.072.455 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.455 I ggml_metal_init: allocating
0.00.072.459 I ggml_metal_init: found device: Apple M4
0.00.072.461 I ggml_metal_init: picking default device: Apple M4
0.00.073.114 I ggml_metal_init: using embedded metal library
0.00.075.919 I ggml_metal_init: GPU name:   Apple M4
0.00.075.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.922 I ggml_metal_init: simdgroup reduction   = true
0.00.075.922 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.922 I ggml_metal_init: has bfloat            = true
0.00.075.922 I ggml_metal_init: use bfloat            = true
0.00.075.923 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.518 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.881 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.088.883 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.088.898 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.068 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.090.069 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.090.070 I llama_init_from_model: graph nodes  = 967
0.00.090.070 I llama_init_from_model: graph splits = 2
0.00.090.071 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.090.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.921 I 
0.00.689.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.998 I perplexity: tokenizing the input ..
0.00.698.428 I perplexity: tokenization took 8.427 ms
0.00.698.444 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.248 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.822.467 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.822.492 I llama_perf_context_print:        load time =     672.14 ms
0.00.822.493 I llama_perf_context_print: prompt eval time =     122.58 ms /   128 tokens (    0.96 ms per token,  1044.22 tokens per second)
0.00.822.494 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.822.495 I llama_perf_context_print:       total time =     132.58 ms /   129 tokens
0.00.822.982 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.087s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.723 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.110 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.114 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.115 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.116 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.116 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.116 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.116 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.117 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.118 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.118 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.119 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.119 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.119 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.120 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.122 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.123 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.123 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.817 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.780 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.537 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.539 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.539 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.540 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.540 I llama_model_loader: - type  f32:  194 tensors
0.00.031.540 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.541 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.541 I print_info: file format = GGUF V3 (latest)
0.00.031.542 I print_info: file type   = Q4_1
0.00.031.542 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.051.699 I load: special tokens cache size = 25
0.00.057.839 I load: token to piece cache size = 0.2984 MB
0.00.057.842 I print_info: arch             = gptneox
0.00.057.842 I print_info: vocab_only       = 0
0.00.057.842 I print_info: n_ctx_train      = 2048
0.00.057.842 I print_info: n_embd           = 2048
0.00.057.843 I print_info: n_layer          = 24
0.00.057.845 I print_info: n_head           = 16
0.00.057.846 I print_info: n_head_kv        = 16
0.00.057.846 I print_info: n_rot            = 32
0.00.057.853 I print_info: n_swa            = 0
0.00.057.854 I print_info: n_embd_head_k    = 128
0.00.057.854 I print_info: n_embd_head_v    = 128
0.00.057.856 I print_info: n_gqa            = 1
0.00.057.857 I print_info: n_embd_k_gqa     = 2048
0.00.057.858 I print_info: n_embd_v_gqa     = 2048
0.00.057.859 I print_info: f_norm_eps       = 1.0e-05
0.00.057.859 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.859 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.859 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.859 I print_info: f_logit_scale    = 0.0e+00
0.00.057.860 I print_info: n_ff             = 8192
0.00.057.865 I print_info: n_expert         = 0
0.00.057.867 I print_info: n_expert_used    = 0
0.00.057.867 I print_info: causal attn      = 1
0.00.057.867 I print_info: pooling type     = 0
0.00.057.867 I print_info: rope type        = 2
0.00.057.867 I print_info: rope scaling     = linear
0.00.057.868 I print_info: freq_base_train  = 10000.0
0.00.057.868 I print_info: freq_scale_train = 1
0.00.057.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.868 I print_info: rope_finetuned   = unknown
0.00.057.869 I print_info: ssm_d_conv       = 0
0.00.057.869 I print_info: ssm_d_inner      = 0
0.00.057.870 I print_info: ssm_d_state      = 0
0.00.057.870 I print_info: ssm_dt_rank      = 0
0.00.057.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.870 I print_info: model type       = 1.4B
0.00.057.871 I print_info: model params     = 1.41 B
0.00.057.871 I print_info: general.name     = 1.4B
0.00.057.872 I print_info: vocab type       = BPE
0.00.057.872 I print_info: n_vocab          = 50304
0.00.057.872 I print_info: n_merges         = 50009
0.00.057.873 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.873 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.873 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.873 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.874 I print_info: LF token         = 128 'Ä'
0.00.057.874 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.874 I print_info: max token length = 1024
0.00.059.817 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.817 I load_tensors: offloading output layer to GPU
0.00.059.817 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.828 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.059.829 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.060.099 I llama_init_from_model: n_seq_max     = 1
0.00.060.100 I llama_init_from_model: n_ctx         = 128
0.00.060.100 I llama_init_from_model: n_ctx_per_seq = 128
0.00.060.100 I llama_init_from_model: n_batch       = 128
0.00.060.100 I llama_init_from_model: n_ubatch      = 128
0.00.060.100 I llama_init_from_model: flash_attn    = 0
0.00.060.101 I llama_init_from_model: freq_base     = 10000.0
0.00.060.101 I llama_init_from_model: freq_scale    = 1
0.00.060.101 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.060.102 I ggml_metal_init: allocating
0.00.060.105 I ggml_metal_init: found device: Apple M4
0.00.060.107 I ggml_metal_init: picking default device: Apple M4
0.00.060.691 I ggml_metal_init: using embedded metal library
0.00.063.075 I ggml_metal_init: GPU name:   Apple M4
0.00.063.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.077 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.077 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.078 I ggml_metal_init: simdgroup reduction   = true
0.00.063.078 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.078 I ggml_metal_init: has bfloat            = true
0.00.063.078 I ggml_metal_init: use bfloat            = true
0.00.063.078 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.079 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.170 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.073.421 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.073.423 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.073.436 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.074.367 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.074.368 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.074.369 I llama_init_from_model: graph nodes  = 967
0.00.074.369 I llama_init_from_model: graph splits = 2
0.00.074.370 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.074.370 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.025 I 
0.00.697.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.072 I perplexity: tokenizing the input ..
0.00.705.145 I perplexity: tokenization took 8.073 ms
0.00.705.155 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.934 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.829.200 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.829.217 I llama_perf_context_print:        load time =     688.30 ms
0.00.829.219 I llama_perf_context_print: prompt eval time =     122.53 ms /   128 tokens (    0.96 ms per token,  1044.63 tokens per second)
0.00.829.219 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.220 I llama_perf_context_print:       total time =     132.19 ms /   129 tokens
0.00.829.788 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.078s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.258 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.043 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.050 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.051 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.051 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.052 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.052 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.053 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.054 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.054 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.055 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.055 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.056 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.057 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.057 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.146 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.267 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.355 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.357 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.357 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.358 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.037.358 I llama_model_loader: - type  f32:  194 tensors
0.00.037.358 I llama_model_loader: - type q5_0:   97 tensors
0.00.037.359 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.359 I print_info: file format = GGUF V3 (latest)
0.00.037.359 I print_info: file type   = Q5_0
0.00.037.360 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.058.747 I load: special tokens cache size = 25
0.00.064.947 I load: token to piece cache size = 0.2984 MB
0.00.064.951 I print_info: arch             = gptneox
0.00.064.952 I print_info: vocab_only       = 0
0.00.064.952 I print_info: n_ctx_train      = 2048
0.00.064.953 I print_info: n_embd           = 2048
0.00.064.953 I print_info: n_layer          = 24
0.00.064.956 I print_info: n_head           = 16
0.00.064.957 I print_info: n_head_kv        = 16
0.00.064.957 I print_info: n_rot            = 32
0.00.064.957 I print_info: n_swa            = 0
0.00.064.957 I print_info: n_embd_head_k    = 128
0.00.064.957 I print_info: n_embd_head_v    = 128
0.00.064.958 I print_info: n_gqa            = 1
0.00.064.959 I print_info: n_embd_k_gqa     = 2048
0.00.064.960 I print_info: n_embd_v_gqa     = 2048
0.00.064.960 I print_info: f_norm_eps       = 1.0e-05
0.00.064.960 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.961 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.961 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.961 I print_info: f_logit_scale    = 0.0e+00
0.00.064.962 I print_info: n_ff             = 8192
0.00.064.962 I print_info: n_expert         = 0
0.00.064.963 I print_info: n_expert_used    = 0
0.00.064.963 I print_info: causal attn      = 1
0.00.064.963 I print_info: pooling type     = 0
0.00.064.963 I print_info: rope type        = 2
0.00.064.964 I print_info: rope scaling     = linear
0.00.064.964 I print_info: freq_base_train  = 10000.0
0.00.064.964 I print_info: freq_scale_train = 1
0.00.064.964 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.964 I print_info: rope_finetuned   = unknown
0.00.064.965 I print_info: ssm_d_conv       = 0
0.00.064.965 I print_info: ssm_d_inner      = 0
0.00.064.965 I print_info: ssm_d_state      = 0
0.00.064.965 I print_info: ssm_dt_rank      = 0
0.00.064.965 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.965 I print_info: model type       = 1.4B
0.00.064.965 I print_info: model params     = 1.41 B
0.00.064.966 I print_info: general.name     = 1.4B
0.00.064.969 I print_info: vocab type       = BPE
0.00.064.969 I print_info: n_vocab          = 50304
0.00.064.970 I print_info: n_merges         = 50009
0.00.064.970 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.970 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.970 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.970 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.970 I print_info: LF token         = 128 'Ä'
0.00.064.971 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.971 I print_info: max token length = 1024
0.00.067.062 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.062 I load_tensors: offloading output layer to GPU
0.00.067.062 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.073 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.067.074 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.067.436 I llama_init_from_model: n_seq_max     = 1
0.00.067.436 I llama_init_from_model: n_ctx         = 128
0.00.067.436 I llama_init_from_model: n_ctx_per_seq = 128
0.00.067.437 I llama_init_from_model: n_batch       = 128
0.00.067.437 I llama_init_from_model: n_ubatch      = 128
0.00.067.437 I llama_init_from_model: flash_attn    = 0
0.00.067.437 I llama_init_from_model: freq_base     = 10000.0
0.00.067.438 I llama_init_from_model: freq_scale    = 1
0.00.067.438 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.438 I ggml_metal_init: allocating
0.00.067.441 I ggml_metal_init: found device: Apple M4
0.00.067.443 I ggml_metal_init: picking default device: Apple M4
0.00.068.051 I ggml_metal_init: using embedded metal library
0.00.070.455 I ggml_metal_init: GPU name:   Apple M4
0.00.070.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.457 I ggml_metal_init: simdgroup reduction   = true
0.00.070.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.457 I ggml_metal_init: has bfloat            = true
0.00.070.457 I ggml_metal_init: use bfloat            = true
0.00.070.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.005 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.080.299 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.301 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.315 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.081.267 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.081.268 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.081.269 I llama_init_from_model: graph nodes  = 967
0.00.081.269 I llama_init_from_model: graph splits = 2
0.00.081.270 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.499 I 
0.00.655.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.557 I perplexity: tokenizing the input ..
0.00.663.822 I perplexity: tokenization took 8.263 ms
0.00.663.837 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.027 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.800.194 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.800.221 I llama_perf_context_print:        load time =     637.24 ms
0.00.800.222 I llama_perf_context_print: prompt eval time =     134.96 ms /   128 tokens (    1.05 ms per token,   948.41 tokens per second)
0.00.800.222 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.223 I llama_perf_context_print:       total time =     144.72 ms /   129 tokens
0.00.800.681 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.080s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.471 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.492 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.498 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.499 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.500 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.500 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.500 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.505 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.506 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.506 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.724 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.725 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.726 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.726 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.726 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.727 I llama_model_loader: - type  f32:  194 tensors
0.00.033.727 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.727 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.728 I print_info: file format = GGUF V3 (latest)
0.00.033.728 I print_info: file type   = Q5_1
0.00.033.733 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.052.881 I load: special tokens cache size = 25
0.00.058.947 I load: token to piece cache size = 0.2984 MB
0.00.058.950 I print_info: arch             = gptneox
0.00.058.950 I print_info: vocab_only       = 0
0.00.058.950 I print_info: n_ctx_train      = 2048
0.00.058.951 I print_info: n_embd           = 2048
0.00.058.951 I print_info: n_layer          = 24
0.00.058.954 I print_info: n_head           = 16
0.00.058.954 I print_info: n_head_kv        = 16
0.00.058.954 I print_info: n_rot            = 32
0.00.058.957 I print_info: n_swa            = 0
0.00.058.957 I print_info: n_embd_head_k    = 128
0.00.058.957 I print_info: n_embd_head_v    = 128
0.00.058.958 I print_info: n_gqa            = 1
0.00.058.958 I print_info: n_embd_k_gqa     = 2048
0.00.058.959 I print_info: n_embd_v_gqa     = 2048
0.00.058.964 I print_info: f_norm_eps       = 1.0e-05
0.00.058.964 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.964 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.964 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.965 I print_info: f_logit_scale    = 0.0e+00
0.00.058.965 I print_info: n_ff             = 8192
0.00.058.966 I print_info: n_expert         = 0
0.00.058.966 I print_info: n_expert_used    = 0
0.00.058.966 I print_info: causal attn      = 1
0.00.058.966 I print_info: pooling type     = 0
0.00.058.969 I print_info: rope type        = 2
0.00.058.970 I print_info: rope scaling     = linear
0.00.058.970 I print_info: freq_base_train  = 10000.0
0.00.058.970 I print_info: freq_scale_train = 1
0.00.058.971 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.971 I print_info: rope_finetuned   = unknown
0.00.058.971 I print_info: ssm_d_conv       = 0
0.00.058.971 I print_info: ssm_d_inner      = 0
0.00.058.971 I print_info: ssm_d_state      = 0
0.00.058.971 I print_info: ssm_dt_rank      = 0
0.00.058.972 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.972 I print_info: model type       = 1.4B
0.00.058.972 I print_info: model params     = 1.41 B
0.00.058.972 I print_info: general.name     = 1.4B
0.00.058.973 I print_info: vocab type       = BPE
0.00.058.973 I print_info: n_vocab          = 50304
0.00.058.973 I print_info: n_merges         = 50009
0.00.058.973 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.973 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.974 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.974 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.975 I print_info: LF token         = 128 'Ä'
0.00.058.975 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.975 I print_info: max token length = 1024
0.00.060.924 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.924 I load_tensors: offloading output layer to GPU
0.00.060.924 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.935 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.060.936 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.061.238 I llama_init_from_model: n_seq_max     = 1
0.00.061.238 I llama_init_from_model: n_ctx         = 128
0.00.061.238 I llama_init_from_model: n_ctx_per_seq = 128
0.00.061.239 I llama_init_from_model: n_batch       = 128
0.00.061.239 I llama_init_from_model: n_ubatch      = 128
0.00.061.239 I llama_init_from_model: flash_attn    = 0
0.00.061.239 I llama_init_from_model: freq_base     = 10000.0
0.00.061.239 I llama_init_from_model: freq_scale    = 1
0.00.061.240 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.061.240 I ggml_metal_init: allocating
0.00.061.244 I ggml_metal_init: found device: Apple M4
0.00.061.245 I ggml_metal_init: picking default device: Apple M4
0.00.061.819 I ggml_metal_init: using embedded metal library
0.00.064.156 I ggml_metal_init: GPU name:   Apple M4
0.00.064.158 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.158 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.158 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.158 I ggml_metal_init: simdgroup reduction   = true
0.00.064.159 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.159 I ggml_metal_init: has bfloat            = true
0.00.064.159 I ggml_metal_init: use bfloat            = true
0.00.064.159 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.160 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.671 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.074.857 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.859 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.873 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.075.797 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.075.799 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.075.799 I llama_init_from_model: graph nodes  = 967
0.00.075.799 I llama_init_from_model: graph splits = 2
0.00.075.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.075.801 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.862.238 I 
0.00.862.288 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.862.304 I perplexity: tokenizing the input ..
0.00.870.714 I perplexity: tokenization took 8.407 ms
0.00.870.725 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.006.125 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.01.007.415 I Final estimate: PPL = 10.1971 +/- 3.18866

0.01.007.430 I llama_perf_context_print:        load time =     846.76 ms
0.01.007.431 I llama_perf_context_print: prompt eval time =     135.15 ms /   128 tokens (    1.06 ms per token,   947.08 tokens per second)
0.01.007.432 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.007.432 I llama_perf_context_print:       total time =     145.19 ms /   129 tokens
0.01.007.789 I ggml_metal_free: deallocating

real	0m1.021s
user	0m0.078s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.990 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.086 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.027.092 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.093 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.094 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.096 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.096 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.097 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.921 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.922 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.923 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.923 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.923 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.036.924 I llama_model_loader: - type  f32:  194 tensors
0.00.036.925 I llama_model_loader: - type q2_K:   49 tensors
0.00.036.925 I llama_model_loader: - type q3_K:   48 tensors
0.00.036.925 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.926 I print_info: file format = GGUF V3 (latest)
0.00.036.926 I print_info: file type   = Q2_K - Medium
0.00.036.927 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.059.974 I load: special tokens cache size = 25
0.00.066.642 I load: token to piece cache size = 0.2984 MB
0.00.066.645 I print_info: arch             = gptneox
0.00.066.645 I print_info: vocab_only       = 0
0.00.066.645 I print_info: n_ctx_train      = 2048
0.00.066.646 I print_info: n_embd           = 2048
0.00.066.646 I print_info: n_layer          = 24
0.00.066.649 I print_info: n_head           = 16
0.00.066.650 I print_info: n_head_kv        = 16
0.00.066.650 I print_info: n_rot            = 32
0.00.066.650 I print_info: n_swa            = 0
0.00.066.650 I print_info: n_embd_head_k    = 128
0.00.066.651 I print_info: n_embd_head_v    = 128
0.00.066.652 I print_info: n_gqa            = 1
0.00.066.653 I print_info: n_embd_k_gqa     = 2048
0.00.066.653 I print_info: n_embd_v_gqa     = 2048
0.00.066.654 I print_info: f_norm_eps       = 1.0e-05
0.00.066.654 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.654 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.655 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.655 I print_info: f_logit_scale    = 0.0e+00
0.00.066.655 I print_info: n_ff             = 8192
0.00.066.655 I print_info: n_expert         = 0
0.00.066.656 I print_info: n_expert_used    = 0
0.00.066.656 I print_info: causal attn      = 1
0.00.066.656 I print_info: pooling type     = 0
0.00.066.656 I print_info: rope type        = 2
0.00.066.656 I print_info: rope scaling     = linear
0.00.066.657 I print_info: freq_base_train  = 10000.0
0.00.066.657 I print_info: freq_scale_train = 1
0.00.066.657 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.657 I print_info: rope_finetuned   = unknown
0.00.066.657 I print_info: ssm_d_conv       = 0
0.00.066.657 I print_info: ssm_d_inner      = 0
0.00.066.658 I print_info: ssm_d_state      = 0
0.00.066.658 I print_info: ssm_dt_rank      = 0
0.00.066.658 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.658 I print_info: model type       = 1.4B
0.00.066.658 I print_info: model params     = 1.41 B
0.00.066.659 I print_info: general.name     = 1.4B
0.00.066.659 I print_info: vocab type       = BPE
0.00.066.659 I print_info: n_vocab          = 50304
0.00.066.659 I print_info: n_merges         = 50009
0.00.066.660 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.660 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.660 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.660 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.660 I print_info: LF token         = 128 'Ä'
0.00.066.661 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.661 I print_info: max token length = 1024
0.00.068.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.741 I load_tensors: offloading output layer to GPU
0.00.068.742 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.752 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.068.753 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.069.048 I llama_init_from_model: n_seq_max     = 1
0.00.069.049 I llama_init_from_model: n_ctx         = 128
0.00.069.049 I llama_init_from_model: n_ctx_per_seq = 128
0.00.069.049 I llama_init_from_model: n_batch       = 128
0.00.069.049 I llama_init_from_model: n_ubatch      = 128
0.00.069.049 I llama_init_from_model: flash_attn    = 0
0.00.069.050 I llama_init_from_model: freq_base     = 10000.0
0.00.069.050 I llama_init_from_model: freq_scale    = 1
0.00.069.050 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.051 I ggml_metal_init: allocating
0.00.069.054 I ggml_metal_init: found device: Apple M4
0.00.069.056 I ggml_metal_init: picking default device: Apple M4
0.00.069.676 I ggml_metal_init: using embedded metal library
0.00.072.330 I ggml_metal_init: GPU name:   Apple M4
0.00.072.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.332 I ggml_metal_init: simdgroup reduction   = true
0.00.072.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.333 I ggml_metal_init: has bfloat            = true
0.00.072.333 I ggml_metal_init: use bfloat            = true
0.00.072.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.334 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.648 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.189 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.194 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.211 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.367 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.085.368 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.085.369 I llama_init_from_model: graph nodes  = 967
0.00.085.369 I llama_init_from_model: graph splits = 2
0.00.085.370 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.370 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.456.212 I 
0.00.456.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.456.266 I perplexity: tokenizing the input ..
0.00.464.323 I perplexity: tokenization took 8.056 ms
0.00.464.335 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.596.977 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.598.183 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.598.198 I llama_perf_context_print:        load time =     436.22 ms
0.00.598.199 I llama_perf_context_print: prompt eval time =     132.42 ms /   128 tokens (    1.03 ms per token,   966.64 tokens per second)
0.00.598.200 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.598.200 I llama_perf_context_print:       total time =     141.99 ms /   129 tokens
0.00.598.654 I ggml_metal_free: deallocating

real	0m0.613s
user	0m0.085s
sys	0m0.068s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.953 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.636 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.022.640 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.641 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.642 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.642 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.642 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.643 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.644 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.644 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.644 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.646 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.648 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.648 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.649 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.650 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.651 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.652 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.662 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.750 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.751 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.031.752 I llama_model_loader: - type  f32:  194 tensors
0.00.031.752 I llama_model_loader: - type q3_K:   25 tensors
0.00.031.752 I llama_model_loader: - type q4_K:   71 tensors
0.00.031.752 I llama_model_loader: - type q5_K:    1 tensors
0.00.031.752 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.753 I print_info: file format = GGUF V3 (latest)
0.00.031.753 I print_info: file type   = Q3_K - Medium
0.00.031.754 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.051.116 I load: special tokens cache size = 25
0.00.057.338 I load: token to piece cache size = 0.2984 MB
0.00.057.341 I print_info: arch             = gptneox
0.00.057.341 I print_info: vocab_only       = 0
0.00.057.342 I print_info: n_ctx_train      = 2048
0.00.057.342 I print_info: n_embd           = 2048
0.00.057.342 I print_info: n_layer          = 24
0.00.057.345 I print_info: n_head           = 16
0.00.057.346 I print_info: n_head_kv        = 16
0.00.057.346 I print_info: n_rot            = 32
0.00.057.346 I print_info: n_swa            = 0
0.00.057.346 I print_info: n_embd_head_k    = 128
0.00.057.349 I print_info: n_embd_head_v    = 128
0.00.057.349 I print_info: n_gqa            = 1
0.00.057.350 I print_info: n_embd_k_gqa     = 2048
0.00.057.351 I print_info: n_embd_v_gqa     = 2048
0.00.057.351 I print_info: f_norm_eps       = 1.0e-05
0.00.057.352 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.352 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.352 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.352 I print_info: f_logit_scale    = 0.0e+00
0.00.057.353 I print_info: n_ff             = 8192
0.00.057.353 I print_info: n_expert         = 0
0.00.057.353 I print_info: n_expert_used    = 0
0.00.057.353 I print_info: causal attn      = 1
0.00.057.353 I print_info: pooling type     = 0
0.00.057.353 I print_info: rope type        = 2
0.00.057.354 I print_info: rope scaling     = linear
0.00.057.354 I print_info: freq_base_train  = 10000.0
0.00.057.358 I print_info: freq_scale_train = 1
0.00.057.358 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.359 I print_info: rope_finetuned   = unknown
0.00.057.359 I print_info: ssm_d_conv       = 0
0.00.057.359 I print_info: ssm_d_inner      = 0
0.00.057.359 I print_info: ssm_d_state      = 0
0.00.057.359 I print_info: ssm_dt_rank      = 0
0.00.057.359 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.360 I print_info: model type       = 1.4B
0.00.057.360 I print_info: model params     = 1.41 B
0.00.057.360 I print_info: general.name     = 1.4B
0.00.057.361 I print_info: vocab type       = BPE
0.00.057.361 I print_info: n_vocab          = 50304
0.00.057.361 I print_info: n_merges         = 50009
0.00.057.362 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.362 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.362 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.362 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.362 I print_info: LF token         = 128 'Ä'
0.00.057.368 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.369 I print_info: max token length = 1024
0.00.059.222 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.223 I load_tensors: offloading output layer to GPU
0.00.059.223 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.233 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.059.234 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.059.510 I llama_init_from_model: n_seq_max     = 1
0.00.059.510 I llama_init_from_model: n_ctx         = 128
0.00.059.510 I llama_init_from_model: n_ctx_per_seq = 128
0.00.059.511 I llama_init_from_model: n_batch       = 128
0.00.059.511 I llama_init_from_model: n_ubatch      = 128
0.00.059.511 I llama_init_from_model: flash_attn    = 0
0.00.059.511 I llama_init_from_model: freq_base     = 10000.0
0.00.059.511 I llama_init_from_model: freq_scale    = 1
0.00.059.512 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.059.512 I ggml_metal_init: allocating
0.00.059.515 I ggml_metal_init: found device: Apple M4
0.00.059.517 I ggml_metal_init: picking default device: Apple M4
0.00.060.068 I ggml_metal_init: using embedded metal library
0.00.064.203 I ggml_metal_init: GPU name:   Apple M4
0.00.064.204 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.205 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.205 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.205 I ggml_metal_init: simdgroup reduction   = true
0.00.064.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.206 I ggml_metal_init: has bfloat            = true
0.00.064.206 I ggml_metal_init: use bfloat            = true
0.00.064.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.580 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.074.806 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.811 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.823 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.075.709 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.075.710 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.075.710 I llama_init_from_model: graph nodes  = 967
0.00.075.710 I llama_init_from_model: graph splits = 2
0.00.075.712 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.075.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.307 I 
0.00.570.368 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.570.384 I perplexity: tokenizing the input ..
0.00.578.592 I perplexity: tokenization took 8.207 ms
0.00.578.603 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.068 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.712.366 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.712.374 I llama_perf_context_print:        load time =     561.35 ms
0.00.712.376 I llama_perf_context_print: prompt eval time =     132.22 ms /   128 tokens (    1.03 ms per token,   968.08 tokens per second)
0.00.712.376 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.712.377 I llama_perf_context_print:       total time =     142.07 ms /   129 tokens
0.00.712.942 I ggml_metal_free: deallocating

real	0m0.727s
user	0m0.078s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.344 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.533 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.023.539 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.540 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.541 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.541 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.542 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.543 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.546 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.546 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.546 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.547 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.549 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.549 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.704 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.706 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.706 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.706 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.706 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.707 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.032.707 I llama_model_loader: - type  f32:  194 tensors
0.00.032.708 I llama_model_loader: - type q4_K:   61 tensors
0.00.032.708 I llama_model_loader: - type q5_K:   24 tensors
0.00.032.708 I llama_model_loader: - type q6_K:   13 tensors
0.00.032.709 I print_info: file format = GGUF V3 (latest)
0.00.032.709 I print_info: file type   = Q4_K - Medium
0.00.032.710 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.054.085 I load: special tokens cache size = 25
0.00.059.877 I load: token to piece cache size = 0.2984 MB
0.00.059.879 I print_info: arch             = gptneox
0.00.059.880 I print_info: vocab_only       = 0
0.00.059.880 I print_info: n_ctx_train      = 2048
0.00.059.880 I print_info: n_embd           = 2048
0.00.059.880 I print_info: n_layer          = 24
0.00.059.884 I print_info: n_head           = 16
0.00.059.885 I print_info: n_head_kv        = 16
0.00.059.885 I print_info: n_rot            = 32
0.00.059.885 I print_info: n_swa            = 0
0.00.059.885 I print_info: n_embd_head_k    = 128
0.00.059.885 I print_info: n_embd_head_v    = 128
0.00.059.886 I print_info: n_gqa            = 1
0.00.059.887 I print_info: n_embd_k_gqa     = 2048
0.00.059.887 I print_info: n_embd_v_gqa     = 2048
0.00.059.888 I print_info: f_norm_eps       = 1.0e-05
0.00.059.889 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.889 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.889 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.889 I print_info: f_logit_scale    = 0.0e+00
0.00.059.890 I print_info: n_ff             = 8192
0.00.059.890 I print_info: n_expert         = 0
0.00.059.890 I print_info: n_expert_used    = 0
0.00.059.890 I print_info: causal attn      = 1
0.00.059.890 I print_info: pooling type     = 0
0.00.059.891 I print_info: rope type        = 2
0.00.059.891 I print_info: rope scaling     = linear
0.00.059.891 I print_info: freq_base_train  = 10000.0
0.00.059.892 I print_info: freq_scale_train = 1
0.00.059.892 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.894 I print_info: rope_finetuned   = unknown
0.00.059.894 I print_info: ssm_d_conv       = 0
0.00.059.894 I print_info: ssm_d_inner      = 0
0.00.059.894 I print_info: ssm_d_state      = 0
0.00.059.894 I print_info: ssm_dt_rank      = 0
0.00.059.895 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.895 I print_info: model type       = 1.4B
0.00.059.895 I print_info: model params     = 1.41 B
0.00.059.895 I print_info: general.name     = 1.4B
0.00.059.896 I print_info: vocab type       = BPE
0.00.059.896 I print_info: n_vocab          = 50304
0.00.059.896 I print_info: n_merges         = 50009
0.00.059.900 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.901 I print_info: LF token         = 128 'Ä'
0.00.059.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.902 I print_info: max token length = 1024
0.00.061.666 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.666 I load_tensors: offloading output layer to GPU
0.00.061.667 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.672 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.061.673 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.061.948 I llama_init_from_model: n_seq_max     = 1
0.00.061.949 I llama_init_from_model: n_ctx         = 128
0.00.061.949 I llama_init_from_model: n_ctx_per_seq = 128
0.00.061.949 I llama_init_from_model: n_batch       = 128
0.00.061.949 I llama_init_from_model: n_ubatch      = 128
0.00.061.949 I llama_init_from_model: flash_attn    = 0
0.00.061.950 I llama_init_from_model: freq_base     = 10000.0
0.00.061.950 I llama_init_from_model: freq_scale    = 1
0.00.061.951 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.061.951 I ggml_metal_init: allocating
0.00.061.954 I ggml_metal_init: found device: Apple M4
0.00.061.956 I ggml_metal_init: picking default device: Apple M4
0.00.062.505 I ggml_metal_init: using embedded metal library
0.00.064.971 I ggml_metal_init: GPU name:   Apple M4
0.00.064.973 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.973 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.974 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.974 I ggml_metal_init: simdgroup reduction   = true
0.00.064.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.974 I ggml_metal_init: has bfloat            = true
0.00.064.974 I ggml_metal_init: use bfloat            = true
0.00.064.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.389 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.075.590 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.594 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.608 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.076.505 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.076.507 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.076.507 I llama_init_from_model: graph nodes  = 967
0.00.076.507 I llama_init_from_model: graph splits = 2
0.00.076.508 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.076.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.151 I 
0.00.711.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.241 I perplexity: tokenizing the input ..
0.00.719.295 I perplexity: tokenization took 8.053 ms
0.00.719.311 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.667 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.854.831 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.854.853 I llama_perf_context_print:        load time =     701.80 ms
0.00.854.855 I llama_perf_context_print: prompt eval time =     134.12 ms /   128 tokens (    1.05 ms per token,   954.36 tokens per second)
0.00.854.856 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.856 I llama_perf_context_print:       total time =     143.71 ms /   129 tokens
0.00.855.356 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.081s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.896 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.721 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.023.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.729 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.729 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.730 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.730 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.731 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.731 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.733 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.733 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.734 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.799 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.919 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.927 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.928 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.929 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.929 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.930 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.032.930 I llama_model_loader: - type  f32:  194 tensors
0.00.032.930 I llama_model_loader: - type q5_K:   61 tensors
0.00.032.930 I llama_model_loader: - type q6_K:   37 tensors
0.00.032.931 I print_info: file format = GGUF V3 (latest)
0.00.032.931 I print_info: file type   = Q5_K - Medium
0.00.032.936 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.056.560 I load: special tokens cache size = 25
0.00.064.123 I load: token to piece cache size = 0.2984 MB
0.00.064.126 I print_info: arch             = gptneox
0.00.064.126 I print_info: vocab_only       = 0
0.00.064.127 I print_info: n_ctx_train      = 2048
0.00.064.127 I print_info: n_embd           = 2048
0.00.064.127 I print_info: n_layer          = 24
0.00.064.130 I print_info: n_head           = 16
0.00.064.131 I print_info: n_head_kv        = 16
0.00.064.131 I print_info: n_rot            = 32
0.00.064.131 I print_info: n_swa            = 0
0.00.064.131 I print_info: n_embd_head_k    = 128
0.00.064.132 I print_info: n_embd_head_v    = 128
0.00.064.133 I print_info: n_gqa            = 1
0.00.064.134 I print_info: n_embd_k_gqa     = 2048
0.00.064.135 I print_info: n_embd_v_gqa     = 2048
0.00.064.135 I print_info: f_norm_eps       = 1.0e-05
0.00.064.137 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.137 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.137 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.138 I print_info: f_logit_scale    = 0.0e+00
0.00.064.138 I print_info: n_ff             = 8192
0.00.064.138 I print_info: n_expert         = 0
0.00.064.139 I print_info: n_expert_used    = 0
0.00.064.139 I print_info: causal attn      = 1
0.00.064.139 I print_info: pooling type     = 0
0.00.064.139 I print_info: rope type        = 2
0.00.064.139 I print_info: rope scaling     = linear
0.00.064.140 I print_info: freq_base_train  = 10000.0
0.00.064.140 I print_info: freq_scale_train = 1
0.00.064.140 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.140 I print_info: rope_finetuned   = unknown
0.00.064.140 I print_info: ssm_d_conv       = 0
0.00.064.141 I print_info: ssm_d_inner      = 0
0.00.064.141 I print_info: ssm_d_state      = 0
0.00.064.141 I print_info: ssm_dt_rank      = 0
0.00.064.141 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.141 I print_info: model type       = 1.4B
0.00.064.143 I print_info: model params     = 1.41 B
0.00.064.143 I print_info: general.name     = 1.4B
0.00.064.143 I print_info: vocab type       = BPE
0.00.064.144 I print_info: n_vocab          = 50304
0.00.064.144 I print_info: n_merges         = 50009
0.00.064.145 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.145 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.145 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.146 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.146 I print_info: LF token         = 128 'Ä'
0.00.064.146 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.146 I print_info: max token length = 1024
0.00.066.429 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.429 I load_tensors: offloading output layer to GPU
0.00.066.429 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.440 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.066.442 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.066.785 I llama_init_from_model: n_seq_max     = 1
0.00.066.786 I llama_init_from_model: n_ctx         = 128
0.00.066.787 I llama_init_from_model: n_ctx_per_seq = 128
0.00.066.787 I llama_init_from_model: n_batch       = 128
0.00.066.787 I llama_init_from_model: n_ubatch      = 128
0.00.066.787 I llama_init_from_model: flash_attn    = 0
0.00.066.788 I llama_init_from_model: freq_base     = 10000.0
0.00.066.788 I llama_init_from_model: freq_scale    = 1
0.00.066.788 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.789 I ggml_metal_init: allocating
0.00.066.792 I ggml_metal_init: found device: Apple M4
0.00.066.794 I ggml_metal_init: picking default device: Apple M4
0.00.067.433 I ggml_metal_init: using embedded metal library
0.00.070.309 I ggml_metal_init: GPU name:   Apple M4
0.00.070.311 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.312 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.312 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.312 I ggml_metal_init: simdgroup reduction   = true
0.00.070.312 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.313 I ggml_metal_init: has bfloat            = true
0.00.070.313 I ggml_metal_init: use bfloat            = true
0.00.070.313 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.314 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.839 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.156 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.158 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.172 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.082.059 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.082.060 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.082.060 I llama_init_from_model: graph nodes  = 967
0.00.082.060 I llama_init_from_model: graph splits = 2
0.00.082.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.999 I 
0.00.694.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.053 I perplexity: tokenizing the input ..
0.00.702.567 I perplexity: tokenization took 8.512 ms
0.00.702.581 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.580 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.844.748 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.844.760 I llama_perf_context_print:        load time =     677.10 ms
0.00.844.762 I llama_perf_context_print: prompt eval time =     140.75 ms /   128 tokens (    1.10 ms per token,   909.41 tokens per second)
0.00.844.763 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.844.763 I llama_perf_context_print:       total time =     150.76 ms /   129 tokens
0.00.845.095 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.086s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.976 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.830 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.835 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.837 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.839 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.839 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.840 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.840 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.840 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.841 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.841 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.843 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.843 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.837 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.889 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.889 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.889 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.890 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.890 I llama_model_loader: - type  f32:  194 tensors
0.00.024.890 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.891 I print_info: file format = GGUF V3 (latest)
0.00.024.891 I print_info: file type   = Q6_K
0.00.024.892 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.152 I load: special tokens cache size = 25
0.00.050.289 I load: token to piece cache size = 0.2984 MB
0.00.050.292 I print_info: arch             = gptneox
0.00.050.292 I print_info: vocab_only       = 0
0.00.050.293 I print_info: n_ctx_train      = 2048
0.00.050.293 I print_info: n_embd           = 2048
0.00.050.293 I print_info: n_layer          = 24
0.00.050.296 I print_info: n_head           = 16
0.00.050.297 I print_info: n_head_kv        = 16
0.00.050.297 I print_info: n_rot            = 32
0.00.050.297 I print_info: n_swa            = 0
0.00.050.298 I print_info: n_embd_head_k    = 128
0.00.050.298 I print_info: n_embd_head_v    = 128
0.00.050.298 I print_info: n_gqa            = 1
0.00.050.299 I print_info: n_embd_k_gqa     = 2048
0.00.050.300 I print_info: n_embd_v_gqa     = 2048
0.00.050.300 I print_info: f_norm_eps       = 1.0e-05
0.00.050.301 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.301 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.301 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.301 I print_info: f_logit_scale    = 0.0e+00
0.00.050.302 I print_info: n_ff             = 8192
0.00.050.302 I print_info: n_expert         = 0
0.00.050.302 I print_info: n_expert_used    = 0
0.00.050.302 I print_info: causal attn      = 1
0.00.050.302 I print_info: pooling type     = 0
0.00.050.302 I print_info: rope type        = 2
0.00.050.303 I print_info: rope scaling     = linear
0.00.050.303 I print_info: freq_base_train  = 10000.0
0.00.050.303 I print_info: freq_scale_train = 1
0.00.050.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.306 I print_info: rope_finetuned   = unknown
0.00.050.307 I print_info: ssm_d_conv       = 0
0.00.050.307 I print_info: ssm_d_inner      = 0
0.00.050.307 I print_info: ssm_d_state      = 0
0.00.050.307 I print_info: ssm_dt_rank      = 0
0.00.050.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.307 I print_info: model type       = 1.4B
0.00.050.308 I print_info: model params     = 1.41 B
0.00.050.308 I print_info: general.name     = 1.4B
0.00.050.309 I print_info: vocab type       = BPE
0.00.050.309 I print_info: n_vocab          = 50304
0.00.050.309 I print_info: n_merges         = 50009
0.00.050.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.309 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.309 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.310 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.310 I print_info: LF token         = 128 'Ä'
0.00.050.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.314 I print_info: max token length = 1024
0.00.052.296 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.296 I load_tensors: offloading output layer to GPU
0.00.052.296 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.306 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.308 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.580 I llama_init_from_model: n_seq_max     = 1
0.00.052.580 I llama_init_from_model: n_ctx         = 128
0.00.052.580 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.581 I llama_init_from_model: n_batch       = 128
0.00.052.581 I llama_init_from_model: n_ubatch      = 128
0.00.052.581 I llama_init_from_model: flash_attn    = 0
0.00.052.581 I llama_init_from_model: freq_base     = 10000.0
0.00.052.582 I llama_init_from_model: freq_scale    = 1
0.00.052.582 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.582 I ggml_metal_init: allocating
0.00.052.585 I ggml_metal_init: found device: Apple M4
0.00.052.587 I ggml_metal_init: picking default device: Apple M4
0.00.053.138 I ggml_metal_init: using embedded metal library
0.00.055.482 I ggml_metal_init: GPU name:   Apple M4
0.00.055.483 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.484 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.484 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.484 I ggml_metal_init: simdgroup reduction   = true
0.00.055.485 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.485 I ggml_metal_init: has bfloat            = true
0.00.055.485 I ggml_metal_init: use bfloat            = true
0.00.055.485 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.486 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.950 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.170 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.172 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.187 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.090 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.091 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.092 I llama_init_from_model: graph nodes  = 967
0.00.067.092 I llama_init_from_model: graph splits = 2
0.00.067.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.470.151 I 
0.00.470.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.470.212 I perplexity: tokenizing the input ..
0.00.477.939 I perplexity: tokenization took 7.724 ms
0.00.477.953 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.618.000 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.619.184 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.619.200 I llama_perf_context_print:        load time =     461.17 ms
0.00.619.201 I llama_perf_context_print: prompt eval time =     139.82 ms /   128 tokens (    1.09 ms per token,   915.45 tokens per second)
0.00.619.202 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.202 I llama_perf_context_print:       total time =     149.05 ms /   129 tokens
0.00.619.645 I ggml_metal_free: deallocating

real	0m0.634s
user	0m0.078s
sys	0m0.088s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.232 I build: 4539 (564804b7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.215 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.013 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.018 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.020 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.021 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.027 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.027 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.028 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.030 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.031 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.033 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.036 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.037 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.441 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.646 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.647 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.647 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.648 I llama_model_loader: - type  f32:  194 tensors
0.00.053.648 I llama_model_loader: - type  f16:   98 tensors
0.00.053.649 I print_info: file format = GGUF V3 (latest)
0.00.053.650 I print_info: file type   = all F32 (guessed)
0.00.053.656 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.821 I load: special tokens cache size = 25
0.00.088.673 I load: token to piece cache size = 0.2984 MB
0.00.088.676 I print_info: arch             = gptneox
0.00.088.676 I print_info: vocab_only       = 0
0.00.088.677 I print_info: n_ctx_train      = 2048
0.00.088.677 I print_info: n_embd           = 2048
0.00.088.677 I print_info: n_layer          = 24
0.00.088.680 I print_info: n_head           = 16
0.00.088.681 I print_info: n_head_kv        = 16
0.00.088.681 I print_info: n_rot            = 32
0.00.088.682 I print_info: n_swa            = 0
0.00.088.682 I print_info: n_embd_head_k    = 128
0.00.088.684 I print_info: n_embd_head_v    = 128
0.00.088.685 I print_info: n_gqa            = 1
0.00.088.686 I print_info: n_embd_k_gqa     = 2048
0.00.088.686 I print_info: n_embd_v_gqa     = 2048
0.00.088.687 I print_info: f_norm_eps       = 1.0e-05
0.00.088.688 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.688 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.688 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.688 I print_info: f_logit_scale    = 0.0e+00
0.00.088.689 I print_info: n_ff             = 8192
0.00.088.689 I print_info: n_expert         = 0
0.00.088.689 I print_info: n_expert_used    = 0
0.00.088.689 I print_info: causal attn      = 1
0.00.088.689 I print_info: pooling type     = 0
0.00.088.689 I print_info: rope type        = 2
0.00.088.690 I print_info: rope scaling     = linear
0.00.088.690 I print_info: freq_base_train  = 10000.0
0.00.088.690 I print_info: freq_scale_train = 1
0.00.088.690 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.691 I print_info: rope_finetuned   = unknown
0.00.088.691 I print_info: ssm_d_conv       = 0
0.00.088.691 I print_info: ssm_d_inner      = 0
0.00.088.691 I print_info: ssm_d_state      = 0
0.00.088.691 I print_info: ssm_dt_rank      = 0
0.00.088.691 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.691 I print_info: model type       = 1.4B
0.00.088.692 I print_info: model params     = 1.41 B
0.00.088.692 I print_info: general.name     = 1.4B
0.00.088.692 I print_info: vocab type       = BPE
0.00.088.692 I print_info: n_vocab          = 50304
0.00.088.693 I print_info: n_merges         = 50009
0.00.088.693 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.693 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.693 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.693 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.694 I print_info: LF token         = 128 'Ä'
0.00.088.694 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.694 I print_info: max token length = 1024
0.00.091.229 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.229 I load_tensors: offloading output layer to GPU
0.00.091.230 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.240 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.242 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.091.496 I llama_init_from_model: n_seq_max     = 1
0.00.091.497 I llama_init_from_model: n_ctx         = 128
0.00.091.497 I llama_init_from_model: n_ctx_per_seq = 128
0.00.091.497 I llama_init_from_model: n_batch       = 128
0.00.091.497 I llama_init_from_model: n_ubatch      = 128
0.00.091.498 I llama_init_from_model: flash_attn    = 0
0.00.091.498 I llama_init_from_model: freq_base     = 10000.0
0.00.091.498 I llama_init_from_model: freq_scale    = 1
0.00.091.499 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.499 I ggml_metal_init: allocating
0.00.091.502 I ggml_metal_init: found device: Apple M4
0.00.091.504 I ggml_metal_init: picking default device: Apple M4
0.00.092.131 I ggml_metal_init: using embedded metal library
0.00.094.718 I ggml_metal_init: GPU name:   Apple M4
0.00.094.719 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.720 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.720 I ggml_metal_init: simdgroup reduction   = true
0.00.094.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.721 I ggml_metal_init: has bfloat            = true
0.00.094.721 I ggml_metal_init: use bfloat            = true
0.00.094.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.192 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.493 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.495 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.509 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.106.337 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.106.338 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.106.339 I llama_init_from_model: graph nodes  = 967
0.00.106.339 I llama_init_from_model: graph splits = 2
0.00.106.340 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.341 I 
0.00.106.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.377 I compute_imatrix: tokenizing the input ..
0.00.113.244 I compute_imatrix: tokenization took 6.866 ms
0.00.113.245 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.667.021 I compute_imatrix: 1.55 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.669.594 I llama_perf_context_print:        load time =    1644.80 ms
0.01.669.595 I llama_perf_context_print: prompt eval time =    1553.15 ms /   128 tokens (   12.13 ms per token,    82.41 tokens per second)
0.01.669.596 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.669.596 I llama_perf_context_print:       total time =    1647.36 ms /   129 tokens
0.01.670.191 I ggml_metal_free: deallocating

real	0m1.854s
user	0m0.167s
sys	0m0.243s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4539 (564804b7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x110e0a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x110e0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x110e0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x110e0b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x110e0bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x110e0c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x110e0c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x110e0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x110e0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x110e0d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x110e0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x110e0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x110e0ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x110e0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x110e0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x110e10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x110e10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x110e11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x110e11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x110e12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x110e12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x110e12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x110e135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x110e13e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x110e14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x110e14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x110e14e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x110e15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x110e15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x110e162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x110e16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x110e16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x110e17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x110e177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x110e17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x110e17f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x110e183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x110e18870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x110e18d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x110e191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x110e19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x110e19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x110e19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x110e1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x110e1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x110e1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x110e1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x110e1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x110e1c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x110e1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x110e1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x110e1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x110e1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x110e1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x110e1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x110e1ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x110e1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x110e1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x110e1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x110e20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x110e20540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x110e209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x110e20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x110e21320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x110e217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x110e21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x110e22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x110e225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x110e22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x110e22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x110e23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x110e23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x110e23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x110e24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x110e24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x110e24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x110e25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x110e25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x110e25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x110e261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x110e26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x110e26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x110e271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x110e27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x110e27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x110e281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x110e28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x110e28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x110e291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x110e29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x110e29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x110e2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x110e2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x110e2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x110e2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x110e2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x110e2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x110e1b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x110e2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x110e2c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x110e2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x110e2d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x110e2d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x110e2dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x110e2e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x110e2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x110e2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x110e2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x110e2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x110e2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x110e302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x110e30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x110e30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x110e31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x110e316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x110e31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x110e31ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x110e32490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x110e32930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x110e32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x110e33270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x110e33710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x110e33bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x110e34050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x110e344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x110e34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x110e34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x110e352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x110e35770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x110e35c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x110e360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x110e36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x110e369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x110e36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x110e37330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x110e377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x110e37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x110e38110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x110e385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x110e38a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x110e38ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x110e39390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x110e39830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x110e39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x110e3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x110e3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x110e3aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x110e3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x110e3b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x110e3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x110e3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x110e3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x110e3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x110e3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x110e3cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x110e3d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x110e3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x110e3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x110e3e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x110e3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x110e3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x110e3f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x110e3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x110e3f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x110e3fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x110e40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x110e40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x110e40bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x110e41070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x110e41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x110e419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x110e41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x110e422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x110e42790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x110e42c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x110e430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x110e43570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x110e43a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x110e43eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x110e44350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x110e447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x110e44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x110e45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x110e455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x110e45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x110e45f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x110e463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x110e46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x110e46cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x110e47190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x110e47630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x110e47ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x110e47f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x110e484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x110e48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x110e48f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x110e494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x110e49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x110e49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x110e4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x110e4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x110e4b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x110e4b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x110e4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x110e4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x110e4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x110e4cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x110e4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x110e4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x110e4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x110e4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x110e4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x110e4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x110e4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x110e4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x110e4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x110e50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x110e507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x110e50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x110e51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x110e517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x110e51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x110e52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x110e527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x110e52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x110e53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x110e53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x110e53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x110e54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x110e54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x110e54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x110e55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x110e55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x110e55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x110e56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x110e56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x110e56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x110e57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x110e57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x110e57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x110e581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x110e58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x110e58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x110e591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x110e59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x110e59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x110e5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x110e5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x110e5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x110e5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x110e5b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x110e5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x110e5c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x110e5c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x110e5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x110e5d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x110e5d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x110e5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x110e5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x110e5e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x110e5ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x110e5f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x110e5f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x110e5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x110e60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x110e606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x110e60c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x110e610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x110e61550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x110e619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x110e61e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x110e62330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x110e627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x110e62c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x110e63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x110e635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x110e63a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x110e63ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x110e64390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x110e64830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x110e64cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x110e65170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x110e656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x110e65de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x110e66500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x110e66c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x110e67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x110e67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x110e67df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x110e680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x110e686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.141.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120e08280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120e086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120e08b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120e08fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120e09440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120e098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120e09d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120e0a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120e0a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120e0aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120e0aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120e0b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120e0c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120e0c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120e0d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120e0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120e0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120e0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120e0ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120e0f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120e0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120e10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120e10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120e11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120e11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120e11b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120e11df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120e12260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120e126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120e12b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120e13040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120e13550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120e139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120e13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120e140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120e14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120e14ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120e14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120e154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120e159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120e15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120e163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120e168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120e16dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120e172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120e17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120e17ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120e18010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120e18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120e188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120e18d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120e191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120e19640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120e19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120e19f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120e1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120e1ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120e1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120e1b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120e1bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120e1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120e1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120e1ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120e1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120e1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120e1d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120e1dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120e1e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120e1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120e1ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120e1ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120e1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120e1f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120e1fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120e20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120e20860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120e20db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120e21300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120e21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120e21da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120e222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120e22840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120e22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120e232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120e23830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120e23d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120e242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120e24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120e24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120e252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120e25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120e25d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120e262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120e26800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120e26d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120e272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120e277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120e27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120e28290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120e287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120e28d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120e29280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120e297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120e29d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120e2a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120e2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120e2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120e2b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120e2b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120e2bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120e2c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120e2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120e2ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120e2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120e2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120e2dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120e2df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120e2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120e2e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120e2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120e2f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120e2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120e2fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120e2ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120e30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120e30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120e30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120e31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120e316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120e31b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120e32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120e324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120e32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120e32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120e332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120e33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120e33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120e34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120e34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120e349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120e34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120e35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120e357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120e35c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120e360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120e36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120e36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120e36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120e37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120e37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120e37cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x110e4bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x110e4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x110e68370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x110e49a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x110e4a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x110e1d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x110e1d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x110e1f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x110e4c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x110e14ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x110e1b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x110e1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x110e1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x110e1afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x110e1a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x110e1dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x110e1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x110e13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x110e0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x110e09960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x110e1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x110e1fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x110e2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x110e678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x110e16cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x110e16f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x110e4c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x110e4ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x110e150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x110e153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x110e15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x110e68b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x110e68de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x110e690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x110e69360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x110e69620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x110e698e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x110e69ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x110e69e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x110e6a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x110e6a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x110e6a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x110e6a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x110e6ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x110e6aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x110e6b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x110e6b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x110e6b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x110e6b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x110e6bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x110e6bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x110e6c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x110e6c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x110e6c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x110e6ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x110e6cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x110e6cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x110e6d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x110e6d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x110e6db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x110e6ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x110e6e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x110e6e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x110e6e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x110e6e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x110e6ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x110e6ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x110e6f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x110e6f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x110e6f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x110e6f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x110e6fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x110e6fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x110e701b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x110e70470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x110e70730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x110e709f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x110e70cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x110e70f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x110e71230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x110e714f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x110e717b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x110e71a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x110e71d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x110e71ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x110e722b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x110e72570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x110e72830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x110e72af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x110e72db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x110e73070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x110e73330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x110e735f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x110e738b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x110e73b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x110e73e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x110e740f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x110e743b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x110e74670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x110e74930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x110e74bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x110e74eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x110e75170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x110e75430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x110e756f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x110e759b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x110e75c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x110e75f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x110e761f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x110e764b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x110e76770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x110e76a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x110e76cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x110e76fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x110e77270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x110e77530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x110e777f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x110f04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x110e77ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x110e77d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x110e78030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x110e782f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x110e785b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x110e78870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x110e78b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x110e78df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x110e790b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x110e79370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x110e79630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x110e798f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x110e79bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x110e79e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x110e7a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x110e7a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x110e7a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x110e7a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x110e7ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x110e7aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x110e7b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x110e7b470 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x110e7b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x110e7bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x110e7c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x110e7c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x110e7c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x110e7c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x110e7cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x110e7cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x110e7d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x110e7d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x110e7d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x110e7d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x110e7df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x110e7e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x110e7eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x110e7ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x110e7f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x110e7f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x110e7f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x110e7f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x110e7fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x110e7feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x110e80170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x110e80430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x110e806f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x110e809b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x110e80c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x110e80f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x110e811f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x110e814b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x110e81770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x110e81a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x110e81cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x110e81fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x110e82270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x110e82530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x110e827f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x110e82ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x110e82d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x110e83030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x110e832f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x110e835b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x110e83870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x110e83b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x110e83df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x110e840b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x110e84370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x110e84630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x110e848f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x110e84bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x110e84e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x110e85130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x110e853f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x110e856b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x110e85970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x110e85c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x110e85ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x110e861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x110e86470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x110e86730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x110e869f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x110e86cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x110e86f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x110e87230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x110e874f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x110e877b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x110e87a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x110e87d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x110e87ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x110e882b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x110e88570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x110e88830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x110e88af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x110e88db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x110e89070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x110e89330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x110e895f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x110e898b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x110e89b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x110e89e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x110e8a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x110e8a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x110e8a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x110e8a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x110e8abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x110e8aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x110e8b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x110e8b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x110e8b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x110e8b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x110e8bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x110e8bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x110e8c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x110e8c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x110e8c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x110e8ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x110e8ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x110e8cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x110e8d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x110e8d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x110e8d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x110e8dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x110e8dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x110e8e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x110e8e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x110e8e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x110e8e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x110e8eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x110e8edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x110e8f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x110e8f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x110e8f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x110e8f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x110e8fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x110e8fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x110e90130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x110e903f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x110e906b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x110e90970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x110e90c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x110e90ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x110e911b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x110e91470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x110e91730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x110e919f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x110e91cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x110e91f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x110e92230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x110e924f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x110e927b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x110e92a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x110e92d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x110e92ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x110e932b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x110e93570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x110e93830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x110e93af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x110e93db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x110e94070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x110e94330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x110e945f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x110e948b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x110e94b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x110e94e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x110e950f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x110e953b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x110e95670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x110e95930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x110e95bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x110e95eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x110e96170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x110e96430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x110e966f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x110e969b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x110e96c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x110e96f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x110e971f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x110e974b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x110e97770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x110e97a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x110e97cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x110e97fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x110e98270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x110e98530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x110e987f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x110e98ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x110e98d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x110e99030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x110e992f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x110e995b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x110e99870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x110e99b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x110e99df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x110e9a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x110e9a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x110e9a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x110e9a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x110e9abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x110e9ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x110e9b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x110e9b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x110e9b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x110e9b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x110e9bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x110e9bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x110e9c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x110e9c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x110e9c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x110e9c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x110e9ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x110e9cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x110e9d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x110e9d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x110e9d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x110e9da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x110e9dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x110e9dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x110e9e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x110e9e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x110e9e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x110e9eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x110e9edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x110e9f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x110e9f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x110e9f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x110e9f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x110e9fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x110e9fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x110ea00f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x110ea03b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x110ea0980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x110ea0c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x110ea0f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x110ea11c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x110ea1480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x110ea1740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x110ea1a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x110ea1cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x110ea1f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x110ea2240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x110ea2500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x110ea27c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x110ea2a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x110ea2d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x110ea3000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x110ea32c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x110ea3580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x110ea3840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x110ea3b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x110ea3dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x110ea4080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x110ea4340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x110ea4600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x110ea4b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x110ea50a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x110ea55f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x110ea5b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x110ea6090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x110ea65e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x110ea6b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x110ea7080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x110ea75d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x110ea7b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x110ea8070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x110ea85c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x110ea8b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x110ea9060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x110ea95b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x110ea9b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x110eaa050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x110eaa5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x110eaaaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x110eab040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x110eab590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x110eabae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x110eac030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x110eac580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x110eacad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x110ead020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x110ead570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x110eadac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x110eae010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x110eae560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x110eaeab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x110eaf000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x110eaf550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x110eafaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x110eafd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x110eb0020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x110eb0520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x110eb0a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x110eb0f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x110eb1420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x110eb1920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x110eb1e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x110eb2320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x110eb2820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x110eb2d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x110eb3220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x110eb3720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x110eb3c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x110eb4120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x110eb4620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x110eb5030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x110eb5750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x110eb5e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x110eb6590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x110eb6850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x110eb7040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x110eb7300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x110eb7910 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.840s
user	0m0.285s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4539 (564804b7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13460b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13460bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13460c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13460c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13460cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13460d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13460d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13460de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13460e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13460e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13460ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13460f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13460fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1346105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134610de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134611500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134611c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134612340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134612a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134613230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134613950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134614070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134614790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134615030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134615750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134615a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134616020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134616c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1346171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134617490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134617930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134618480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1346189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134618c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1346195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134619a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134619f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13461a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13461a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13461ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13461b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13461b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13461b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13461bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13461c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13461ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13461d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13461da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13461e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13461e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13461ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13461f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13461fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13461ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1346203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134620670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134620c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134621bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134622070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134622510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1346229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134622e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1346232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134623790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134623c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1346240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134624570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134624a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134625ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1346263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134626940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1346273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134627930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134627e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1346283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134628920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134628e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1346293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134629910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13462a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13462a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13462ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13462b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13462b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13462be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13462c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13462c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13462ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13461cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13462d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13462da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13462dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13462e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13462ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13462ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13462f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13462fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13462ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1346304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134630f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1346314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134631a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134631f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134632400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1346328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134632d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1346331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134633680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134633b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134634460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134634900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134634da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1346356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134635b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134636020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1346364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134636e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1346372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134637740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134637be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134638080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134638520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1346389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134638e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134639300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1346397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134639c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13463a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13463a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13463aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13463aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13463b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13463b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13463bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13463c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13463c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13463ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13463cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13463d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13463d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13463dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13463e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13463e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13463eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13463ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13463f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13463f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13463fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134640200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1346406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134640fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134641480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134641920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134641dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134642260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134642700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134643040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1346434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134643980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134643e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1346442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134644760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134644c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1346450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1346459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134645e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134646320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1346467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134646c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134647100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1346475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134647a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134647ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134648380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134648820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134648cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134649160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1346496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134649c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13464a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13464a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13464a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13464af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13464b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13464bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13464c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13464c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13464cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13464d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13464d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13464def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13464e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13464e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13464ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13464f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13464f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13464ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134650470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1346509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134650f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134651460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1346519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134652450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1346529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134653440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134653ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134654430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134654980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134654ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134655420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134655970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134656410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134656960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134656eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134657ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1346583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1346593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134659e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13465a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13465a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13465ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13465b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13465b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13465be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13465c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13465c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13465ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13465d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13465d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13465de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13465e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13465e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13465ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13465f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13465f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13465fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1346608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1346618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1346622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134662740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134662be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134663080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134663520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1346639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134663e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134664300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1346647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134664c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1346650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134665580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134665a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134665ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134666360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1346668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134666fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1346676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134667e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134668530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1346687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134668fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1346692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1346698b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.086.901 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13470ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13470b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13470b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13470b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13470be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13470c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13470c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13470cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13470cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13470d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13470d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13470e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13470eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13470f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13470fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134710260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134710980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1347110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1347117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134711f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1347126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134712dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1347134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134713c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134714330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1347145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1347148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134714d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134715190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134715600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134715b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134716010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134716480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134716bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134717020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134717580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134717a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134718480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134718980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134718e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134719380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134719880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134719d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13471a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13471a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13471aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13471af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13471b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13471b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13471bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13471c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13471c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13471c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13471d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13471d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13471d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13471df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13471e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13471ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13471f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13471f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13471f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13471fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1347202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134720770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134720c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1347210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134721550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1347219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134721e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134722330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134722880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134722dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134723320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134723870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134723dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134724310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134724860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134724db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134725850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134725da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1347262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134726840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134726d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1347272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134727830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134727d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1347282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134728820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134728d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1347292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134729810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134729d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13472a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13472a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13472ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13472b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13472b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13472bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13472c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13472c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13472cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13472d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13472d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13472dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13472e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13472e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13472ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13472f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13472f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13472fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1347300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134730590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134730a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134730ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134731370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134731810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134731cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134732150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1347325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134732a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134732f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1347333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134733870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134733d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1347341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134734650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134734af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134734f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134735430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1347358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134735d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134736210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1347366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134736b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134736ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134737490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134737dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134738270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134738710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134738bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134739050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1347394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134739990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134739e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13473a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13473a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13473ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13473b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13473b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13473b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13473be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13473c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13473c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13473cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13473d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13473d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13473da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13473def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13473e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13473e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13473ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13473f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13473f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13473fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13473ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1347403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134740890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134740d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1347411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134741670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134741b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134741fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134742450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1347428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134742d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134743230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1347436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134743b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134744010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1347444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134744950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134744df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134745290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134745730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134745bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134746070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134746510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1347469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134746f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134747450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1347479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134747ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1347481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1347487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134748dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1347493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134749bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13474a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13474a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13474a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13474af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13474b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13474bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13474c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13474c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13474ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13474d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13474d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13474dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13474e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13474e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13474ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13474f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13474f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13474fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1347501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134750740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134750c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1347511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134751730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134751c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1347521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134752720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134752c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1347531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134753710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134753c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1347541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134754700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134754c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1347551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1347556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134755c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134756190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1347566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134756c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134757180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1347576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134757c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134758170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1347586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134758c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134759160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1347596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134759c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13475a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13475a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13475abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13475b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13475b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13475bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13475c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13475c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13475cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13475d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13475d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13475dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13475e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13475e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13475ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13475f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13475f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13475faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13475ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134760430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1347608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134760d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134761210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1347616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134761b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134761ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134762490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134762930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134762dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134763270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134763710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134763bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134764100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134764820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134764f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134765660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134765d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134766040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134766830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134766af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134767100 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1358046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1358058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1358065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135807a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135808530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135808ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1358094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135809c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13580a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13580aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13580b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13580b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13580c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13580c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13580cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13580d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13580dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13580dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13580e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13580e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13580eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13580efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13580f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13580f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13580fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135810080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1358104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135810960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135810dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135811240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1358116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135811b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135811f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135812400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135812870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135812ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135813150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1358135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135813a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135813ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135814310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135814780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135814bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135815060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1358154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135815940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135815db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135816220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135816790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135816c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135817100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135817570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1358179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135817e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1358182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135818730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135818ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135819010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135819480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1358198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135819d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13581a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13581a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13581aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13581af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13581b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13581b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13581bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13581c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13581c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13581c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13581ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13581d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13581d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13581db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13581dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13581e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13581e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13581ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13581f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13581f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13581fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13581ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135820370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1358207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135820c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1358210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x135821530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1358219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135821e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135822280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1358226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135822b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135822fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135823440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135823cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135823f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135824400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135824870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135824ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x135825150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1358255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135825a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135825ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135826310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135826780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135826bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135827060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1358274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135827940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135827db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135828220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135828690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135828b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135828f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1358293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135829850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135829cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13582a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13582a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13582aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13582ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13582b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13582b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13582bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13582c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13582c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13582c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13582cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13582d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13582d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13582dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13582df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13582e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13582e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13582eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13582f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13582f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13582f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13582fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1358302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135830740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135830bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135831020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135831490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135831900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135831d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1358321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135832650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135832ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135832f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1358333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135833810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135833c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1358340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135834560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1358349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135834e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1358352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135835720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135835b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135836000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135836470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1358368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135836d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1358371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135837630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x135837aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135837f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135838380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1358387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135838c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1358390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135839540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1358399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135839e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13583a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13583a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13583ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13583afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13583b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13583b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13583bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13583c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13583c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13583ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13583cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13583d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13583d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13583dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13583e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13583e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13583e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13583ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13583f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13583f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13583fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13583ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135840430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1358408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135840d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x135841180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135841d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135841fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135842280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1358426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135842b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135842fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135843440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1358438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135843d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135844190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135844600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135844a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135844ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135845350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1358457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135845c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1358460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135846510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135846980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135846df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x135847260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1358476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135847b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135847fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135848420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135848890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135848d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135849170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1358495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135849a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135849ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13584a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13584a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13584ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13584b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13584b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13584b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13584bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13584c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13584c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13584cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13584cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13584d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13584d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13584dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13584e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13584e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13584ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13584eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13584f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13584f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13584fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x135850060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1358504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135850940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135850db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135851220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135851690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135851b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135851f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1358523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135852850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135852cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135853130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1358535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135853a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135853e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1358542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135854760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135854bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x135855040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1358554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135855920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135856390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135856ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1358571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1358578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135857bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135858020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135858620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135858c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.913s
user	0m0.243s
sys	0m0.134s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
