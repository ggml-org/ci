### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.74 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.01 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.19 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.26 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.16 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.20 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.22 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.16 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  171.87 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    1.01 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.75 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.34 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.26 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 213.90 sec*proc (27 tests)

Total Test time (real) = 213.91 sec

real	3m33.941s
user	7m18.813s
sys	0m5.504s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.91 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   28.41 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.28 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.10 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.10 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  49.96 sec*proc (27 tests)

Total Test time (real) =  49.97 sec

real	0m49.984s
user	1m10.312s
sys	0m5.008s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.146 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.873 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.063 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.072 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.073 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.074 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.075 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.075 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.077 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.078 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.078 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.079 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.079 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.083 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.083 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.084 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.085 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.085 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.086 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.086 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.502 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.504 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.505 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.505 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.506 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.027.507 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.507 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.027.508 I llama_model_loader: - type  f32:  124 tensors
0.00.027.509 I llama_model_loader: - type  f16:   73 tensors
0.00.032.274 I llm_load_vocab: special tokens cache size = 5
0.00.034.405 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.034.408 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.034.409 I llm_load_print_meta: arch             = bert
0.00.034.409 I llm_load_print_meta: vocab type       = WPM
0.00.034.410 I llm_load_print_meta: n_vocab          = 30522
0.00.034.410 I llm_load_print_meta: n_merges         = 0
0.00.034.410 I llm_load_print_meta: vocab_only       = 0
0.00.034.410 I llm_load_print_meta: n_ctx_train      = 512
0.00.034.410 I llm_load_print_meta: n_embd           = 384
0.00.034.411 I llm_load_print_meta: n_layer          = 12
0.00.034.413 I llm_load_print_meta: n_head           = 12
0.00.034.414 I llm_load_print_meta: n_head_kv        = 12
0.00.034.415 I llm_load_print_meta: n_rot            = 32
0.00.034.417 I llm_load_print_meta: n_swa            = 0
0.00.034.418 I llm_load_print_meta: n_embd_head_k    = 32
0.00.034.418 I llm_load_print_meta: n_embd_head_v    = 32
0.00.034.419 I llm_load_print_meta: n_gqa            = 1
0.00.034.420 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.034.420 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.034.428 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.034.429 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.034.429 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.034.429 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.034.429 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.034.430 I llm_load_print_meta: n_ff             = 1536
0.00.034.431 I llm_load_print_meta: n_expert         = 0
0.00.034.431 I llm_load_print_meta: n_expert_used    = 0
0.00.034.431 I llm_load_print_meta: causal attn      = 0
0.00.034.431 I llm_load_print_meta: pooling type     = 2
0.00.034.432 I llm_load_print_meta: rope type        = 2
0.00.034.432 I llm_load_print_meta: rope scaling     = linear
0.00.034.432 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.034.433 I llm_load_print_meta: freq_scale_train = 1
0.00.034.433 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.034.433 I llm_load_print_meta: rope_finetuned   = unknown
0.00.034.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.034.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.034.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.034.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.034.434 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.034.448 I llm_load_print_meta: model type       = 33M
0.00.034.449 I llm_load_print_meta: model ftype      = F16
0.00.034.450 I llm_load_print_meta: model params     = 33.21 M
0.00.034.450 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.034.455 I llm_load_print_meta: general.name     = Bge Small
0.00.034.456 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.034.456 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.034.456 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.034.457 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.034.457 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.034.457 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.034.457 I llm_load_print_meta: max token length = 21
0.00.036.505 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.036.507 I llm_load_tensors: offloading output layer to GPU
0.00.036.507 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.036.533 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.535 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.126 I llama_new_context_with_model: n_seq_max     = 1
0.00.037.128 I llama_new_context_with_model: n_ctx         = 512
0.00.037.128 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.037.128 I llama_new_context_with_model: n_batch       = 2048
0.00.037.128 I llama_new_context_with_model: n_ubatch      = 2048
0.00.037.129 I llama_new_context_with_model: flash_attn    = 0
0.00.037.129 I llama_new_context_with_model: freq_base     = 10000.0
0.00.037.130 I llama_new_context_with_model: freq_scale    = 1
0.00.037.130 I ggml_metal_init: allocating
0.00.037.140 I ggml_metal_init: found device: Apple M4
0.00.037.143 I ggml_metal_init: picking default device: Apple M4
0.00.037.954 I ggml_metal_init: using embedded metal library
0.00.041.671 I ggml_metal_init: GPU name:   Apple M4
0.00.041.674 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.675 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.675 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.676 I ggml_metal_init: simdgroup reduction   = true
0.00.041.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.676 I ggml_metal_init: has bfloat            = true
0.00.041.676 I ggml_metal_init: use bfloat            = true
0.00.041.677 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.140 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.143 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.144 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.054.077 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.054.079 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.054.080 I llama_new_context_with_model: graph nodes  = 429
0.00.054.080 I llama_new_context_with_model: graph splits = 2
0.00.054.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.990 I 
0.00.061.020 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.061.700 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.066.430 I llama_perf_context_print:        load time =      44.11 ms
0.00.066.431 I llama_perf_context_print: prompt eval time =       4.58 ms /     9 tokens (    0.51 ms per token,  1966.78 tokens per second)
0.00.066.432 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.066.432 I llama_perf_context_print:       total time =       5.44 ms /    10 tokens
0.00.066.565 I ggml_metal_free: deallocating

real	0m0.249s
user	0m0.050s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.471 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.570 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.573 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.575 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.575 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.576 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.576 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.576 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.577 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.577 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.578 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.578 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.579 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.581 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.581 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.582 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.582 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.582 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.582 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.583 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.770 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.771 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.772 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.772 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.773 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.773 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.773 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.774 I llama_model_loader: - type  f32:  124 tensors
0.00.014.774 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.401 I llm_load_vocab: special tokens cache size = 5
0.00.018.794 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.796 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.796 I llm_load_print_meta: arch             = bert
0.00.018.797 I llm_load_print_meta: vocab type       = WPM
0.00.018.797 I llm_load_print_meta: n_vocab          = 30522
0.00.018.797 I llm_load_print_meta: n_merges         = 0
0.00.018.797 I llm_load_print_meta: vocab_only       = 0
0.00.018.797 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.797 I llm_load_print_meta: n_embd           = 384
0.00.018.797 I llm_load_print_meta: n_layer          = 12
0.00.018.799 I llm_load_print_meta: n_head           = 12
0.00.018.800 I llm_load_print_meta: n_head_kv        = 12
0.00.018.800 I llm_load_print_meta: n_rot            = 32
0.00.018.800 I llm_load_print_meta: n_swa            = 0
0.00.018.800 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.800 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.801 I llm_load_print_meta: n_gqa            = 1
0.00.018.803 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.804 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.804 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.804 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.804 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.805 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.805 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.805 I llm_load_print_meta: n_ff             = 1536
0.00.018.805 I llm_load_print_meta: n_expert         = 0
0.00.018.806 I llm_load_print_meta: n_expert_used    = 0
0.00.018.806 I llm_load_print_meta: causal attn      = 0
0.00.018.806 I llm_load_print_meta: pooling type     = 2
0.00.018.806 I llm_load_print_meta: rope type        = 2
0.00.018.806 I llm_load_print_meta: rope scaling     = linear
0.00.018.807 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.807 I llm_load_print_meta: freq_scale_train = 1
0.00.018.807 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.807 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.807 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.810 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.811 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.811 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.811 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.817 I llm_load_print_meta: model type       = 33M
0.00.018.817 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.818 I llm_load_print_meta: model params     = 33.21 M
0.00.018.818 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.818 I llm_load_print_meta: general.name     = Bge Small
0.00.018.819 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.819 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.819 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.819 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.820 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.820 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.820 I llm_load_print_meta: max token length = 21
0.00.020.114 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.114 I llm_load_tensors: offloading output layer to GPU
0.00.020.115 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.123 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.124 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.480 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.481 I llama_new_context_with_model: n_ctx         = 512
0.00.020.481 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.482 I llama_new_context_with_model: n_batch       = 2048
0.00.020.482 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.482 I llama_new_context_with_model: flash_attn    = 0
0.00.020.482 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.483 I llama_new_context_with_model: freq_scale    = 1
0.00.020.483 I ggml_metal_init: allocating
0.00.020.485 I ggml_metal_init: found device: Apple M4
0.00.020.487 I ggml_metal_init: picking default device: Apple M4
0.00.020.998 I ggml_metal_init: using embedded metal library
0.00.023.117 I ggml_metal_init: GPU name:   Apple M4
0.00.023.118 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.118 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.119 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.119 I ggml_metal_init: simdgroup reduction   = true
0.00.023.119 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.119 I ggml_metal_init: has bfloat            = true
0.00.023.120 I ggml_metal_init: use bfloat            = true
0.00.023.120 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.121 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.394 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.031.396 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.031.397 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.003 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.004 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.004 I llama_new_context_with_model: graph nodes  = 429
0.00.032.004 I llama_new_context_with_model: graph splits = 2
0.00.032.017 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.036.337 I 
0.00.036.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.036.884 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.041.138 I llama_perf_context_print:        load time =      26.86 ms
0.00.041.140 I llama_perf_context_print: prompt eval time =       4.11 ms /     9 tokens (    0.46 ms per token,  2189.25 tokens per second)
0.00.041.141 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.041.141 I llama_perf_context_print:       total time =       4.80 ms /    10 tokens
0.00.041.315 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.223 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.689 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.376 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.380 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.382 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.030.383 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.384 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.030.385 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.030.385 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.030.386 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.030.387 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.030.388 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.030.388 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.030.392 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.030.394 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.395 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.395 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.030.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.402 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.037.664 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.039.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.101 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.044.103 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.103 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.044.104 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.044.104 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.044.105 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.044.105 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.044.106 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.044.106 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.044.106 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.044.107 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.044.107 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.044.107 I llama_model_loader: - type  f32:   41 tensors
0.00.044.108 I llama_model_loader: - type  f16:   29 tensors
0.00.061.827 W llm_load_vocab: empty token at index 5
0.00.066.106 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.067.369 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.067.395 I llm_load_vocab: special tokens cache size = 5
0.00.308.646 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.308.652 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.308.653 I llm_load_print_meta: arch             = jina-bert-v2
0.00.308.653 I llm_load_print_meta: vocab type       = BPE
0.00.308.654 I llm_load_print_meta: n_vocab          = 61056
0.00.308.654 I llm_load_print_meta: n_merges         = 39382
0.00.308.654 I llm_load_print_meta: vocab_only       = 0
0.00.308.654 I llm_load_print_meta: n_ctx_train      = 8192
0.00.308.654 I llm_load_print_meta: n_embd           = 384
0.00.308.654 I llm_load_print_meta: n_layer          = 4
0.00.308.659 I llm_load_print_meta: n_head           = 12
0.00.308.659 I llm_load_print_meta: n_head_kv        = 12
0.00.308.660 I llm_load_print_meta: n_rot            = 32
0.00.308.660 I llm_load_print_meta: n_swa            = 0
0.00.308.663 I llm_load_print_meta: n_embd_head_k    = 32
0.00.308.663 I llm_load_print_meta: n_embd_head_v    = 32
0.00.308.663 I llm_load_print_meta: n_gqa            = 1
0.00.308.664 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.308.664 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.308.665 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.308.666 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.308.666 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.308.667 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.308.667 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.308.667 I llm_load_print_meta: n_ff             = 1536
0.00.308.668 I llm_load_print_meta: n_expert         = 0
0.00.308.668 I llm_load_print_meta: n_expert_used    = 0
0.00.308.668 I llm_load_print_meta: causal attn      = 0
0.00.308.668 I llm_load_print_meta: pooling type     = -1
0.00.308.668 I llm_load_print_meta: rope type        = -1
0.00.308.669 I llm_load_print_meta: rope scaling     = linear
0.00.308.669 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.308.669 I llm_load_print_meta: freq_scale_train = 1
0.00.308.670 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.308.671 I llm_load_print_meta: rope_finetuned   = unknown
0.00.308.671 I llm_load_print_meta: ssm_d_conv       = 0
0.00.308.671 I llm_load_print_meta: ssm_d_inner      = 0
0.00.308.671 I llm_load_print_meta: ssm_d_state      = 0
0.00.308.671 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.308.672 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.308.694 I llm_load_print_meta: model type       = 33M
0.00.308.695 I llm_load_print_meta: model ftype      = F16
0.00.308.695 I llm_load_print_meta: model params     = 32.90 M
0.00.308.696 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.308.696 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.308.697 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.308.697 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.308.697 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.308.698 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.308.698 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.308.698 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.308.698 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.308.698 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.308.698 I llm_load_print_meta: max token length = 45
0.00.309.716 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.309.716 I llm_load_tensors: offloading output layer to GPU
0.00.309.716 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.309.736 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.309.737 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.310.411 I llama_new_context_with_model: n_seq_max     = 1
0.00.310.412 I llama_new_context_with_model: n_ctx         = 8192
0.00.310.412 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.310.412 I llama_new_context_with_model: n_batch       = 2048
0.00.310.412 I llama_new_context_with_model: n_ubatch      = 2048
0.00.310.412 I llama_new_context_with_model: flash_attn    = 0
0.00.310.413 I llama_new_context_with_model: freq_base     = 10000.0
0.00.310.413 I llama_new_context_with_model: freq_scale    = 1
0.00.310.414 I ggml_metal_init: allocating
0.00.310.417 I ggml_metal_init: found device: Apple M4
0.00.310.419 I ggml_metal_init: picking default device: Apple M4
0.00.311.121 I ggml_metal_init: using embedded metal library
0.00.313.466 I ggml_metal_init: GPU name:   Apple M4
0.00.313.468 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.313.469 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.313.469 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.313.469 I ggml_metal_init: simdgroup reduction   = true
0.00.313.469 I ggml_metal_init: simdgroup matrix mul. = true
0.00.313.470 I ggml_metal_init: has bfloat            = true
0.00.313.470 I ggml_metal_init: use bfloat            = true
0.00.313.470 I ggml_metal_init: hasUnifiedMemory      = true
0.00.313.471 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.323.836 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.323.838 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.323.839 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.324.366 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.324.367 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.324.367 I llama_new_context_with_model: graph nodes  = 154
0.00.324.367 I llama_new_context_with_model: graph splits = 2
0.00.324.385 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.335.620 I 
0.00.335.650 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.335.801 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.335.802 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.335.804 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.335.805 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.335.808 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.335.809 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.336.334 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.340.056 I llama_perf_context_print:        load time =     314.93 ms
0.00.340.056 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16707.09 tokens per second)
0.00.340.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.340.058 I llama_perf_context_print:       total time =       4.44 ms /    63 tokens
0.00.340.297 I ggml_metal_free: deallocating

real	0m1.040s
user	0m0.317s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.151 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.269 I main: llama backend init
0.00.000.290 I main: load the model and apply lora adapter, if any
0.00.029.104 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.537 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.557 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.562 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.562 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.565 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.566 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.567 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.568 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.569 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.715 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.716 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.716 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.718 I llama_model_loader: - type  f32:  194 tensors
0.00.059.719 I llama_model_loader: - type  f16:   98 tensors
0.00.091.347 I llm_load_vocab: special tokens cache size = 25
0.00.098.354 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.357 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.358 I llm_load_print_meta: arch             = gptneox
0.00.098.358 I llm_load_print_meta: vocab type       = BPE
0.00.098.358 I llm_load_print_meta: n_vocab          = 50304
0.00.098.358 I llm_load_print_meta: n_merges         = 50009
0.00.098.358 I llm_load_print_meta: vocab_only       = 0
0.00.098.359 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.359 I llm_load_print_meta: n_embd           = 2048
0.00.098.359 I llm_load_print_meta: n_layer          = 24
0.00.098.361 I llm_load_print_meta: n_head           = 16
0.00.098.362 I llm_load_print_meta: n_head_kv        = 16
0.00.098.362 I llm_load_print_meta: n_rot            = 32
0.00.098.363 I llm_load_print_meta: n_swa            = 0
0.00.098.363 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.363 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.363 I llm_load_print_meta: n_gqa            = 1
0.00.098.364 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.365 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.365 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.366 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.366 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.366 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.366 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.367 I llm_load_print_meta: n_ff             = 8192
0.00.098.367 I llm_load_print_meta: n_expert         = 0
0.00.098.367 I llm_load_print_meta: n_expert_used    = 0
0.00.098.367 I llm_load_print_meta: causal attn      = 1
0.00.098.368 I llm_load_print_meta: pooling type     = 0
0.00.098.368 I llm_load_print_meta: rope type        = 2
0.00.098.369 I llm_load_print_meta: rope scaling     = linear
0.00.098.369 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.369 I llm_load_print_meta: freq_scale_train = 1
0.00.098.369 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.370 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.370 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.370 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.370 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.370 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.370 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.383 I llm_load_print_meta: model type       = 1.4B
0.00.098.383 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.098.383 I llm_load_print_meta: model params     = 1.41 B
0.00.098.384 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.384 I llm_load_print_meta: general.name     = 1.4B
0.00.098.385 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.385 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.385 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.385 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.385 I llm_load_print_meta: LF token         = 128 ''
0.00.098.386 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.386 I llm_load_print_meta: max token length = 1024
0.00.101.024 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.024 I llm_load_tensors: offloading output layer to GPU
0.00.101.024 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.041 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.042 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.996 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.998 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.998 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.998 I llama_new_context_with_model: n_batch       = 2048
0.00.101.998 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.998 I llama_new_context_with_model: flash_attn    = 0
0.00.101.999 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.999 I llama_new_context_with_model: freq_scale    = 1
0.00.101.999 I ggml_metal_init: allocating
0.00.102.010 I ggml_metal_init: found device: Apple M4
0.00.102.012 I ggml_metal_init: picking default device: Apple M4
0.00.102.664 I ggml_metal_init: using embedded metal library
0.00.113.342 I ggml_metal_init: GPU name:   Apple M4
0.00.113.344 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.345 I ggml_metal_init: simdgroup reduction   = true
0.00.113.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.345 I ggml_metal_init: has bfloat            = true
0.00.113.345 I ggml_metal_init: use bfloat            = true
0.00.113.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.346 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.192.898 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.192.903 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.192.921 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.193.946 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.193.947 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.193.948 I llama_new_context_with_model: graph nodes  = 967
0.00.193.948 I llama_new_context_with_model: graph splits = 2
0.00.193.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.273.022 I main: llama threadpool init, n_threads = 4
0.00.273.054 I 
0.00.273.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.273.088 I 
0.00.273.159 I sampler seed: 1234
0.00.273.164 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.273.189 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.273.191 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.273.191 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.122.750 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54826.25 tokens per second)
0.02.122.750 I llama_perf_context_print:        load time =     243.91 ms
0.02.122.751 I llama_perf_context_print: prompt eval time =      37.64 ms /     7 tokens (    5.38 ms per token,   185.95 tokens per second)
0.02.122.752 I llama_perf_context_print:        eval time =    1808.91 ms /    63 runs   (   28.71 ms per token,    34.83 tokens per second)
0.02.122.752 I llama_perf_context_print:       total time =    1849.73 ms /    70 tokens
0.02.122.928 I ggml_metal_free: deallocating

real	0m2.506s
user	0m0.147s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.676 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.239 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.999 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.003 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.005 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.007 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.007 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.008 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.011 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.013 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.700 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.085 I llama_model_loader: - type  f32:  194 tensors
0.00.050.085 I llama_model_loader: - type  f16:   98 tensors
0.00.078.084 I llm_load_vocab: special tokens cache size = 25
0.00.084.568 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.572 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.572 I llm_load_print_meta: arch             = gptneox
0.00.084.572 I llm_load_print_meta: vocab type       = BPE
0.00.084.573 I llm_load_print_meta: n_vocab          = 50304
0.00.084.573 I llm_load_print_meta: n_merges         = 50009
0.00.084.573 I llm_load_print_meta: vocab_only       = 0
0.00.084.573 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.573 I llm_load_print_meta: n_embd           = 2048
0.00.084.573 I llm_load_print_meta: n_layer          = 24
0.00.084.576 I llm_load_print_meta: n_head           = 16
0.00.084.577 I llm_load_print_meta: n_head_kv        = 16
0.00.084.577 I llm_load_print_meta: n_rot            = 32
0.00.084.578 I llm_load_print_meta: n_swa            = 0
0.00.084.578 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.578 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.578 I llm_load_print_meta: n_gqa            = 1
0.00.084.579 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.580 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.580 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.581 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.581 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.581 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.581 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.582 I llm_load_print_meta: n_ff             = 8192
0.00.084.582 I llm_load_print_meta: n_expert         = 0
0.00.084.582 I llm_load_print_meta: n_expert_used    = 0
0.00.084.582 I llm_load_print_meta: causal attn      = 1
0.00.084.582 I llm_load_print_meta: pooling type     = 0
0.00.084.582 I llm_load_print_meta: rope type        = 2
0.00.084.582 I llm_load_print_meta: rope scaling     = linear
0.00.084.583 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.583 I llm_load_print_meta: freq_scale_train = 1
0.00.084.583 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.583 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.584 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.584 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.584 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.584 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.584 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.595 I llm_load_print_meta: model type       = 1.4B
0.00.084.596 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.084.598 I llm_load_print_meta: model params     = 1.41 B
0.00.084.599 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.084.599 I llm_load_print_meta: general.name     = 1.4B
0.00.084.599 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.599 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.599 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.600 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.600 I llm_load_print_meta: LF token         = 128 ''
0.00.084.600 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.600 I llm_load_print_meta: max token length = 1024
0.00.086.525 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.086.525 I llm_load_tensors: offloading output layer to GPU
0.00.086.525 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.086.536 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.537 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.087.432 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.433 I llama_new_context_with_model: n_ctx         = 128
0.00.087.433 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.087.433 I llama_new_context_with_model: n_batch       = 128
0.00.087.433 I llama_new_context_with_model: n_ubatch      = 128
0.00.087.433 I llama_new_context_with_model: flash_attn    = 0
0.00.087.434 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.434 I llama_new_context_with_model: freq_scale    = 1
0.00.087.434 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.435 I ggml_metal_init: allocating
0.00.087.438 I ggml_metal_init: found device: Apple M4
0.00.087.440 I ggml_metal_init: picking default device: Apple M4
0.00.088.073 I ggml_metal_init: using embedded metal library
0.00.090.208 I ggml_metal_init: GPU name:   Apple M4
0.00.090.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.210 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.211 I ggml_metal_init: simdgroup reduction   = true
0.00.090.211 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.211 I ggml_metal_init: has bfloat            = true
0.00.090.211 I ggml_metal_init: use bfloat            = true
0.00.090.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.909 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.099.912 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.099.927 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.816 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.100.817 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.100.817 I llama_new_context_with_model: graph nodes  = 967
0.00.100.817 I llama_new_context_with_model: graph splits = 2
0.00.100.829 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.037.173 I 
0.01.037.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.037.230 I perplexity: tokenizing the input ..
0.01.051.155 I perplexity: tokenization took 13.923 ms
0.01.051.190 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.172.164 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.174.186 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.174.207 I llama_perf_context_print:        load time =    1015.93 ms
0.01.174.208 I llama_perf_context_print: prompt eval time =     120.10 ms /   128 tokens (    0.94 ms per token,  1065.81 tokens per second)
0.01.174.210 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.174.210 I llama_perf_context_print:       total time =     137.03 ms /   129 tokens
0.01.174.846 I ggml_metal_free: deallocating

real	0m1.370s
user	0m0.123s
sys	0m0.197s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.612 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.510 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.510 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.511 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.512 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.513 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.580 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.582 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.582 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.582 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.583 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.583 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.584 I llama_model_loader: - type  f32:  194 tensors
0.00.034.585 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.708 I llm_load_vocab: special tokens cache size = 25
0.00.062.617 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.620 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.621 I llm_load_print_meta: arch             = gptneox
0.00.062.621 I llm_load_print_meta: vocab type       = BPE
0.00.062.621 I llm_load_print_meta: n_vocab          = 50304
0.00.062.622 I llm_load_print_meta: n_merges         = 50009
0.00.062.622 I llm_load_print_meta: vocab_only       = 0
0.00.062.622 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.622 I llm_load_print_meta: n_embd           = 2048
0.00.062.623 I llm_load_print_meta: n_layer          = 24
0.00.062.627 I llm_load_print_meta: n_head           = 16
0.00.062.628 I llm_load_print_meta: n_head_kv        = 16
0.00.062.629 I llm_load_print_meta: n_rot            = 32
0.00.062.629 I llm_load_print_meta: n_swa            = 0
0.00.062.629 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.629 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.630 I llm_load_print_meta: n_gqa            = 1
0.00.062.631 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.631 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.632 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.632 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.633 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.633 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.633 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.634 I llm_load_print_meta: n_ff             = 8192
0.00.062.634 I llm_load_print_meta: n_expert         = 0
0.00.062.634 I llm_load_print_meta: n_expert_used    = 0
0.00.062.636 I llm_load_print_meta: causal attn      = 1
0.00.062.636 I llm_load_print_meta: pooling type     = 0
0.00.062.636 I llm_load_print_meta: rope type        = 2
0.00.062.637 I llm_load_print_meta: rope scaling     = linear
0.00.062.637 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.637 I llm_load_print_meta: freq_scale_train = 1
0.00.062.638 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.638 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.638 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.638 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.638 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.639 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.639 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.653 I llm_load_print_meta: model type       = 1.4B
0.00.062.653 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.653 I llm_load_print_meta: model params     = 1.41 B
0.00.062.654 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.654 I llm_load_print_meta: general.name     = 1.4B
0.00.062.654 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.654 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.654 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.655 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.655 I llm_load_print_meta: LF token         = 128 ''
0.00.062.655 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.655 I llm_load_print_meta: max token length = 1024
0.00.065.021 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.021 I llm_load_tensors: offloading output layer to GPU
0.00.065.022 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.032 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.033 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.043 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.044 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.045 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.045 I llama_new_context_with_model: n_batch       = 2048
0.00.066.045 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.045 I llama_new_context_with_model: flash_attn    = 0
0.00.066.045 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.046 I llama_new_context_with_model: freq_scale    = 1
0.00.066.046 I ggml_metal_init: allocating
0.00.066.049 I ggml_metal_init: found device: Apple M4
0.00.066.051 I ggml_metal_init: picking default device: Apple M4
0.00.066.720 I ggml_metal_init: using embedded metal library
0.00.068.798 I ggml_metal_init: GPU name:   Apple M4
0.00.068.799 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.800 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.800 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.800 I ggml_metal_init: simdgroup reduction   = true
0.00.068.801 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.801 I ggml_metal_init: has bfloat            = true
0.00.068.801 I ggml_metal_init: use bfloat            = true
0.00.068.801 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.802 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.106 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.115 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.137 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.163 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.166 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.166 I llama_new_context_with_model: graph nodes  = 967
0.00.103.166 I llama_new_context_with_model: graph splits = 2
0.00.103.181 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.294.123 I main: llama threadpool init, n_threads = 4
0.01.294.153 I 
0.01.294.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.294.185 I 
0.01.294.406 I sampler seed: 1234
0.01.294.410 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.294.421 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.294.421 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.294.421 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.381.963 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61101.55 tokens per second)
0.02.381.964 I llama_perf_context_print:        load time =    1284.51 ms
0.02.381.964 I llama_perf_context_print: prompt eval time =      33.56 ms /     7 tokens (    4.79 ms per token,   208.59 tokens per second)
0.02.381.965 I llama_perf_context_print:        eval time =    1051.03 ms /    63 runs   (   16.68 ms per token,    59.94 tokens per second)
0.02.381.966 I llama_perf_context_print:       total time =    1087.84 ms /    70 tokens
0.02.382.139 I ggml_metal_free: deallocating

real	0m2.400s
user	0m0.113s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.123 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.500 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.186 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.191 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.193 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.193 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.198 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.198 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.199 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.200 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.200 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.201 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.201 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.204 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.206 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.206 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.207 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.245 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.667 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.168 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.169 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.169 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.169 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.170 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.170 I llama_model_loader: - type  f32:  194 tensors
0.00.029.171 I llama_model_loader: - type q8_0:   98 tensors
0.00.051.630 I llm_load_vocab: special tokens cache size = 25
0.00.057.610 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.613 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.613 I llm_load_print_meta: arch             = gptneox
0.00.057.613 I llm_load_print_meta: vocab type       = BPE
0.00.057.614 I llm_load_print_meta: n_vocab          = 50304
0.00.057.614 I llm_load_print_meta: n_merges         = 50009
0.00.057.614 I llm_load_print_meta: vocab_only       = 0
0.00.057.614 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.614 I llm_load_print_meta: n_embd           = 2048
0.00.057.614 I llm_load_print_meta: n_layer          = 24
0.00.057.618 I llm_load_print_meta: n_head           = 16
0.00.057.618 I llm_load_print_meta: n_head_kv        = 16
0.00.057.618 I llm_load_print_meta: n_rot            = 32
0.00.057.619 I llm_load_print_meta: n_swa            = 0
0.00.057.620 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.620 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.621 I llm_load_print_meta: n_gqa            = 1
0.00.057.621 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.622 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.623 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.623 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.623 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.623 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.624 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.624 I llm_load_print_meta: n_ff             = 8192
0.00.057.624 I llm_load_print_meta: n_expert         = 0
0.00.057.625 I llm_load_print_meta: n_expert_used    = 0
0.00.057.625 I llm_load_print_meta: causal attn      = 1
0.00.057.625 I llm_load_print_meta: pooling type     = 0
0.00.057.625 I llm_load_print_meta: rope type        = 2
0.00.057.625 I llm_load_print_meta: rope scaling     = linear
0.00.057.626 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.626 I llm_load_print_meta: freq_scale_train = 1
0.00.057.626 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.627 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.627 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.628 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.628 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.628 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.628 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.640 I llm_load_print_meta: model type       = 1.4B
0.00.057.640 I llm_load_print_meta: model ftype      = Q8_0
0.00.057.641 I llm_load_print_meta: model params     = 1.41 B
0.00.057.641 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.057.641 I llm_load_print_meta: general.name     = 1.4B
0.00.057.642 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.642 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.642 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.642 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.642 I llm_load_print_meta: LF token         = 128 ''
0.00.057.643 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.643 I llm_load_print_meta: max token length = 1024
0.00.059.707 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.708 I llm_load_tensors: offloading output layer to GPU
0.00.059.708 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.718 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.059.719 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.060.615 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.616 I llama_new_context_with_model: n_ctx         = 128
0.00.060.616 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.060.617 I llama_new_context_with_model: n_batch       = 128
0.00.060.617 I llama_new_context_with_model: n_ubatch      = 128
0.00.060.617 I llama_new_context_with_model: flash_attn    = 0
0.00.060.617 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.618 I llama_new_context_with_model: freq_scale    = 1
0.00.060.618 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.060.618 I ggml_metal_init: allocating
0.00.060.625 I ggml_metal_init: found device: Apple M4
0.00.060.627 I ggml_metal_init: picking default device: Apple M4
0.00.061.181 I ggml_metal_init: using embedded metal library
0.00.063.134 I ggml_metal_init: GPU name:   Apple M4
0.00.063.135 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.136 I ggml_metal_init: simdgroup reduction   = true
0.00.063.136 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.136 I ggml_metal_init: has bfloat            = true
0.00.063.136 I ggml_metal_init: use bfloat            = true
0.00.063.137 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.137 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.165 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.167 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.192 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.065 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.066 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.066 I llama_new_context_with_model: graph nodes  = 967
0.00.072.066 I llama_new_context_with_model: graph splits = 2
0.00.072.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.882.810 I 
0.00.882.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.882.858 I perplexity: tokenizing the input ..
0.00.891.088 I perplexity: tokenization took 8.228 ms
0.00.891.099 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.012.277 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.013.607 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.013.624 I llama_perf_context_print:        load time =     872.31 ms
0.01.013.624 I llama_perf_context_print: prompt eval time =     120.95 ms /   128 tokens (    0.94 ms per token,  1058.26 tokens per second)
0.01.013.625 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.013.626 I llama_perf_context_print:       total time =     130.81 ms /   129 tokens
0.01.013.921 I ggml_metal_free: deallocating

real	0m1.029s
user	0m0.086s
sys	0m0.162s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.017.142 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.792 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.797 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.799 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.803 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.804 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.804 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.804 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.805 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.806 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.806 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.808 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.809 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.809 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.812 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.812 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.812 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.673 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.194 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.194 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.195 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.195 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.196 I llama_model_loader: - type  f32:  194 tensors
0.00.044.196 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.197 I llama_model_loader: - type q6_K:    1 tensors
0.00.075.193 I llm_load_vocab: special tokens cache size = 25
0.00.085.473 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.476 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.476 I llm_load_print_meta: arch             = gptneox
0.00.085.477 I llm_load_print_meta: vocab type       = BPE
0.00.085.477 I llm_load_print_meta: n_vocab          = 50304
0.00.085.477 I llm_load_print_meta: n_merges         = 50009
0.00.085.478 I llm_load_print_meta: vocab_only       = 0
0.00.085.478 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.478 I llm_load_print_meta: n_embd           = 2048
0.00.085.478 I llm_load_print_meta: n_layer          = 24
0.00.085.483 I llm_load_print_meta: n_head           = 16
0.00.085.484 I llm_load_print_meta: n_head_kv        = 16
0.00.085.484 I llm_load_print_meta: n_rot            = 32
0.00.085.484 I llm_load_print_meta: n_swa            = 0
0.00.085.484 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.484 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.485 I llm_load_print_meta: n_gqa            = 1
0.00.085.486 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.487 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.488 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.488 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.489 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.489 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.489 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.490 I llm_load_print_meta: n_ff             = 8192
0.00.085.490 I llm_load_print_meta: n_expert         = 0
0.00.085.490 I llm_load_print_meta: n_expert_used    = 0
0.00.085.491 I llm_load_print_meta: causal attn      = 1
0.00.085.494 I llm_load_print_meta: pooling type     = 0
0.00.085.494 I llm_load_print_meta: rope type        = 2
0.00.085.494 I llm_load_print_meta: rope scaling     = linear
0.00.085.495 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.495 I llm_load_print_meta: freq_scale_train = 1
0.00.085.495 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.496 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.496 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.496 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.496 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.496 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.496 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.509 I llm_load_print_meta: model type       = 1.4B
0.00.085.509 I llm_load_print_meta: model ftype      = Q4_0
0.00.085.510 I llm_load_print_meta: model params     = 1.41 B
0.00.085.510 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.085.510 I llm_load_print_meta: general.name     = 1.4B
0.00.085.511 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.511 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.511 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.511 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.512 I llm_load_print_meta: LF token         = 128 ''
0.00.085.514 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.514 I llm_load_print_meta: max token length = 1024
0.00.088.092 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.092 I llm_load_tensors: offloading output layer to GPU
0.00.088.092 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.103 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.088.104 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.089.326 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.328 I llama_new_context_with_model: n_ctx         = 2048
0.00.089.328 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.089.328 I llama_new_context_with_model: n_batch       = 2048
0.00.089.329 I llama_new_context_with_model: n_ubatch      = 512
0.00.089.329 I llama_new_context_with_model: flash_attn    = 0
0.00.089.329 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.330 I llama_new_context_with_model: freq_scale    = 1
0.00.089.330 I ggml_metal_init: allocating
0.00.089.336 I ggml_metal_init: found device: Apple M4
0.00.089.339 I ggml_metal_init: picking default device: Apple M4
0.00.090.181 I ggml_metal_init: using embedded metal library
0.00.093.134 I ggml_metal_init: GPU name:   Apple M4
0.00.093.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.137 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.137 I ggml_metal_init: simdgroup reduction   = true
0.00.093.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.137 I ggml_metal_init: has bfloat            = true
0.00.093.138 I ggml_metal_init: use bfloat            = true
0.00.093.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.139 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.127.362 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.127.370 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.127.395 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.128.512 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.128.514 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.128.514 I llama_new_context_with_model: graph nodes  = 967
0.00.128.514 I llama_new_context_with_model: graph splits = 2
0.00.128.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.838.761 I main: llama threadpool init, n_threads = 4
0.00.838.823 I 
0.00.838.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.838.864 I 
0.00.839.172 I sampler seed: 1234
0.00.839.177 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.839.247 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.839.252 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.839.252 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.519.073 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.519.073 I llama_perf_context_print:        load time =     821.61 ms
0.01.519.074 I llama_perf_context_print: prompt eval time =      37.89 ms /     7 tokens (    5.41 ms per token,   184.74 tokens per second)
0.01.519.076 I llama_perf_context_print:        eval time =     638.87 ms /    63 runs   (   10.14 ms per token,    98.61 tokens per second)
0.01.519.076 I llama_perf_context_print:       total time =     680.32 ms /    70 tokens
0.01.519.267 I ggml_metal_free: deallocating

real	0m1.544s
user	0m0.135s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.622 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.467 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.477 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.294 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.337 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.129 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.129 I llama_model_loader: - type  f32:  194 tensors
0.00.024.130 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.130 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.844 I llm_load_vocab: special tokens cache size = 25
0.00.050.934 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.937 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.937 I llm_load_print_meta: arch             = gptneox
0.00.050.937 I llm_load_print_meta: vocab type       = BPE
0.00.050.938 I llm_load_print_meta: n_vocab          = 50304
0.00.050.938 I llm_load_print_meta: n_merges         = 50009
0.00.050.938 I llm_load_print_meta: vocab_only       = 0
0.00.050.938 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.938 I llm_load_print_meta: n_embd           = 2048
0.00.050.938 I llm_load_print_meta: n_layer          = 24
0.00.050.941 I llm_load_print_meta: n_head           = 16
0.00.050.942 I llm_load_print_meta: n_head_kv        = 16
0.00.050.942 I llm_load_print_meta: n_rot            = 32
0.00.050.942 I llm_load_print_meta: n_swa            = 0
0.00.050.942 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.942 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.943 I llm_load_print_meta: n_gqa            = 1
0.00.050.944 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.945 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.946 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.946 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.946 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.946 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.946 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.947 I llm_load_print_meta: n_ff             = 8192
0.00.050.947 I llm_load_print_meta: n_expert         = 0
0.00.050.947 I llm_load_print_meta: n_expert_used    = 0
0.00.050.948 I llm_load_print_meta: causal attn      = 1
0.00.050.948 I llm_load_print_meta: pooling type     = 0
0.00.050.948 I llm_load_print_meta: rope type        = 2
0.00.050.948 I llm_load_print_meta: rope scaling     = linear
0.00.050.949 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.949 I llm_load_print_meta: freq_scale_train = 1
0.00.050.949 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.949 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.949 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.950 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.950 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.950 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.950 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.961 I llm_load_print_meta: model type       = 1.4B
0.00.050.962 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.962 I llm_load_print_meta: model params     = 1.41 B
0.00.050.962 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.963 I llm_load_print_meta: general.name     = 1.4B
0.00.050.963 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.963 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.963 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.963 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.964 I llm_load_print_meta: LF token         = 128 ''
0.00.050.964 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.964 I llm_load_print_meta: max token length = 1024
0.00.052.524 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.525 I llm_load_tensors: offloading output layer to GPU
0.00.052.525 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.534 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.535 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.388 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.389 I llama_new_context_with_model: n_ctx         = 128
0.00.053.389 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.389 I llama_new_context_with_model: n_batch       = 128
0.00.053.389 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.389 I llama_new_context_with_model: flash_attn    = 0
0.00.053.390 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.390 I llama_new_context_with_model: freq_scale    = 1
0.00.053.390 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.391 I ggml_metal_init: allocating
0.00.053.397 I ggml_metal_init: found device: Apple M4
0.00.053.399 I ggml_metal_init: picking default device: Apple M4
0.00.053.925 I ggml_metal_init: using embedded metal library
0.00.055.848 I ggml_metal_init: GPU name:   Apple M4
0.00.055.850 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.850 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.851 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.851 I ggml_metal_init: simdgroup reduction   = true
0.00.055.851 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.851 I ggml_metal_init: has bfloat            = true
0.00.055.851 I ggml_metal_init: use bfloat            = true
0.00.055.852 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.852 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.839 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.841 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.855 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.707 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.708 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.708 I llama_new_context_with_model: graph nodes  = 967
0.00.065.709 I llama_new_context_with_model: graph splits = 2
0.00.065.720 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.249 I 
0.00.581.286 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.581.300 I perplexity: tokenizing the input ..
0.00.589.053 I perplexity: tokenization took 7.749 ms
0.00.589.064 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.646 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.712.953 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.712.969 I llama_perf_context_print:        load time =     571.62 ms
0.00.712.970 I llama_perf_context_print: prompt eval time =     122.36 ms /   128 tokens (    0.96 ms per token,  1046.13 tokens per second)
0.00.712.971 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.712.975 I llama_perf_context_print:       total time =     131.72 ms /   129 tokens
0.00.713.456 I ggml_metal_free: deallocating

real	0m0.730s
user	0m0.078s
sys	0m0.119s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.013.571 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.337 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.338 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.344 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.344 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.345 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.346 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.348 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.349 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.349 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.351 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.351 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.535 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.968 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.056 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.057 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.058 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.058 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.059 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.059 I llama_model_loader: - type  f32:  194 tensors
0.00.033.060 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.060 I llama_model_loader: - type q6_K:    1 tensors
0.00.066.133 I llm_load_vocab: special tokens cache size = 25
0.00.075.949 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.952 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.953 I llm_load_print_meta: arch             = gptneox
0.00.075.953 I llm_load_print_meta: vocab type       = BPE
0.00.075.953 I llm_load_print_meta: n_vocab          = 50304
0.00.075.953 I llm_load_print_meta: n_merges         = 50009
0.00.075.954 I llm_load_print_meta: vocab_only       = 0
0.00.075.954 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.954 I llm_load_print_meta: n_embd           = 2048
0.00.075.954 I llm_load_print_meta: n_layer          = 24
0.00.075.958 I llm_load_print_meta: n_head           = 16
0.00.075.959 I llm_load_print_meta: n_head_kv        = 16
0.00.075.959 I llm_load_print_meta: n_rot            = 32
0.00.075.959 I llm_load_print_meta: n_swa            = 0
0.00.075.959 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.962 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.963 I llm_load_print_meta: n_gqa            = 1
0.00.075.964 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.964 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.965 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.966 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.966 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.966 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.966 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.967 I llm_load_print_meta: n_ff             = 8192
0.00.075.968 I llm_load_print_meta: n_expert         = 0
0.00.075.968 I llm_load_print_meta: n_expert_used    = 0
0.00.075.968 I llm_load_print_meta: causal attn      = 1
0.00.075.968 I llm_load_print_meta: pooling type     = 0
0.00.075.968 I llm_load_print_meta: rope type        = 2
0.00.075.969 I llm_load_print_meta: rope scaling     = linear
0.00.075.969 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.970 I llm_load_print_meta: freq_scale_train = 1
0.00.075.970 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.970 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.970 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.971 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.971 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.971 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.971 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.978 I llm_load_print_meta: model type       = 1.4B
0.00.075.979 I llm_load_print_meta: model ftype      = Q4_1
0.00.075.979 I llm_load_print_meta: model params     = 1.41 B
0.00.075.980 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.075.980 I llm_load_print_meta: general.name     = 1.4B
0.00.075.981 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.981 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.981 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.981 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.982 I llm_load_print_meta: LF token         = 128 ''
0.00.075.982 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.982 I llm_load_print_meta: max token length = 1024
0.00.078.252 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.078.252 I llm_load_tensors: offloading output layer to GPU
0.00.078.253 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.078.258 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.078.259 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.079.529 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.530 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.531 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.531 I llama_new_context_with_model: n_batch       = 2048
0.00.079.531 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.531 I llama_new_context_with_model: flash_attn    = 0
0.00.079.532 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.532 I llama_new_context_with_model: freq_scale    = 1
0.00.079.532 I ggml_metal_init: allocating
0.00.079.536 I ggml_metal_init: found device: Apple M4
0.00.079.539 I ggml_metal_init: picking default device: Apple M4
0.00.080.221 I ggml_metal_init: using embedded metal library
0.00.082.998 I ggml_metal_init: GPU name:   Apple M4
0.00.083.000 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.001 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.002 I ggml_metal_init: simdgroup reduction   = true
0.00.083.002 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.002 I ggml_metal_init: has bfloat            = true
0.00.083.002 I ggml_metal_init: use bfloat            = true
0.00.083.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.092 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.097 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.116 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.100 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.114.101 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.114.101 I llama_new_context_with_model: graph nodes  = 967
0.00.114.101 I llama_new_context_with_model: graph splits = 2
0.00.114.125 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.378 I main: llama threadpool init, n_threads = 4
0.00.756.442 I 
0.00.756.490 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.756.492 I 
0.00.756.763 I sampler seed: 1234
0.00.756.770 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.847 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.852 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.853 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.491.990 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 67107.75 tokens per second)
0.01.491.991 I llama_perf_context_print:        load time =     742.80 ms
0.01.491.993 I llama_perf_context_print: prompt eval time =      41.80 ms /     7 tokens (    5.97 ms per token,   167.44 tokens per second)
0.01.491.993 I llama_perf_context_print:        eval time =     690.45 ms /    63 runs   (   10.96 ms per token,    91.24 tokens per second)
0.01.491.994 I llama_perf_context_print:       total time =     735.62 ms /    70 tokens
0.01.492.178 I ggml_metal_free: deallocating

real	0m1.532s
user	0m0.137s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.982 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.912 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.916 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.920 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.921 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.923 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.923 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.924 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.925 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.929 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.929 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.931 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.932 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.932 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.613 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.615 I llama_model_loader: - type  f32:  194 tensors
0.00.023.615 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.615 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.459 I llm_load_vocab: special tokens cache size = 25
0.00.050.648 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.651 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.651 I llm_load_print_meta: arch             = gptneox
0.00.050.652 I llm_load_print_meta: vocab type       = BPE
0.00.050.652 I llm_load_print_meta: n_vocab          = 50304
0.00.050.652 I llm_load_print_meta: n_merges         = 50009
0.00.050.652 I llm_load_print_meta: vocab_only       = 0
0.00.050.652 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.652 I llm_load_print_meta: n_embd           = 2048
0.00.050.653 I llm_load_print_meta: n_layer          = 24
0.00.050.656 I llm_load_print_meta: n_head           = 16
0.00.050.657 I llm_load_print_meta: n_head_kv        = 16
0.00.050.657 I llm_load_print_meta: n_rot            = 32
0.00.050.659 I llm_load_print_meta: n_swa            = 0
0.00.050.659 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.659 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.660 I llm_load_print_meta: n_gqa            = 1
0.00.050.661 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.662 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.662 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.663 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.664 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.664 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.664 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.665 I llm_load_print_meta: n_ff             = 8192
0.00.050.671 I llm_load_print_meta: n_expert         = 0
0.00.050.672 I llm_load_print_meta: n_expert_used    = 0
0.00.050.673 I llm_load_print_meta: causal attn      = 1
0.00.050.673 I llm_load_print_meta: pooling type     = 0
0.00.050.673 I llm_load_print_meta: rope type        = 2
0.00.050.673 I llm_load_print_meta: rope scaling     = linear
0.00.050.674 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.674 I llm_load_print_meta: freq_scale_train = 1
0.00.050.674 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.674 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.675 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.675 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.675 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.675 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.675 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.686 I llm_load_print_meta: model type       = 1.4B
0.00.050.687 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.687 I llm_load_print_meta: model params     = 1.41 B
0.00.050.687 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.688 I llm_load_print_meta: general.name     = 1.4B
0.00.050.688 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.688 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.688 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.688 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.689 I llm_load_print_meta: LF token         = 128 ''
0.00.050.689 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.689 I llm_load_print_meta: max token length = 1024
0.00.052.221 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.222 I llm_load_tensors: offloading output layer to GPU
0.00.052.222 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.231 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.232 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.051 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.051 I llama_new_context_with_model: n_ctx         = 128
0.00.053.052 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.052 I llama_new_context_with_model: n_batch       = 128
0.00.053.052 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.052 I llama_new_context_with_model: flash_attn    = 0
0.00.053.052 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.053 I llama_new_context_with_model: freq_scale    = 1
0.00.053.053 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.053 I ggml_metal_init: allocating
0.00.053.056 I ggml_metal_init: found device: Apple M4
0.00.053.058 I ggml_metal_init: picking default device: Apple M4
0.00.053.588 I ggml_metal_init: using embedded metal library
0.00.055.528 I ggml_metal_init: GPU name:   Apple M4
0.00.055.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.531 I ggml_metal_init: simdgroup reduction   = true
0.00.055.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.531 I ggml_metal_init: has bfloat            = true
0.00.055.531 I ggml_metal_init: use bfloat            = true
0.00.055.531 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.424 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.426 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.438 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.294 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.295 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.295 I llama_new_context_with_model: graph nodes  = 967
0.00.065.295 I llama_new_context_with_model: graph splits = 2
0.00.065.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.649 I 
0.00.644.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.644.691 I perplexity: tokenizing the input ..
0.00.652.510 I perplexity: tokenization took 7.817 ms
0.00.652.521 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.112 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.776.444 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.776.463 I llama_perf_context_print:        load time =     635.66 ms
0.00.776.464 I llama_perf_context_print: prompt eval time =     122.37 ms /   128 tokens (    0.96 ms per token,  1046.05 tokens per second)
0.00.776.465 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.466 I llama_perf_context_print:       total time =     131.82 ms /   129 tokens
0.00.776.886 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.078s
sys	0m0.118s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.766 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.999 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.004 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.005 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.006 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.006 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.007 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.008 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.008 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.008 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.009 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.009 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.009 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.014 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.881 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.975 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.980 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.981 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.981 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.982 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.982 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.982 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.036.983 I llama_model_loader: - type  f32:  194 tensors
0.00.036.984 I llama_model_loader: - type q5_0:   97 tensors
0.00.036.984 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.636 I llm_load_vocab: special tokens cache size = 25
0.00.069.761 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.765 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.765 I llm_load_print_meta: arch             = gptneox
0.00.069.765 I llm_load_print_meta: vocab type       = BPE
0.00.069.766 I llm_load_print_meta: n_vocab          = 50304
0.00.069.766 I llm_load_print_meta: n_merges         = 50009
0.00.069.766 I llm_load_print_meta: vocab_only       = 0
0.00.069.766 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.766 I llm_load_print_meta: n_embd           = 2048
0.00.069.767 I llm_load_print_meta: n_layer          = 24
0.00.069.770 I llm_load_print_meta: n_head           = 16
0.00.069.772 I llm_load_print_meta: n_head_kv        = 16
0.00.069.772 I llm_load_print_meta: n_rot            = 32
0.00.069.772 I llm_load_print_meta: n_swa            = 0
0.00.069.772 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.772 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.773 I llm_load_print_meta: n_gqa            = 1
0.00.069.774 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.775 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.776 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.777 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.778 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.778 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.778 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.779 I llm_load_print_meta: n_ff             = 8192
0.00.069.780 I llm_load_print_meta: n_expert         = 0
0.00.069.780 I llm_load_print_meta: n_expert_used    = 0
0.00.069.782 I llm_load_print_meta: causal attn      = 1
0.00.069.784 I llm_load_print_meta: pooling type     = 0
0.00.069.784 I llm_load_print_meta: rope type        = 2
0.00.069.784 I llm_load_print_meta: rope scaling     = linear
0.00.069.784 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.785 I llm_load_print_meta: freq_scale_train = 1
0.00.069.785 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.785 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.785 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.785 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.786 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.786 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.786 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.798 I llm_load_print_meta: model type       = 1.4B
0.00.069.798 I llm_load_print_meta: model ftype      = Q5_0
0.00.069.800 I llm_load_print_meta: model params     = 1.41 B
0.00.069.801 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.069.801 I llm_load_print_meta: general.name     = 1.4B
0.00.069.801 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.801 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.802 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.802 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.802 I llm_load_print_meta: LF token         = 128 ''
0.00.069.803 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.803 I llm_load_print_meta: max token length = 1024
0.00.072.287 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.287 I llm_load_tensors: offloading output layer to GPU
0.00.072.287 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.297 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.072.299 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.073.582 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.583 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.583 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.584 I llama_new_context_with_model: n_batch       = 2048
0.00.073.584 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.584 I llama_new_context_with_model: flash_attn    = 0
0.00.073.585 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.585 I llama_new_context_with_model: freq_scale    = 1
0.00.073.585 I ggml_metal_init: allocating
0.00.073.589 I ggml_metal_init: found device: Apple M4
0.00.073.592 I ggml_metal_init: picking default device: Apple M4
0.00.074.312 I ggml_metal_init: using embedded metal library
0.00.077.083 I ggml_metal_init: GPU name:   Apple M4
0.00.077.086 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.086 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.086 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.087 I ggml_metal_init: simdgroup reduction   = true
0.00.077.087 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.087 I ggml_metal_init: has bfloat            = true
0.00.077.087 I ggml_metal_init: use bfloat            = true
0.00.077.088 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.326 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.336 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.356 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.441 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.442 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.442 I llama_new_context_with_model: graph nodes  = 967
0.00.109.442 I llama_new_context_with_model: graph splits = 2
0.00.109.465 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.881.776 I main: llama threadpool init, n_threads = 4
0.00.881.820 I 
0.00.881.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.881.844 I 
0.00.882.062 I sampler seed: 1234
0.00.882.066 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.882.125 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.882.126 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.882.127 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.668.086 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.668.087 I llama_perf_context_print:        load time =     873.01 ms
0.01.668.088 I llama_perf_context_print: prompt eval time =      40.47 ms /     7 tokens (    5.78 ms per token,   172.98 tokens per second)
0.01.668.089 I llama_perf_context_print:        eval time =     742.65 ms /    63 runs   (   11.79 ms per token,    84.83 tokens per second)
0.01.668.089 I llama_perf_context_print:       total time =     786.31 ms /    70 tokens
0.01.668.275 I ggml_metal_free: deallocating

real	0m1.683s
user	0m0.118s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.881 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.363 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.367 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.369 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.369 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.369 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.370 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.370 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.371 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.371 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.372 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.372 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.373 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.373 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.375 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.375 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.110 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.184 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.943 I llama_model_loader: - type  f32:  194 tensors
0.00.023.944 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.944 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.855 I llm_load_vocab: special tokens cache size = 25
0.00.049.953 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.955 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.956 I llm_load_print_meta: arch             = gptneox
0.00.049.956 I llm_load_print_meta: vocab type       = BPE
0.00.049.956 I llm_load_print_meta: n_vocab          = 50304
0.00.049.956 I llm_load_print_meta: n_merges         = 50009
0.00.049.957 I llm_load_print_meta: vocab_only       = 0
0.00.049.957 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.957 I llm_load_print_meta: n_embd           = 2048
0.00.049.957 I llm_load_print_meta: n_layer          = 24
0.00.049.960 I llm_load_print_meta: n_head           = 16
0.00.049.961 I llm_load_print_meta: n_head_kv        = 16
0.00.049.961 I llm_load_print_meta: n_rot            = 32
0.00.049.961 I llm_load_print_meta: n_swa            = 0
0.00.049.961 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.962 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.964 I llm_load_print_meta: n_gqa            = 1
0.00.049.965 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.966 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.966 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.967 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.967 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.967 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.967 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.968 I llm_load_print_meta: n_ff             = 8192
0.00.049.968 I llm_load_print_meta: n_expert         = 0
0.00.049.969 I llm_load_print_meta: n_expert_used    = 0
0.00.049.969 I llm_load_print_meta: causal attn      = 1
0.00.049.969 I llm_load_print_meta: pooling type     = 0
0.00.049.969 I llm_load_print_meta: rope type        = 2
0.00.049.969 I llm_load_print_meta: rope scaling     = linear
0.00.049.970 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.970 I llm_load_print_meta: freq_scale_train = 1
0.00.049.970 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.970 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.971 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.971 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.971 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.971 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.971 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.982 I llm_load_print_meta: model type       = 1.4B
0.00.049.982 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.983 I llm_load_print_meta: model params     = 1.41 B
0.00.049.983 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.984 I llm_load_print_meta: general.name     = 1.4B
0.00.049.984 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.984 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.984 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.984 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.985 I llm_load_print_meta: LF token         = 128 ''
0.00.049.985 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.985 I llm_load_print_meta: max token length = 1024
0.00.051.519 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.519 I llm_load_tensors: offloading output layer to GPU
0.00.051.519 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.529 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.530 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.367 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.368 I llama_new_context_with_model: n_ctx         = 128
0.00.052.368 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.368 I llama_new_context_with_model: n_batch       = 128
0.00.052.368 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.369 I llama_new_context_with_model: flash_attn    = 0
0.00.052.369 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.369 I llama_new_context_with_model: freq_scale    = 1
0.00.052.370 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.370 I ggml_metal_init: allocating
0.00.052.373 I ggml_metal_init: found device: Apple M4
0.00.052.375 I ggml_metal_init: picking default device: Apple M4
0.00.052.899 I ggml_metal_init: using embedded metal library
0.00.054.842 I ggml_metal_init: GPU name:   Apple M4
0.00.054.843 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.844 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.844 I ggml_metal_init: simdgroup reduction   = true
0.00.054.844 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.845 I ggml_metal_init: has bfloat            = true
0.00.054.845 I ggml_metal_init: use bfloat            = true
0.00.054.845 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.839 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.845 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.869 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.739 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.740 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.740 I llama_new_context_with_model: graph nodes  = 967
0.00.064.740 I llama_new_context_with_model: graph splits = 2
0.00.064.752 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.589 I 
0.00.709.625 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.709.631 I perplexity: tokenizing the input ..
0.00.717.832 I perplexity: tokenization took 8.2 ms
0.00.717.849 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.852.023 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.853.343 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.853.357 I llama_perf_context_print:        load time =     699.70 ms
0.00.853.359 I llama_perf_context_print: prompt eval time =     133.95 ms /   128 tokens (    1.05 ms per token,   955.60 tokens per second)
0.00.853.360 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.853.360 I llama_perf_context_print:       total time =     143.77 ms /   129 tokens
0.00.853.773 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.078s
sys	0m0.130s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.055 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.511 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.513 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.513 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.514 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.514 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.514 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.515 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.516 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.516 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.516 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.519 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.519 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.522 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.526 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.527 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.379 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.380 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.380 I llama_model_loader: - type  f32:  194 tensors
0.00.025.381 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.381 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.362 I llm_load_vocab: special tokens cache size = 25
0.00.052.280 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.283 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.283 I llm_load_print_meta: arch             = gptneox
0.00.052.283 I llm_load_print_meta: vocab type       = BPE
0.00.052.284 I llm_load_print_meta: n_vocab          = 50304
0.00.052.284 I llm_load_print_meta: n_merges         = 50009
0.00.052.284 I llm_load_print_meta: vocab_only       = 0
0.00.052.284 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.284 I llm_load_print_meta: n_embd           = 2048
0.00.052.285 I llm_load_print_meta: n_layer          = 24
0.00.052.287 I llm_load_print_meta: n_head           = 16
0.00.052.287 I llm_load_print_meta: n_head_kv        = 16
0.00.052.290 I llm_load_print_meta: n_rot            = 32
0.00.052.290 I llm_load_print_meta: n_swa            = 0
0.00.052.290 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.290 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.291 I llm_load_print_meta: n_gqa            = 1
0.00.052.292 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.292 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.293 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.293 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.293 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.294 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.294 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.294 I llm_load_print_meta: n_ff             = 8192
0.00.052.295 I llm_load_print_meta: n_expert         = 0
0.00.052.295 I llm_load_print_meta: n_expert_used    = 0
0.00.052.296 I llm_load_print_meta: causal attn      = 1
0.00.052.297 I llm_load_print_meta: pooling type     = 0
0.00.052.298 I llm_load_print_meta: rope type        = 2
0.00.052.298 I llm_load_print_meta: rope scaling     = linear
0.00.052.298 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.298 I llm_load_print_meta: freq_scale_train = 1
0.00.052.299 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.299 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.299 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.299 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.299 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.299 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.299 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.311 I llm_load_print_meta: model type       = 1.4B
0.00.052.311 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.312 I llm_load_print_meta: model params     = 1.41 B
0.00.052.312 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.313 I llm_load_print_meta: general.name     = 1.4B
0.00.052.313 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.314 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.314 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.315 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.315 I llm_load_print_meta: LF token         = 128 ''
0.00.052.315 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.315 I llm_load_print_meta: max token length = 1024
0.00.054.352 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.353 I llm_load_tensors: offloading output layer to GPU
0.00.054.353 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.363 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.364 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.279 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.279 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.279 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.280 I llama_new_context_with_model: n_batch       = 2048
0.00.055.280 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.280 I llama_new_context_with_model: flash_attn    = 0
0.00.055.280 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.281 I llama_new_context_with_model: freq_scale    = 1
0.00.055.281 I ggml_metal_init: allocating
0.00.055.286 I ggml_metal_init: found device: Apple M4
0.00.055.289 I ggml_metal_init: picking default device: Apple M4
0.00.055.825 I ggml_metal_init: using embedded metal library
0.00.057.728 I ggml_metal_init: GPU name:   Apple M4
0.00.057.729 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.730 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.730 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.730 I ggml_metal_init: simdgroup reduction   = true
0.00.057.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.732 I ggml_metal_init: has bfloat            = true
0.00.057.732 I ggml_metal_init: use bfloat            = true
0.00.057.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.803 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.814 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.830 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.824 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.825 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.826 I llama_new_context_with_model: graph nodes  = 967
0.00.086.826 I llama_new_context_with_model: graph splits = 2
0.00.086.839 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.934 I main: llama threadpool init, n_threads = 4
0.00.773.966 I 
0.00.773.996 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.773.997 I 
0.00.774.153 I sampler seed: 1234
0.00.774.158 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.195 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.195 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.195 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.611.368 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.611.369 I llama_perf_context_print:        load time =     763.88 ms
0.01.611.370 I llama_perf_context_print: prompt eval time =      36.63 ms /     7 tokens (    5.23 ms per token,   191.08 tokens per second)
0.01.611.371 I llama_perf_context_print:        eval time =     797.56 ms /    63 runs   (   12.66 ms per token,    78.99 tokens per second)
0.01.611.372 I llama_perf_context_print:       total time =     837.44 ms /    70 tokens
0.01.611.544 I ggml_metal_free: deallocating

real	0m1.631s
user	0m0.109s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.742 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.565 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.569 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.571 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.571 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.572 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.572 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.573 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.574 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.574 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.576 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.578 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.338 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.405 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.102 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.104 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.104 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.105 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.105 I llama_model_loader: - type  f32:  194 tensors
0.00.023.106 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.106 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.010 I llm_load_vocab: special tokens cache size = 25
0.00.049.183 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.186 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.186 I llm_load_print_meta: arch             = gptneox
0.00.049.187 I llm_load_print_meta: vocab type       = BPE
0.00.049.187 I llm_load_print_meta: n_vocab          = 50304
0.00.049.187 I llm_load_print_meta: n_merges         = 50009
0.00.049.187 I llm_load_print_meta: vocab_only       = 0
0.00.049.187 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.188 I llm_load_print_meta: n_embd           = 2048
0.00.049.188 I llm_load_print_meta: n_layer          = 24
0.00.049.190 I llm_load_print_meta: n_head           = 16
0.00.049.193 I llm_load_print_meta: n_head_kv        = 16
0.00.049.193 I llm_load_print_meta: n_rot            = 32
0.00.049.193 I llm_load_print_meta: n_swa            = 0
0.00.049.194 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.194 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.195 I llm_load_print_meta: n_gqa            = 1
0.00.049.195 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.196 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.197 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.197 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.197 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.197 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.197 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.202 I llm_load_print_meta: n_ff             = 8192
0.00.049.203 I llm_load_print_meta: n_expert         = 0
0.00.049.203 I llm_load_print_meta: n_expert_used    = 0
0.00.049.203 I llm_load_print_meta: causal attn      = 1
0.00.049.203 I llm_load_print_meta: pooling type     = 0
0.00.049.203 I llm_load_print_meta: rope type        = 2
0.00.049.204 I llm_load_print_meta: rope scaling     = linear
0.00.049.204 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.204 I llm_load_print_meta: freq_scale_train = 1
0.00.049.204 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.205 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.207 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.207 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.207 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.207 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.207 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.218 I llm_load_print_meta: model type       = 1.4B
0.00.049.218 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.220 I llm_load_print_meta: model params     = 1.41 B
0.00.049.220 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.220 I llm_load_print_meta: general.name     = 1.4B
0.00.049.221 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.221 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.221 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.221 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.221 I llm_load_print_meta: LF token         = 128 ''
0.00.049.222 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.222 I llm_load_print_meta: max token length = 1024
0.00.050.738 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.739 I llm_load_tensors: offloading output layer to GPU
0.00.050.739 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.748 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.749 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.596 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.597 I llama_new_context_with_model: n_ctx         = 128
0.00.051.597 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.597 I llama_new_context_with_model: n_batch       = 128
0.00.051.597 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.597 I llama_new_context_with_model: flash_attn    = 0
0.00.051.598 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.598 I llama_new_context_with_model: freq_scale    = 1
0.00.051.598 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.599 I ggml_metal_init: allocating
0.00.051.604 I ggml_metal_init: found device: Apple M4
0.00.051.606 I ggml_metal_init: picking default device: Apple M4
0.00.052.144 I ggml_metal_init: using embedded metal library
0.00.054.028 I ggml_metal_init: GPU name:   Apple M4
0.00.054.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.030 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.030 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.031 I ggml_metal_init: simdgroup reduction   = true
0.00.054.031 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.031 I ggml_metal_init: has bfloat            = true
0.00.054.031 I ggml_metal_init: use bfloat            = true
0.00.054.031 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.041 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.046 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.059 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.920 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.921 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.921 I llama_new_context_with_model: graph nodes  = 967
0.00.063.922 I llama_new_context_with_model: graph splits = 2
0.00.063.933 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.860 I 
0.00.726.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.726.946 I perplexity: tokenizing the input ..
0.00.735.129 I perplexity: tokenization took 8.181 ms
0.00.735.141 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.869.293 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.870.612 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.870.626 I llama_perf_context_print:        load time =     718.11 ms
0.00.870.627 I llama_perf_context_print: prompt eval time =     133.93 ms /   128 tokens (    1.05 ms per token,   955.74 tokens per second)
0.00.870.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.870.628 I llama_perf_context_print:       total time =     143.77 ms /   129 tokens
0.00.870.954 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.078s
sys	0m0.137s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.530 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.119 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.136 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.137 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.137 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.137 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.986 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.130 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.865 I llama_model_loader: - type  f32:  194 tensors
0.00.023.865 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.866 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.866 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.969 I llm_load_vocab: special tokens cache size = 25
0.00.051.195 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.198 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.198 I llm_load_print_meta: arch             = gptneox
0.00.051.199 I llm_load_print_meta: vocab type       = BPE
0.00.051.199 I llm_load_print_meta: n_vocab          = 50304
0.00.051.199 I llm_load_print_meta: n_merges         = 50009
0.00.051.199 I llm_load_print_meta: vocab_only       = 0
0.00.051.199 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.200 I llm_load_print_meta: n_embd           = 2048
0.00.051.200 I llm_load_print_meta: n_layer          = 24
0.00.051.202 I llm_load_print_meta: n_head           = 16
0.00.051.203 I llm_load_print_meta: n_head_kv        = 16
0.00.051.203 I llm_load_print_meta: n_rot            = 32
0.00.051.204 I llm_load_print_meta: n_swa            = 0
0.00.051.204 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.204 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.207 I llm_load_print_meta: n_gqa            = 1
0.00.051.208 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.209 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.209 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.209 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.210 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.210 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.210 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.211 I llm_load_print_meta: n_ff             = 8192
0.00.051.211 I llm_load_print_meta: n_expert         = 0
0.00.051.211 I llm_load_print_meta: n_expert_used    = 0
0.00.051.211 I llm_load_print_meta: causal attn      = 1
0.00.051.212 I llm_load_print_meta: pooling type     = 0
0.00.051.212 I llm_load_print_meta: rope type        = 2
0.00.051.212 I llm_load_print_meta: rope scaling     = linear
0.00.051.212 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.213 I llm_load_print_meta: freq_scale_train = 1
0.00.051.213 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.213 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.213 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.215 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.215 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.215 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.215 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.228 I llm_load_print_meta: model type       = 1.4B
0.00.051.228 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.228 I llm_load_print_meta: model params     = 1.41 B
0.00.051.229 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.229 I llm_load_print_meta: general.name     = 1.4B
0.00.051.229 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.229 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: LF token         = 128 ''
0.00.051.230 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.230 I llm_load_print_meta: max token length = 1024
0.00.053.150 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.150 I llm_load_tensors: offloading output layer to GPU
0.00.053.150 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.160 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.161 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.092 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.093 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.093 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.093 I llama_new_context_with_model: n_batch       = 2048
0.00.054.093 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.093 I llama_new_context_with_model: flash_attn    = 0
0.00.054.094 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.094 I llama_new_context_with_model: freq_scale    = 1
0.00.054.094 I ggml_metal_init: allocating
0.00.054.098 I ggml_metal_init: found device: Apple M4
0.00.054.100 I ggml_metal_init: picking default device: Apple M4
0.00.054.651 I ggml_metal_init: using embedded metal library
0.00.056.575 I ggml_metal_init: GPU name:   Apple M4
0.00.056.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.577 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.577 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.578 I ggml_metal_init: simdgroup reduction   = true
0.00.056.578 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.578 I ggml_metal_init: has bfloat            = true
0.00.056.578 I ggml_metal_init: use bfloat            = true
0.00.056.579 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.579 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.125 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.132 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.149 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.264 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.266 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.266 I llama_new_context_with_model: graph nodes  = 967
0.00.086.266 I llama_new_context_with_model: graph splits = 2
0.00.086.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.061 I main: llama threadpool init, n_threads = 4
0.00.478.095 I 
0.00.478.123 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.478.123 I 
0.00.478.351 I sampler seed: 1234
0.00.478.355 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.478.375 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.478.375 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.478.375 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.159.359 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.159.360 I llama_perf_context_print:        load time =     468.52 ms
0.01.159.361 I llama_perf_context_print: prompt eval time =      36.02 ms /     7 tokens (    5.15 ms per token,   194.35 tokens per second)
0.01.159.361 I llama_perf_context_print:        eval time =     641.99 ms /    63 runs   (   10.19 ms per token,    98.13 tokens per second)
0.01.159.362 I llama_perf_context_print:       total time =     681.30 ms /    70 tokens
0.01.159.530 I ggml_metal_free: deallocating

real	0m1.180s
user	0m0.108s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.330 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.829 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.841 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.842 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.843 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.843 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.844 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.844 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.845 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.846 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.846 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.847 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.557 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.615 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.344 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.345 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.345 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.346 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.346 I llama_model_loader: - type  f32:  194 tensors
0.00.023.347 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.347 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.347 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.329 I llm_load_vocab: special tokens cache size = 25
0.00.049.390 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.392 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.393 I llm_load_print_meta: arch             = gptneox
0.00.049.393 I llm_load_print_meta: vocab type       = BPE
0.00.049.393 I llm_load_print_meta: n_vocab          = 50304
0.00.049.393 I llm_load_print_meta: n_merges         = 50009
0.00.049.394 I llm_load_print_meta: vocab_only       = 0
0.00.049.394 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.394 I llm_load_print_meta: n_embd           = 2048
0.00.049.394 I llm_load_print_meta: n_layer          = 24
0.00.049.397 I llm_load_print_meta: n_head           = 16
0.00.049.398 I llm_load_print_meta: n_head_kv        = 16
0.00.049.398 I llm_load_print_meta: n_rot            = 32
0.00.049.398 I llm_load_print_meta: n_swa            = 0
0.00.049.398 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.398 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.399 I llm_load_print_meta: n_gqa            = 1
0.00.049.400 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.400 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.401 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.401 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.402 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.403 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.403 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.404 I llm_load_print_meta: n_ff             = 8192
0.00.049.404 I llm_load_print_meta: n_expert         = 0
0.00.049.404 I llm_load_print_meta: n_expert_used    = 0
0.00.049.406 I llm_load_print_meta: causal attn      = 1
0.00.049.406 I llm_load_print_meta: pooling type     = 0
0.00.049.406 I llm_load_print_meta: rope type        = 2
0.00.049.406 I llm_load_print_meta: rope scaling     = linear
0.00.049.407 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.407 I llm_load_print_meta: freq_scale_train = 1
0.00.049.407 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.408 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.408 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.408 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.408 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.408 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.408 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.419 I llm_load_print_meta: model type       = 1.4B
0.00.049.420 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.420 I llm_load_print_meta: model params     = 1.41 B
0.00.049.421 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.421 I llm_load_print_meta: general.name     = 1.4B
0.00.049.421 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.421 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.421 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.422 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.422 I llm_load_print_meta: LF token         = 128 ''
0.00.049.422 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.422 I llm_load_print_meta: max token length = 1024
0.00.050.941 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.941 I llm_load_tensors: offloading output layer to GPU
0.00.050.941 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.951 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.952 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.841 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.842 I llama_new_context_with_model: n_ctx         = 128
0.00.051.842 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.842 I llama_new_context_with_model: n_batch       = 128
0.00.051.842 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.842 I llama_new_context_with_model: flash_attn    = 0
0.00.051.843 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.843 I llama_new_context_with_model: freq_scale    = 1
0.00.051.843 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.844 I ggml_metal_init: allocating
0.00.051.847 I ggml_metal_init: found device: Apple M4
0.00.051.849 I ggml_metal_init: picking default device: Apple M4
0.00.052.399 I ggml_metal_init: using embedded metal library
0.00.054.359 I ggml_metal_init: GPU name:   Apple M4
0.00.054.361 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.361 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.362 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.362 I ggml_metal_init: simdgroup reduction   = true
0.00.054.362 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.362 I ggml_metal_init: has bfloat            = true
0.00.054.362 I ggml_metal_init: use bfloat            = true
0.00.054.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.408 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.411 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.427 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.307 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.308 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.308 I llama_new_context_with_model: graph nodes  = 967
0.00.064.308 I llama_new_context_with_model: graph splits = 2
0.00.064.321 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.427.497 I 
0.00.427.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.427.527 I perplexity: tokenizing the input ..
0.00.435.105 I perplexity: tokenization took 7.576 ms
0.00.435.116 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.566.920 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.568.268 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.568.280 I llama_perf_context_print:        load time =     418.16 ms
0.00.568.281 I llama_perf_context_print: prompt eval time =     131.58 ms /   128 tokens (    1.03 ms per token,   972.81 tokens per second)
0.00.568.282 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.568.282 I llama_perf_context_print:       total time =     140.78 ms /   129 tokens
0.00.568.759 I ggml_metal_free: deallocating

real	0m0.585s
user	0m0.078s
sys	0m0.084s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.176 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.508 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.513 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.515 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.515 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.515 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.517 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.517 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.518 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.518 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.521 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.521 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.480 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.428 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.429 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.429 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.429 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.430 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.430 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.431 I llama_model_loader: - type  f32:  194 tensors
0.00.024.431 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.431 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.431 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.432 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.376 I llm_load_vocab: special tokens cache size = 25
0.00.051.493 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.496 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.496 I llm_load_print_meta: arch             = gptneox
0.00.051.497 I llm_load_print_meta: vocab type       = BPE
0.00.051.497 I llm_load_print_meta: n_vocab          = 50304
0.00.051.497 I llm_load_print_meta: n_merges         = 50009
0.00.051.497 I llm_load_print_meta: vocab_only       = 0
0.00.051.498 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.498 I llm_load_print_meta: n_embd           = 2048
0.00.051.498 I llm_load_print_meta: n_layer          = 24
0.00.051.500 I llm_load_print_meta: n_head           = 16
0.00.051.501 I llm_load_print_meta: n_head_kv        = 16
0.00.051.501 I llm_load_print_meta: n_rot            = 32
0.00.051.501 I llm_load_print_meta: n_swa            = 0
0.00.051.501 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.501 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.502 I llm_load_print_meta: n_gqa            = 1
0.00.051.505 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.506 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.507 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.507 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.507 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.508 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.508 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.509 I llm_load_print_meta: n_ff             = 8192
0.00.051.510 I llm_load_print_meta: n_expert         = 0
0.00.051.511 I llm_load_print_meta: n_expert_used    = 0
0.00.051.512 I llm_load_print_meta: causal attn      = 1
0.00.051.512 I llm_load_print_meta: pooling type     = 0
0.00.051.512 I llm_load_print_meta: rope type        = 2
0.00.051.512 I llm_load_print_meta: rope scaling     = linear
0.00.051.512 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.513 I llm_load_print_meta: freq_scale_train = 1
0.00.051.513 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.513 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.513 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.513 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.513 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.514 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.514 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.526 I llm_load_print_meta: model type       = 1.4B
0.00.051.526 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.526 I llm_load_print_meta: model params     = 1.41 B
0.00.051.527 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.527 I llm_load_print_meta: general.name     = 1.4B
0.00.051.527 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.527 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.528 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.528 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.528 I llm_load_print_meta: LF token         = 128 ''
0.00.051.528 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.528 I llm_load_print_meta: max token length = 1024
0.00.053.358 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.358 I llm_load_tensors: offloading output layer to GPU
0.00.053.358 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.363 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.363 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.292 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.293 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.293 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.293 I llama_new_context_with_model: n_batch       = 2048
0.00.054.294 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.294 I llama_new_context_with_model: flash_attn    = 0
0.00.054.294 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.295 I llama_new_context_with_model: freq_scale    = 1
0.00.054.295 I ggml_metal_init: allocating
0.00.054.302 I ggml_metal_init: found device: Apple M4
0.00.054.304 I ggml_metal_init: picking default device: Apple M4
0.00.054.854 I ggml_metal_init: using embedded metal library
0.00.056.780 I ggml_metal_init: GPU name:   Apple M4
0.00.056.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.784 I ggml_metal_init: simdgroup reduction   = true
0.00.056.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.784 I ggml_metal_init: has bfloat            = true
0.00.056.784 I ggml_metal_init: use bfloat            = true
0.00.056.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.785 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.523 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.532 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.553 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.432 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.434 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.434 I llama_new_context_with_model: graph nodes  = 967
0.00.085.434 I llama_new_context_with_model: graph splits = 2
0.00.085.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.622 I main: llama threadpool init, n_threads = 4
0.00.549.661 I 
0.00.549.703 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.549.704 I 
0.00.549.949 I sampler seed: 1234
0.00.549.954 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.549.994 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.549.998 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.549.998 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.295.347 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.01.295.348 I llama_perf_context_print:        load time =     540.44 ms
0.01.295.348 I llama_perf_context_print: prompt eval time =      39.63 ms /     7 tokens (    5.66 ms per token,   176.65 tokens per second)
0.01.295.349 I llama_perf_context_print:        eval time =     702.71 ms /    63 runs   (   11.15 ms per token,    89.65 tokens per second)
0.01.295.349 I llama_perf_context_print:       total time =     745.73 ms /    70 tokens
0.01.295.517 I ggml_metal_free: deallocating

real	0m1.310s
user	0m0.109s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.517 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.211 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.213 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.213 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.214 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.215 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.215 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.967 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.025 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.814 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.815 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.815 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.815 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.816 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.816 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.816 I llama_model_loader: - type  f32:  194 tensors
0.00.022.817 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.817 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.817 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.817 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.730 I llm_load_vocab: special tokens cache size = 25
0.00.048.808 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.811 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.811 I llm_load_print_meta: arch             = gptneox
0.00.048.812 I llm_load_print_meta: vocab type       = BPE
0.00.048.812 I llm_load_print_meta: n_vocab          = 50304
0.00.048.812 I llm_load_print_meta: n_merges         = 50009
0.00.048.812 I llm_load_print_meta: vocab_only       = 0
0.00.048.812 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.813 I llm_load_print_meta: n_embd           = 2048
0.00.048.813 I llm_load_print_meta: n_layer          = 24
0.00.048.815 I llm_load_print_meta: n_head           = 16
0.00.048.816 I llm_load_print_meta: n_head_kv        = 16
0.00.048.816 I llm_load_print_meta: n_rot            = 32
0.00.048.816 I llm_load_print_meta: n_swa            = 0
0.00.048.816 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.816 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.817 I llm_load_print_meta: n_gqa            = 1
0.00.048.818 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.819 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.819 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.820 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.820 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.820 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.820 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.821 I llm_load_print_meta: n_ff             = 8192
0.00.048.821 I llm_load_print_meta: n_expert         = 0
0.00.048.821 I llm_load_print_meta: n_expert_used    = 0
0.00.048.824 I llm_load_print_meta: causal attn      = 1
0.00.048.824 I llm_load_print_meta: pooling type     = 0
0.00.048.824 I llm_load_print_meta: rope type        = 2
0.00.048.825 I llm_load_print_meta: rope scaling     = linear
0.00.048.825 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.825 I llm_load_print_meta: freq_scale_train = 1
0.00.048.825 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.826 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.826 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.826 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.826 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.826 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.826 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.837 I llm_load_print_meta: model type       = 1.4B
0.00.048.838 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.838 I llm_load_print_meta: model params     = 1.41 B
0.00.048.838 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.839 I llm_load_print_meta: general.name     = 1.4B
0.00.048.839 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.839 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.839 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.839 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.840 I llm_load_print_meta: LF token         = 128 ''
0.00.048.840 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.840 I llm_load_print_meta: max token length = 1024
0.00.050.397 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.397 I llm_load_tensors: offloading output layer to GPU
0.00.050.397 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.406 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.407 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.285 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.286 I llama_new_context_with_model: n_ctx         = 128
0.00.051.286 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.286 I llama_new_context_with_model: n_batch       = 128
0.00.051.286 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.286 I llama_new_context_with_model: flash_attn    = 0
0.00.051.287 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.287 I llama_new_context_with_model: freq_scale    = 1
0.00.051.287 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.288 I ggml_metal_init: allocating
0.00.051.293 I ggml_metal_init: found device: Apple M4
0.00.051.296 I ggml_metal_init: picking default device: Apple M4
0.00.051.850 I ggml_metal_init: using embedded metal library
0.00.053.762 I ggml_metal_init: GPU name:   Apple M4
0.00.053.764 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.764 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.765 I ggml_metal_init: simdgroup reduction   = true
0.00.053.765 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.765 I ggml_metal_init: has bfloat            = true
0.00.053.765 I ggml_metal_init: use bfloat            = true
0.00.053.765 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.766 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.620 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.622 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.636 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.488 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.489 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.489 I llama_new_context_with_model: graph nodes  = 967
0.00.063.489 I llama_new_context_with_model: graph splits = 2
0.00.063.501 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.396 I 
0.00.494.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.494.440 I perplexity: tokenizing the input ..
0.00.502.521 I perplexity: tokenization took 8.079 ms
0.00.502.532 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.634.723 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.636.080 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.636.097 I llama_perf_context_print:        load time =     485.87 ms
0.00.636.098 I llama_perf_context_print: prompt eval time =     131.93 ms /   128 tokens (    1.03 ms per token,   970.19 tokens per second)
0.00.636.099 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.636.099 I llama_perf_context_print:       total time =     141.71 ms /   129 tokens
0.00.636.642 I ggml_metal_free: deallocating

real	0m0.650s
user	0m0.078s
sys	0m0.097s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.576 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.849 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.853 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.859 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.860 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.861 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.861 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.862 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.862 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.863 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.863 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.863 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.864 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.864 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.866 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.866 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.866 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.770 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.540 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.543 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.543 I llama_model_loader: - type  f32:  194 tensors
0.00.024.544 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.544 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.544 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.486 I llm_load_vocab: special tokens cache size = 25
0.00.050.316 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.319 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.320 I llm_load_print_meta: arch             = gptneox
0.00.050.320 I llm_load_print_meta: vocab type       = BPE
0.00.050.320 I llm_load_print_meta: n_vocab          = 50304
0.00.050.320 I llm_load_print_meta: n_merges         = 50009
0.00.050.321 I llm_load_print_meta: vocab_only       = 0
0.00.050.321 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.321 I llm_load_print_meta: n_embd           = 2048
0.00.050.321 I llm_load_print_meta: n_layer          = 24
0.00.050.323 I llm_load_print_meta: n_head           = 16
0.00.050.324 I llm_load_print_meta: n_head_kv        = 16
0.00.050.324 I llm_load_print_meta: n_rot            = 32
0.00.050.325 I llm_load_print_meta: n_swa            = 0
0.00.050.325 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.325 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.326 I llm_load_print_meta: n_gqa            = 1
0.00.050.326 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.327 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.328 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.328 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.328 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.329 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.329 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.329 I llm_load_print_meta: n_ff             = 8192
0.00.050.330 I llm_load_print_meta: n_expert         = 0
0.00.050.330 I llm_load_print_meta: n_expert_used    = 0
0.00.050.330 I llm_load_print_meta: causal attn      = 1
0.00.050.330 I llm_load_print_meta: pooling type     = 0
0.00.050.330 I llm_load_print_meta: rope type        = 2
0.00.050.330 I llm_load_print_meta: rope scaling     = linear
0.00.050.332 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.332 I llm_load_print_meta: freq_scale_train = 1
0.00.050.332 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.333 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.333 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.333 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.333 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.333 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.334 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.345 I llm_load_print_meta: model type       = 1.4B
0.00.050.346 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.346 I llm_load_print_meta: model params     = 1.41 B
0.00.050.346 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.346 I llm_load_print_meta: general.name     = 1.4B
0.00.050.347 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.347 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.347 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.347 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.348 I llm_load_print_meta: LF token         = 128 ''
0.00.050.348 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.348 I llm_load_print_meta: max token length = 1024
0.00.051.890 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.890 I llm_load_tensors: offloading output layer to GPU
0.00.051.890 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.900 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.901 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.774 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.774 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.774 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.775 I llama_new_context_with_model: n_batch       = 2048
0.00.052.775 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.775 I llama_new_context_with_model: flash_attn    = 0
0.00.052.775 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.776 I llama_new_context_with_model: freq_scale    = 1
0.00.052.776 I ggml_metal_init: allocating
0.00.052.782 I ggml_metal_init: found device: Apple M4
0.00.052.785 I ggml_metal_init: picking default device: Apple M4
0.00.053.315 I ggml_metal_init: using embedded metal library
0.00.055.283 I ggml_metal_init: GPU name:   Apple M4
0.00.055.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.285 I ggml_metal_init: simdgroup reduction   = true
0.00.055.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.286 I ggml_metal_init: has bfloat            = true
0.00.055.286 I ggml_metal_init: use bfloat            = true
0.00.055.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.768 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.777 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.800 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.798 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.799 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.799 I llama_new_context_with_model: graph nodes  = 967
0.00.083.799 I llama_new_context_with_model: graph splits = 2
0.00.083.822 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.104 I main: llama threadpool init, n_threads = 4
0.00.624.143 I 
0.00.624.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.624.168 I 
0.00.624.385 I sampler seed: 1234
0.00.624.389 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.624.429 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.624.430 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.624.430 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.379.982 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.01.379.984 I llama_perf_context_print:        load time =     614.52 ms
0.01.379.985 I llama_perf_context_print: prompt eval time =      42.03 ms /     7 tokens (    6.00 ms per token,   166.54 tokens per second)
0.01.379.985 I llama_perf_context_print:        eval time =     710.40 ms /    63 runs   (   11.28 ms per token,    88.68 tokens per second)
0.01.379.986 I llama_perf_context_print:       total time =     755.88 ms /    70 tokens
0.01.380.144 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.108s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.927 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.814 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.818 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.820 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.821 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.821 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.822 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.824 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.825 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.825 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.825 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.826 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.826 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.608 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.657 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.467 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.470 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.470 I llama_model_loader: - type  f32:  194 tensors
0.00.024.471 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.471 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.471 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.273 I llm_load_vocab: special tokens cache size = 25
0.00.051.336 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.339 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.340 I llm_load_print_meta: arch             = gptneox
0.00.051.340 I llm_load_print_meta: vocab type       = BPE
0.00.051.340 I llm_load_print_meta: n_vocab          = 50304
0.00.051.340 I llm_load_print_meta: n_merges         = 50009
0.00.051.341 I llm_load_print_meta: vocab_only       = 0
0.00.051.341 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.341 I llm_load_print_meta: n_embd           = 2048
0.00.051.341 I llm_load_print_meta: n_layer          = 24
0.00.051.343 I llm_load_print_meta: n_head           = 16
0.00.051.344 I llm_load_print_meta: n_head_kv        = 16
0.00.051.344 I llm_load_print_meta: n_rot            = 32
0.00.051.345 I llm_load_print_meta: n_swa            = 0
0.00.051.345 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.345 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.348 I llm_load_print_meta: n_gqa            = 1
0.00.051.348 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.350 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.351 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.351 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.351 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.352 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.352 I llm_load_print_meta: n_ff             = 8192
0.00.051.353 I llm_load_print_meta: n_expert         = 0
0.00.051.353 I llm_load_print_meta: n_expert_used    = 0
0.00.051.353 I llm_load_print_meta: causal attn      = 1
0.00.051.353 I llm_load_print_meta: pooling type     = 0
0.00.051.353 I llm_load_print_meta: rope type        = 2
0.00.051.353 I llm_load_print_meta: rope scaling     = linear
0.00.051.354 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.354 I llm_load_print_meta: freq_scale_train = 1
0.00.051.354 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.355 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.355 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.355 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.355 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.355 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.356 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.367 I llm_load_print_meta: model type       = 1.4B
0.00.051.367 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.368 I llm_load_print_meta: model params     = 1.41 B
0.00.051.368 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.368 I llm_load_print_meta: general.name     = 1.4B
0.00.051.368 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.369 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.370 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.370 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.370 I llm_load_print_meta: LF token         = 128 ''
0.00.051.370 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.370 I llm_load_print_meta: max token length = 1024
0.00.052.888 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.888 I llm_load_tensors: offloading output layer to GPU
0.00.052.888 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.897 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.898 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.719 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.719 I llama_new_context_with_model: n_ctx         = 128
0.00.053.720 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.720 I llama_new_context_with_model: n_batch       = 128
0.00.053.720 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.720 I llama_new_context_with_model: flash_attn    = 0
0.00.053.720 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.721 I llama_new_context_with_model: freq_scale    = 1
0.00.053.721 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.721 I ggml_metal_init: allocating
0.00.053.727 I ggml_metal_init: found device: Apple M4
0.00.053.729 I ggml_metal_init: picking default device: Apple M4
0.00.054.251 I ggml_metal_init: using embedded metal library
0.00.056.202 I ggml_metal_init: GPU name:   Apple M4
0.00.056.204 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.204 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.204 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.205 I ggml_metal_init: simdgroup reduction   = true
0.00.056.205 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.205 I ggml_metal_init: has bfloat            = true
0.00.056.205 I ggml_metal_init: use bfloat            = true
0.00.056.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.206 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.229 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.234 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.247 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.143 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.144 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.144 I llama_new_context_with_model: graph nodes  = 967
0.00.066.144 I llama_new_context_with_model: graph splits = 2
0.00.066.156 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.107 I 
0.00.583.142 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.583.145 I perplexity: tokenizing the input ..
0.00.591.002 I perplexity: tokenization took 7.855 ms
0.00.591.014 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.177 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.726.524 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.726.543 I llama_perf_context_print:        load time =     573.17 ms
0.00.726.544 I llama_perf_context_print: prompt eval time =     133.94 ms /   128 tokens (    1.05 ms per token,   955.67 tokens per second)
0.00.726.545 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.726.545 I llama_perf_context_print:       total time =     143.44 ms /   129 tokens
0.00.727.038 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.078s
sys	0m0.114s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.462 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.196 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.200 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.206 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.207 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.210 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.211 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.211 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.211 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.212 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.212 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.214 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.214 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.214 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.216 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.127 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.129 I llama_model_loader: - type  f32:  194 tensors
0.00.024.130 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.130 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.116 I llm_load_vocab: special tokens cache size = 25
0.00.051.250 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.253 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.253 I llm_load_print_meta: arch             = gptneox
0.00.051.253 I llm_load_print_meta: vocab type       = BPE
0.00.051.253 I llm_load_print_meta: n_vocab          = 50304
0.00.051.254 I llm_load_print_meta: n_merges         = 50009
0.00.051.254 I llm_load_print_meta: vocab_only       = 0
0.00.051.254 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.254 I llm_load_print_meta: n_embd           = 2048
0.00.051.254 I llm_load_print_meta: n_layer          = 24
0.00.051.257 I llm_load_print_meta: n_head           = 16
0.00.051.257 I llm_load_print_meta: n_head_kv        = 16
0.00.051.258 I llm_load_print_meta: n_rot            = 32
0.00.051.258 I llm_load_print_meta: n_swa            = 0
0.00.051.258 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.258 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.259 I llm_load_print_meta: n_gqa            = 1
0.00.051.259 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.260 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.261 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.263 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.264 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.264 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.264 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.265 I llm_load_print_meta: n_ff             = 8192
0.00.051.265 I llm_load_print_meta: n_expert         = 0
0.00.051.265 I llm_load_print_meta: n_expert_used    = 0
0.00.051.265 I llm_load_print_meta: causal attn      = 1
0.00.051.265 I llm_load_print_meta: pooling type     = 0
0.00.051.265 I llm_load_print_meta: rope type        = 2
0.00.051.266 I llm_load_print_meta: rope scaling     = linear
0.00.051.266 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.266 I llm_load_print_meta: freq_scale_train = 1
0.00.051.267 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.267 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.267 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.267 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.267 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.267 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.267 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.280 I llm_load_print_meta: model type       = 1.4B
0.00.051.280 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.281 I llm_load_print_meta: model params     = 1.41 B
0.00.051.281 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.281 I llm_load_print_meta: general.name     = 1.4B
0.00.051.281 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.282 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.282 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.282 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.282 I llm_load_print_meta: LF token         = 128 ''
0.00.051.282 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.283 I llm_load_print_meta: max token length = 1024
0.00.053.342 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.342 I llm_load_tensors: offloading output layer to GPU
0.00.053.342 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.352 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.353 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.273 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.273 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.274 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.274 I llama_new_context_with_model: n_batch       = 2048
0.00.054.274 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.274 I llama_new_context_with_model: flash_attn    = 0
0.00.054.275 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.275 I llama_new_context_with_model: freq_scale    = 1
0.00.054.275 I ggml_metal_init: allocating
0.00.054.278 I ggml_metal_init: found device: Apple M4
0.00.054.280 I ggml_metal_init: picking default device: Apple M4
0.00.054.857 I ggml_metal_init: using embedded metal library
0.00.056.804 I ggml_metal_init: GPU name:   Apple M4
0.00.056.806 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.806 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.807 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.807 I ggml_metal_init: simdgroup reduction   = true
0.00.056.807 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.807 I ggml_metal_init: has bfloat            = true
0.00.056.807 I ggml_metal_init: use bfloat            = true
0.00.056.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.808 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.275 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.283 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.302 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.370 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.372 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.372 I llama_new_context_with_model: graph nodes  = 967
0.00.085.372 I llama_new_context_with_model: graph splits = 2
0.00.085.384 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.304 I main: llama threadpool init, n_threads = 4
0.00.696.337 I 
0.00.696.363 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.696.365 I 
0.00.696.506 I sampler seed: 1234
0.00.696.510 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.549 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.558 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.561 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.536.278 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.536.278 I llama_perf_context_print:        load time =     687.84 ms
0.01.536.279 I llama_perf_context_print: prompt eval time =      38.65 ms /     7 tokens (    5.52 ms per token,   181.13 tokens per second)
0.01.536.280 I llama_perf_context_print:        eval time =     798.01 ms /    63 runs   (   12.67 ms per token,    78.95 tokens per second)
0.01.536.280 I llama_perf_context_print:       total time =     839.98 ms /    70 tokens
0.01.536.458 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.110s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.762 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.459 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.460 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.460 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.461 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.461 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.462 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.462 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.462 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.463 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.463 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.465 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.465 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.070 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.070 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.071 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.071 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.071 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.072 I llama_model_loader: - type  f32:  194 tensors
0.00.023.072 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.073 I llama_model_loader: - type q6_K:   37 tensors
0.00.042.902 I llm_load_vocab: special tokens cache size = 25
0.00.048.984 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.986 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.986 I llm_load_print_meta: arch             = gptneox
0.00.048.987 I llm_load_print_meta: vocab type       = BPE
0.00.048.987 I llm_load_print_meta: n_vocab          = 50304
0.00.048.987 I llm_load_print_meta: n_merges         = 50009
0.00.048.987 I llm_load_print_meta: vocab_only       = 0
0.00.048.987 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.988 I llm_load_print_meta: n_embd           = 2048
0.00.048.988 I llm_load_print_meta: n_layer          = 24
0.00.048.990 I llm_load_print_meta: n_head           = 16
0.00.048.991 I llm_load_print_meta: n_head_kv        = 16
0.00.048.991 I llm_load_print_meta: n_rot            = 32
0.00.048.991 I llm_load_print_meta: n_swa            = 0
0.00.048.992 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.992 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.993 I llm_load_print_meta: n_gqa            = 1
0.00.048.993 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.994 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.995 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.995 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.995 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.995 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.996 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.996 I llm_load_print_meta: n_ff             = 8192
0.00.048.997 I llm_load_print_meta: n_expert         = 0
0.00.048.997 I llm_load_print_meta: n_expert_used    = 0
0.00.048.997 I llm_load_print_meta: causal attn      = 1
0.00.048.997 I llm_load_print_meta: pooling type     = 0
0.00.048.997 I llm_load_print_meta: rope type        = 2
0.00.048.997 I llm_load_print_meta: rope scaling     = linear
0.00.048.998 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.998 I llm_load_print_meta: freq_scale_train = 1
0.00.048.999 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.999 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.999 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.999 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.999 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.999 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.999 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.011 I llm_load_print_meta: model type       = 1.4B
0.00.049.011 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.011 I llm_load_print_meta: model params     = 1.41 B
0.00.049.012 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.012 I llm_load_print_meta: general.name     = 1.4B
0.00.049.012 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.012 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.013 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.013 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.013 I llm_load_print_meta: LF token         = 128 ''
0.00.049.013 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.013 I llm_load_print_meta: max token length = 1024
0.00.050.550 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.551 I llm_load_tensors: offloading output layer to GPU
0.00.050.551 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.560 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.561 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.398 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.398 I llama_new_context_with_model: n_ctx         = 128
0.00.051.399 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.399 I llama_new_context_with_model: n_batch       = 128
0.00.051.399 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.399 I llama_new_context_with_model: flash_attn    = 0
0.00.051.399 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.400 I llama_new_context_with_model: freq_scale    = 1
0.00.051.400 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.400 I ggml_metal_init: allocating
0.00.051.407 I ggml_metal_init: found device: Apple M4
0.00.051.409 I ggml_metal_init: picking default device: Apple M4
0.00.051.942 I ggml_metal_init: using embedded metal library
0.00.053.856 I ggml_metal_init: GPU name:   Apple M4
0.00.053.857 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.858 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.858 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.858 I ggml_metal_init: simdgroup reduction   = true
0.00.053.859 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.859 I ggml_metal_init: has bfloat            = true
0.00.053.859 I ggml_metal_init: use bfloat            = true
0.00.053.859 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.860 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.915 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.920 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.934 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.798 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.799 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.799 I llama_new_context_with_model: graph nodes  = 967
0.00.063.799 I llama_new_context_with_model: graph splits = 2
0.00.063.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.509 I 
0.00.659.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.659.543 I perplexity: tokenizing the input ..
0.00.667.638 I perplexity: tokenization took 8.093 ms
0.00.667.648 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.789 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.809.119 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.809.131 I llama_perf_context_print:        load time =     650.74 ms
0.00.809.132 I llama_perf_context_print: prompt eval time =     139.92 ms /   128 tokens (    1.09 ms per token,   914.83 tokens per second)
0.00.809.133 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.136 I llama_perf_context_print:       total time =     149.62 ms /   129 tokens
0.00.809.431 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.078s
sys	0m0.129s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.744 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.407 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.408 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.410 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.257 I llama_model_loader: - type  f32:  194 tensors
0.00.025.257 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.377 I llm_load_vocab: special tokens cache size = 25
0.00.051.543 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.546 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.546 I llm_load_print_meta: arch             = gptneox
0.00.051.546 I llm_load_print_meta: vocab type       = BPE
0.00.051.547 I llm_load_print_meta: n_vocab          = 50304
0.00.051.547 I llm_load_print_meta: n_merges         = 50009
0.00.051.547 I llm_load_print_meta: vocab_only       = 0
0.00.051.547 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.547 I llm_load_print_meta: n_embd           = 2048
0.00.051.547 I llm_load_print_meta: n_layer          = 24
0.00.051.550 I llm_load_print_meta: n_head           = 16
0.00.051.551 I llm_load_print_meta: n_head_kv        = 16
0.00.051.551 I llm_load_print_meta: n_rot            = 32
0.00.051.551 I llm_load_print_meta: n_swa            = 0
0.00.051.552 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.552 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.552 I llm_load_print_meta: n_gqa            = 1
0.00.051.553 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.554 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.555 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.555 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.555 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.555 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.556 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.556 I llm_load_print_meta: n_ff             = 8192
0.00.051.556 I llm_load_print_meta: n_expert         = 0
0.00.051.557 I llm_load_print_meta: n_expert_used    = 0
0.00.051.557 I llm_load_print_meta: causal attn      = 1
0.00.051.558 I llm_load_print_meta: pooling type     = 0
0.00.051.560 I llm_load_print_meta: rope type        = 2
0.00.051.560 I llm_load_print_meta: rope scaling     = linear
0.00.051.561 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.561 I llm_load_print_meta: freq_scale_train = 1
0.00.051.561 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.561 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.561 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.562 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.562 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.562 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.562 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.574 I llm_load_print_meta: model type       = 1.4B
0.00.051.575 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.575 I llm_load_print_meta: model params     = 1.41 B
0.00.051.576 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.576 I llm_load_print_meta: general.name     = 1.4B
0.00.051.576 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.576 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.576 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.577 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.577 I llm_load_print_meta: LF token         = 128 ''
0.00.051.577 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.577 I llm_load_print_meta: max token length = 1024
0.00.053.607 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.607 I llm_load_tensors: offloading output layer to GPU
0.00.053.607 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.617 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.618 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.564 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.564 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.565 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.565 I llama_new_context_with_model: n_batch       = 2048
0.00.054.565 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.565 I llama_new_context_with_model: flash_attn    = 0
0.00.054.566 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.566 I llama_new_context_with_model: freq_scale    = 1
0.00.054.566 I ggml_metal_init: allocating
0.00.054.570 I ggml_metal_init: found device: Apple M4
0.00.054.572 I ggml_metal_init: picking default device: Apple M4
0.00.055.140 I ggml_metal_init: using embedded metal library
0.00.057.046 I ggml_metal_init: GPU name:   Apple M4
0.00.057.048 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.048 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.048 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.049 I ggml_metal_init: simdgroup reduction   = true
0.00.057.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.050 I ggml_metal_init: has bfloat            = true
0.00.057.050 I ggml_metal_init: use bfloat            = true
0.00.057.051 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.051 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.085 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.095 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.112 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.176 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.177 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.178 I llama_new_context_with_model: graph nodes  = 967
0.00.085.178 I llama_new_context_with_model: graph splits = 2
0.00.085.200 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.441 I main: llama threadpool init, n_threads = 4
0.00.758.478 I 
0.00.758.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.758.510 I 
0.00.758.734 I sampler seed: 1234
0.00.758.739 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.792 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.800 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.805 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.634.220 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.634.221 I llama_perf_context_print:        load time =     748.69 ms
0.01.634.222 I llama_perf_context_print: prompt eval time =      38.42 ms /     7 tokens (    5.49 ms per token,   182.18 tokens per second)
0.01.634.222 I llama_perf_context_print:        eval time =     833.87 ms /    63 runs   (   13.24 ms per token,    75.55 tokens per second)
0.01.634.224 I llama_perf_context_print:       total time =     875.78 ms /    70 tokens
0.01.634.397 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.108s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4213 (6c595676) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.719 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.255 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.259 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.261 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.261 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.262 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.262 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.262 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.263 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.263 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.264 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.268 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.952 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.974 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.699 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.700 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.701 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.702 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.702 I llama_model_loader: - type  f32:  194 tensors
0.00.023.702 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.474 I llm_load_vocab: special tokens cache size = 25
0.00.049.499 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.502 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.502 I llm_load_print_meta: arch             = gptneox
0.00.049.503 I llm_load_print_meta: vocab type       = BPE
0.00.049.503 I llm_load_print_meta: n_vocab          = 50304
0.00.049.503 I llm_load_print_meta: n_merges         = 50009
0.00.049.503 I llm_load_print_meta: vocab_only       = 0
0.00.049.503 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.504 I llm_load_print_meta: n_embd           = 2048
0.00.049.504 I llm_load_print_meta: n_layer          = 24
0.00.049.506 I llm_load_print_meta: n_head           = 16
0.00.049.507 I llm_load_print_meta: n_head_kv        = 16
0.00.049.509 I llm_load_print_meta: n_rot            = 32
0.00.049.509 I llm_load_print_meta: n_swa            = 0
0.00.049.510 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.510 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.511 I llm_load_print_meta: n_gqa            = 1
0.00.049.511 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.512 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.513 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.513 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.513 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.513 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.513 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.514 I llm_load_print_meta: n_ff             = 8192
0.00.049.514 I llm_load_print_meta: n_expert         = 0
0.00.049.514 I llm_load_print_meta: n_expert_used    = 0
0.00.049.515 I llm_load_print_meta: causal attn      = 1
0.00.049.515 I llm_load_print_meta: pooling type     = 0
0.00.049.515 I llm_load_print_meta: rope type        = 2
0.00.049.515 I llm_load_print_meta: rope scaling     = linear
0.00.049.515 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.520 I llm_load_print_meta: freq_scale_train = 1
0.00.049.520 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.520 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.521 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.521 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.521 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.521 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.521 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.532 I llm_load_print_meta: model type       = 1.4B
0.00.049.533 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.534 I llm_load_print_meta: model params     = 1.41 B
0.00.049.535 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.535 I llm_load_print_meta: general.name     = 1.4B
0.00.049.535 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.535 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.535 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.535 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.536 I llm_load_print_meta: LF token         = 128 ''
0.00.049.536 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.536 I llm_load_print_meta: max token length = 1024
0.00.051.045 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.045 I llm_load_tensors: offloading output layer to GPU
0.00.051.045 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.054 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.055 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.920 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.921 I llama_new_context_with_model: n_ctx         = 128
0.00.051.921 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.921 I llama_new_context_with_model: n_batch       = 128
0.00.051.921 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.922 I llama_new_context_with_model: flash_attn    = 0
0.00.051.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.922 I llama_new_context_with_model: freq_scale    = 1
0.00.051.923 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.923 I ggml_metal_init: allocating
0.00.051.928 I ggml_metal_init: found device: Apple M4
0.00.051.930 I ggml_metal_init: picking default device: Apple M4
0.00.052.451 I ggml_metal_init: using embedded metal library
0.00.054.351 I ggml_metal_init: GPU name:   Apple M4
0.00.054.352 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.352 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.353 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.353 I ggml_metal_init: simdgroup reduction   = true
0.00.054.353 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.353 I ggml_metal_init: has bfloat            = true
0.00.054.353 I ggml_metal_init: use bfloat            = true
0.00.054.354 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.354 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.271 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.273 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.287 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.152 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.154 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.154 I llama_new_context_with_model: graph nodes  = 967
0.00.064.154 I llama_new_context_with_model: graph splits = 2
0.00.064.166 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.150.033 I 
0.00.150.068 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.150.073 I perplexity: tokenizing the input ..
0.00.157.119 I perplexity: tokenization took 7.045 ms
0.00.157.136 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.297.737 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.299.114 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.299.131 I llama_perf_context_print:        load time =     140.31 ms
0.00.299.132 I llama_perf_context_print: prompt eval time =     140.38 ms /   128 tokens (    1.10 ms per token,   911.82 tokens per second)
0.00.299.133 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.299.134 I llama_perf_context_print:       total time =     149.10 ms /   129 tokens
0.00.299.523 I ggml_metal_free: deallocating

real	0m0.316s
user	0m0.076s
sys	0m0.040s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4213 (6c595676)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b50a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b50ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b50b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b50b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b50bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b50c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b50ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b50cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b50d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b50da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b50df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b50e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b50ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b50f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b50ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b510660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b510d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b5114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b511bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b512390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b512ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b5131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b5138f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b514190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b5148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b514b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b515180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b515df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b516330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b5165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b516a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b516d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b5175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b517b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b517de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b518280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b518720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b518bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b519060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b519500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b5199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b519e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b51a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b51a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b51aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b51b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b51b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b51bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b51c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b51cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b51d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b51d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b51ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b51e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b51ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b51f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b51f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b51f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b51fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b5205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b520890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b520d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b5211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b521670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b521b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b521fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b522450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b5228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b522d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b523230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b5236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b523b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b524010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b5244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b524950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b524df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b525290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b525730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b525bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b526070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b526510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b5269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b526e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b5272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b527790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b527c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b5280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b528570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b528a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b528eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b529350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b5297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b529c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b52a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b52a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b52aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b52af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b51bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b52b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b52ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b52bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b52c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b52c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b52cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b52d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b52d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b52da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b52df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b52e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b52e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b52ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b52f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b52f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b52fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b52ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b530400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b5308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b530d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b5311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b531680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b531b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b531fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b532460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b532900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b532da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b533240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b5336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b533b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b534020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b5344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b534960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b534e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b5352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b535740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b535be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b536080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b536520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b5369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b536e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b537300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b5377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b537c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b5380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b538580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b538a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b538ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b539360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b539800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b539ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b53a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b53a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b53aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b53af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b53b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b53b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b53bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b53c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b53c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b53cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b53d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b53d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b53df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b53e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b53ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b53f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b53f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b53fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b5402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b540840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b540d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b5412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b541830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b541d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b5422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b542820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b542d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b5432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b543810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b543d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b5442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b544800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b544d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b5452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b5457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b545d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b546290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b5467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b546d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b547280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b5477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b547d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b548270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b5487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b548d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b549260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b5497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b549d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b54a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b54a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b54acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b54b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b54b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b54bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b54c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b54c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b54ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b54d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b54d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b54dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b54e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b54e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b54ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b54f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b54f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b54fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b5501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b550740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b550c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b5511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b551730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b551c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b5521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b552720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b552c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b553110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b5535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b553a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b553ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b554390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b554830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b554cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b555170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b555610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b555ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b555f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b5563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b556890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b556de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b557500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b557c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b558340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b558a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b558d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b559330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b559940 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.133.965 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b405980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b405df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b406260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b4066d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b406b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b406fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b407420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b407890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b407d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b408220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b408690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b408d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b409830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b409fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b40a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b40af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b40b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b40bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b40c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b40cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b40d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b40da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b40e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b40e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b40efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b40f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b40f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b40f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b40fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b4102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b410720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b410c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b4110c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b411380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b4117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b411c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b4120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b412540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b4129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b412e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b413290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b413700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b413b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b413fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b414450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b4148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b414d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b4151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b415610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b415a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b415ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b416360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b4167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b416c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b4170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b417520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b417a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b417f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b418400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b418870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b418ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b419150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b4195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b419a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b419ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b41a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b41a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b41abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b41b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b41b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b41b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b41bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b41c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b41c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b41cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b41cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b41d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b41d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b41dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b41e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b41e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b41ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b41ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b41f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b41f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b41fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b420040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b4204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b420920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b420d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b421200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b421670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b421ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b421f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b4223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b422830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b422ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b423110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b423580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b4239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b423e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b4242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b424740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b424bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b425020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b425490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b425900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b425d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b4261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b426650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b426ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b426f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b4273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b427810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b427c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b4280f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b428560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b4289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b428e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b4292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b429720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b429b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b42a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b42a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b42a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b42ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b42b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b42b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b42baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b42bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b42c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b42c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b42cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b42d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b42d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b42d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b42de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b42e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b42e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b42eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b42efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b42f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b42f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b42fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b4301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b430610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b430a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b430ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b431360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b4317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b431c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b4320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b432520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b432990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b432e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b433270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b4336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b433b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b433fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b434430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b4348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b434d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b435180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b4355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b435a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b435ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b436340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b436ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b437190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b437450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b4378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b437d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b4381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b438610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b438a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b438ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b439360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b4397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b439c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b43a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b43a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b43a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b43ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b43b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b43b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b43bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b43bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b43c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b43c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b43cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b43d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b43d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b43da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b43ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b43e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b43e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b43ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b43f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b43f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b43f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b43fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b440250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b4406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b440b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b440fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b441410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b441880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b441cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b442160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b4425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b442a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b442eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b443320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b443790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b443c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b444070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b4444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b444950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b444dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b445230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b4456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b445b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b445f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b4463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b446860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b446cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b447140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b4475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b447a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b447e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b448300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b448770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b448be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b449050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b4494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b449930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b449da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b44a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b44ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b44b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b44bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b44c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b44c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b44c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b44cca0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b405980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b405df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b406260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b4066d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b406b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b406fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b407420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b407890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b407d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b408170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b4085e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b408bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b4094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b409c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b40a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b40ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b40b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b40b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b40bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b40c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b40d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b40d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b40de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b40e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b40ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b40f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b40f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b40f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b40fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b410230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b4106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b410b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b410f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b411240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b4116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b411b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b411f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b412400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b412870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b412ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b413150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b4135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b413a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b413ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b414310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b414780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b414bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b415060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b4154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b415940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b415db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b416220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b416690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b416b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b416f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b4173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b417850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b417cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b418130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b4185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b418a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b418e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b4192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b419760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b419bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b41a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b41a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b41a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b41ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b41b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b41b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b41bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b41bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b41c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b41c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b41cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b41d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b41d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b41d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b41de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b41e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b41e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b41ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b41f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b41f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b41f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b41fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b4201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b420650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b420ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b420f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b4213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b421810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b421c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b4220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b422560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b4229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b422e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b4232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b423720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b423b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b424000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b424470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b4248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b424d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b4251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b425630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b425aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b425f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b426380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b4267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b426c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b4270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b427540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b4279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b427e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b428290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b428700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b428b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b428fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b429450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b4298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b429d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b42a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b42a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b42aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b42aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b42b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b42b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b42bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b42c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b42c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b42c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b42ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b42d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b42d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b42db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b42dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b42e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b42e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b42ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b42f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b42f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b42fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b42fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b430340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b4307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b430c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b431090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b431500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b431970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b431de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b432250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b4326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b432b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b432fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b433410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b433880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b433cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b434160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b4345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b434a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b434eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b435320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b435790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b435c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b436070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b4367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b436c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b4370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b437540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b4379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b437e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b438290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b438700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b438b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b438fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b439450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b4398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b439d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b43a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b43a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b43aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b43aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b43b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b43b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b43bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b43c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b43c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b43c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b43ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b43d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b43d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b43db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b43dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b43e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b43e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b43ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b43f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b43f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b43fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b43fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b440340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b4407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b440c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b441090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b441500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b441970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b441de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b442250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b4426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b442b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b442fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b443410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b443880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b443cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b444160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b4445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b444a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b444eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b445320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b445790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b445c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b446070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b4464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b446950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b446dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b447230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b4476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b447b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b447f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b4483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b448860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b448cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b449140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b4495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b449a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b449e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b44a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b44ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b44b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b44ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b44bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b44c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b44c7a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.773s
user	0m0.289s
sys	0m0.308s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4213 (6c595676)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ae0b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ae0b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ae0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ae0c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ae0ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ae0d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ae0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ae0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ae0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ae0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ae0eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ae0f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ae0fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ae102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ae10b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ae11220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ae11940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ae12060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ae12780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ae12f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ae13670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ae13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ae144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ae14d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ae15470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ae15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ae15d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ae169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ae16ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ae171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ae17650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ae17910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ae181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ae186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ae189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ae18e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ae192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ae19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ae19c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ae1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ae1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ae1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ae1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ae1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ae1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ae1bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ae1c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ae1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ae1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ae1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ae1dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ae1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ae1e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ae1efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ae1f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ae1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ae200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ae20390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ae209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ae21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ae21450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ae218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ae21d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ae22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ae226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ae22b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ae23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ae234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ae23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ae23df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ae24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ae24730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ae24bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ae25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ae25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ae259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ae25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ae262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ae26790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ae26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ae270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ae27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ae27a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ae27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ae28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ae287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ae28c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ae29130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ae295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ae29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ae29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ae2a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ae2a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ae2acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ae2b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ae2b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ae2bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ae1c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ae2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ae2c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ae2ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ae2cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ae2d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ae2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ae2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ae2e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ae2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ae2eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ae2ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ae2f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ae2f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ae2fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ae301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ae30680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ae30b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ae30fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ae31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ae31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ae31da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ae32240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ae326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ae32b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ae33020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ae334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ae33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ae33e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ae342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ae34740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ae34be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ae35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ae35520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ae359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ae35e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ae36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ae367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ae36c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ae370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ae37580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ae37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ae37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ae38360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ae38800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ae38ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ae39140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ae395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ae39a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ae39f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ae3a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ae3a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ae3ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ae3b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ae3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ae3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ae3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ae3c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ae3cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ae3d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ae3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ae3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ae3df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ae3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ae3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ae3f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ae3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ae3fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ae40260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ae40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ae40eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ae41400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ae41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ae41ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ae423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ae42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ae42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ae433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ae43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ae43e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ae443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ae44920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ae44e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ae453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ae45910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ae45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ae463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ae46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ae46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ae473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ae478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ae47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ae48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ae488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ae48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ae49380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ae498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ae49e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ae4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ae4a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ae4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ae4b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ae4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ae4be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ae4c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ae4c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ae4cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ae4d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ae4d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ae4dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ae4e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ae4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ae4edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ae4f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ae4f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ae4fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ae50310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ae50860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ae50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ae51300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ae51850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ae51da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ae522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ae52840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ae52d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ae532e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ae53830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ae53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ae54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ae54610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ae54ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ae54f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ae553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ae55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ae55d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ae561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ae56670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ae56b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ae56fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ae57450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ae579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ae580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ae587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ae58f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ae59620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ae598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ae59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ae5a500 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c005000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c0052c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c005730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c005ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c006010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c006480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c0068f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c006d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c0071d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c007640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c007ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c0081d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c008cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c0094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c009cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c00a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c00aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c00b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c00b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c00c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c00c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c00cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c00d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c00dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c00e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c00e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c00e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c00edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c00f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c00f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c00fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c010070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c0104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c0107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c010c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c011080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c0114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c011960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c011dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c012240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c0126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c012b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c012f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c013400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c013870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c013ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c014150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c0145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c014a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c014ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c015310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c015780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c015bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c016060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c0164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c016940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c016eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c0173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c017820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c017c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c018100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c018570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c0189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c018e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c0192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c019730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c019ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c01a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c01a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c01a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c01ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c01b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c01b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c01bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c01bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c01c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c01c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c01cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c01d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c01d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c01d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c01de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c01e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c01e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c01eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c01eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c01f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c01f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c01fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c0201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c020620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c020a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c020f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c021370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c0217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c021c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c0220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c022530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c0229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c022e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c023280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c0236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c023b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c023fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c024440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c0248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c024d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c025190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c025600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c025a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c025ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c026350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c0267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c026c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c0270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c027510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c027980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c027df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c028260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c0286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c028b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c028fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c029420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c029890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c029d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c02a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c02a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c02aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c02aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c02b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c02b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c02bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c02c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c02c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c02c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c02cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c02d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c02d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c02db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c02df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c02e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c02e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c02ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c02f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c02f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c02fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c02fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c030310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c030780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c030bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c031060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c0314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c031940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c031db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c032220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c032690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c032b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c032f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c0333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c033850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c033cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c034130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c0345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c034a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c034e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c0352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c035760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c0362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c0365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c036870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c036ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c037150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c0375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c037a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c037ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c038310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c038780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c038bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c039060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c0394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c039940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c039db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c03a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c03a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c03ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c03af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c03b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c03b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c03bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c03c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c03c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c03ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c03ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c03d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c03d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c03dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c03e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c03e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c03e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c03ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c03f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c03f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c03fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c03ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c0403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c040830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c040ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c041110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c041580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c0419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c041e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c0422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c042740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c042bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c043020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c043490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c043900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c043d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c0441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c044650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c044ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c044f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c0453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c045810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c045c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c0460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c046560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c0469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c046e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c0472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c047720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c047b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c048000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c048470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c0488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c048d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c0491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c049630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c04a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c04a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c04afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c04b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c04b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c04bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c04c0c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f6044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f604950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f604dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f605230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f6056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f605b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f605f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f6063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f606860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f606db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f607220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f6078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f6083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f608b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f609380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f609aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f60a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f60a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f60b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f60b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f60bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f60c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f60cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f60d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f60db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f60de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f60e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f60e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f60e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f60ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f60f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f60f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f60fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f60ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f610380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f6107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f610c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f6110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f611540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f6119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f611e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f612290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f612700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f612b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f612fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f613450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f6138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f613d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f6141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f614610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f614a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f614ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f615360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f6157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f615c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f6160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f616b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f616f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f617400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f617870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f617ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f618150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f6185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f618a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f618ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f619310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f619780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f619bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f61a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f61a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f61a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f61adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f61b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f61b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f61bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f61bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f61c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f61c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f61ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f61d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f61d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f61da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f61de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f61e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f61e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f61ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f61f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f61f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f61f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f61fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f620200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f620670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f620ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f620f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f6213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f621830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f621ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f622110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f622580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f6229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f622e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f6232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f623740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f623bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f624020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f624490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f624900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f624d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f6251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f625650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f625ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f625f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f6263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f626810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f626c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f6270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f627560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f6279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f627e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f6282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f628720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f628b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f629000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f629470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f6298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f629d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f62a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f62a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f62aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f62af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f62b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f62b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f62bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f62c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f62c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f62c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f62ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f62d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f62d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f62db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f62dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f62e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f62e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f62ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f62f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f62f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f62fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f62fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14f630360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14f6307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14f630c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14f6310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14f631520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14f631990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14f631e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14f632270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14f6326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14f632b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14f632fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14f633430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14f6338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14f633d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14f634180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14f6345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14f634a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14f634ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14f635a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14f635d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14f635fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14f636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f6368c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14f636d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14f6371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14f637610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14f637a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14f637ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14f638360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14f6387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14f638c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f6390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14f639520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14f639990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14f639e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14f63a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14f63a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14f63ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14f63afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14f63b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f63b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14f63bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14f63c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14f63c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14f63ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f63ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14f63d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f63d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14f63dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14f63e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14f63e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14f63e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14f63ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f63f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f63f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f63fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f63ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f640410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14f640880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14f640cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14f641160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f6415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14f641a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14f641eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14f642320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14f642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14f642c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14f643070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14f6434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14f643950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14f643dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14f644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14f6446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14f644b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14f644f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14f6453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14f645860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14f645cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14f646140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14f6465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14f646a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14f646e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14f647300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14f647770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14f647be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14f648050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14f6484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14f648930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14f648da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14f6498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14f64a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14f64a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14f64ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14f64b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f64b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f64b830 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.938s
user	0m0.240s
sys	0m0.138s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.53 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.10 sec*proc (2 tests)

Total Test time (real) =   1.11 sec
        1.13 real         0.71 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.26 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.15 user         0.04 sys
```
