Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.578s
user	0m0.908s
sys	0m1.249s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Built target test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-sampling
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Built target test-log
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-gguf
[ 63%] Built target test-autorelease
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-chat-template
[ 63%] Built target test-backend-ops
[ 63%] Built target test-barrier
[ 63%] Built target test-quantize-fns
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-quantize-perf
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Built target llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target test-rope
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Built target llama-bench
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookahead
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-passkey
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-parallel
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-cli
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Built target llama-perplexity
[ 86%] Generating index.html.gz.hpp
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-quantize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-run
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.138s
user	0m6.210s
sys	0m9.704s

main: quantize time =  3697.03 ms
main:    total time =  3697.03 ms

main: quantize time =  1782.69 ms
main:    total time =  1782.69 ms

main: quantize time =  2391.95 ms
main:    total time =  2391.95 ms

main: quantize time =  2621.03 ms
main:    total time =  2621.03 ms

main: quantize time =  3021.83 ms
main:    total time =  3021.83 ms

main: quantize time =  6075.97 ms
main:    total time =  6075.97 ms

main: quantize time =  5773.98 ms
main:    total time =  5773.98 ms

main: quantize time =  7285.45 ms
main:    total time =  7285.45 ms

main: quantize time =  6151.85 ms
main:    total time =  6151.85 ms

main: quantize time =  4502.75 ms
main:    total time =  4502.75 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.146 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.316 I main: llama backend init
0.00.000.322 I main: load the model and apply lora adapter, if any
0.00.031.972 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.045.786 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.813 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.813 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.814 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.818 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.819 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.820 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.063.758 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.759 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.760 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.760 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.761 I llama_model_loader: - type  f32:  194 tensors
0.00.063.762 I llama_model_loader: - type  f16:   98 tensors
0.00.063.763 I print_info: file format = GGUF V3 (latest)
0.00.063.765 I print_info: file type   = all F32 (guessed)
0.00.063.766 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.080.563 I load: special tokens cache size = 25
0.00.089.957 I load: token to piece cache size = 0.2984 MB
0.00.089.960 I print_info: arch             = gptneox
0.00.089.961 I print_info: vocab_only       = 0
0.00.089.961 I print_info: n_ctx_train      = 2048
0.00.089.961 I print_info: n_embd           = 2048
0.00.089.961 I print_info: n_layer          = 24
0.00.089.965 I print_info: n_head           = 16
0.00.089.966 I print_info: n_head_kv        = 16
0.00.089.966 I print_info: n_rot            = 32
0.00.089.966 I print_info: n_swa            = 0
0.00.089.967 I print_info: n_embd_head_k    = 128
0.00.089.967 I print_info: n_embd_head_v    = 128
0.00.089.968 I print_info: n_gqa            = 1
0.00.089.969 I print_info: n_embd_k_gqa     = 2048
0.00.089.969 I print_info: n_embd_v_gqa     = 2048
0.00.089.970 I print_info: f_norm_eps       = 1.0e-05
0.00.089.971 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.971 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.971 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.971 I print_info: f_logit_scale    = 0.0e+00
0.00.089.972 I print_info: n_ff             = 8192
0.00.089.972 I print_info: n_expert         = 0
0.00.089.972 I print_info: n_expert_used    = 0
0.00.089.973 I print_info: causal attn      = 1
0.00.089.973 I print_info: pooling type     = 0
0.00.089.973 I print_info: rope type        = 2
0.00.089.976 I print_info: rope scaling     = linear
0.00.089.977 I print_info: freq_base_train  = 10000.0
0.00.089.977 I print_info: freq_scale_train = 1
0.00.089.977 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.978 I print_info: rope_finetuned   = unknown
0.00.089.979 I print_info: ssm_d_conv       = 0
0.00.089.980 I print_info: ssm_d_inner      = 0
0.00.089.980 I print_info: ssm_d_state      = 0
0.00.089.980 I print_info: ssm_dt_rank      = 0
0.00.089.980 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.980 I print_info: model type       = 1.4B
0.00.089.981 I print_info: model params     = 1.41 B
0.00.089.981 I print_info: general.name     = 1.4B
0.00.089.982 I print_info: vocab type       = BPE
0.00.089.982 I print_info: n_vocab          = 50304
0.00.089.982 I print_info: n_merges         = 50009
0.00.089.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.983 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.987 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.988 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.988 I print_info: LF token         = 128 'Ä'
0.00.089.988 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.989 I print_info: max token length = 1024
0.00.128.858 I load_tensors: offloading 24 repeating layers to GPU
0.00.128.862 I load_tensors: offloading output layer to GPU
0.00.128.863 I load_tensors: offloaded 25/25 layers to GPU
0.00.128.880 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.128.882 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.129.152 I llama_init_from_model: n_seq_max     = 1
0.00.129.153 I llama_init_from_model: n_ctx         = 2048
0.00.129.153 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.129.153 I llama_init_from_model: n_batch       = 2048
0.00.129.154 I llama_init_from_model: n_ubatch      = 512
0.00.129.154 I llama_init_from_model: flash_attn    = 0
0.00.129.154 I llama_init_from_model: freq_base     = 10000.0
0.00.129.155 I llama_init_from_model: freq_scale    = 1
0.00.129.155 I ggml_metal_init: allocating
0.00.129.172 I ggml_metal_init: found device: Apple M4
0.00.129.177 I ggml_metal_init: picking default device: Apple M4
0.00.129.769 I ggml_metal_init: using embedded metal library
0.00.138.637 I ggml_metal_init: GPU name:   Apple M4
0.00.138.639 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.138.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.138.640 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.138.640 I ggml_metal_init: simdgroup reduction   = true
0.00.138.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.138.641 I ggml_metal_init: has residency sets    = true
0.00.138.641 I ggml_metal_init: has bfloat            = true
0.00.138.641 I ggml_metal_init: use bfloat            = true
0.00.138.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.138.642 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.163.652 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.194.066 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.194.073 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.194.096 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.198.394 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.198.396 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.198.396 I llama_init_from_model: graph nodes  = 967
0.00.198.397 I llama_init_from_model: graph splits = 2
0.00.198.400 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.198.529 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.198.530 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.266.082 I main: llama threadpool init, n_threads = 4
0.00.266.124 I 
0.00.266.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.266.155 I 
0.00.266.195 I sampler seed: 1234
0.00.266.200 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.266.224 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.266.226 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.266.226 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.106.176 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.02.106.177 I llama_perf_context_print:        load time =     233.07 ms
0.02.106.178 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.20 tokens per second)
0.02.106.178 I llama_perf_context_print:        eval time =    1793.45 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.106.179 I llama_perf_context_print:       total time =    1841.13 ms /    70 tokens
0.02.106.413 I ggml_metal_free: deallocating

real	0m2.404s
user	0m0.134s
sys	0m0.134s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.892 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.062 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.068 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.070 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.071 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.071 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.072 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.072 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.073 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.073 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.074 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.074 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.076 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.077 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.079 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.079 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.979 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.952 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.953 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.953 I llama_model_loader: - type  f32:  194 tensors
0.00.033.954 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.955 I print_info: file format = GGUF V3 (latest)
0.00.033.955 I print_info: file type   = Q8_0
0.00.033.956 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.049 I load: special tokens cache size = 25
0.00.049.326 I load: token to piece cache size = 0.2984 MB
0.00.049.331 I print_info: arch             = gptneox
0.00.049.331 I print_info: vocab_only       = 0
0.00.049.331 I print_info: n_ctx_train      = 2048
0.00.049.331 I print_info: n_embd           = 2048
0.00.049.332 I print_info: n_layer          = 24
0.00.049.339 I print_info: n_head           = 16
0.00.049.340 I print_info: n_head_kv        = 16
0.00.049.341 I print_info: n_rot            = 32
0.00.049.341 I print_info: n_swa            = 0
0.00.049.341 I print_info: n_embd_head_k    = 128
0.00.049.341 I print_info: n_embd_head_v    = 128
0.00.049.342 I print_info: n_gqa            = 1
0.00.049.342 I print_info: n_embd_k_gqa     = 2048
0.00.049.343 I print_info: n_embd_v_gqa     = 2048
0.00.049.344 I print_info: f_norm_eps       = 1.0e-05
0.00.049.344 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.344 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.345 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.347 I print_info: f_logit_scale    = 0.0e+00
0.00.049.348 I print_info: n_ff             = 8192
0.00.049.359 I print_info: n_expert         = 0
0.00.049.361 I print_info: n_expert_used    = 0
0.00.049.362 I print_info: causal attn      = 1
0.00.049.362 I print_info: pooling type     = 0
0.00.049.362 I print_info: rope type        = 2
0.00.049.362 I print_info: rope scaling     = linear
0.00.049.364 I print_info: freq_base_train  = 10000.0
0.00.049.364 I print_info: freq_scale_train = 1
0.00.049.365 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.365 I print_info: rope_finetuned   = unknown
0.00.049.365 I print_info: ssm_d_conv       = 0
0.00.049.365 I print_info: ssm_d_inner      = 0
0.00.049.365 I print_info: ssm_d_state      = 0
0.00.049.365 I print_info: ssm_dt_rank      = 0
0.00.049.365 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.366 I print_info: model type       = 1.4B
0.00.049.367 I print_info: model params     = 1.41 B
0.00.049.367 I print_info: general.name     = 1.4B
0.00.049.367 I print_info: vocab type       = BPE
0.00.049.368 I print_info: n_vocab          = 50304
0.00.049.368 I print_info: n_merges         = 50009
0.00.049.368 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.368 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.368 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.369 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.369 I print_info: LF token         = 128 'Ä'
0.00.049.370 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.370 I print_info: max token length = 1024
0.01.058.199 I load_tensors: offloading 24 repeating layers to GPU
0.01.058.204 I load_tensors: offloading output layer to GPU
0.01.058.205 I load_tensors: offloaded 25/25 layers to GPU
0.01.058.231 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.058.235 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.058.914 I llama_init_from_model: n_seq_max     = 1
0.01.058.916 I llama_init_from_model: n_ctx         = 2048
0.01.058.917 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.058.917 I llama_init_from_model: n_batch       = 2048
0.01.058.917 I llama_init_from_model: n_ubatch      = 512
0.01.058.918 I llama_init_from_model: flash_attn    = 0
0.01.058.918 I llama_init_from_model: freq_base     = 10000.0
0.01.058.919 I llama_init_from_model: freq_scale    = 1
0.01.058.920 I ggml_metal_init: allocating
0.01.058.936 I ggml_metal_init: found device: Apple M4
0.01.058.944 I ggml_metal_init: picking default device: Apple M4
0.01.060.157 I ggml_metal_init: using embedded metal library
0.01.066.071 I ggml_metal_init: GPU name:   Apple M4
0.01.066.076 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.066.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.066.077 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.066.078 I ggml_metal_init: simdgroup reduction   = true
0.01.066.078 I ggml_metal_init: simdgroup matrix mul. = true
0.01.066.078 I ggml_metal_init: has residency sets    = true
0.01.066.079 I ggml_metal_init: has bfloat            = true
0.01.066.079 I ggml_metal_init: use bfloat            = true
0.01.066.080 I ggml_metal_init: hasUnifiedMemory      = true
0.01.066.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.085.795 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.143.528 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.143.542 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.143.566 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.147.854 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.147.856 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.147.856 I llama_init_from_model: graph nodes  = 967
0.01.147.856 I llama_init_from_model: graph splits = 2
0.01.147.860 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.147.988 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.147.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.196.080 I main: llama threadpool init, n_threads = 4
0.01.196.119 I 
0.01.196.141 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.196.141 I 
0.01.196.289 I sampler seed: 1234
0.01.196.294 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.196.327 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.196.330 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.196.330 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.283.158 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55124.22 tokens per second)
0.02.283.158 I llama_perf_context_print:        load time =    1185.28 ms
0.02.283.159 I llama_perf_context_print: prompt eval time =      39.93 ms /     7 tokens (    5.70 ms per token,   175.30 tokens per second)
0.02.283.160 I llama_perf_context_print:        eval time =    1044.05 ms /    63 runs   (   16.57 ms per token,    60.34 tokens per second)
0.02.283.160 I llama_perf_context_print:       total time =    1087.98 ms /    70 tokens
0.02.283.428 I ggml_metal_free: deallocating

real	0m2.302s
user	0m0.113s
sys	0m0.257s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.015.400 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.190 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.193 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.193 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.193 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.193 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.196 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.196 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.197 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.197 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.206 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.394 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.773 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.773 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.774 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.774 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.774 I llama_model_loader: - type  f32:  194 tensors
0.00.038.775 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.775 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.776 I print_info: file format = GGUF V3 (latest)
0.00.038.777 I print_info: file type   = Q4_0
0.00.038.778 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.289 I load: special tokens cache size = 25
0.00.056.190 I load: token to piece cache size = 0.2984 MB
0.00.056.194 I print_info: arch             = gptneox
0.00.056.194 I print_info: vocab_only       = 0
0.00.056.194 I print_info: n_ctx_train      = 2048
0.00.056.195 I print_info: n_embd           = 2048
0.00.056.195 I print_info: n_layer          = 24
0.00.056.200 I print_info: n_head           = 16
0.00.056.201 I print_info: n_head_kv        = 16
0.00.056.201 I print_info: n_rot            = 32
0.00.056.203 I print_info: n_swa            = 0
0.00.056.203 I print_info: n_embd_head_k    = 128
0.00.056.204 I print_info: n_embd_head_v    = 128
0.00.056.204 I print_info: n_gqa            = 1
0.00.056.205 I print_info: n_embd_k_gqa     = 2048
0.00.056.206 I print_info: n_embd_v_gqa     = 2048
0.00.056.207 I print_info: f_norm_eps       = 1.0e-05
0.00.056.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.208 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.208 I print_info: f_logit_scale    = 0.0e+00
0.00.056.210 I print_info: n_ff             = 8192
0.00.056.210 I print_info: n_expert         = 0
0.00.056.210 I print_info: n_expert_used    = 0
0.00.056.210 I print_info: causal attn      = 1
0.00.056.211 I print_info: pooling type     = 0
0.00.056.211 I print_info: rope type        = 2
0.00.056.212 I print_info: rope scaling     = linear
0.00.056.213 I print_info: freq_base_train  = 10000.0
0.00.056.213 I print_info: freq_scale_train = 1
0.00.056.213 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.213 I print_info: rope_finetuned   = unknown
0.00.056.214 I print_info: ssm_d_conv       = 0
0.00.056.214 I print_info: ssm_d_inner      = 0
0.00.056.214 I print_info: ssm_d_state      = 0
0.00.056.214 I print_info: ssm_dt_rank      = 0
0.00.056.214 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.215 I print_info: model type       = 1.4B
0.00.056.215 I print_info: model params     = 1.41 B
0.00.056.215 I print_info: general.name     = 1.4B
0.00.056.216 I print_info: vocab type       = BPE
0.00.056.216 I print_info: n_vocab          = 50304
0.00.056.216 I print_info: n_merges         = 50009
0.00.056.216 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.218 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.218 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.218 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.218 I print_info: LF token         = 128 'Ä'
0.00.056.219 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.219 I print_info: max token length = 1024
0.00.560.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.560.520 I load_tensors: offloading output layer to GPU
0.00.560.521 I load_tensors: offloaded 25/25 layers to GPU
0.00.560.551 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.560.552 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.561.948 I llama_init_from_model: n_seq_max     = 1
0.00.561.957 I llama_init_from_model: n_ctx         = 2048
0.00.561.957 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.561.958 I llama_init_from_model: n_batch       = 2048
0.00.561.958 I llama_init_from_model: n_ubatch      = 512
0.00.561.958 I llama_init_from_model: flash_attn    = 0
0.00.561.959 I llama_init_from_model: freq_base     = 10000.0
0.00.561.960 I llama_init_from_model: freq_scale    = 1
0.00.561.967 I ggml_metal_init: allocating
0.00.562.028 I ggml_metal_init: found device: Apple M4
0.00.562.042 I ggml_metal_init: picking default device: Apple M4
0.00.563.734 I ggml_metal_init: using embedded metal library
0.00.569.568 I ggml_metal_init: GPU name:   Apple M4
0.00.569.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.569.578 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.569.579 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.569.579 I ggml_metal_init: simdgroup reduction   = true
0.00.569.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.569.591 I ggml_metal_init: has residency sets    = true
0.00.569.591 I ggml_metal_init: has bfloat            = true
0.00.569.592 I ggml_metal_init: use bfloat            = true
0.00.569.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.569.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.589.138 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.328 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.651.333 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.651.358 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.655.927 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.655.929 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.655.929 I llama_init_from_model: graph nodes  = 967
0.00.655.929 I llama_init_from_model: graph splits = 2
0.00.655.934 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.656.064 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.656.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.539 I main: llama threadpool init, n_threads = 4
0.00.710.584 I 
0.00.710.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.610 I 
0.00.710.799 I sampler seed: 1234
0.00.710.804 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.837 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.839 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.839 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.383.570 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.01.383.570 I llama_perf_context_print:        load time =     694.22 ms
0.01.383.572 I llama_perf_context_print: prompt eval time =      39.69 ms /     7 tokens (    5.67 ms per token,   176.38 tokens per second)
0.01.383.574 I llama_perf_context_print:        eval time =     630.10 ms /    63 runs   (   10.00 ms per token,    99.98 tokens per second)
0.01.383.574 I llama_perf_context_print:       total time =     673.95 ms /    70 tokens
0.01.383.815 I ggml_metal_free: deallocating

real	0m1.407s
user	0m0.115s
sys	0m0.224s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.104 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.672 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.678 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.679 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.679 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.680 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.680 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.681 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.681 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.682 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.682 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.682 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.683 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.683 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.687 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.522 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.565 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.602 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.605 I llama_model_loader: - type  f32:  194 tensors
0.00.035.605 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.606 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.606 I print_info: file format = GGUF V3 (latest)
0.00.035.607 I print_info: file type   = Q4_1
0.00.035.609 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.845 I load: special tokens cache size = 25
0.00.050.868 I load: token to piece cache size = 0.2984 MB
0.00.050.871 I print_info: arch             = gptneox
0.00.050.871 I print_info: vocab_only       = 0
0.00.050.871 I print_info: n_ctx_train      = 2048
0.00.050.872 I print_info: n_embd           = 2048
0.00.050.872 I print_info: n_layer          = 24
0.00.050.875 I print_info: n_head           = 16
0.00.050.876 I print_info: n_head_kv        = 16
0.00.050.876 I print_info: n_rot            = 32
0.00.050.876 I print_info: n_swa            = 0
0.00.050.876 I print_info: n_embd_head_k    = 128
0.00.050.876 I print_info: n_embd_head_v    = 128
0.00.050.877 I print_info: n_gqa            = 1
0.00.050.878 I print_info: n_embd_k_gqa     = 2048
0.00.050.878 I print_info: n_embd_v_gqa     = 2048
0.00.050.879 I print_info: f_norm_eps       = 1.0e-05
0.00.050.879 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.879 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.879 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.880 I print_info: f_logit_scale    = 0.0e+00
0.00.050.880 I print_info: n_ff             = 8192
0.00.050.880 I print_info: n_expert         = 0
0.00.050.881 I print_info: n_expert_used    = 0
0.00.050.881 I print_info: causal attn      = 1
0.00.050.881 I print_info: pooling type     = 0
0.00.050.882 I print_info: rope type        = 2
0.00.050.884 I print_info: rope scaling     = linear
0.00.050.885 I print_info: freq_base_train  = 10000.0
0.00.050.885 I print_info: freq_scale_train = 1
0.00.050.885 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.885 I print_info: rope_finetuned   = unknown
0.00.050.885 I print_info: ssm_d_conv       = 0
0.00.050.886 I print_info: ssm_d_inner      = 0
0.00.050.886 I print_info: ssm_d_state      = 0
0.00.050.886 I print_info: ssm_dt_rank      = 0
0.00.050.886 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.886 I print_info: model type       = 1.4B
0.00.050.887 I print_info: model params     = 1.41 B
0.00.050.887 I print_info: general.name     = 1.4B
0.00.050.887 I print_info: vocab type       = BPE
0.00.050.887 I print_info: n_vocab          = 50304
0.00.050.887 I print_info: n_merges         = 50009
0.00.050.888 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.888 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.888 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.892 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.892 I print_info: LF token         = 128 'Ä'
0.00.050.892 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.893 I print_info: max token length = 1024
0.00.629.278 I load_tensors: offloading 24 repeating layers to GPU
0.00.629.292 I load_tensors: offloading output layer to GPU
0.00.629.293 I load_tensors: offloaded 25/25 layers to GPU
0.00.629.326 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.629.327 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.630.876 I llama_init_from_model: n_seq_max     = 1
0.00.630.882 I llama_init_from_model: n_ctx         = 2048
0.00.630.882 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.630.883 I llama_init_from_model: n_batch       = 2048
0.00.630.883 I llama_init_from_model: n_ubatch      = 512
0.00.630.883 I llama_init_from_model: flash_attn    = 0
0.00.630.886 I llama_init_from_model: freq_base     = 10000.0
0.00.630.886 I llama_init_from_model: freq_scale    = 1
0.00.630.894 I ggml_metal_init: allocating
0.00.630.973 I ggml_metal_init: found device: Apple M4
0.00.630.987 I ggml_metal_init: picking default device: Apple M4
0.00.632.806 I ggml_metal_init: using embedded metal library
0.00.639.526 I ggml_metal_init: GPU name:   Apple M4
0.00.639.531 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.532 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.532 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.533 I ggml_metal_init: simdgroup reduction   = true
0.00.639.533 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.533 I ggml_metal_init: has residency sets    = true
0.00.639.534 I ggml_metal_init: has bfloat            = true
0.00.639.534 I ggml_metal_init: use bfloat            = true
0.00.639.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.136 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.450 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.709.459 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.709.487 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.713.996 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.713.997 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.713.998 I llama_init_from_model: graph nodes  = 967
0.00.713.998 I llama_init_from_model: graph splits = 2
0.00.714.003 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.120 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.120 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.238 I main: llama threadpool init, n_threads = 4
0.00.770.285 I 
0.00.770.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.312 I 
0.00.770.489 I sampler seed: 1234
0.00.770.494 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.549 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.553 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.553 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.512.262 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.512.263 I llama_perf_context_print:        load time =     758.24 ms
0.01.512.264 I llama_perf_context_print: prompt eval time =      49.17 ms /     7 tokens (    7.02 ms per token,   142.37 tokens per second)
0.01.512.265 I llama_perf_context_print:        eval time =     689.70 ms /    63 runs   (   10.95 ms per token,    91.34 tokens per second)
0.01.512.265 I llama_perf_context_print:       total time =     742.92 ms /    70 tokens
0.01.512.552 I ggml_metal_free: deallocating

real	0m1.529s
user	0m0.111s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.832 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.125 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.026.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.131 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.132 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.137 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.137 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.138 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.138 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.139 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.139 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.140 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.144 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.145 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.145 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.910 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.854 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.855 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.034.856 I llama_model_loader: - type  f32:  194 tensors
0.00.034.856 I llama_model_loader: - type q5_0:   97 tensors
0.00.034.856 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.857 I print_info: file format = GGUF V3 (latest)
0.00.034.857 I print_info: file type   = Q5_0
0.00.034.858 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.109 I load: special tokens cache size = 25
0.00.049.644 I load: token to piece cache size = 0.2984 MB
0.00.049.648 I print_info: arch             = gptneox
0.00.049.648 I print_info: vocab_only       = 0
0.00.049.648 I print_info: n_ctx_train      = 2048
0.00.049.648 I print_info: n_embd           = 2048
0.00.049.648 I print_info: n_layer          = 24
0.00.049.651 I print_info: n_head           = 16
0.00.049.652 I print_info: n_head_kv        = 16
0.00.049.652 I print_info: n_rot            = 32
0.00.049.652 I print_info: n_swa            = 0
0.00.049.654 I print_info: n_embd_head_k    = 128
0.00.049.654 I print_info: n_embd_head_v    = 128
0.00.049.655 I print_info: n_gqa            = 1
0.00.049.655 I print_info: n_embd_k_gqa     = 2048
0.00.049.656 I print_info: n_embd_v_gqa     = 2048
0.00.049.657 I print_info: f_norm_eps       = 1.0e-05
0.00.049.657 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.657 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.657 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.657 I print_info: f_logit_scale    = 0.0e+00
0.00.049.658 I print_info: n_ff             = 8192
0.00.049.658 I print_info: n_expert         = 0
0.00.049.658 I print_info: n_expert_used    = 0
0.00.049.658 I print_info: causal attn      = 1
0.00.049.659 I print_info: pooling type     = 0
0.00.049.660 I print_info: rope type        = 2
0.00.049.661 I print_info: rope scaling     = linear
0.00.049.662 I print_info: freq_base_train  = 10000.0
0.00.049.662 I print_info: freq_scale_train = 1
0.00.049.662 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.662 I print_info: rope_finetuned   = unknown
0.00.049.662 I print_info: ssm_d_conv       = 0
0.00.049.662 I print_info: ssm_d_inner      = 0
0.00.049.663 I print_info: ssm_d_state      = 0
0.00.049.663 I print_info: ssm_dt_rank      = 0
0.00.049.663 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.664 I print_info: model type       = 1.4B
0.00.049.664 I print_info: model params     = 1.41 B
0.00.049.664 I print_info: general.name     = 1.4B
0.00.049.665 I print_info: vocab type       = BPE
0.00.049.665 I print_info: n_vocab          = 50304
0.00.049.665 I print_info: n_merges         = 50009
0.00.049.666 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.669 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.669 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.670 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.670 I print_info: LF token         = 128 'Ä'
0.00.049.670 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.670 I print_info: max token length = 1024
0.00.721.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.721.043 I load_tensors: offloading output layer to GPU
0.00.721.043 I load_tensors: offloaded 25/25 layers to GPU
0.00.721.075 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.721.076 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.722.464 I llama_init_from_model: n_seq_max     = 1
0.00.722.469 I llama_init_from_model: n_ctx         = 2048
0.00.722.470 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.722.470 I llama_init_from_model: n_batch       = 2048
0.00.722.471 I llama_init_from_model: n_ubatch      = 512
0.00.722.471 I llama_init_from_model: flash_attn    = 0
0.00.722.473 I llama_init_from_model: freq_base     = 10000.0
0.00.722.474 I llama_init_from_model: freq_scale    = 1
0.00.722.481 I ggml_metal_init: allocating
0.00.722.565 I ggml_metal_init: found device: Apple M4
0.00.722.580 I ggml_metal_init: picking default device: Apple M4
0.00.724.366 I ggml_metal_init: using embedded metal library
0.00.730.586 I ggml_metal_init: GPU name:   Apple M4
0.00.730.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.730.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.730.593 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.730.594 I ggml_metal_init: simdgroup reduction   = true
0.00.730.595 I ggml_metal_init: simdgroup matrix mul. = true
0.00.730.595 I ggml_metal_init: has residency sets    = true
0.00.730.595 I ggml_metal_init: has bfloat            = true
0.00.730.595 I ggml_metal_init: use bfloat            = true
0.00.730.596 I ggml_metal_init: hasUnifiedMemory      = true
0.00.730.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.750.130 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.805.012 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.805.020 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.805.042 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.809.838 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.809.840 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.809.840 I llama_init_from_model: graph nodes  = 967
0.00.809.841 I llama_init_from_model: graph splits = 2
0.00.809.847 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.809.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.809.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.861.937 I main: llama threadpool init, n_threads = 4
0.00.861.976 I 
0.00.861.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.862.000 I 
0.00.862.191 I sampler seed: 1234
0.00.862.198 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.862.212 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.862.214 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.862.215 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.655.882 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48102.98 tokens per second)
0.01.655.883 I llama_perf_context_print:        load time =     852.21 ms
0.01.655.884 I llama_perf_context_print: prompt eval time =      43.15 ms /     7 tokens (    6.16 ms per token,   162.22 tokens per second)
0.01.655.885 I llama_perf_context_print:        eval time =     747.88 ms /    63 runs   (   11.87 ms per token,    84.24 tokens per second)
0.01.655.886 I llama_perf_context_print:       total time =     794.84 ms /    70 tokens
0.01.656.162 I ggml_metal_free: deallocating

real	0m1.672s
user	0m0.111s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.222 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.474 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.021.479 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.481 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.482 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.483 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.484 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.484 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.489 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.393 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.428 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.256 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.256 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.257 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.257 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.030.258 I llama_model_loader: - type  f32:  194 tensors
0.00.030.258 I llama_model_loader: - type q5_1:   97 tensors
0.00.030.258 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.259 I print_info: file format = GGUF V3 (latest)
0.00.030.260 I print_info: file type   = Q5_1
0.00.030.261 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.038.828 I load: special tokens cache size = 25
0.00.045.115 I load: token to piece cache size = 0.2984 MB
0.00.045.120 I print_info: arch             = gptneox
0.00.045.120 I print_info: vocab_only       = 0
0.00.045.121 I print_info: n_ctx_train      = 2048
0.00.045.121 I print_info: n_embd           = 2048
0.00.045.121 I print_info: n_layer          = 24
0.00.045.125 I print_info: n_head           = 16
0.00.045.126 I print_info: n_head_kv        = 16
0.00.045.126 I print_info: n_rot            = 32
0.00.045.126 I print_info: n_swa            = 0
0.00.045.126 I print_info: n_embd_head_k    = 128
0.00.045.126 I print_info: n_embd_head_v    = 128
0.00.045.127 I print_info: n_gqa            = 1
0.00.045.128 I print_info: n_embd_k_gqa     = 2048
0.00.045.128 I print_info: n_embd_v_gqa     = 2048
0.00.045.129 I print_info: f_norm_eps       = 1.0e-05
0.00.045.129 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.129 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.129 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.130 I print_info: f_logit_scale    = 0.0e+00
0.00.045.130 I print_info: n_ff             = 8192
0.00.045.130 I print_info: n_expert         = 0
0.00.045.131 I print_info: n_expert_used    = 0
0.00.045.134 I print_info: causal attn      = 1
0.00.045.134 I print_info: pooling type     = 0
0.00.045.135 I print_info: rope type        = 2
0.00.045.136 I print_info: rope scaling     = linear
0.00.045.136 I print_info: freq_base_train  = 10000.0
0.00.045.136 I print_info: freq_scale_train = 1
0.00.045.136 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.137 I print_info: rope_finetuned   = unknown
0.00.045.137 I print_info: ssm_d_conv       = 0
0.00.045.137 I print_info: ssm_d_inner      = 0
0.00.045.138 I print_info: ssm_d_state      = 0
0.00.045.139 I print_info: ssm_dt_rank      = 0
0.00.045.139 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.139 I print_info: model type       = 1.4B
0.00.045.139 I print_info: model params     = 1.41 B
0.00.045.139 I print_info: general.name     = 1.4B
0.00.045.140 I print_info: vocab type       = BPE
0.00.045.140 I print_info: n_vocab          = 50304
0.00.045.140 I print_info: n_merges         = 50009
0.00.045.140 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.140 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.141 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.141 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.141 I print_info: LF token         = 128 'Ä'
0.00.045.141 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.142 I print_info: max token length = 1024
0.00.701.052 I load_tensors: offloading 24 repeating layers to GPU
0.00.701.059 I load_tensors: offloading output layer to GPU
0.00.701.059 I load_tensors: offloaded 25/25 layers to GPU
0.00.701.072 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.701.072 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.701.842 I llama_init_from_model: n_seq_max     = 1
0.00.701.845 I llama_init_from_model: n_ctx         = 2048
0.00.701.845 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.701.845 I llama_init_from_model: n_batch       = 2048
0.00.701.846 I llama_init_from_model: n_ubatch      = 512
0.00.701.846 I llama_init_from_model: flash_attn    = 0
0.00.701.847 I llama_init_from_model: freq_base     = 10000.0
0.00.701.847 I llama_init_from_model: freq_scale    = 1
0.00.701.850 I ggml_metal_init: allocating
0.00.701.883 I ggml_metal_init: found device: Apple M4
0.00.701.892 I ggml_metal_init: picking default device: Apple M4
0.00.702.956 I ggml_metal_init: using embedded metal library
0.00.707.220 I ggml_metal_init: GPU name:   Apple M4
0.00.707.224 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.707.225 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.707.225 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.707.226 I ggml_metal_init: simdgroup reduction   = true
0.00.707.226 I ggml_metal_init: simdgroup matrix mul. = true
0.00.707.226 I ggml_metal_init: has residency sets    = true
0.00.707.227 I ggml_metal_init: has bfloat            = true
0.00.707.227 I ggml_metal_init: use bfloat            = true
0.00.707.230 I ggml_metal_init: hasUnifiedMemory      = true
0.00.707.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.722.252 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.753.337 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.753.344 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.753.365 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.759.418 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.759.421 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.759.421 I llama_init_from_model: graph nodes  = 967
0.00.759.421 I llama_init_from_model: graph splits = 2
0.00.759.428 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.759.557 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.759.558 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.078 I main: llama threadpool init, n_threads = 4
0.00.818.124 I 
0.00.818.146 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.153 I 
0.00.818.327 I sampler seed: 1234
0.00.818.334 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.344 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.344 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.658.613 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50750.54 tokens per second)
0.01.658.614 I llama_perf_context_print:        load time =     806.95 ms
0.01.658.616 I llama_perf_context_print: prompt eval time =      51.87 ms /     7 tokens (    7.41 ms per token,   134.94 tokens per second)
0.01.658.616 I llama_perf_context_print:        eval time =     785.45 ms /    63 runs   (   12.47 ms per token,    80.21 tokens per second)
0.01.658.617 I llama_perf_context_print:       total time =     841.43 ms /    70 tokens
0.01.658.846 I ggml_metal_free: deallocating

real	0m1.678s
user	0m0.106s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.884 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.744 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.813 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.600 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.601 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.602 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.602 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.602 I llama_model_loader: - type  f32:  194 tensors
0.00.025.603 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.603 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.603 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.604 I print_info: file format = GGUF V3 (latest)
0.00.025.604 I print_info: file type   = Q2_K - Medium
0.00.025.605 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.956 I load: special tokens cache size = 25
0.00.039.882 I load: token to piece cache size = 0.2984 MB
0.00.039.884 I print_info: arch             = gptneox
0.00.039.885 I print_info: vocab_only       = 0
0.00.039.885 I print_info: n_ctx_train      = 2048
0.00.039.885 I print_info: n_embd           = 2048
0.00.039.885 I print_info: n_layer          = 24
0.00.039.888 I print_info: n_head           = 16
0.00.039.889 I print_info: n_head_kv        = 16
0.00.039.889 I print_info: n_rot            = 32
0.00.039.890 I print_info: n_swa            = 0
0.00.039.890 I print_info: n_embd_head_k    = 128
0.00.039.890 I print_info: n_embd_head_v    = 128
0.00.039.891 I print_info: n_gqa            = 1
0.00.039.892 I print_info: n_embd_k_gqa     = 2048
0.00.039.892 I print_info: n_embd_v_gqa     = 2048
0.00.039.893 I print_info: f_norm_eps       = 1.0e-05
0.00.039.893 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.893 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.893 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.893 I print_info: f_logit_scale    = 0.0e+00
0.00.039.894 I print_info: n_ff             = 8192
0.00.039.894 I print_info: n_expert         = 0
0.00.039.894 I print_info: n_expert_used    = 0
0.00.039.894 I print_info: causal attn      = 1
0.00.039.895 I print_info: pooling type     = 0
0.00.039.895 I print_info: rope type        = 2
0.00.039.895 I print_info: rope scaling     = linear
0.00.039.897 I print_info: freq_base_train  = 10000.0
0.00.039.897 I print_info: freq_scale_train = 1
0.00.039.897 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.897 I print_info: rope_finetuned   = unknown
0.00.039.897 I print_info: ssm_d_conv       = 0
0.00.039.898 I print_info: ssm_d_inner      = 0
0.00.039.898 I print_info: ssm_d_state      = 0
0.00.039.900 I print_info: ssm_dt_rank      = 0
0.00.039.900 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.900 I print_info: model type       = 1.4B
0.00.039.901 I print_info: model params     = 1.41 B
0.00.039.901 I print_info: general.name     = 1.4B
0.00.039.901 I print_info: vocab type       = BPE
0.00.039.901 I print_info: n_vocab          = 50304
0.00.039.902 I print_info: n_merges         = 50009
0.00.039.902 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.902 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.902 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.902 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.903 I print_info: LF token         = 128 'Ä'
0.00.039.903 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.907 I print_info: max token length = 1024
0.00.374.120 I load_tensors: offloading 24 repeating layers to GPU
0.00.374.136 I load_tensors: offloading output layer to GPU
0.00.374.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.374.169 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.374.171 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.375.578 I llama_init_from_model: n_seq_max     = 1
0.00.375.583 I llama_init_from_model: n_ctx         = 2048
0.00.375.584 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.375.584 I llama_init_from_model: n_batch       = 2048
0.00.375.585 I llama_init_from_model: n_ubatch      = 512
0.00.375.585 I llama_init_from_model: flash_attn    = 0
0.00.375.587 I llama_init_from_model: freq_base     = 10000.0
0.00.375.587 I llama_init_from_model: freq_scale    = 1
0.00.375.590 I ggml_metal_init: allocating
0.00.375.682 I ggml_metal_init: found device: Apple M4
0.00.375.697 I ggml_metal_init: picking default device: Apple M4
0.00.377.521 I ggml_metal_init: using embedded metal library
0.00.383.149 I ggml_metal_init: GPU name:   Apple M4
0.00.383.170 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.383.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.383.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.383.172 I ggml_metal_init: simdgroup reduction   = true
0.00.383.173 I ggml_metal_init: simdgroup matrix mul. = true
0.00.383.173 I ggml_metal_init: has residency sets    = true
0.00.383.173 I ggml_metal_init: has bfloat            = true
0.00.383.173 I ggml_metal_init: use bfloat            = true
0.00.383.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.383.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.404.801 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.460.479 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.460.489 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.460.511 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.464.950 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.464.952 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.464.953 I llama_init_from_model: graph nodes  = 967
0.00.464.953 I llama_init_from_model: graph splits = 2
0.00.464.960 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.465.084 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.465.085 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.453 I main: llama threadpool init, n_threads = 4
0.00.524.502 I 
0.00.524.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.529 I 
0.00.524.703 I sampler seed: 1234
0.00.524.708 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.524.754 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.524.757 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.524.757 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.208.035 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.208.036 I llama_perf_context_print:        load time =     513.59 ms
0.01.208.037 I llama_perf_context_print: prompt eval time =      44.78 ms /     7 tokens (    6.40 ms per token,   156.31 tokens per second)
0.01.208.038 I llama_perf_context_print:        eval time =     635.51 ms /    63 runs   (   10.09 ms per token,    99.13 tokens per second)
0.01.208.038 I llama_perf_context_print:       total time =     684.49 ms /    70 tokens
0.01.208.289 I ggml_metal_free: deallocating

real	0m1.227s
user	0m0.111s
sys	0m0.175s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.534 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.549 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.491 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.275 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.276 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.276 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.276 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.277 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.277 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.278 I llama_model_loader: - type  f32:  194 tensors
0.00.025.278 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.278 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.278 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.279 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.279 I print_info: file format = GGUF V3 (latest)
0.00.025.280 I print_info: file type   = Q3_K - Medium
0.00.025.281 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.464 I load: special tokens cache size = 25
0.00.039.382 I load: token to piece cache size = 0.2984 MB
0.00.039.385 I print_info: arch             = gptneox
0.00.039.385 I print_info: vocab_only       = 0
0.00.039.386 I print_info: n_ctx_train      = 2048
0.00.039.386 I print_info: n_embd           = 2048
0.00.039.386 I print_info: n_layer          = 24
0.00.039.389 I print_info: n_head           = 16
0.00.039.389 I print_info: n_head_kv        = 16
0.00.039.390 I print_info: n_rot            = 32
0.00.039.390 I print_info: n_swa            = 0
0.00.039.390 I print_info: n_embd_head_k    = 128
0.00.039.392 I print_info: n_embd_head_v    = 128
0.00.039.393 I print_info: n_gqa            = 1
0.00.039.393 I print_info: n_embd_k_gqa     = 2048
0.00.039.394 I print_info: n_embd_v_gqa     = 2048
0.00.039.399 I print_info: f_norm_eps       = 1.0e-05
0.00.039.400 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.401 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.402 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.402 I print_info: f_logit_scale    = 0.0e+00
0.00.039.403 I print_info: n_ff             = 8192
0.00.039.403 I print_info: n_expert         = 0
0.00.039.403 I print_info: n_expert_used    = 0
0.00.039.404 I print_info: causal attn      = 1
0.00.039.405 I print_info: pooling type     = 0
0.00.039.405 I print_info: rope type        = 2
0.00.039.406 I print_info: rope scaling     = linear
0.00.039.409 I print_info: freq_base_train  = 10000.0
0.00.039.409 I print_info: freq_scale_train = 1
0.00.039.410 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.410 I print_info: rope_finetuned   = unknown
0.00.039.410 I print_info: ssm_d_conv       = 0
0.00.039.410 I print_info: ssm_d_inner      = 0
0.00.039.410 I print_info: ssm_d_state      = 0
0.00.039.410 I print_info: ssm_dt_rank      = 0
0.00.039.410 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.410 I print_info: model type       = 1.4B
0.00.039.411 I print_info: model params     = 1.41 B
0.00.039.411 I print_info: general.name     = 1.4B
0.00.039.411 I print_info: vocab type       = BPE
0.00.039.411 I print_info: n_vocab          = 50304
0.00.039.412 I print_info: n_merges         = 50009
0.00.039.412 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.412 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.413 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.414 I print_info: LF token         = 128 'Ä'
0.00.039.414 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.414 I print_info: max token length = 1024
0.00.458.604 I load_tensors: offloading 24 repeating layers to GPU
0.00.458.618 I load_tensors: offloading output layer to GPU
0.00.458.619 I load_tensors: offloaded 25/25 layers to GPU
0.00.458.647 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.458.648 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.459.985 I llama_init_from_model: n_seq_max     = 1
0.00.459.991 I llama_init_from_model: n_ctx         = 2048
0.00.459.992 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.459.993 I llama_init_from_model: n_batch       = 2048
0.00.459.993 I llama_init_from_model: n_ubatch      = 512
0.00.459.993 I llama_init_from_model: flash_attn    = 0
0.00.460.002 I llama_init_from_model: freq_base     = 10000.0
0.00.460.003 I llama_init_from_model: freq_scale    = 1
0.00.460.005 I ggml_metal_init: allocating
0.00.460.056 I ggml_metal_init: found device: Apple M4
0.00.460.071 I ggml_metal_init: picking default device: Apple M4
0.00.461.744 I ggml_metal_init: using embedded metal library
0.00.467.444 I ggml_metal_init: GPU name:   Apple M4
0.00.467.458 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.467.459 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.467.460 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.467.460 I ggml_metal_init: simdgroup reduction   = true
0.00.467.461 I ggml_metal_init: simdgroup matrix mul. = true
0.00.467.461 I ggml_metal_init: has residency sets    = true
0.00.467.461 I ggml_metal_init: has bfloat            = true
0.00.467.461 I ggml_metal_init: use bfloat            = true
0.00.467.466 I ggml_metal_init: hasUnifiedMemory      = true
0.00.467.469 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.488.456 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.552.053 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.552.062 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.552.091 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.556.898 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.556.900 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.556.900 I llama_init_from_model: graph nodes  = 967
0.00.556.901 I llama_init_from_model: graph splits = 2
0.00.556.906 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.557.029 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.557.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.969 I main: llama threadpool init, n_threads = 4
0.00.614.011 I 
0.00.614.040 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.040 I 
0.00.614.192 I sampler seed: 1234
0.00.614.196 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.238 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.241 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.241 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.361.008 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.361.008 I llama_perf_context_print:        load time =     604.33 ms
0.01.361.009 I llama_perf_context_print: prompt eval time =      50.06 ms /     7 tokens (    7.15 ms per token,   139.84 tokens per second)
0.01.361.010 I llama_perf_context_print:        eval time =     693.84 ms /    63 runs   (   11.01 ms per token,    90.80 tokens per second)
0.01.361.010 I llama_perf_context_print:       total time =     747.95 ms /    70 tokens
0.01.361.246 I ggml_metal_free: deallocating

real	0m1.378s
user	0m0.112s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.692 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.179 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.184 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.186 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.193 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.193 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.194 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.194 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.195 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.197 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.913 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.606 I llama_model_loader: - type  f32:  194 tensors
0.00.026.606 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.606 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.606 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.607 I print_info: file format = GGUF V3 (latest)
0.00.026.607 I print_info: file type   = Q4_K - Medium
0.00.026.608 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.548 I load: special tokens cache size = 25
0.00.040.496 I load: token to piece cache size = 0.2984 MB
0.00.040.498 I print_info: arch             = gptneox
0.00.040.499 I print_info: vocab_only       = 0
0.00.040.499 I print_info: n_ctx_train      = 2048
0.00.040.499 I print_info: n_embd           = 2048
0.00.040.499 I print_info: n_layer          = 24
0.00.040.502 I print_info: n_head           = 16
0.00.040.503 I print_info: n_head_kv        = 16
0.00.040.503 I print_info: n_rot            = 32
0.00.040.504 I print_info: n_swa            = 0
0.00.040.504 I print_info: n_embd_head_k    = 128
0.00.040.504 I print_info: n_embd_head_v    = 128
0.00.040.505 I print_info: n_gqa            = 1
0.00.040.506 I print_info: n_embd_k_gqa     = 2048
0.00.040.506 I print_info: n_embd_v_gqa     = 2048
0.00.040.507 I print_info: f_norm_eps       = 1.0e-05
0.00.040.507 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.507 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.509 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.509 I print_info: f_logit_scale    = 0.0e+00
0.00.040.510 I print_info: n_ff             = 8192
0.00.040.510 I print_info: n_expert         = 0
0.00.040.510 I print_info: n_expert_used    = 0
0.00.040.510 I print_info: causal attn      = 1
0.00.040.511 I print_info: pooling type     = 0
0.00.040.513 I print_info: rope type        = 2
0.00.040.513 I print_info: rope scaling     = linear
0.00.040.514 I print_info: freq_base_train  = 10000.0
0.00.040.514 I print_info: freq_scale_train = 1
0.00.040.514 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.514 I print_info: rope_finetuned   = unknown
0.00.040.515 I print_info: ssm_d_conv       = 0
0.00.040.515 I print_info: ssm_d_inner      = 0
0.00.040.515 I print_info: ssm_d_state      = 0
0.00.040.515 I print_info: ssm_dt_rank      = 0
0.00.040.515 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.515 I print_info: model type       = 1.4B
0.00.040.516 I print_info: model params     = 1.41 B
0.00.040.516 I print_info: general.name     = 1.4B
0.00.040.517 I print_info: vocab type       = BPE
0.00.040.517 I print_info: n_vocab          = 50304
0.00.040.518 I print_info: n_merges         = 50009
0.00.040.519 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.519 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.519 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.519 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.519 I print_info: LF token         = 128 'Ä'
0.00.040.520 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.520 I print_info: max token length = 1024
0.00.533.725 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.736 I load_tensors: offloading output layer to GPU
0.00.533.737 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.767 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.768 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.535.221 I llama_init_from_model: n_seq_max     = 1
0.00.535.230 I llama_init_from_model: n_ctx         = 2048
0.00.535.231 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.535.231 I llama_init_from_model: n_batch       = 2048
0.00.535.232 I llama_init_from_model: n_ubatch      = 512
0.00.535.232 I llama_init_from_model: flash_attn    = 0
0.00.535.233 I llama_init_from_model: freq_base     = 10000.0
0.00.535.237 I llama_init_from_model: freq_scale    = 1
0.00.535.241 I ggml_metal_init: allocating
0.00.535.287 I ggml_metal_init: found device: Apple M4
0.00.535.297 I ggml_metal_init: picking default device: Apple M4
0.00.536.977 I ggml_metal_init: using embedded metal library
0.00.542.959 I ggml_metal_init: GPU name:   Apple M4
0.00.542.964 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.542.965 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.542.966 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.542.966 I ggml_metal_init: simdgroup reduction   = true
0.00.542.967 I ggml_metal_init: simdgroup matrix mul. = true
0.00.542.967 I ggml_metal_init: has residency sets    = true
0.00.542.967 I ggml_metal_init: has bfloat            = true
0.00.542.968 I ggml_metal_init: use bfloat            = true
0.00.542.969 I ggml_metal_init: hasUnifiedMemory      = true
0.00.542.971 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.562.369 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.350 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.619.356 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.619.377 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.795 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.624.798 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.624.798 I llama_init_from_model: graph nodes  = 967
0.00.624.798 I llama_init_from_model: graph splits = 2
0.00.624.804 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.624.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.624.916 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.736 I main: llama threadpool init, n_threads = 4
0.00.683.779 I 
0.00.683.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.803 I 
0.00.683.972 I sampler seed: 1234
0.00.683.976 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.987 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.987 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.988 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.448.664 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.448.665 I llama_perf_context_print:        load time =     672.13 ms
0.01.448.665 I llama_perf_context_print: prompt eval time =      58.48 ms /     7 tokens (    8.35 ms per token,   119.70 tokens per second)
0.01.448.667 I llama_perf_context_print:        eval time =     703.17 ms /    63 runs   (   11.16 ms per token,    89.59 tokens per second)
0.01.448.668 I llama_perf_context_print:       total time =     765.84 ms /    70 tokens
0.01.448.916 I ggml_metal_free: deallocating

real	0m1.467s
user	0m0.108s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.890 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.193 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.200 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.205 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.205 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.207 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.209 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.217 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.217 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.067 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.907 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.908 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.909 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.909 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.909 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.910 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.910 I llama_model_loader: - type  f32:  194 tensors
0.00.025.911 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.911 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.911 I print_info: file format = GGUF V3 (latest)
0.00.025.912 I print_info: file type   = Q5_K - Medium
0.00.025.913 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.083 I load: special tokens cache size = 25
0.00.040.056 I load: token to piece cache size = 0.2984 MB
0.00.040.059 I print_info: arch             = gptneox
0.00.040.060 I print_info: vocab_only       = 0
0.00.040.060 I print_info: n_ctx_train      = 2048
0.00.040.060 I print_info: n_embd           = 2048
0.00.040.060 I print_info: n_layer          = 24
0.00.040.063 I print_info: n_head           = 16
0.00.040.063 I print_info: n_head_kv        = 16
0.00.040.064 I print_info: n_rot            = 32
0.00.040.064 I print_info: n_swa            = 0
0.00.040.064 I print_info: n_embd_head_k    = 128
0.00.040.064 I print_info: n_embd_head_v    = 128
0.00.040.065 I print_info: n_gqa            = 1
0.00.040.066 I print_info: n_embd_k_gqa     = 2048
0.00.040.066 I print_info: n_embd_v_gqa     = 2048
0.00.040.067 I print_info: f_norm_eps       = 1.0e-05
0.00.040.067 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.067 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.068 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.068 I print_info: f_logit_scale    = 0.0e+00
0.00.040.069 I print_info: n_ff             = 8192
0.00.040.069 I print_info: n_expert         = 0
0.00.040.069 I print_info: n_expert_used    = 0
0.00.040.069 I print_info: causal attn      = 1
0.00.040.069 I print_info: pooling type     = 0
0.00.040.071 I print_info: rope type        = 2
0.00.040.072 I print_info: rope scaling     = linear
0.00.040.072 I print_info: freq_base_train  = 10000.0
0.00.040.072 I print_info: freq_scale_train = 1
0.00.040.073 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.073 I print_info: rope_finetuned   = unknown
0.00.040.073 I print_info: ssm_d_conv       = 0
0.00.040.073 I print_info: ssm_d_inner      = 0
0.00.040.073 I print_info: ssm_d_state      = 0
0.00.040.074 I print_info: ssm_dt_rank      = 0
0.00.040.074 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.074 I print_info: model type       = 1.4B
0.00.040.074 I print_info: model params     = 1.41 B
0.00.040.074 I print_info: general.name     = 1.4B
0.00.040.077 I print_info: vocab type       = BPE
0.00.040.077 I print_info: n_vocab          = 50304
0.00.040.077 I print_info: n_merges         = 50009
0.00.040.077 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.079 I print_info: LF token         = 128 'Ä'
0.00.040.080 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.080 I print_info: max token length = 1024
0.00.589.793 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.805 I load_tensors: offloading output layer to GPU
0.00.589.806 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.835 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.836 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.591.273 I llama_init_from_model: n_seq_max     = 1
0.00.591.279 I llama_init_from_model: n_ctx         = 2048
0.00.591.279 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.591.280 I llama_init_from_model: n_batch       = 2048
0.00.591.280 I llama_init_from_model: n_ubatch      = 512
0.00.591.281 I llama_init_from_model: flash_attn    = 0
0.00.591.281 I llama_init_from_model: freq_base     = 10000.0
0.00.591.282 I llama_init_from_model: freq_scale    = 1
0.00.591.287 I ggml_metal_init: allocating
0.00.591.335 I ggml_metal_init: found device: Apple M4
0.00.591.348 I ggml_metal_init: picking default device: Apple M4
0.00.593.026 I ggml_metal_init: using embedded metal library
0.00.599.886 I ggml_metal_init: GPU name:   Apple M4
0.00.599.891 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.892 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.892 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.893 I ggml_metal_init: simdgroup reduction   = true
0.00.599.894 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.894 I ggml_metal_init: has residency sets    = true
0.00.599.894 I ggml_metal_init: has bfloat            = true
0.00.599.894 I ggml_metal_init: use bfloat            = true
0.00.599.895 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.896 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.089 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.100 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.672.110 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.672.143 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.262 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.677.264 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.677.265 I llama_init_from_model: graph nodes  = 967
0.00.677.265 I llama_init_from_model: graph splits = 2
0.00.677.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.677.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.677.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.205 I main: llama threadpool init, n_threads = 4
0.00.741.246 I 
0.00.741.269 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.270 I 
0.00.741.442 I sampler seed: 1234
0.00.741.447 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.477 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.480 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.481 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.583.293 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.01.583.293 I llama_perf_context_print:        load time =     731.40 ms
0.01.583.294 I llama_perf_context_print: prompt eval time =      51.68 ms /     7 tokens (    7.38 ms per token,   135.44 tokens per second)
0.01.583.295 I llama_perf_context_print:        eval time =     787.20 ms /    63 runs   (   12.50 ms per token,    80.03 tokens per second)
0.01.583.295 I llama_perf_context_print:       total time =     843.00 ms /    70 tokens
0.01.583.529 I ggml_metal_free: deallocating

real	0m1.602s
user	0m0.110s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.749 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.753 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.755 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.755 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.756 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.756 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.757 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.757 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.758 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.758 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.759 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.761 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.761 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.762 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.577 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.620 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.363 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.365 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.365 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.365 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.366 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.366 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.366 I llama_model_loader: - type  f32:  194 tensors
0.00.026.367 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.367 I print_info: file format = GGUF V3 (latest)
0.00.026.368 I print_info: file type   = Q6_K
0.00.026.369 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.616 I load: special tokens cache size = 25
0.00.040.554 I load: token to piece cache size = 0.2984 MB
0.00.040.557 I print_info: arch             = gptneox
0.00.040.557 I print_info: vocab_only       = 0
0.00.040.557 I print_info: n_ctx_train      = 2048
0.00.040.557 I print_info: n_embd           = 2048
0.00.040.557 I print_info: n_layer          = 24
0.00.040.560 I print_info: n_head           = 16
0.00.040.561 I print_info: n_head_kv        = 16
0.00.040.561 I print_info: n_rot            = 32
0.00.040.561 I print_info: n_swa            = 0
0.00.040.561 I print_info: n_embd_head_k    = 128
0.00.040.561 I print_info: n_embd_head_v    = 128
0.00.040.562 I print_info: n_gqa            = 1
0.00.040.565 I print_info: n_embd_k_gqa     = 2048
0.00.040.566 I print_info: n_embd_v_gqa     = 2048
0.00.040.567 I print_info: f_norm_eps       = 1.0e-05
0.00.040.567 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.567 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.567 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.568 I print_info: f_logit_scale    = 0.0e+00
0.00.040.568 I print_info: n_ff             = 8192
0.00.040.568 I print_info: n_expert         = 0
0.00.040.569 I print_info: n_expert_used    = 0
0.00.040.569 I print_info: causal attn      = 1
0.00.040.569 I print_info: pooling type     = 0
0.00.040.569 I print_info: rope type        = 2
0.00.040.569 I print_info: rope scaling     = linear
0.00.040.570 I print_info: freq_base_train  = 10000.0
0.00.040.570 I print_info: freq_scale_train = 1
0.00.040.572 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.572 I print_info: rope_finetuned   = unknown
0.00.040.572 I print_info: ssm_d_conv       = 0
0.00.040.572 I print_info: ssm_d_inner      = 0
0.00.040.572 I print_info: ssm_d_state      = 0
0.00.040.572 I print_info: ssm_dt_rank      = 0
0.00.040.573 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.573 I print_info: model type       = 1.4B
0.00.040.573 I print_info: model params     = 1.41 B
0.00.040.573 I print_info: general.name     = 1.4B
0.00.040.574 I print_info: vocab type       = BPE
0.00.040.574 I print_info: n_vocab          = 50304
0.00.040.574 I print_info: n_merges         = 50009
0.00.040.575 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.579 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.579 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.579 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.580 I print_info: LF token         = 128 'Ä'
0.00.040.580 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.580 I print_info: max token length = 1024
0.00.674.203 I load_tensors: offloading 24 repeating layers to GPU
0.00.674.213 I load_tensors: offloading output layer to GPU
0.00.674.213 I load_tensors: offloaded 25/25 layers to GPU
0.00.674.249 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.674.250 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.675.851 I llama_init_from_model: n_seq_max     = 1
0.00.675.855 I llama_init_from_model: n_ctx         = 2048
0.00.675.856 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.675.856 I llama_init_from_model: n_batch       = 2048
0.00.675.856 I llama_init_from_model: n_ubatch      = 512
0.00.675.857 I llama_init_from_model: flash_attn    = 0
0.00.675.859 I llama_init_from_model: freq_base     = 10000.0
0.00.675.859 I llama_init_from_model: freq_scale    = 1
0.00.675.862 I ggml_metal_init: allocating
0.00.675.946 I ggml_metal_init: found device: Apple M4
0.00.675.960 I ggml_metal_init: picking default device: Apple M4
0.00.677.785 I ggml_metal_init: using embedded metal library
0.00.684.427 I ggml_metal_init: GPU name:   Apple M4
0.00.684.431 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.684.432 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.684.432 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.684.433 I ggml_metal_init: simdgroup reduction   = true
0.00.684.433 I ggml_metal_init: simdgroup matrix mul. = true
0.00.684.434 I ggml_metal_init: has residency sets    = true
0.00.684.434 I ggml_metal_init: has bfloat            = true
0.00.684.434 I ggml_metal_init: use bfloat            = true
0.00.684.435 I ggml_metal_init: hasUnifiedMemory      = true
0.00.684.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.701.815 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.764.030 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.764.040 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.764.072 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.768.263 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.768.265 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.768.266 I llama_init_from_model: graph nodes  = 967
0.00.768.266 I llama_init_from_model: graph splits = 2
0.00.768.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.768.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.768.388 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.338 I main: llama threadpool init, n_threads = 4
0.00.836.380 I 
0.00.836.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.403 I 
0.00.836.556 I sampler seed: 1234
0.00.836.561 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.571 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.571 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.571 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.707.360 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.707.361 I llama_perf_context_print:        load time =     825.49 ms
0.01.707.362 I llama_perf_context_print: prompt eval time =      54.38 ms /     7 tokens (    7.77 ms per token,   128.74 tokens per second)
0.01.707.363 I llama_perf_context_print:        eval time =     813.47 ms /    63 runs   (   12.91 ms per token,    77.45 tokens per second)
0.01.707.364 I llama_perf_context_print:       total time =     871.93 ms /    70 tokens
0.01.707.586 I ggml_metal_free: deallocating

real	0m1.726s
user	0m0.109s
sys	0m0.249s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.683 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.032.420 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.047.392 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.398 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.413 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.839 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.554 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.554 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.555 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.555 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.556 I llama_model_loader: - type  f32:  194 tensors
0.00.061.556 I llama_model_loader: - type  f16:   98 tensors
0.00.061.557 I print_info: file format = GGUF V3 (latest)
0.00.061.558 I print_info: file type   = all F32 (guessed)
0.00.061.565 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.074.399 I load: special tokens cache size = 25
0.00.082.341 I load: token to piece cache size = 0.2984 MB
0.00.082.344 I print_info: arch             = gptneox
0.00.082.344 I print_info: vocab_only       = 0
0.00.082.345 I print_info: n_ctx_train      = 2048
0.00.082.345 I print_info: n_embd           = 2048
0.00.082.345 I print_info: n_layer          = 24
0.00.082.348 I print_info: n_head           = 16
0.00.082.349 I print_info: n_head_kv        = 16
0.00.082.349 I print_info: n_rot            = 32
0.00.082.350 I print_info: n_swa            = 0
0.00.082.350 I print_info: n_embd_head_k    = 128
0.00.082.350 I print_info: n_embd_head_v    = 128
0.00.082.351 I print_info: n_gqa            = 1
0.00.082.353 I print_info: n_embd_k_gqa     = 2048
0.00.082.354 I print_info: n_embd_v_gqa     = 2048
0.00.082.354 I print_info: f_norm_eps       = 1.0e-05
0.00.082.355 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.082.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.082.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.082.355 I print_info: f_logit_scale    = 0.0e+00
0.00.082.356 I print_info: n_ff             = 8192
0.00.082.356 I print_info: n_expert         = 0
0.00.082.356 I print_info: n_expert_used    = 0
0.00.082.356 I print_info: causal attn      = 1
0.00.082.357 I print_info: pooling type     = 0
0.00.082.357 I print_info: rope type        = 2
0.00.082.357 I print_info: rope scaling     = linear
0.00.082.357 I print_info: freq_base_train  = 10000.0
0.00.082.358 I print_info: freq_scale_train = 1
0.00.082.358 I print_info: n_ctx_orig_yarn  = 2048
0.00.082.358 I print_info: rope_finetuned   = unknown
0.00.082.368 I print_info: ssm_d_conv       = 0
0.00.082.370 I print_info: ssm_d_inner      = 0
0.00.082.371 I print_info: ssm_d_state      = 0
0.00.082.371 I print_info: ssm_dt_rank      = 0
0.00.082.371 I print_info: ssm_dt_b_c_rms   = 0
0.00.082.371 I print_info: model type       = 1.4B
0.00.082.372 I print_info: model params     = 1.41 B
0.00.082.372 I print_info: general.name     = 1.4B
0.00.082.373 I print_info: vocab type       = BPE
0.00.082.373 I print_info: n_vocab          = 50304
0.00.082.373 I print_info: n_merges         = 50009
0.00.082.374 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.082.374 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.082.374 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.082.374 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.082.375 I print_info: LF token         = 128 'Ä'
0.00.082.377 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.082.377 I print_info: max token length = 1024
0.00.977.252 I load_tensors: offloading 24 repeating layers to GPU
0.00.977.257 I load_tensors: offloading output layer to GPU
0.00.977.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.977.288 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.977.290 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.977.908 I llama_init_from_model: n_seq_max     = 1
0.00.977.909 I llama_init_from_model: n_ctx         = 128
0.00.977.910 I llama_init_from_model: n_ctx_per_seq = 128
0.00.977.910 I llama_init_from_model: n_batch       = 128
0.00.977.910 I llama_init_from_model: n_ubatch      = 128
0.00.977.910 I llama_init_from_model: flash_attn    = 0
0.00.977.911 I llama_init_from_model: freq_base     = 10000.0
0.00.977.911 I llama_init_from_model: freq_scale    = 1
0.00.977.911 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.977.913 I ggml_metal_init: allocating
0.00.977.991 I ggml_metal_init: found device: Apple M4
0.00.977.998 I ggml_metal_init: picking default device: Apple M4
0.00.979.162 I ggml_metal_init: using embedded metal library
0.00.982.937 I ggml_metal_init: GPU name:   Apple M4
0.00.982.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.982.941 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.982.941 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.982.942 I ggml_metal_init: simdgroup reduction   = true
0.00.982.942 I ggml_metal_init: simdgroup matrix mul. = true
0.00.982.942 I ggml_metal_init: has residency sets    = true
0.00.982.942 I ggml_metal_init: has bfloat            = true
0.00.982.942 I ggml_metal_init: use bfloat            = true
0.00.982.943 I ggml_metal_init: hasUnifiedMemory      = true
0.00.982.943 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.994.280 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.995.993 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.995.995 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.996.009 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.997.713 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.997.714 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.997.714 I llama_init_from_model: graph nodes  = 967
0.00.997.714 I llama_init_from_model: graph splits = 2
0.00.997.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.997.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.032.976 I 
0.01.033.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.033.035 I perplexity: tokenizing the input ..
0.01.037.990 I perplexity: tokenization took 4.953 ms
0.01.038.010 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.157.121 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.158.533 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.158.545 I llama_perf_context_print:        load time =    1000.55 ms
0.01.158.546 I llama_perf_context_print: prompt eval time =     118.83 ms /   128 tokens (    0.93 ms per token,  1077.15 tokens per second)
0.01.158.547 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.158.547 I llama_perf_context_print:       total time =     125.57 ms /   129 tokens
0.01.158.931 I ggml_metal_free: deallocating

real	0m1.346s
user	0m0.096s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.104 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.080 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.085 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.087 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.094 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.096 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.051 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.887 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.887 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.888 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.888 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.888 I llama_model_loader: - type  f32:  194 tensors
0.00.026.889 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.889 I print_info: file format = GGUF V3 (latest)
0.00.026.890 I print_info: file type   = Q8_0
0.00.026.891 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.089 I load: special tokens cache size = 25
0.00.041.171 I load: token to piece cache size = 0.2984 MB
0.00.041.174 I print_info: arch             = gptneox
0.00.041.174 I print_info: vocab_only       = 0
0.00.041.174 I print_info: n_ctx_train      = 2048
0.00.041.174 I print_info: n_embd           = 2048
0.00.041.175 I print_info: n_layer          = 24
0.00.041.178 I print_info: n_head           = 16
0.00.041.179 I print_info: n_head_kv        = 16
0.00.041.179 I print_info: n_rot            = 32
0.00.041.179 I print_info: n_swa            = 0
0.00.041.179 I print_info: n_embd_head_k    = 128
0.00.041.180 I print_info: n_embd_head_v    = 128
0.00.041.180 I print_info: n_gqa            = 1
0.00.041.181 I print_info: n_embd_k_gqa     = 2048
0.00.041.181 I print_info: n_embd_v_gqa     = 2048
0.00.041.182 I print_info: f_norm_eps       = 1.0e-05
0.00.041.182 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.182 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.182 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.183 I print_info: f_logit_scale    = 0.0e+00
0.00.041.183 I print_info: n_ff             = 8192
0.00.041.183 I print_info: n_expert         = 0
0.00.041.184 I print_info: n_expert_used    = 0
0.00.041.184 I print_info: causal attn      = 1
0.00.041.184 I print_info: pooling type     = 0
0.00.041.184 I print_info: rope type        = 2
0.00.041.184 I print_info: rope scaling     = linear
0.00.041.185 I print_info: freq_base_train  = 10000.0
0.00.041.185 I print_info: freq_scale_train = 1
0.00.041.185 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.185 I print_info: rope_finetuned   = unknown
0.00.041.186 I print_info: ssm_d_conv       = 0
0.00.041.186 I print_info: ssm_d_inner      = 0
0.00.041.186 I print_info: ssm_d_state      = 0
0.00.041.186 I print_info: ssm_dt_rank      = 0
0.00.041.186 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.186 I print_info: model type       = 1.4B
0.00.041.187 I print_info: model params     = 1.41 B
0.00.041.187 I print_info: general.name     = 1.4B
0.00.041.187 I print_info: vocab type       = BPE
0.00.041.187 I print_info: n_vocab          = 50304
0.00.041.187 I print_info: n_merges         = 50009
0.00.041.189 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.189 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.190 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.190 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.190 I print_info: LF token         = 128 'Ä'
0.00.041.190 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.190 I print_info: max token length = 1024
0.00.871.599 I load_tensors: offloading 24 repeating layers to GPU
0.00.871.605 I load_tensors: offloading output layer to GPU
0.00.871.606 I load_tensors: offloaded 25/25 layers to GPU
0.00.871.634 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.871.637 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.872.839 I llama_init_from_model: n_seq_max     = 1
0.00.872.842 I llama_init_from_model: n_ctx         = 128
0.00.872.842 I llama_init_from_model: n_ctx_per_seq = 128
0.00.872.842 I llama_init_from_model: n_batch       = 128
0.00.872.843 I llama_init_from_model: n_ubatch      = 128
0.00.872.843 I llama_init_from_model: flash_attn    = 0
0.00.872.843 I llama_init_from_model: freq_base     = 10000.0
0.00.872.844 I llama_init_from_model: freq_scale    = 1
0.00.872.844 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.872.846 I ggml_metal_init: allocating
0.00.872.905 I ggml_metal_init: found device: Apple M4
0.00.872.919 I ggml_metal_init: picking default device: Apple M4
0.00.874.176 I ggml_metal_init: using embedded metal library
0.00.879.303 I ggml_metal_init: GPU name:   Apple M4
0.00.879.306 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.879.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.879.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.879.309 I ggml_metal_init: simdgroup reduction   = true
0.00.879.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.879.309 I ggml_metal_init: has residency sets    = true
0.00.879.309 I ggml_metal_init: has bfloat            = true
0.00.879.310 I ggml_metal_init: use bfloat            = true
0.00.879.311 I ggml_metal_init: hasUnifiedMemory      = true
0.00.879.312 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.894.519 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.897.974 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.897.977 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.898.006 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.901.184 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.901.186 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.901.186 I llama_init_from_model: graph nodes  = 967
0.00.901.187 I llama_init_from_model: graph splits = 2
0.00.901.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.901.190 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.928.138 I 
0.00.928.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.928.222 I perplexity: tokenizing the input ..
0.00.935.628 I perplexity: tokenization took 7.402 ms
0.00.935.649 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.073.075 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.074.425 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.074.434 I llama_perf_context_print:        load time =     917.02 ms
0.01.074.435 I llama_perf_context_print: prompt eval time =     136.50 ms /   128 tokens (    1.07 ms per token,   937.70 tokens per second)
0.01.074.436 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.074.437 I llama_perf_context_print:       total time =     146.30 ms /   129 tokens
0.01.074.868 I ggml_metal_free: deallocating

real	0m1.090s
user	0m0.077s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.441 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.702 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.707 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.709 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.715 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.715 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.717 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.718 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.718 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.719 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.722 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.722 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.032 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.033 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.033 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.034 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.034 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.034 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.035 I llama_model_loader: - type  f32:  194 tensors
0.00.026.035 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.035 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.036 I print_info: file format = GGUF V3 (latest)
0.00.026.036 I print_info: file type   = Q4_0
0.00.026.037 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.111 I load: special tokens cache size = 25
0.00.039.824 I load: token to piece cache size = 0.2984 MB
0.00.039.827 I print_info: arch             = gptneox
0.00.039.827 I print_info: vocab_only       = 0
0.00.039.828 I print_info: n_ctx_train      = 2048
0.00.039.828 I print_info: n_embd           = 2048
0.00.039.828 I print_info: n_layer          = 24
0.00.039.831 I print_info: n_head           = 16
0.00.039.832 I print_info: n_head_kv        = 16
0.00.039.832 I print_info: n_rot            = 32
0.00.039.832 I print_info: n_swa            = 0
0.00.039.833 I print_info: n_embd_head_k    = 128
0.00.039.833 I print_info: n_embd_head_v    = 128
0.00.039.835 I print_info: n_gqa            = 1
0.00.039.836 I print_info: n_embd_k_gqa     = 2048
0.00.039.837 I print_info: n_embd_v_gqa     = 2048
0.00.039.837 I print_info: f_norm_eps       = 1.0e-05
0.00.039.838 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.838 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.839 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.842 I print_info: f_logit_scale    = 0.0e+00
0.00.039.843 I print_info: n_ff             = 8192
0.00.039.843 I print_info: n_expert         = 0
0.00.039.843 I print_info: n_expert_used    = 0
0.00.039.844 I print_info: causal attn      = 1
0.00.039.844 I print_info: pooling type     = 0
0.00.039.844 I print_info: rope type        = 2
0.00.039.844 I print_info: rope scaling     = linear
0.00.039.845 I print_info: freq_base_train  = 10000.0
0.00.039.845 I print_info: freq_scale_train = 1
0.00.039.845 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.846 I print_info: rope_finetuned   = unknown
0.00.039.846 I print_info: ssm_d_conv       = 0
0.00.039.846 I print_info: ssm_d_inner      = 0
0.00.039.847 I print_info: ssm_d_state      = 0
0.00.039.847 I print_info: ssm_dt_rank      = 0
0.00.039.847 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.854 I print_info: model type       = 1.4B
0.00.039.856 I print_info: model params     = 1.41 B
0.00.039.856 I print_info: general.name     = 1.4B
0.00.039.857 I print_info: vocab type       = BPE
0.00.039.857 I print_info: n_vocab          = 50304
0.00.039.857 I print_info: n_merges         = 50009
0.00.039.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.858 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.858 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.858 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.858 I print_info: LF token         = 128 'Ä'
0.00.039.858 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.859 I print_info: max token length = 1024
0.00.550.300 I load_tensors: offloading 24 repeating layers to GPU
0.00.550.316 I load_tensors: offloading output layer to GPU
0.00.550.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.550.350 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.550.352 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.551.880 I llama_init_from_model: n_seq_max     = 1
0.00.551.885 I llama_init_from_model: n_ctx         = 128
0.00.551.885 I llama_init_from_model: n_ctx_per_seq = 128
0.00.551.886 I llama_init_from_model: n_batch       = 128
0.00.551.886 I llama_init_from_model: n_ubatch      = 128
0.00.551.887 I llama_init_from_model: flash_attn    = 0
0.00.551.889 I llama_init_from_model: freq_base     = 10000.0
0.00.551.889 I llama_init_from_model: freq_scale    = 1
0.00.551.890 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.551.898 I ggml_metal_init: allocating
0.00.551.976 I ggml_metal_init: found device: Apple M4
0.00.551.991 I ggml_metal_init: picking default device: Apple M4
0.00.553.711 I ggml_metal_init: using embedded metal library
0.00.559.409 I ggml_metal_init: GPU name:   Apple M4
0.00.559.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.559.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.559.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.559.417 I ggml_metal_init: simdgroup reduction   = true
0.00.559.418 I ggml_metal_init: simdgroup matrix mul. = true
0.00.559.418 I ggml_metal_init: has residency sets    = true
0.00.559.418 I ggml_metal_init: has bfloat            = true
0.00.559.419 I ggml_metal_init: use bfloat            = true
0.00.559.420 I ggml_metal_init: hasUnifiedMemory      = true
0.00.559.422 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.578.911 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.582.556 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.582.562 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.582.608 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.585.818 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.585.820 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.585.820 I llama_init_from_model: graph nodes  = 967
0.00.585.821 I llama_init_from_model: graph splits = 2
0.00.585.823 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.585.824 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.537 I 
0.00.613.604 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.623 I perplexity: tokenizing the input ..
0.00.620.695 I perplexity: tokenization took 7.07 ms
0.00.620.709 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.089 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.753.394 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.753.407 I llama_perf_context_print:        load time =     604.09 ms
0.00.753.408 I llama_perf_context_print: prompt eval time =     131.09 ms /   128 tokens (    1.02 ms per token,   976.41 tokens per second)
0.00.753.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.753.409 I llama_perf_context_print:       total time =     139.87 ms /   129 tokens
0.00.753.790 I ggml_metal_free: deallocating

real	0m0.769s
user	0m0.079s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.330 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.336 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.338 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.343 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.343 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.345 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.345 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.346 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.348 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.349 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.349 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.064 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.718 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.719 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.720 I llama_model_loader: - type  f32:  194 tensors
0.00.024.720 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.720 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.721 I print_info: file format = GGUF V3 (latest)
0.00.024.721 I print_info: file type   = Q4_1
0.00.024.723 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.699 I load: special tokens cache size = 25
0.00.038.589 I load: token to piece cache size = 0.2984 MB
0.00.038.591 I print_info: arch             = gptneox
0.00.038.592 I print_info: vocab_only       = 0
0.00.038.592 I print_info: n_ctx_train      = 2048
0.00.038.592 I print_info: n_embd           = 2048
0.00.038.592 I print_info: n_layer          = 24
0.00.038.596 I print_info: n_head           = 16
0.00.038.596 I print_info: n_head_kv        = 16
0.00.038.597 I print_info: n_rot            = 32
0.00.038.597 I print_info: n_swa            = 0
0.00.038.597 I print_info: n_embd_head_k    = 128
0.00.038.598 I print_info: n_embd_head_v    = 128
0.00.038.599 I print_info: n_gqa            = 1
0.00.038.600 I print_info: n_embd_k_gqa     = 2048
0.00.038.600 I print_info: n_embd_v_gqa     = 2048
0.00.038.601 I print_info: f_norm_eps       = 1.0e-05
0.00.038.601 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.601 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.601 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.601 I print_info: f_logit_scale    = 0.0e+00
0.00.038.602 I print_info: n_ff             = 8192
0.00.038.604 I print_info: n_expert         = 0
0.00.038.604 I print_info: n_expert_used    = 0
0.00.038.604 I print_info: causal attn      = 1
0.00.038.605 I print_info: pooling type     = 0
0.00.038.605 I print_info: rope type        = 2
0.00.038.605 I print_info: rope scaling     = linear
0.00.038.605 I print_info: freq_base_train  = 10000.0
0.00.038.605 I print_info: freq_scale_train = 1
0.00.038.606 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.606 I print_info: rope_finetuned   = unknown
0.00.038.606 I print_info: ssm_d_conv       = 0
0.00.038.606 I print_info: ssm_d_inner      = 0
0.00.038.606 I print_info: ssm_d_state      = 0
0.00.038.606 I print_info: ssm_dt_rank      = 0
0.00.038.606 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.607 I print_info: model type       = 1.4B
0.00.038.607 I print_info: model params     = 1.41 B
0.00.038.607 I print_info: general.name     = 1.4B
0.00.038.608 I print_info: vocab type       = BPE
0.00.038.608 I print_info: n_vocab          = 50304
0.00.038.608 I print_info: n_merges         = 50009
0.00.038.608 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.612 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.612 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.612 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.619 I print_info: LF token         = 128 'Ä'
0.00.038.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.620 I print_info: max token length = 1024
0.00.609.229 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.239 I load_tensors: offloading output layer to GPU
0.00.609.239 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.270 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.609.272 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.610.472 I llama_init_from_model: n_seq_max     = 1
0.00.610.477 I llama_init_from_model: n_ctx         = 128
0.00.610.477 I llama_init_from_model: n_ctx_per_seq = 128
0.00.610.478 I llama_init_from_model: n_batch       = 128
0.00.610.479 I llama_init_from_model: n_ubatch      = 128
0.00.610.479 I llama_init_from_model: flash_attn    = 0
0.00.610.481 I llama_init_from_model: freq_base     = 10000.0
0.00.610.482 I llama_init_from_model: freq_scale    = 1
0.00.610.482 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.485 I ggml_metal_init: allocating
0.00.610.550 I ggml_metal_init: found device: Apple M4
0.00.610.564 I ggml_metal_init: picking default device: Apple M4
0.00.612.403 I ggml_metal_init: using embedded metal library
0.00.619.278 I ggml_metal_init: GPU name:   Apple M4
0.00.619.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.284 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.285 I ggml_metal_init: simdgroup reduction   = true
0.00.619.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.286 I ggml_metal_init: has residency sets    = true
0.00.619.286 I ggml_metal_init: has bfloat            = true
0.00.619.286 I ggml_metal_init: use bfloat            = true
0.00.619.287 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.289 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.165 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.152 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.641.156 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.641.192 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.644.501 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.644.503 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.644.504 I llama_init_from_model: graph nodes  = 967
0.00.644.504 I llama_init_from_model: graph splits = 2
0.00.644.508 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.644.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.776 I 
0.00.668.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.872 I perplexity: tokenizing the input ..
0.00.676.199 I perplexity: tokenization took 7.324 ms
0.00.676.218 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.591 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.802.008 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.802.020 I llama_perf_context_print:        load time =     659.81 ms
0.00.802.021 I llama_perf_context_print: prompt eval time =     123.50 ms /   128 tokens (    0.96 ms per token,  1036.41 tokens per second)
0.00.802.022 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.022 I llama_perf_context_print:       total time =     133.25 ms /   129 tokens
0.00.802.401 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.079s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.984 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.941 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.947 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.948 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.953 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.954 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.955 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.956 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.957 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.861 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.676 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.678 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.678 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.679 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.679 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.679 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.680 I llama_model_loader: - type  f32:  194 tensors
0.00.025.680 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.681 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.681 I print_info: file format = GGUF V3 (latest)
0.00.025.686 I print_info: file type   = Q5_0
0.00.025.687 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.729 I load: special tokens cache size = 25
0.00.039.701 I load: token to piece cache size = 0.2984 MB
0.00.039.704 I print_info: arch             = gptneox
0.00.039.704 I print_info: vocab_only       = 0
0.00.039.705 I print_info: n_ctx_train      = 2048
0.00.039.705 I print_info: n_embd           = 2048
0.00.039.705 I print_info: n_layer          = 24
0.00.039.709 I print_info: n_head           = 16
0.00.039.710 I print_info: n_head_kv        = 16
0.00.039.710 I print_info: n_rot            = 32
0.00.039.711 I print_info: n_swa            = 0
0.00.039.713 I print_info: n_embd_head_k    = 128
0.00.039.713 I print_info: n_embd_head_v    = 128
0.00.039.714 I print_info: n_gqa            = 1
0.00.039.715 I print_info: n_embd_k_gqa     = 2048
0.00.039.716 I print_info: n_embd_v_gqa     = 2048
0.00.039.716 I print_info: f_norm_eps       = 1.0e-05
0.00.039.717 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.717 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.717 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.717 I print_info: f_logit_scale    = 0.0e+00
0.00.039.718 I print_info: n_ff             = 8192
0.00.039.718 I print_info: n_expert         = 0
0.00.039.718 I print_info: n_expert_used    = 0
0.00.039.718 I print_info: causal attn      = 1
0.00.039.718 I print_info: pooling type     = 0
0.00.039.719 I print_info: rope type        = 2
0.00.039.720 I print_info: rope scaling     = linear
0.00.039.721 I print_info: freq_base_train  = 10000.0
0.00.039.721 I print_info: freq_scale_train = 1
0.00.039.721 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.721 I print_info: rope_finetuned   = unknown
0.00.039.722 I print_info: ssm_d_conv       = 0
0.00.039.722 I print_info: ssm_d_inner      = 0
0.00.039.722 I print_info: ssm_d_state      = 0
0.00.039.722 I print_info: ssm_dt_rank      = 0
0.00.039.722 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.722 I print_info: model type       = 1.4B
0.00.039.723 I print_info: model params     = 1.41 B
0.00.039.723 I print_info: general.name     = 1.4B
0.00.039.727 I print_info: vocab type       = BPE
0.00.039.727 I print_info: n_vocab          = 50304
0.00.039.727 I print_info: n_merges         = 50009
0.00.039.727 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.727 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.732 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.734 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.735 I print_info: LF token         = 128 'Ä'
0.00.039.738 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.741 I print_info: max token length = 1024
0.00.636.151 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.162 I load_tensors: offloading output layer to GPU
0.00.636.163 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.196 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.636.202 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.637.765 I llama_init_from_model: n_seq_max     = 1
0.00.637.769 I llama_init_from_model: n_ctx         = 128
0.00.637.770 I llama_init_from_model: n_ctx_per_seq = 128
0.00.637.770 I llama_init_from_model: n_batch       = 128
0.00.637.771 I llama_init_from_model: n_ubatch      = 128
0.00.637.771 I llama_init_from_model: flash_attn    = 0
0.00.637.773 I llama_init_from_model: freq_base     = 10000.0
0.00.637.774 I llama_init_from_model: freq_scale    = 1
0.00.637.775 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.637.777 I ggml_metal_init: allocating
0.00.637.848 I ggml_metal_init: found device: Apple M4
0.00.637.862 I ggml_metal_init: picking default device: Apple M4
0.00.639.342 I ggml_metal_init: using embedded metal library
0.00.645.682 I ggml_metal_init: GPU name:   Apple M4
0.00.645.686 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.687 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.688 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.688 I ggml_metal_init: simdgroup reduction   = true
0.00.645.688 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.689 I ggml_metal_init: has residency sets    = true
0.00.645.689 I ggml_metal_init: has bfloat            = true
0.00.645.689 I ggml_metal_init: use bfloat            = true
0.00.645.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.050 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.666.630 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.666.633 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.666.659 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.670.004 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.670.006 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.670.006 I llama_init_from_model: graph nodes  = 967
0.00.670.007 I llama_init_from_model: graph splits = 2
0.00.670.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.670.010 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.079 I 
0.00.700.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.186 I perplexity: tokenizing the input ..
0.00.707.492 I perplexity: tokenization took 7.302 ms
0.00.707.514 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.027 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.844.363 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.844.372 I llama_perf_context_print:        load time =     690.09 ms
0.00.844.374 I llama_perf_context_print: prompt eval time =     134.92 ms /   128 tokens (    1.05 ms per token,   948.73 tokens per second)
0.00.844.375 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.844.375 I llama_perf_context_print:       total time =     144.30 ms /   129 tokens
0.00.844.789 I ggml_metal_free: deallocating

real	0m0.860s
user	0m0.079s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.079 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.454 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.454 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.455 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.455 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.455 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.456 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.457 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.457 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.458 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.458 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.458 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.459 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.460 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.461 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.461 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.137 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.139 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.139 I llama_model_loader: - type  f32:  194 tensors
0.00.025.139 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.140 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.140 I print_info: file format = GGUF V3 (latest)
0.00.025.141 I print_info: file type   = Q5_1
0.00.025.142 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.316 I load: special tokens cache size = 25
0.00.039.257 I load: token to piece cache size = 0.2984 MB
0.00.039.260 I print_info: arch             = gptneox
0.00.039.260 I print_info: vocab_only       = 0
0.00.039.260 I print_info: n_ctx_train      = 2048
0.00.039.261 I print_info: n_embd           = 2048
0.00.039.261 I print_info: n_layer          = 24
0.00.039.264 I print_info: n_head           = 16
0.00.039.265 I print_info: n_head_kv        = 16
0.00.039.265 I print_info: n_rot            = 32
0.00.039.265 I print_info: n_swa            = 0
0.00.039.265 I print_info: n_embd_head_k    = 128
0.00.039.265 I print_info: n_embd_head_v    = 128
0.00.039.266 I print_info: n_gqa            = 1
0.00.039.267 I print_info: n_embd_k_gqa     = 2048
0.00.039.268 I print_info: n_embd_v_gqa     = 2048
0.00.039.269 I print_info: f_norm_eps       = 1.0e-05
0.00.039.269 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.269 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.271 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.271 I print_info: f_logit_scale    = 0.0e+00
0.00.039.272 I print_info: n_ff             = 8192
0.00.039.272 I print_info: n_expert         = 0
0.00.039.272 I print_info: n_expert_used    = 0
0.00.039.273 I print_info: causal attn      = 1
0.00.039.273 I print_info: pooling type     = 0
0.00.039.273 I print_info: rope type        = 2
0.00.039.274 I print_info: rope scaling     = linear
0.00.039.275 I print_info: freq_base_train  = 10000.0
0.00.039.275 I print_info: freq_scale_train = 1
0.00.039.275 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.275 I print_info: rope_finetuned   = unknown
0.00.039.275 I print_info: ssm_d_conv       = 0
0.00.039.276 I print_info: ssm_d_inner      = 0
0.00.039.276 I print_info: ssm_d_state      = 0
0.00.039.276 I print_info: ssm_dt_rank      = 0
0.00.039.276 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.276 I print_info: model type       = 1.4B
0.00.039.277 I print_info: model params     = 1.41 B
0.00.039.277 I print_info: general.name     = 1.4B
0.00.039.277 I print_info: vocab type       = BPE
0.00.039.278 I print_info: n_vocab          = 50304
0.00.039.278 I print_info: n_merges         = 50009
0.00.039.283 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.284 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: LF token         = 128 'Ä'
0.00.039.285 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: max token length = 1024
0.00.682.605 I load_tensors: offloading 24 repeating layers to GPU
0.00.682.618 I load_tensors: offloading output layer to GPU
0.00.682.619 I load_tensors: offloaded 25/25 layers to GPU
0.00.682.655 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.682.657 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.684.171 I llama_init_from_model: n_seq_max     = 1
0.00.684.174 I llama_init_from_model: n_ctx         = 128
0.00.684.175 I llama_init_from_model: n_ctx_per_seq = 128
0.00.684.180 I llama_init_from_model: n_batch       = 128
0.00.684.180 I llama_init_from_model: n_ubatch      = 128
0.00.684.181 I llama_init_from_model: flash_attn    = 0
0.00.684.182 I llama_init_from_model: freq_base     = 10000.0
0.00.684.185 I llama_init_from_model: freq_scale    = 1
0.00.684.186 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.684.187 I ggml_metal_init: allocating
0.00.684.213 I ggml_metal_init: found device: Apple M4
0.00.684.221 I ggml_metal_init: picking default device: Apple M4
0.00.685.595 I ggml_metal_init: using embedded metal library
0.00.691.919 I ggml_metal_init: GPU name:   Apple M4
0.00.691.923 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.924 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.925 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.925 I ggml_metal_init: simdgroup reduction   = true
0.00.691.925 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.926 I ggml_metal_init: has residency sets    = true
0.00.691.926 I ggml_metal_init: has bfloat            = true
0.00.691.926 I ggml_metal_init: use bfloat            = true
0.00.691.927 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.708.371 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.803 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.711.806 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.711.833 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.159 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.715.161 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.715.162 I llama_init_from_model: graph nodes  = 967
0.00.715.162 I llama_init_from_model: graph splits = 2
0.00.715.164 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.715.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.645 I 
0.00.742.736 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.756 I perplexity: tokenizing the input ..
0.00.750.227 I perplexity: tokenization took 7.468 ms
0.00.750.246 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.886.203 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.887.544 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.887.561 I llama_perf_context_print:        load time =     733.56 ms
0.00.887.562 I llama_perf_context_print: prompt eval time =     134.99 ms /   128 tokens (    1.05 ms per token,   948.23 tokens per second)
0.00.887.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.887.563 I llama_perf_context_print:       total time =     144.92 ms /   129 tokens
0.00.887.953 I ggml_metal_free: deallocating

real	0m0.901s
user	0m0.079s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.914 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.377 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.020.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.385 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.386 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.386 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.387 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.387 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.388 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.388 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.390 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.393 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.192 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.685 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.424 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.031.425 I llama_model_loader: - type  f32:  194 tensors
0.00.031.425 I llama_model_loader: - type q2_K:   49 tensors
0.00.031.425 I llama_model_loader: - type q3_K:   48 tensors
0.00.031.426 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.426 I print_info: file format = GGUF V3 (latest)
0.00.031.432 I print_info: file type   = Q2_K - Medium
0.00.031.434 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.042.221 I load: special tokens cache size = 25
0.00.050.362 I load: token to piece cache size = 0.2984 MB
0.00.050.365 I print_info: arch             = gptneox
0.00.050.365 I print_info: vocab_only       = 0
0.00.050.366 I print_info: n_ctx_train      = 2048
0.00.050.366 I print_info: n_embd           = 2048
0.00.050.366 I print_info: n_layer          = 24
0.00.050.368 I print_info: n_head           = 16
0.00.050.369 I print_info: n_head_kv        = 16
0.00.050.369 I print_info: n_rot            = 32
0.00.050.370 I print_info: n_swa            = 0
0.00.050.370 I print_info: n_embd_head_k    = 128
0.00.050.370 I print_info: n_embd_head_v    = 128
0.00.050.371 I print_info: n_gqa            = 1
0.00.050.371 I print_info: n_embd_k_gqa     = 2048
0.00.050.372 I print_info: n_embd_v_gqa     = 2048
0.00.050.373 I print_info: f_norm_eps       = 1.0e-05
0.00.050.373 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.373 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.373 I print_info: f_logit_scale    = 0.0e+00
0.00.050.374 I print_info: n_ff             = 8192
0.00.050.374 I print_info: n_expert         = 0
0.00.050.374 I print_info: n_expert_used    = 0
0.00.050.374 I print_info: causal attn      = 1
0.00.050.374 I print_info: pooling type     = 0
0.00.050.374 I print_info: rope type        = 2
0.00.050.375 I print_info: rope scaling     = linear
0.00.050.375 I print_info: freq_base_train  = 10000.0
0.00.050.375 I print_info: freq_scale_train = 1
0.00.050.376 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.376 I print_info: rope_finetuned   = unknown
0.00.050.376 I print_info: ssm_d_conv       = 0
0.00.050.376 I print_info: ssm_d_inner      = 0
0.00.050.376 I print_info: ssm_d_state      = 0
0.00.050.376 I print_info: ssm_dt_rank      = 0
0.00.050.376 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.378 I print_info: model type       = 1.4B
0.00.050.378 I print_info: model params     = 1.41 B
0.00.050.379 I print_info: general.name     = 1.4B
0.00.050.379 I print_info: vocab type       = BPE
0.00.050.379 I print_info: n_vocab          = 50304
0.00.050.379 I print_info: n_merges         = 50009
0.00.050.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.380 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.380 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.380 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.380 I print_info: LF token         = 128 'Ä'
0.00.050.381 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.381 I print_info: max token length = 1024
0.00.394.764 I load_tensors: offloading 24 repeating layers to GPU
0.00.394.778 I load_tensors: offloading output layer to GPU
0.00.394.779 I load_tensors: offloaded 25/25 layers to GPU
0.00.394.807 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.394.809 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.396.306 I llama_init_from_model: n_seq_max     = 1
0.00.396.315 I llama_init_from_model: n_ctx         = 128
0.00.396.315 I llama_init_from_model: n_ctx_per_seq = 128
0.00.396.315 I llama_init_from_model: n_batch       = 128
0.00.396.316 I llama_init_from_model: n_ubatch      = 128
0.00.396.316 I llama_init_from_model: flash_attn    = 0
0.00.396.317 I llama_init_from_model: freq_base     = 10000.0
0.00.396.317 I llama_init_from_model: freq_scale    = 1
0.00.396.318 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.396.320 I ggml_metal_init: allocating
0.00.396.372 I ggml_metal_init: found device: Apple M4
0.00.396.386 I ggml_metal_init: picking default device: Apple M4
0.00.398.044 I ggml_metal_init: using embedded metal library
0.00.404.164 I ggml_metal_init: GPU name:   Apple M4
0.00.404.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.404.180 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.404.181 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.404.181 I ggml_metal_init: simdgroup reduction   = true
0.00.404.181 I ggml_metal_init: simdgroup matrix mul. = true
0.00.404.182 I ggml_metal_init: has residency sets    = true
0.00.404.182 I ggml_metal_init: has bfloat            = true
0.00.404.182 I ggml_metal_init: use bfloat            = true
0.00.404.187 I ggml_metal_init: hasUnifiedMemory      = true
0.00.404.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.427.236 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.431.066 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.431.077 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.431.127 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.434.628 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.434.630 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.434.631 I llama_init_from_model: graph nodes  = 967
0.00.434.631 I llama_init_from_model: graph splits = 2
0.00.434.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.434.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.467.120 I 
0.00.467.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.467.219 I perplexity: tokenizing the input ..
0.00.473.695 I perplexity: tokenization took 6.473 ms
0.00.473.711 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.614.079 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.615.420 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.615.437 I llama_perf_context_print:        load time =     457.19 ms
0.00.615.438 I llama_perf_context_print: prompt eval time =     139.78 ms /   128 tokens (    1.09 ms per token,   915.74 tokens per second)
0.00.615.438 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.439 I llama_perf_context_print:       total time =     148.32 ms /   129 tokens
0.00.615.862 I ggml_metal_free: deallocating

real	0m0.631s
user	0m0.092s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.016 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.168 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.169 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.171 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.173 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.174 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.179 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.179 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.994 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.007 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.852 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.853 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.854 I llama_model_loader: - type  f32:  194 tensors
0.00.024.854 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.854 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.854 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.855 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.855 I print_info: file format = GGUF V3 (latest)
0.00.024.860 I print_info: file type   = Q3_K - Medium
0.00.024.861 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.759 I load: special tokens cache size = 25
0.00.038.680 I load: token to piece cache size = 0.2984 MB
0.00.038.682 I print_info: arch             = gptneox
0.00.038.683 I print_info: vocab_only       = 0
0.00.038.683 I print_info: n_ctx_train      = 2048
0.00.038.683 I print_info: n_embd           = 2048
0.00.038.683 I print_info: n_layer          = 24
0.00.038.686 I print_info: n_head           = 16
0.00.038.686 I print_info: n_head_kv        = 16
0.00.038.686 I print_info: n_rot            = 32
0.00.038.687 I print_info: n_swa            = 0
0.00.038.687 I print_info: n_embd_head_k    = 128
0.00.038.687 I print_info: n_embd_head_v    = 128
0.00.038.688 I print_info: n_gqa            = 1
0.00.038.689 I print_info: n_embd_k_gqa     = 2048
0.00.038.689 I print_info: n_embd_v_gqa     = 2048
0.00.038.690 I print_info: f_norm_eps       = 1.0e-05
0.00.038.690 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.690 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.690 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.691 I print_info: f_logit_scale    = 0.0e+00
0.00.038.691 I print_info: n_ff             = 8192
0.00.038.691 I print_info: n_expert         = 0
0.00.038.691 I print_info: n_expert_used    = 0
0.00.038.692 I print_info: causal attn      = 1
0.00.038.692 I print_info: pooling type     = 0
0.00.038.693 I print_info: rope type        = 2
0.00.038.695 I print_info: rope scaling     = linear
0.00.038.696 I print_info: freq_base_train  = 10000.0
0.00.038.696 I print_info: freq_scale_train = 1
0.00.038.696 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.696 I print_info: rope_finetuned   = unknown
0.00.038.697 I print_info: ssm_d_conv       = 0
0.00.038.697 I print_info: ssm_d_inner      = 0
0.00.038.697 I print_info: ssm_d_state      = 0
0.00.038.697 I print_info: ssm_dt_rank      = 0
0.00.038.697 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.697 I print_info: model type       = 1.4B
0.00.038.698 I print_info: model params     = 1.41 B
0.00.038.698 I print_info: general.name     = 1.4B
0.00.038.698 I print_info: vocab type       = BPE
0.00.038.698 I print_info: n_vocab          = 50304
0.00.038.699 I print_info: n_merges         = 50009
0.00.038.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: LF token         = 128 'Ä'
0.00.038.701 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: max token length = 1024
0.00.440.709 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.723 I load_tensors: offloading output layer to GPU
0.00.440.723 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.762 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.764 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.442.235 I llama_init_from_model: n_seq_max     = 1
0.00.442.241 I llama_init_from_model: n_ctx         = 128
0.00.442.241 I llama_init_from_model: n_ctx_per_seq = 128
0.00.442.242 I llama_init_from_model: n_batch       = 128
0.00.442.242 I llama_init_from_model: n_ubatch      = 128
0.00.442.242 I llama_init_from_model: flash_attn    = 0
0.00.442.244 I llama_init_from_model: freq_base     = 10000.0
0.00.442.245 I llama_init_from_model: freq_scale    = 1
0.00.442.249 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.442.252 I ggml_metal_init: allocating
0.00.442.327 I ggml_metal_init: found device: Apple M4
0.00.442.341 I ggml_metal_init: picking default device: Apple M4
0.00.444.087 I ggml_metal_init: using embedded metal library
0.00.449.651 I ggml_metal_init: GPU name:   Apple M4
0.00.449.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.449.657 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.449.658 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.449.659 I ggml_metal_init: simdgroup reduction   = true
0.00.449.659 I ggml_metal_init: simdgroup matrix mul. = true
0.00.449.659 I ggml_metal_init: has residency sets    = true
0.00.449.660 I ggml_metal_init: has bfloat            = true
0.00.449.660 I ggml_metal_init: use bfloat            = true
0.00.449.661 I ggml_metal_init: hasUnifiedMemory      = true
0.00.449.663 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.468.866 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.472.411 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.472.415 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.472.485 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.475.608 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.475.610 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.475.611 I llama_init_from_model: graph nodes  = 967
0.00.475.611 I llama_init_from_model: graph splits = 2
0.00.475.614 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.475.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.461 I 
0.00.503.532 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.553 I perplexity: tokenizing the input ..
0.00.510.644 I perplexity: tokenization took 7.087 ms
0.00.510.666 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.653.426 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.654.768 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.654.784 I llama_perf_context_print:        load time =     494.43 ms
0.00.654.786 I llama_perf_context_print: prompt eval time =     141.81 ms /   128 tokens (    1.11 ms per token,   902.58 tokens per second)
0.00.654.786 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.654.787 I llama_perf_context_print:       total time =     151.33 ms /   129 tokens
0.00.655.185 I ggml_metal_free: deallocating

real	0m0.670s
user	0m0.079s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.061 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.170 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.172 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.173 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.174 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.175 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.176 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.176 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.720 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.721 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.722 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.722 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.722 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.723 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.723 I llama_model_loader: - type  f32:  194 tensors
0.00.025.724 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.724 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.724 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.725 I print_info: file format = GGUF V3 (latest)
0.00.025.725 I print_info: file type   = Q4_K - Medium
0.00.025.726 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.973 I load: special tokens cache size = 25
0.00.039.896 I load: token to piece cache size = 0.2984 MB
0.00.039.899 I print_info: arch             = gptneox
0.00.039.899 I print_info: vocab_only       = 0
0.00.039.900 I print_info: n_ctx_train      = 2048
0.00.039.900 I print_info: n_embd           = 2048
0.00.039.900 I print_info: n_layer          = 24
0.00.039.903 I print_info: n_head           = 16
0.00.039.904 I print_info: n_head_kv        = 16
0.00.039.904 I print_info: n_rot            = 32
0.00.039.904 I print_info: n_swa            = 0
0.00.039.904 I print_info: n_embd_head_k    = 128
0.00.039.905 I print_info: n_embd_head_v    = 128
0.00.039.908 I print_info: n_gqa            = 1
0.00.039.909 I print_info: n_embd_k_gqa     = 2048
0.00.039.910 I print_info: n_embd_v_gqa     = 2048
0.00.039.911 I print_info: f_norm_eps       = 1.0e-05
0.00.039.911 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.911 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.912 I print_info: f_logit_scale    = 0.0e+00
0.00.039.913 I print_info: n_ff             = 8192
0.00.039.913 I print_info: n_expert         = 0
0.00.039.913 I print_info: n_expert_used    = 0
0.00.039.913 I print_info: causal attn      = 1
0.00.039.913 I print_info: pooling type     = 0
0.00.039.913 I print_info: rope type        = 2
0.00.039.913 I print_info: rope scaling     = linear
0.00.039.914 I print_info: freq_base_train  = 10000.0
0.00.039.914 I print_info: freq_scale_train = 1
0.00.039.914 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.915 I print_info: rope_finetuned   = unknown
0.00.039.915 I print_info: ssm_d_conv       = 0
0.00.039.916 I print_info: ssm_d_inner      = 0
0.00.039.916 I print_info: ssm_d_state      = 0
0.00.039.916 I print_info: ssm_dt_rank      = 0
0.00.039.917 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.917 I print_info: model type       = 1.4B
0.00.039.917 I print_info: model params     = 1.41 B
0.00.039.917 I print_info: general.name     = 1.4B
0.00.039.918 I print_info: vocab type       = BPE
0.00.039.918 I print_info: n_vocab          = 50304
0.00.039.918 I print_info: n_merges         = 50009
0.00.039.919 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.919 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.919 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.919 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.919 I print_info: LF token         = 128 'Ä'
0.00.039.921 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.921 I print_info: max token length = 1024
0.00.517.906 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.920 I load_tensors: offloading output layer to GPU
0.00.517.921 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.956 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.958 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.519.505 I llama_init_from_model: n_seq_max     = 1
0.00.519.511 I llama_init_from_model: n_ctx         = 128
0.00.519.511 I llama_init_from_model: n_ctx_per_seq = 128
0.00.519.512 I llama_init_from_model: n_batch       = 128
0.00.519.512 I llama_init_from_model: n_ubatch      = 128
0.00.519.513 I llama_init_from_model: flash_attn    = 0
0.00.519.514 I llama_init_from_model: freq_base     = 10000.0
0.00.519.515 I llama_init_from_model: freq_scale    = 1
0.00.519.515 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.519.524 I ggml_metal_init: allocating
0.00.519.645 I ggml_metal_init: found device: Apple M4
0.00.519.659 I ggml_metal_init: picking default device: Apple M4
0.00.521.522 I ggml_metal_init: using embedded metal library
0.00.527.908 I ggml_metal_init: GPU name:   Apple M4
0.00.527.912 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.527.913 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.527.913 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.527.914 I ggml_metal_init: simdgroup reduction   = true
0.00.527.914 I ggml_metal_init: simdgroup matrix mul. = true
0.00.527.914 I ggml_metal_init: has residency sets    = true
0.00.527.915 I ggml_metal_init: has bfloat            = true
0.00.527.915 I ggml_metal_init: use bfloat            = true
0.00.527.915 I ggml_metal_init: hasUnifiedMemory      = true
0.00.527.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.544.669 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.548.268 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.548.271 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.548.302 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.551.375 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.551.377 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.551.377 I llama_init_from_model: graph nodes  = 967
0.00.551.378 I llama_init_from_model: graph splits = 2
0.00.551.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.551.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.580.397 I 
0.00.580.474 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.580.493 I perplexity: tokenizing the input ..
0.00.587.593 I perplexity: tokenization took 7.097 ms
0.00.587.616 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.736.405 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.737.863 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.737.877 I llama_perf_context_print:        load time =     570.33 ms
0.00.737.878 I llama_perf_context_print: prompt eval time =     147.83 ms /   128 tokens (    1.15 ms per token,   865.84 tokens per second)
0.00.737.878 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.879 I llama_perf_context_print:       total time =     157.49 ms /   129 tokens
0.00.738.232 I ggml_metal_free: deallocating

real	0m0.753s
user	0m0.078s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.052 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.227 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.238 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.238 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.240 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.240 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.241 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.241 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.241 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.243 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.243 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.170 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.061 I llama_model_loader: - type  f32:  194 tensors
0.00.025.061 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.061 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.062 I print_info: file format = GGUF V3 (latest)
0.00.025.063 I print_info: file type   = Q5_K - Medium
0.00.025.064 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.922 I load: special tokens cache size = 25
0.00.038.811 I load: token to piece cache size = 0.2984 MB
0.00.038.815 I print_info: arch             = gptneox
0.00.038.816 I print_info: vocab_only       = 0
0.00.038.816 I print_info: n_ctx_train      = 2048
0.00.038.816 I print_info: n_embd           = 2048
0.00.038.816 I print_info: n_layer          = 24
0.00.038.819 I print_info: n_head           = 16
0.00.038.820 I print_info: n_head_kv        = 16
0.00.038.820 I print_info: n_rot            = 32
0.00.038.820 I print_info: n_swa            = 0
0.00.038.820 I print_info: n_embd_head_k    = 128
0.00.038.820 I print_info: n_embd_head_v    = 128
0.00.038.821 I print_info: n_gqa            = 1
0.00.038.822 I print_info: n_embd_k_gqa     = 2048
0.00.038.824 I print_info: n_embd_v_gqa     = 2048
0.00.038.824 I print_info: f_norm_eps       = 1.0e-05
0.00.038.824 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.825 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.825 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.827 I print_info: f_logit_scale    = 0.0e+00
0.00.038.827 I print_info: n_ff             = 8192
0.00.038.827 I print_info: n_expert         = 0
0.00.038.828 I print_info: n_expert_used    = 0
0.00.038.828 I print_info: causal attn      = 1
0.00.038.828 I print_info: pooling type     = 0
0.00.038.828 I print_info: rope type        = 2
0.00.038.828 I print_info: rope scaling     = linear
0.00.038.829 I print_info: freq_base_train  = 10000.0
0.00.038.829 I print_info: freq_scale_train = 1
0.00.038.829 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.829 I print_info: rope_finetuned   = unknown
0.00.038.830 I print_info: ssm_d_conv       = 0
0.00.038.830 I print_info: ssm_d_inner      = 0
0.00.038.830 I print_info: ssm_d_state      = 0
0.00.038.830 I print_info: ssm_dt_rank      = 0
0.00.038.830 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.830 I print_info: model type       = 1.4B
0.00.038.831 I print_info: model params     = 1.41 B
0.00.038.831 I print_info: general.name     = 1.4B
0.00.038.831 I print_info: vocab type       = BPE
0.00.038.831 I print_info: n_vocab          = 50304
0.00.038.832 I print_info: n_merges         = 50009
0.00.038.832 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.836 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.836 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.837 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.837 I print_info: LF token         = 128 'Ä'
0.00.038.837 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.837 I print_info: max token length = 1024
0.00.596.637 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.654 I load_tensors: offloading output layer to GPU
0.00.596.654 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.688 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.596.689 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.597.647 I llama_init_from_model: n_seq_max     = 1
0.00.597.650 I llama_init_from_model: n_ctx         = 128
0.00.597.650 I llama_init_from_model: n_ctx_per_seq = 128
0.00.597.654 I llama_init_from_model: n_batch       = 128
0.00.597.655 I llama_init_from_model: n_ubatch      = 128
0.00.597.656 I llama_init_from_model: flash_attn    = 0
0.00.597.657 I llama_init_from_model: freq_base     = 10000.0
0.00.597.666 I llama_init_from_model: freq_scale    = 1
0.00.597.667 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.668 I ggml_metal_init: allocating
0.00.597.715 I ggml_metal_init: found device: Apple M4
0.00.597.726 I ggml_metal_init: picking default device: Apple M4
0.00.599.230 I ggml_metal_init: using embedded metal library
0.00.605.397 I ggml_metal_init: GPU name:   Apple M4
0.00.605.400 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.401 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.402 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.403 I ggml_metal_init: simdgroup reduction   = true
0.00.605.403 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.403 I ggml_metal_init: has residency sets    = true
0.00.605.403 I ggml_metal_init: has bfloat            = true
0.00.605.404 I ggml_metal_init: use bfloat            = true
0.00.605.404 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.081 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.531 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.625.534 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.625.561 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.899 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.628.901 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.628.901 I llama_init_from_model: graph nodes  = 967
0.00.628.902 I llama_init_from_model: graph splits = 2
0.00.628.904 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.628.905 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.081 I 
0.00.665.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.177 I perplexity: tokenizing the input ..
0.00.672.068 I perplexity: tokenization took 6.887 ms
0.00.672.089 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.268 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.813.614 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.813.629 I llama_perf_context_print:        load time =     656.02 ms
0.00.813.630 I llama_perf_context_print: prompt eval time =     139.79 ms /   128 tokens (    1.09 ms per token,   915.65 tokens per second)
0.00.813.630 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.631 I llama_perf_context_print:       total time =     148.55 ms /   129 tokens
0.00.814.014 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.077s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.193 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.147 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.149 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.150 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.150 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.151 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.152 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.153 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.154 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.154 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.943 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.711 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.712 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.713 I llama_model_loader: - type  f32:  194 tensors
0.00.025.713 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.713 I print_info: file format = GGUF V3 (latest)
0.00.025.714 I print_info: file type   = Q6_K
0.00.025.715 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.634 I load: special tokens cache size = 25
0.00.039.557 I load: token to piece cache size = 0.2984 MB
0.00.039.560 I print_info: arch             = gptneox
0.00.039.561 I print_info: vocab_only       = 0
0.00.039.561 I print_info: n_ctx_train      = 2048
0.00.039.561 I print_info: n_embd           = 2048
0.00.039.561 I print_info: n_layer          = 24
0.00.039.564 I print_info: n_head           = 16
0.00.039.565 I print_info: n_head_kv        = 16
0.00.039.565 I print_info: n_rot            = 32
0.00.039.565 I print_info: n_swa            = 0
0.00.039.565 I print_info: n_embd_head_k    = 128
0.00.039.565 I print_info: n_embd_head_v    = 128
0.00.039.566 I print_info: n_gqa            = 1
0.00.039.567 I print_info: n_embd_k_gqa     = 2048
0.00.039.569 I print_info: n_embd_v_gqa     = 2048
0.00.039.570 I print_info: f_norm_eps       = 1.0e-05
0.00.039.572 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.572 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.572 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.572 I print_info: f_logit_scale    = 0.0e+00
0.00.039.573 I print_info: n_ff             = 8192
0.00.039.573 I print_info: n_expert         = 0
0.00.039.573 I print_info: n_expert_used    = 0
0.00.039.574 I print_info: causal attn      = 1
0.00.039.574 I print_info: pooling type     = 0
0.00.039.574 I print_info: rope type        = 2
0.00.039.574 I print_info: rope scaling     = linear
0.00.039.574 I print_info: freq_base_train  = 10000.0
0.00.039.576 I print_info: freq_scale_train = 1
0.00.039.576 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.576 I print_info: rope_finetuned   = unknown
0.00.039.576 I print_info: ssm_d_conv       = 0
0.00.039.576 I print_info: ssm_d_inner      = 0
0.00.039.577 I print_info: ssm_d_state      = 0
0.00.039.577 I print_info: ssm_dt_rank      = 0
0.00.039.577 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.577 I print_info: model type       = 1.4B
0.00.039.578 I print_info: model params     = 1.41 B
0.00.039.578 I print_info: general.name     = 1.4B
0.00.039.578 I print_info: vocab type       = BPE
0.00.039.579 I print_info: n_vocab          = 50304
0.00.039.579 I print_info: n_merges         = 50009
0.00.039.579 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: LF token         = 128 'Ä'
0.00.039.581 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: max token length = 1024
0.00.250.973 I load_tensors: offloading 24 repeating layers to GPU
0.00.250.979 I load_tensors: offloading output layer to GPU
0.00.250.980 I load_tensors: offloaded 25/25 layers to GPU
0.00.251.008 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.251.011 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.252.384 I llama_init_from_model: n_seq_max     = 1
0.00.252.387 I llama_init_from_model: n_ctx         = 128
0.00.252.387 I llama_init_from_model: n_ctx_per_seq = 128
0.00.252.387 I llama_init_from_model: n_batch       = 128
0.00.252.388 I llama_init_from_model: n_ubatch      = 128
0.00.252.389 I llama_init_from_model: flash_attn    = 0
0.00.252.390 I llama_init_from_model: freq_base     = 10000.0
0.00.252.390 I llama_init_from_model: freq_scale    = 1
0.00.252.391 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.252.393 I ggml_metal_init: allocating
0.00.252.435 I ggml_metal_init: found device: Apple M4
0.00.252.448 I ggml_metal_init: picking default device: Apple M4
0.00.253.807 I ggml_metal_init: using embedded metal library
0.00.259.764 I ggml_metal_init: GPU name:   Apple M4
0.00.259.768 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.259.769 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.259.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.259.771 I ggml_metal_init: simdgroup reduction   = true
0.00.259.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.259.771 I ggml_metal_init: has residency sets    = true
0.00.259.771 I ggml_metal_init: has bfloat            = true
0.00.259.772 I ggml_metal_init: use bfloat            = true
0.00.259.773 I ggml_metal_init: hasUnifiedMemory      = true
0.00.259.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.276.614 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.280.125 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.280.128 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.280.153 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.283.476 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.283.478 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.283.478 I llama_init_from_model: graph nodes  = 967
0.00.283.479 I llama_init_from_model: graph splits = 2
0.00.283.481 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.283.481 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.319.229 I 
0.00.319.303 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.319.323 I perplexity: tokenizing the input ..
0.00.325.226 I perplexity: tokenization took 5.901 ms
0.00.325.238 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.464.548 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.465.884 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.465.898 I llama_perf_context_print:        load time =     309.03 ms
0.00.465.899 I llama_perf_context_print: prompt eval time =     139.08 ms /   128 tokens (    1.09 ms per token,   920.34 tokens per second)
0.00.465.900 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.465.900 I llama_perf_context_print:       total time =     146.68 ms /   129 tokens
0.00.466.281 I ggml_metal_free: deallocating

real	0m0.481s
user	0m0.076s
sys	0m0.092s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.244 I build: 4573 (d7d1ecca) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.429 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.638 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.642 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.644 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.647 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.649 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.649 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.650 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.653 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.653 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.653 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.654 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.656 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.657 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.657 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.125 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.071 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.431 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.433 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.434 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.434 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.434 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.435 I llama_model_loader: - type  f32:  194 tensors
0.00.052.435 I llama_model_loader: - type  f16:   98 tensors
0.00.052.436 I print_info: file format = GGUF V3 (latest)
0.00.052.437 I print_info: file type   = all F32 (guessed)
0.00.052.438 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.182 I load: special tokens cache size = 25
0.00.071.744 I load: token to piece cache size = 0.2984 MB
0.00.071.747 I print_info: arch             = gptneox
0.00.071.747 I print_info: vocab_only       = 0
0.00.071.747 I print_info: n_ctx_train      = 2048
0.00.071.747 I print_info: n_embd           = 2048
0.00.071.748 I print_info: n_layer          = 24
0.00.071.751 I print_info: n_head           = 16
0.00.071.752 I print_info: n_head_kv        = 16
0.00.071.752 I print_info: n_rot            = 32
0.00.071.752 I print_info: n_swa            = 0
0.00.071.752 I print_info: n_embd_head_k    = 128
0.00.071.755 I print_info: n_embd_head_v    = 128
0.00.071.755 I print_info: n_gqa            = 1
0.00.071.757 I print_info: n_embd_k_gqa     = 2048
0.00.071.757 I print_info: n_embd_v_gqa     = 2048
0.00.071.758 I print_info: f_norm_eps       = 1.0e-05
0.00.071.758 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.759 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.759 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.759 I print_info: f_logit_scale    = 0.0e+00
0.00.071.760 I print_info: n_ff             = 8192
0.00.071.760 I print_info: n_expert         = 0
0.00.071.760 I print_info: n_expert_used    = 0
0.00.071.760 I print_info: causal attn      = 1
0.00.071.760 I print_info: pooling type     = 0
0.00.071.761 I print_info: rope type        = 2
0.00.071.761 I print_info: rope scaling     = linear
0.00.071.761 I print_info: freq_base_train  = 10000.0
0.00.071.762 I print_info: freq_scale_train = 1
0.00.071.762 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.762 I print_info: rope_finetuned   = unknown
0.00.071.762 I print_info: ssm_d_conv       = 0
0.00.071.764 I print_info: ssm_d_inner      = 0
0.00.071.764 I print_info: ssm_d_state      = 0
0.00.071.764 I print_info: ssm_dt_rank      = 0
0.00.071.764 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.765 I print_info: model type       = 1.4B
0.00.071.765 I print_info: model params     = 1.41 B
0.00.071.765 I print_info: general.name     = 1.4B
0.00.071.766 I print_info: vocab type       = BPE
0.00.071.766 I print_info: n_vocab          = 50304
0.00.071.766 I print_info: n_merges         = 50009
0.00.071.766 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.767 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.767 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.767 I print_info: LF token         = 128 'Ä'
0.00.071.767 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.768 I print_info: max token length = 1024
0.01.346.497 I load_tensors: offloading 24 repeating layers to GPU
0.01.346.503 I load_tensors: offloading output layer to GPU
0.01.346.504 I load_tensors: offloaded 25/25 layers to GPU
0.01.346.529 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.346.531 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.347.468 I llama_init_from_model: n_seq_max     = 1
0.01.347.469 I llama_init_from_model: n_ctx         = 128
0.01.347.469 I llama_init_from_model: n_ctx_per_seq = 128
0.01.347.469 I llama_init_from_model: n_batch       = 128
0.01.347.469 I llama_init_from_model: n_ubatch      = 128
0.01.347.470 I llama_init_from_model: flash_attn    = 0
0.01.347.470 I llama_init_from_model: freq_base     = 10000.0
0.01.347.471 I llama_init_from_model: freq_scale    = 1
0.01.347.471 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.347.472 I ggml_metal_init: allocating
0.01.347.528 I ggml_metal_init: found device: Apple M4
0.01.347.535 I ggml_metal_init: picking default device: Apple M4
0.01.348.573 I ggml_metal_init: using embedded metal library
0.01.352.341 I ggml_metal_init: GPU name:   Apple M4
0.01.352.343 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.352.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.352.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.352.345 I ggml_metal_init: simdgroup reduction   = true
0.01.352.345 I ggml_metal_init: simdgroup matrix mul. = true
0.01.352.345 I ggml_metal_init: has residency sets    = true
0.01.352.346 I ggml_metal_init: has bfloat            = true
0.01.352.346 I ggml_metal_init: use bfloat            = true
0.01.352.346 I ggml_metal_init: hasUnifiedMemory      = true
0.01.352.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.362.974 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.364.674 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.364.676 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.364.689 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.366.345 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.366.346 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.366.346 I llama_init_from_model: graph nodes  = 967
0.01.366.347 I llama_init_from_model: graph splits = 2
0.01.366.348 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.366.348 I 
0.01.366.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.366.385 I compute_imatrix: tokenizing the input ..
0.01.370.343 I compute_imatrix: tokenization took 3.957 ms
0.01.370.345 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.639.058 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.641.554 I llama_perf_context_print:        load time =    1617.63 ms
0.01.641.555 I llama_perf_context_print: prompt eval time =     266.97 ms /   128 tokens (    2.09 ms per token,   479.46 tokens per second)
0.01.641.556 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.641.556 I llama_perf_context_print:       total time =    1620.12 ms /   129 tokens
0.01.642.037 I ggml_metal_free: deallocating

real	0m1.827s
user	0m0.125s
sys	0m0.254s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4573 (d7d1ecca)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122204f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1222056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122205c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122206210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1222067c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122206d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122207320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1222078d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122207e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122208380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122208880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122208d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1222098a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12220a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12220a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12220af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12220b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12220bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12220c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12220ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12220d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12220daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12220e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12220eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12220f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12220f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12220faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122210710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122210c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122210f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1222113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122211670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122211f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122212440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122212700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122212ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122213040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1222134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122213980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122213e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1222142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122214760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122214c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1222150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122215360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122215970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122215f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1222168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122216eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1222174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122217ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1222180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1222186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122218d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1222194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122219990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122219e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12221a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12221a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12221aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12221b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12221b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12221baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12221bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12221c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12221c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12221cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12221d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12221d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12221db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12221dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12221e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12221e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12221ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12221f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12221f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12221fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1222203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122220910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122220e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1222213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122221900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122221e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1222223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1222228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122222e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122223390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1222238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122223e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122224380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1222248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122224e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122225370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1222258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122225e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122226360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1222268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122216590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122226d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1222274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122227a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122227f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1222284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122228a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122228f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1222294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122229a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122229f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12222a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12222a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12222af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12222b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12222b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12222be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12222c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12222c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12222cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12222d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12222d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12222da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12222dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12222e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12222e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12222ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12222f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12222f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12222faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12222ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1222303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122230880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122230d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1222311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122231660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122231b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122231fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122232440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1222328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122232d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122233220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1222336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122233b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122234000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1222344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122234940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122234de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122235280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122235720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122235bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122236060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122236500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1222369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122236e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1222372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122237780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122237c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1222380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122238560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122238a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122238ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122239340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1222397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122239c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12223a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12223a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12223aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12223af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12223b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12223b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12223bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12223c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12223c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12223cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12223cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12223d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12223d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12223dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12223e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12223e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12223eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12223efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12223f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12223f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12223fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122240240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1222406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122240b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122241020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1222414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122241960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122241e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1222422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122242740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122242be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122243130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122243680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122243bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122244120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1222443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1222449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122245000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122245610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122245e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1222462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122246560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122246b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122247180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122247970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122247e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1222482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122248750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122248f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122249450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1222499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122249ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12224a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12224a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12224aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12224b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12224b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12224bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12224c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12224c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12224cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12224d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12224d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12224deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12224e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12224e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12224eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12224f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12224f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12224fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1222503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122250930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122250e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1222513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122251920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122251e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1222523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122252910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122252e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1222533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122253900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122253e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1222543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1222548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122254e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122255390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1222558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122255e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122256380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1222568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122256e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122257370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1222578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122257e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122258360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1222588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122258e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122259350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1222598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122259df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12225a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12225a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12225ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12225b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12225b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12225bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12225c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12225c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12225cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12225cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12225d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12225d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12225dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12225e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12225e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12225eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12225f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12225f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12225f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12225fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122260330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122260a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122261170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122261890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122261fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122262270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122262a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122262d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122263330 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.685.111 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106504d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1065051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106505630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106505aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106505f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106506380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1065067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106506c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1065070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106507540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1065079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1065080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106508bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106509370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106509b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10650a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10650a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10650b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10650b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10650bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10650c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10650cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10650d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10650dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10650e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10650e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10650e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10650ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10650f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10650f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10650fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10650ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1065103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106510670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106510ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106510f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1065113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106511830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106511ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106512110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106512580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1065129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106512e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1065132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106513740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106513bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106514020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106514490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106514900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106514d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1065151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106515650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106515ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106515f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1065163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106516810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106516d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106517280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1065176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106517b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106517fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106518440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1065188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106518d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106519190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106519600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106519a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106519ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10651a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10651a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10651ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10651b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10651b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10651b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10651bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10651c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10651c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10651cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10651cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10651d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10651d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10651dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10651e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10651e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10651ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10651eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10651f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10651f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10651fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106520080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1065204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106520960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106520dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106521240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1065216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106521b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106521f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106522400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106522870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106522ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106523150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1065235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106523a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106523ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106524310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106524780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106524bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106525060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1065254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106525940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106525db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106526220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106526690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106526b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106526f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1065273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106527850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106527cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106528130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1065285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106528a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106528e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1065292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106529760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106529bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10652a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10652a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10652a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10652ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10652b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10652b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10652bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10652bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10652c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10652c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10652cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10652d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10652d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10652d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10652de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10652e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10652e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10652ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10652f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10652f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10652f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10652fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1065301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106530650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106530ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106530f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1065313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106531810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106531c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1065320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106532560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1065329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106532e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1065332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106533720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106533b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106534000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106534470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1065348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106534d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1065351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106535df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1065360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106536370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1065367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106536c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1065370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106537530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1065379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106537e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106538280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1065386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106538b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106538fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106539440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1065398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106539d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10653a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10653a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10653aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10653aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10653b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10653b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10653bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10653c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10653c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10653c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10653cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10653d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10653d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122204720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122246820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122244cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122262fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1222446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1222452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1222183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122217d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12221a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122246e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12220f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122216240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122216b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122217170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122215c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1222189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122217780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12220e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122209040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122226fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122262530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122211930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122211bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122247440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1222458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12220fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122210020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1222102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122263790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122263a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122263d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122263fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122264290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122264550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122264810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122264ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x122264d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122265050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122265310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1222655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122265890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122265b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122265e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1222660d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122266390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122266650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122266910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122266bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122266e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122267150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122267410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1222676d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122267990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122267c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122267f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1222681d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122268490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122268750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122268a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122268cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122268f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122269250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122269510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1222697d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122269a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122269d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12226a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12226a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12226a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12226a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12226ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12226add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12226b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12226b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12226b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12226b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12226bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12226be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12226c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12226c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12226c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12226c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12226cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12226ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12226d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12226d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12226d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12226d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12226dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12226df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12226e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12226e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12226e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12226ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12226ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12226efd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10653d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106508360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106535480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106504880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10650bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10653dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10653df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10653e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10653e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10653e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10653ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10653ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10653f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10653fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106540140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106540400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106540940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106540e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1065413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106541b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1065420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106542610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106542b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106543090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1065435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106543b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106543dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106544090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106544350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106544610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1065448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106544b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106544e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106545110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1065453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106545690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106545950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106545c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106545ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106546190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106546450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106546710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1065469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106546c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106546f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106547210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1065474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106547790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106547a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106547d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106547fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106548290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106548550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106548810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106548ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106548d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106549050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106549310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1065495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106549890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106549b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106549e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10654a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10654a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10654a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10654a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10654abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10654ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10654b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10654b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10654b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10654b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10654bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10654bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10654c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10654c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10654c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10654ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10654ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10654cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10654d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10654d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10654d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10654da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10654dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10654e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10654e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10654e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10654e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10654eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10654edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10654f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10654f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10654f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10654f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10654fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10654fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106550110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1065503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106550690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106550950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106550c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106550ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106551190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106551450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106551710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1065519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106551c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106551f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106552210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1065524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106552790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106552a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106552d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106552fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106553290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106553550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106553810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106553ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106553d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106554050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106554310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1065545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106554890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106554b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106554e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1065550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106555390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106555650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106555910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106555bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106555e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106556150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106556410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1065566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106556990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106556c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106556f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1065571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106557490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106557750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106557a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106557cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106557f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106558250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106558510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1065587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106558a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106558d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106559010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1065592d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106559590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106559850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106559b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106559dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10655a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10655a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10655a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10655a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10655ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10655ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10655b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10655b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10655b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10655b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10655bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10655bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10655c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10655c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10655c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10655c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10655cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10655cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10655d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10655d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10655d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10655da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10655dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10655dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10655e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10655e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10655e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10655ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10655ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10655f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10655f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10655f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10655f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10655fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10655ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1065603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106560830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106560ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106561110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106561580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1065619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106561e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1065622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106562740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106562bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106563020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106563490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106563900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106563e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1065642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106564750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106564bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106565030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106565550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106565a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1065665d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106566890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106566e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106567410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1065679d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106567f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106568550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106568b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1065690d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106569690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106569c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10656a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10656a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10656ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10656b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10656b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10656bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10656c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10656ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10656d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10656d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10656db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10656e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10656e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10656ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10656f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10656f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10656fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1065703d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106570990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106570f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106571510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106571ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106572090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106572650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106572c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1065731d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106573790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106573d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106574310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1065748d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106574e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106575450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106575a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106575fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106576590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106576b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106577110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1065776d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106577c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106578250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106578810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106578dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106579390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106579950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106579f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10657a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10657aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10657af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10657b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10657b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10657be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10657c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10657c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10657cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10657d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10657d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10657dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10657e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10657e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10657eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10657f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10657f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10657ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1065806c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106580de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106581500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1065817c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106581fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106582270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106582880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.750s
user	0m0.282s
sys	0m0.324s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4573 (d7d1ecca)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e0b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e0bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e0c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e0cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e0d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e0d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e0dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e0e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e0e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e0f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e0fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e10510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e12280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e13170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e13fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e14f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e15690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e15950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e15f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e17110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e17b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e18900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e19500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e19e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e1a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e1a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e1ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e1b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e1b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e1b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e1be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e1cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e1d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e1df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e1e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e20bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e21670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e21b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e21fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e22450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e23230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e23b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e24010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e24950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e24df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e25890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e25de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e26330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e26880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e26dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e27320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e27870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e27dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e28db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e29da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e2a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e2a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e2b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e2b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e2c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e2c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e2e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e2eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e2f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e2f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e2fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e30410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e31400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e31950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e31ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e32340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e32c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e33120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e33f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e34840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e34ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e35180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e35620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e35ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e35f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e36400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e37680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e37b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e37fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e38900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e39240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e3a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e3a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e3b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e3b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e3bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e3c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e3c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e3c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e3ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e3d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e3e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e3eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e3f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e3f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e3fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e40140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e40a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e40f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e41860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e41d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e42640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e42ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e43420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e43d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e44200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e44b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e44fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e45480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e45920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e45dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e46260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e46700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e47980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e47e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e48760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e48c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e4a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e4a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e4b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e4bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e4c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e4ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e4d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e4de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e4e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e4e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e4f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e4f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e4fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e50900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e50e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e51e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e52e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e53380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e53e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e54e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e55360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e55e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e56350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e568a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e56df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e57340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e57890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e57de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e58330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e58880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e58dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e59320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e59870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e59dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e5a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e5a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e5adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e5b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e5b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e5bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e5c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e5c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e5d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e5d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e5dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e5e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e5ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e5f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e5f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e5fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e60800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e60d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e61d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e621e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e62680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e62b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e62fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e63460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e63900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e63da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e64240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e64b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e65020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126e65960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126e65e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126e662a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126e667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126e66f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126e67630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126e67d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126e68470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126e68730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126e68f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126e691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126e697f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.359 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1280053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1280069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1280072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1280090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12800a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12800a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12800ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12800b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12800bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12800c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12800cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12800d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12800d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12800e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12800e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12800e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12800eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12800ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12800f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12800f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12800fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1280101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1280111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1280123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1280130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1280139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1280142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1280158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1280161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1280170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1280186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12801a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12801a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12801aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12801aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12801b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12801b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12801bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12801c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12801c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12801c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12801cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12801d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12801d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12801db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12801df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12801e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12801e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12801ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12801f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12801f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12801fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12801fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1280214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1280233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1280245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1280252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1280264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1280283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1280299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x128029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12802a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12802a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12802abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12802b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12802b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12802b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12802bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12802c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12802c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12802cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12802cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12802d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12802d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12802dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12802e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12802e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12802e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12802ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12802f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12802f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12802fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1280308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1280311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1280327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1280330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1280339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128035c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128035ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128036190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128036600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128036a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128036ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x128037350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1280377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x128037c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1280380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128038510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128038980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128038df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x128039260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1280396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128039b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x128039fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12803a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12803a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12803ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12803b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12803b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12803ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12803bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12803c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12803c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12803cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12803d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12803d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12803d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12803ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12803e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12803e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12803eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12803ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12803f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12803f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12803fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1280402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128040750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128041030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128041550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128041a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1280425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128042890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128042e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128043410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1280439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128044550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1280450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128045690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128046210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1280467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128046d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128047910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128047ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128048490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128048a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1280495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x128049b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12804a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12804a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12804acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12804b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12804b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12804be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12804c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12804c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12804cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12804d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12804dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12804e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12804e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12804ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12804f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12804f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12804fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128050310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1280508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128050e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128051450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128051a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128051fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128052590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128053110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1280536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128054250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128054810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128054dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128055390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128055950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128055f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1280564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128056a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128056f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128057490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128057990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128057e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128058390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128058890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128058d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128059290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128059c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12805a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12805a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12805ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12805b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12805b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12805bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12805c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12805cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12805d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12805d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12805dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12805e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12805e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116f046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116f04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116f04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116f05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116f058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116f05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116f06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116f065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116f06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116f06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116f07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116f079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116f08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116f08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116f094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x116f09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x116f0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x116f0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x116f0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x116f0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x116f0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116f0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x116f0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x116f0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x116f0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116f0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x116f0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116f0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116f0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x116f0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116f0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116f0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116f0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116f10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116f104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116f10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116f10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116f11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116f11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116f11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116f11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116f123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116f12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116f12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116f13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116f13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116f13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116f13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116f142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116f14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116f14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116f15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116f154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116f15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116f15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116f161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116f16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116f16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116f170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116f17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116f179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116f17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116f18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116f18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116f18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x116f18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116f19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116f198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x116f19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x116f1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x116f1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x116f1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x116f1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x116f1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x116f1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x116f1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x116f1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x116f1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x116f1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x116f1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x116f1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x116f1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x116f1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x116f1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x116f1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x116f1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x116f1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116f1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x116f1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x116f1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116f1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116f20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116f207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116f20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116f21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116f21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116f21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116f21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116f22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116f226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116f22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116f22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116f23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116f23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116f23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116f243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x116f24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116f24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116f25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116f25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116f25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116f25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116f262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x116f26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116f26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116f27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116f274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116f27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116f27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116f281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116f28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116f28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116f28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116f293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116f29820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116f29c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116f2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x116f2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x116f2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x116f2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x116f2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x116f2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116f2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x116f2c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x116f2c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x116f2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x116f2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x116f2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116f2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x116f2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x116f2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x116f2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x116f2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x116f2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x116f2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x116f2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x116f2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x116f2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116f302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116f30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116f30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116f30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116f31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116f318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116f31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116f321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116f32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116f32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116f32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x116f33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116f337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116f33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116f340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x116f34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116f349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116f34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116f35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116f356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116f35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x116f35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116f36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116f368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116f37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116f37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116f37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116f37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116f38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116f387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116f38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116f390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116f39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116f39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116f39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116f3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116f3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x116f3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x116f3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116f3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x116f3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x116f3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x116f3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116f3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x116f3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116f3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116f3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116f3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116f3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x116f3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116f3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116f3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x116f3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116f3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116f3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116f3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116f3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116f40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116f40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116f40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116f41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116f41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116f41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116f42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116f426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116f42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116f42fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116f43410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116f43880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116f43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116f44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116f445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x116f44a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116f44eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116f45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116f45790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116f46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116f464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116f46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116f46dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116f47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116f476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x116f47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x116f47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x116f483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x116f48860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x116f48cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x116f49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x116f495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x116f49a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x116f49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x116f4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x116f4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x116f4abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x116f4b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x116f4b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x116f4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x116f4bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x116f4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x116f4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116f4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116f4cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116f4d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x116f4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116f4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116f4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116f4e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116f4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116f4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116f4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116f4f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x116f4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116f50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116f504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116f50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116f50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116f511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116f51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116f51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116f51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116f523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116f52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116f52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116f53100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116f53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116f539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116f53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116f542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x116f54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x116f54ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x116f55010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116f55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x116f558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116f56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116f56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116f571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116f578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116f57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116f57ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x116f585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116f58c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.963s
user	0m0.236s
sys	0m0.186s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
