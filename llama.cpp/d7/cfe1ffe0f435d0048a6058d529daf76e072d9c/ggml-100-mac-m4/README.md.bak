### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.25 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.08 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.43 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.65 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.59 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.47 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.88 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.93 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  103.82 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.87 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.61 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.32 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 164.11 sec*proc (29 tests)

Total Test time (real) = 164.12 sec

real	2m44.420s
user	4m37.761s
sys	0m5.771s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.87 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.85 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.36 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.38 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.11 sec*proc (29 tests)

Total Test time (real) =  48.12 sec

real	0m48.134s
user	0m54.426s
sys	0m5.176s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.205 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.878 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.481 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.490 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.492 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.493 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.494 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.495 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.496 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.497 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.497 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.498 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.501 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.502 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.503 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.504 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.504 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.505 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.506 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.238 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.481 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.483 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.484 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.484 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.485 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.485 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.030.486 I llama_model_loader: - type  f32:  124 tensors
0.00.030.486 I llama_model_loader: - type  f16:   73 tensors
0.00.030.487 I print_info: file format = GGUF V3 (latest)
0.00.030.488 I print_info: file type   = F16
0.00.030.489 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.035.016 I load: special tokens cache size = 5
0.00.037.391 I load: token to piece cache size = 0.2032 MB
0.00.037.396 I print_info: arch             = bert
0.00.037.396 I print_info: vocab_only       = 0
0.00.037.396 I print_info: n_ctx_train      = 512
0.00.037.396 I print_info: n_embd           = 384
0.00.037.397 I print_info: n_layer          = 12
0.00.037.400 I print_info: n_head           = 12
0.00.037.401 I print_info: n_head_kv        = 12
0.00.037.401 I print_info: n_rot            = 32
0.00.037.405 I print_info: n_swa            = 0
0.00.037.405 I print_info: n_embd_head_k    = 32
0.00.037.405 I print_info: n_embd_head_v    = 32
0.00.037.406 I print_info: n_gqa            = 1
0.00.037.407 I print_info: n_embd_k_gqa     = 384
0.00.037.407 I print_info: n_embd_v_gqa     = 384
0.00.037.410 I print_info: f_norm_eps       = 1.0e-12
0.00.037.410 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.411 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.411 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.411 I print_info: f_logit_scale    = 0.0e+00
0.00.037.412 I print_info: n_ff             = 1536
0.00.037.412 I print_info: n_expert         = 0
0.00.037.412 I print_info: n_expert_used    = 0
0.00.037.413 I print_info: causal attn      = 0
0.00.037.413 I print_info: pooling type     = 2
0.00.037.413 I print_info: rope type        = 2
0.00.037.413 I print_info: rope scaling     = linear
0.00.037.414 I print_info: freq_base_train  = 10000.0
0.00.037.414 I print_info: freq_scale_train = 1
0.00.037.415 I print_info: n_ctx_orig_yarn  = 512
0.00.037.415 I print_info: rope_finetuned   = unknown
0.00.037.415 I print_info: ssm_d_conv       = 0
0.00.037.415 I print_info: ssm_d_inner      = 0
0.00.037.415 I print_info: ssm_d_state      = 0
0.00.037.416 I print_info: ssm_dt_rank      = 0
0.00.037.417 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.417 I print_info: model type       = 33M
0.00.037.418 I print_info: model params     = 33.21 M
0.00.037.418 I print_info: general.name     = Bge Small
0.00.037.419 I print_info: vocab type       = WPM
0.00.037.424 I print_info: n_vocab          = 30522
0.00.037.425 I print_info: n_merges         = 0
0.00.037.425 I print_info: BOS token        = 101 '[CLS]'
0.00.037.425 I print_info: UNK token        = 100 '[UNK]'
0.00.037.426 I print_info: SEP token        = 102 '[SEP]'
0.00.037.426 I print_info: PAD token        = 0 '[PAD]'
0.00.037.426 I print_info: MASK token       = 103 '[MASK]'
0.00.037.426 I print_info: LF token         = 0 '[PAD]'
0.00.037.427 I print_info: max token length = 21
0.00.037.427 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.040.699 I load_tensors: offloading 12 repeating layers to GPU
0.00.040.701 I load_tensors: offloading output layer to GPU
0.00.040.702 I load_tensors: offloaded 13/13 layers to GPU
0.00.040.727 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.729 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.011 I llama_init_from_model: n_seq_max     = 1
0.00.041.012 I llama_init_from_model: n_ctx         = 512
0.00.041.013 I llama_init_from_model: n_ctx_per_seq = 512
0.00.041.013 I llama_init_from_model: n_batch       = 2048
0.00.041.013 I llama_init_from_model: n_ubatch      = 2048
0.00.041.014 I llama_init_from_model: flash_attn    = 0
0.00.041.014 I llama_init_from_model: freq_base     = 10000.0
0.00.041.015 I llama_init_from_model: freq_scale    = 1
0.00.041.015 I ggml_metal_init: allocating
0.00.041.021 I ggml_metal_init: found device: Apple M4
0.00.041.026 I ggml_metal_init: picking default device: Apple M4
0.00.041.797 I ggml_metal_init: using embedded metal library
0.00.045.935 I ggml_metal_init: GPU name:   Apple M4
0.00.045.938 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.939 I ggml_metal_init: simdgroup reduction   = true
0.00.045.939 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.940 I ggml_metal_init: has residency sets    = true
0.00.045.940 I ggml_metal_init: has bfloat            = true
0.00.045.940 I ggml_metal_init: use bfloat            = true
0.00.045.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.827 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.059.514 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.516 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.538 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.060.701 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.060.703 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.060.703 I llama_init_from_model: graph nodes  = 429
0.00.060.703 I llama_init_from_model: graph splits = 2
0.00.060.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.350 I 
0.00.066.378 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.067.027 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.072.118 I llama_perf_context_print:        load time =      47.47 ms
0.00.072.119 I llama_perf_context_print: prompt eval time =       4.96 ms /     9 tokens (    0.55 ms per token,  1813.05 tokens per second)
0.00.072.120 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.072.120 I llama_perf_context_print:       total time =       5.77 ms /    10 tokens
0.00.072.262 I ggml_metal_free: deallocating

real	0m0.257s
user	0m0.051s
sys	0m0.037s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.048 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.345 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.984 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.990 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.992 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.992 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.993 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.994 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.994 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.994 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.995 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.996 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.998 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.999 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.999 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.002 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.002 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.003 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.332 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.065 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.067 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.067 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.067 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.068 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.068 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.068 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.068 I llama_model_loader: - type  f32:  124 tensors
0.00.015.069 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.069 I print_info: file format = GGUF V3 (latest)
0.00.015.070 I print_info: file type   = Q8_0
0.00.015.073 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.463 I load: special tokens cache size = 5
0.00.018.638 I load: token to piece cache size = 0.2032 MB
0.00.018.641 I print_info: arch             = bert
0.00.018.641 I print_info: vocab_only       = 0
0.00.018.641 I print_info: n_ctx_train      = 512
0.00.018.641 I print_info: n_embd           = 384
0.00.018.642 I print_info: n_layer          = 12
0.00.018.645 I print_info: n_head           = 12
0.00.018.645 I print_info: n_head_kv        = 12
0.00.018.646 I print_info: n_rot            = 32
0.00.018.646 I print_info: n_swa            = 0
0.00.018.646 I print_info: n_embd_head_k    = 32
0.00.018.646 I print_info: n_embd_head_v    = 32
0.00.018.647 I print_info: n_gqa            = 1
0.00.018.647 I print_info: n_embd_k_gqa     = 384
0.00.018.648 I print_info: n_embd_v_gqa     = 384
0.00.018.648 I print_info: f_norm_eps       = 1.0e-12
0.00.018.649 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.649 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.649 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.650 I print_info: f_logit_scale    = 0.0e+00
0.00.018.650 I print_info: n_ff             = 1536
0.00.018.650 I print_info: n_expert         = 0
0.00.018.654 I print_info: n_expert_used    = 0
0.00.018.654 I print_info: causal attn      = 0
0.00.018.654 I print_info: pooling type     = 2
0.00.018.654 I print_info: rope type        = 2
0.00.018.654 I print_info: rope scaling     = linear
0.00.018.655 I print_info: freq_base_train  = 10000.0
0.00.018.655 I print_info: freq_scale_train = 1
0.00.018.655 I print_info: n_ctx_orig_yarn  = 512
0.00.018.655 I print_info: rope_finetuned   = unknown
0.00.018.658 I print_info: ssm_d_conv       = 0
0.00.018.658 I print_info: ssm_d_inner      = 0
0.00.018.658 I print_info: ssm_d_state      = 0
0.00.018.658 I print_info: ssm_dt_rank      = 0
0.00.018.658 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.658 I print_info: model type       = 33M
0.00.018.659 I print_info: model params     = 33.21 M
0.00.018.659 I print_info: general.name     = Bge Small
0.00.018.659 I print_info: vocab type       = WPM
0.00.018.659 I print_info: n_vocab          = 30522
0.00.018.660 I print_info: n_merges         = 0
0.00.018.660 I print_info: BOS token        = 101 '[CLS]'
0.00.018.660 I print_info: UNK token        = 100 '[UNK]'
0.00.018.660 I print_info: SEP token        = 102 '[SEP]'
0.00.018.660 I print_info: PAD token        = 0 '[PAD]'
0.00.018.660 I print_info: MASK token       = 103 '[MASK]'
0.00.018.661 I print_info: LF token         = 0 '[PAD]'
0.00.018.661 I print_info: max token length = 21
0.00.018.665 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.308 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.309 I load_tensors: offloading output layer to GPU
0.00.020.309 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.315 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.315 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.481 I llama_init_from_model: n_seq_max     = 1
0.00.020.482 I llama_init_from_model: n_ctx         = 512
0.00.020.482 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.483 I llama_init_from_model: n_batch       = 2048
0.00.020.483 I llama_init_from_model: n_ubatch      = 2048
0.00.020.483 I llama_init_from_model: flash_attn    = 0
0.00.020.483 I llama_init_from_model: freq_base     = 10000.0
0.00.020.484 I llama_init_from_model: freq_scale    = 1
0.00.020.484 I ggml_metal_init: allocating
0.00.020.488 I ggml_metal_init: found device: Apple M4
0.00.020.491 I ggml_metal_init: picking default device: Apple M4
0.00.020.994 I ggml_metal_init: using embedded metal library
0.00.023.572 I ggml_metal_init: GPU name:   Apple M4
0.00.023.574 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.575 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.575 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.575 I ggml_metal_init: simdgroup reduction   = true
0.00.023.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.576 I ggml_metal_init: has residency sets    = true
0.00.023.576 I ggml_metal_init: has bfloat            = true
0.00.023.576 I ggml_metal_init: use bfloat            = true
0.00.023.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.026 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.619 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.621 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.635 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.601 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.602 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.602 I llama_init_from_model: graph nodes  = 429
0.00.035.602 I llama_init_from_model: graph splits = 2
0.00.035.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.604 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.519 I 
0.00.039.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.039 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.498 I llama_perf_context_print:        load time =      30.17 ms
0.00.044.500 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2071.82 tokens per second)
0.00.044.501 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.501 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.044.701 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.177 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.916 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.777 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.781 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.783 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.790 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.791 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.791 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.792 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.793 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.794 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.794 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.795 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.795 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.799 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.800 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.801 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.651 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.800 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.538 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.538 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.539 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.539 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.540 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.540 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.540 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.541 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.542 I llama_model_loader: - type  f32:   40 tensors
0.00.051.542 I llama_model_loader: - type  f16:   30 tensors
0.00.051.543 I print_info: file format = GGUF V3 (latest)
0.00.051.543 I print_info: file type   = F16
0.00.051.545 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.056.261 W load: empty token at index 5
0.00.061.745 W load: model vocab missing newline token, using special_pad_id instead
0.00.063.357 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.063.391 I load: special tokens cache size = 5
0.00.324.400 I load: token to piece cache size = 1.5060 MB
0.00.324.406 I print_info: arch             = jina-bert-v2
0.00.324.406 I print_info: vocab_only       = 0
0.00.324.406 I print_info: n_ctx_train      = 8192
0.00.324.407 I print_info: n_embd           = 384
0.00.324.407 I print_info: n_layer          = 4
0.00.324.414 I print_info: n_head           = 12
0.00.324.414 I print_info: n_head_kv        = 12
0.00.324.414 I print_info: n_rot            = 32
0.00.324.415 I print_info: n_swa            = 0
0.00.324.415 I print_info: n_embd_head_k    = 32
0.00.324.417 I print_info: n_embd_head_v    = 32
0.00.324.418 I print_info: n_gqa            = 1
0.00.324.418 I print_info: n_embd_k_gqa     = 384
0.00.324.419 I print_info: n_embd_v_gqa     = 384
0.00.324.420 I print_info: f_norm_eps       = 1.0e-12
0.00.324.421 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.324.421 I print_info: f_clamp_kqv      = 0.0e+00
0.00.324.422 I print_info: f_max_alibi_bias = 8.0e+00
0.00.324.422 I print_info: f_logit_scale    = 0.0e+00
0.00.324.422 I print_info: n_ff             = 1536
0.00.324.423 I print_info: n_expert         = 0
0.00.324.425 I print_info: n_expert_used    = 0
0.00.324.425 I print_info: causal attn      = 0
0.00.324.426 I print_info: pooling type     = -1
0.00.324.426 I print_info: rope type        = -1
0.00.324.426 I print_info: rope scaling     = linear
0.00.324.427 I print_info: freq_base_train  = 10000.0
0.00.324.427 I print_info: freq_scale_train = 1
0.00.324.427 I print_info: n_ctx_orig_yarn  = 8192
0.00.324.427 I print_info: rope_finetuned   = unknown
0.00.324.427 I print_info: ssm_d_conv       = 0
0.00.324.428 I print_info: ssm_d_inner      = 0
0.00.324.428 I print_info: ssm_d_state      = 0
0.00.324.428 I print_info: ssm_dt_rank      = 0
0.00.324.428 I print_info: ssm_dt_b_c_rms   = 0
0.00.324.428 I print_info: model type       = 33M
0.00.324.429 I print_info: model params     = 32.90 M
0.00.324.434 I print_info: general.name     = Jina Bert Implementation
0.00.324.435 I print_info: vocab type       = BPE
0.00.324.435 I print_info: n_vocab          = 61056
0.00.324.435 I print_info: n_merges         = 39382
0.00.324.436 I print_info: BOS token        = 0 '<s>'
0.00.324.436 I print_info: EOS token        = 2 '</s>'
0.00.324.436 I print_info: UNK token        = 3 '<unk>'
0.00.324.436 I print_info: SEP token        = 2 '</s>'
0.00.324.437 I print_info: PAD token        = 1 '<pad>'
0.00.324.437 I print_info: MASK token       = 4 '<mask>'
0.00.324.437 I print_info: EOG token        = 2 '</s>'
0.00.324.437 I print_info: max token length = 45
0.00.324.438 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.326.429 I load_tensors: offloading 4 repeating layers to GPU
0.00.326.430 I load_tensors: offloading output layer to GPU
0.00.326.430 I load_tensors: offloaded 5/5 layers to GPU
0.00.326.455 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.326.457 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.326.743 I llama_init_from_model: n_seq_max     = 1
0.00.326.744 I llama_init_from_model: n_ctx         = 8192
0.00.326.744 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.326.744 I llama_init_from_model: n_batch       = 2048
0.00.326.745 I llama_init_from_model: n_ubatch      = 2048
0.00.326.745 I llama_init_from_model: flash_attn    = 0
0.00.326.745 I llama_init_from_model: freq_base     = 10000.0
0.00.326.745 I llama_init_from_model: freq_scale    = 1
0.00.326.746 I ggml_metal_init: allocating
0.00.326.750 I ggml_metal_init: found device: Apple M4
0.00.326.753 I ggml_metal_init: picking default device: Apple M4
0.00.327.664 I ggml_metal_init: using embedded metal library
0.00.330.450 I ggml_metal_init: GPU name:   Apple M4
0.00.330.452 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.330.452 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.330.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.330.453 I ggml_metal_init: simdgroup reduction   = true
0.00.330.453 I ggml_metal_init: simdgroup matrix mul. = true
0.00.330.453 I ggml_metal_init: has residency sets    = true
0.00.330.453 I ggml_metal_init: has bfloat            = true
0.00.330.453 I ggml_metal_init: use bfloat            = true
0.00.330.454 I ggml_metal_init: hasUnifiedMemory      = true
0.00.330.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.340.326 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.343.292 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.343.294 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.343.316 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.349.820 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.349.822 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.349.822 I llama_init_from_model: graph nodes  = 154
0.00.349.822 I llama_init_from_model: graph splits = 2
0.00.349.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.349.824 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.242 I 
0.00.357.277 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.357.370 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.357.371 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.357.374 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.357.374 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.357.377 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.357.377 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.357.875 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.361.560 I llama_perf_context_print:        load time =     334.32 ms
0.00.361.561 I llama_perf_context_print: prompt eval time =       3.68 ms /    62 tokens (    0.06 ms per token, 16861.57 tokens per second)
0.00.361.562 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.361.562 I llama_perf_context_print:       total time =       4.32 ms /    63 tokens
0.00.361.747 I ggml_metal_free: deallocating

real	0m1.101s
user	0m0.331s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.193 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.367 I main: llama backend init
0.00.000.372 I main: load the model and apply lora adapter, if any
0.00.042.262 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.055.052 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.055.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.055.088 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.055.089 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.055.090 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.055.090 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.055.091 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.055.093 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.055.094 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.055.094 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.055.095 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.055.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.055.096 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.055.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.055.102 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.055.103 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.055.103 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.062.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.064.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.073.657 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.073.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.073.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.073.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.073.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.073.669 I llama_model_loader: - type  f32:  194 tensors
0.00.073.669 I llama_model_loader: - type  f16:   98 tensors
0.00.073.671 I print_info: file format = GGUF V3 (latest)
0.00.073.676 I print_info: file type   = all F32 (guessed)
0.00.073.678 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.089.775 I load: special tokens cache size = 25
0.00.099.030 I load: token to piece cache size = 0.2984 MB
0.00.099.034 I print_info: arch             = gptneox
0.00.099.035 I print_info: vocab_only       = 0
0.00.099.035 I print_info: n_ctx_train      = 2048
0.00.099.035 I print_info: n_embd           = 2048
0.00.099.035 I print_info: n_layer          = 24
0.00.099.041 I print_info: n_head           = 16
0.00.099.042 I print_info: n_head_kv        = 16
0.00.099.042 I print_info: n_rot            = 32
0.00.099.043 I print_info: n_swa            = 0
0.00.099.043 I print_info: n_embd_head_k    = 128
0.00.099.044 I print_info: n_embd_head_v    = 128
0.00.099.044 I print_info: n_gqa            = 1
0.00.099.045 I print_info: n_embd_k_gqa     = 2048
0.00.099.046 I print_info: n_embd_v_gqa     = 2048
0.00.099.047 I print_info: f_norm_eps       = 1.0e-05
0.00.099.047 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.099.048 I print_info: f_clamp_kqv      = 0.0e+00
0.00.099.048 I print_info: f_max_alibi_bias = 0.0e+00
0.00.099.048 I print_info: f_logit_scale    = 0.0e+00
0.00.099.049 I print_info: n_ff             = 8192
0.00.099.049 I print_info: n_expert         = 0
0.00.099.049 I print_info: n_expert_used    = 0
0.00.099.050 I print_info: causal attn      = 1
0.00.099.050 I print_info: pooling type     = 0
0.00.099.050 I print_info: rope type        = 2
0.00.099.050 I print_info: rope scaling     = linear
0.00.099.051 I print_info: freq_base_train  = 10000.0
0.00.099.051 I print_info: freq_scale_train = 1
0.00.099.051 I print_info: n_ctx_orig_yarn  = 2048
0.00.099.051 I print_info: rope_finetuned   = unknown
0.00.099.052 I print_info: ssm_d_conv       = 0
0.00.099.052 I print_info: ssm_d_inner      = 0
0.00.099.052 I print_info: ssm_d_state      = 0
0.00.099.052 I print_info: ssm_dt_rank      = 0
0.00.099.052 I print_info: ssm_dt_b_c_rms   = 0
0.00.099.053 I print_info: model type       = 1.4B
0.00.099.053 I print_info: model params     = 1.41 B
0.00.099.053 I print_info: general.name     = 1.4B
0.00.099.054 I print_info: vocab type       = BPE
0.00.099.054 I print_info: n_vocab          = 50304
0.00.099.054 I print_info: n_merges         = 50009
0.00.099.055 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.099.055 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.099.055 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.099.055 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.099.055 I print_info: LF token         = 187 ''
0.00.099.056 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.099.056 I print_info: max token length = 1024
0.00.099.057 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.203.143 I load_tensors: offloading 24 repeating layers to GPU
0.00.203.148 I load_tensors: offloading output layer to GPU
0.00.203.148 I load_tensors: offloaded 25/25 layers to GPU
0.00.203.174 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.203.175 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.203.833 I llama_init_from_model: n_seq_max     = 1
0.00.203.833 I llama_init_from_model: n_ctx         = 2048
0.00.203.834 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.203.834 I llama_init_from_model: n_batch       = 2048
0.00.203.834 I llama_init_from_model: n_ubatch      = 512
0.00.203.834 I llama_init_from_model: flash_attn    = 0
0.00.203.835 I llama_init_from_model: freq_base     = 10000.0
0.00.203.835 I llama_init_from_model: freq_scale    = 1
0.00.203.836 I ggml_metal_init: allocating
0.00.203.871 I ggml_metal_init: found device: Apple M4
0.00.203.877 I ggml_metal_init: picking default device: Apple M4
0.00.204.602 I ggml_metal_init: using embedded metal library
0.00.218.150 I ggml_metal_init: GPU name:   Apple M4
0.00.218.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.218.152 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.218.153 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.218.153 I ggml_metal_init: simdgroup reduction   = true
0.00.218.153 I ggml_metal_init: simdgroup matrix mul. = true
0.00.218.153 I ggml_metal_init: has residency sets    = true
0.00.218.153 I ggml_metal_init: has bfloat            = true
0.00.218.153 I ggml_metal_init: use bfloat            = true
0.00.218.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.218.155 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.293.012 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.325.306 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.325.314 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.325.364 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.329.877 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.329.880 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.329.881 I llama_init_from_model: graph nodes  = 967
0.00.329.881 I llama_init_from_model: graph splits = 2
0.00.329.888 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.330.020 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.330.021 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.395.404 I main: llama threadpool init, n_threads = 4
0.00.395.448 I 
0.00.395.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.395.482 I 
0.00.395.657 I sampler seed: 1234
0.00.395.661 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.395.686 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.395.688 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.395.688 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.228.556 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.02.228.556 I llama_perf_context_print:        load time =     352.27 ms
0.02.228.557 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.47 tokens per second)
0.02.228.558 I llama_perf_context_print:        eval time =    1786.39 ms /    63 runs   (   28.36 ms per token,    35.27 tokens per second)
0.02.228.558 I llama_perf_context_print:       total time =    1834.02 ms /    70 tokens
0.02.228.822 I ggml_metal_free: deallocating

real	0m2.558s
user	0m0.136s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.708 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.811 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.571 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.581 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.589 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.591 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.813 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.049.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.952 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.953 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.953 I llama_model_loader: - type  f32:  194 tensors
0.00.049.954 I llama_model_loader: - type  f16:   98 tensors
0.00.049.954 I print_info: file format = GGUF V3 (latest)
0.00.049.955 I print_info: file type   = all F32 (guessed)
0.00.049.956 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.061.679 I load: special tokens cache size = 25
0.00.069.083 I load: token to piece cache size = 0.2984 MB
0.00.069.086 I print_info: arch             = gptneox
0.00.069.086 I print_info: vocab_only       = 0
0.00.069.086 I print_info: n_ctx_train      = 2048
0.00.069.086 I print_info: n_embd           = 2048
0.00.069.087 I print_info: n_layer          = 24
0.00.069.090 I print_info: n_head           = 16
0.00.069.091 I print_info: n_head_kv        = 16
0.00.069.091 I print_info: n_rot            = 32
0.00.069.091 I print_info: n_swa            = 0
0.00.069.091 I print_info: n_embd_head_k    = 128
0.00.069.092 I print_info: n_embd_head_v    = 128
0.00.069.092 I print_info: n_gqa            = 1
0.00.069.093 I print_info: n_embd_k_gqa     = 2048
0.00.069.094 I print_info: n_embd_v_gqa     = 2048
0.00.069.094 I print_info: f_norm_eps       = 1.0e-05
0.00.069.094 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.095 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.095 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.095 I print_info: f_logit_scale    = 0.0e+00
0.00.069.096 I print_info: n_ff             = 8192
0.00.069.096 I print_info: n_expert         = 0
0.00.069.096 I print_info: n_expert_used    = 0
0.00.069.096 I print_info: causal attn      = 1
0.00.069.105 I print_info: pooling type     = 0
0.00.069.107 I print_info: rope type        = 2
0.00.069.107 I print_info: rope scaling     = linear
0.00.069.108 I print_info: freq_base_train  = 10000.0
0.00.069.108 I print_info: freq_scale_train = 1
0.00.069.108 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.109 I print_info: rope_finetuned   = unknown
0.00.069.109 I print_info: ssm_d_conv       = 0
0.00.069.109 I print_info: ssm_d_inner      = 0
0.00.069.109 I print_info: ssm_d_state      = 0
0.00.069.109 I print_info: ssm_dt_rank      = 0
0.00.069.110 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.111 I print_info: model type       = 1.4B
0.00.069.111 I print_info: model params     = 1.41 B
0.00.069.111 I print_info: general.name     = 1.4B
0.00.069.112 I print_info: vocab type       = BPE
0.00.069.112 I print_info: n_vocab          = 50304
0.00.069.112 I print_info: n_merges         = 50009
0.00.069.112 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.113 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.113 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.113 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.113 I print_info: LF token         = 187 ''
0.00.069.116 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.116 I print_info: max token length = 1024
0.00.069.116 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.439.965 I load_tensors: offloading 24 repeating layers to GPU
0.01.439.969 I load_tensors: offloading output layer to GPU
0.01.439.970 I load_tensors: offloaded 25/25 layers to GPU
0.01.440.001 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.440.003 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.440.992 I llama_init_from_model: n_seq_max     = 1
0.01.440.993 I llama_init_from_model: n_ctx         = 128
0.01.440.993 I llama_init_from_model: n_ctx_per_seq = 128
0.01.440.993 I llama_init_from_model: n_batch       = 128
0.01.440.993 I llama_init_from_model: n_ubatch      = 128
0.01.440.994 I llama_init_from_model: flash_attn    = 0
0.01.440.994 I llama_init_from_model: freq_base     = 10000.0
0.01.440.994 I llama_init_from_model: freq_scale    = 1
0.01.440.995 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.440.999 I ggml_metal_init: allocating
0.01.441.040 I ggml_metal_init: found device: Apple M4
0.01.441.046 I ggml_metal_init: picking default device: Apple M4
0.01.442.128 I ggml_metal_init: using embedded metal library
0.01.445.971 I ggml_metal_init: GPU name:   Apple M4
0.01.445.974 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.445.974 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.445.974 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.445.975 I ggml_metal_init: simdgroup reduction   = true
0.01.445.975 I ggml_metal_init: simdgroup matrix mul. = true
0.01.445.975 I ggml_metal_init: has residency sets    = true
0.01.445.975 I ggml_metal_init: has bfloat            = true
0.01.445.976 I ggml_metal_init: use bfloat            = true
0.01.445.976 I ggml_metal_init: hasUnifiedMemory      = true
0.01.445.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.457.052 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.458.760 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.458.763 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.458.783 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.460.372 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.460.373 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.460.373 I llama_init_from_model: graph nodes  = 967
0.01.460.374 I llama_init_from_model: graph splits = 2
0.01.460.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.460.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.496.206 I 
0.01.496.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.496.241 I perplexity: tokenizing the input ..
0.01.501.142 I perplexity: tokenization took 4.9 ms
0.01.501.146 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.620.168 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.621.670 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.621.701 I llama_perf_context_print:        load time =    1477.39 ms
0.01.621.701 I llama_perf_context_print: prompt eval time =     118.72 ms /   128 tokens (    0.93 ms per token,  1078.19 tokens per second)
0.01.621.702 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.621.703 I llama_perf_context_print:       total time =     125.49 ms /   129 tokens
0.01.622.047 I ggml_metal_free: deallocating

real	0m1.807s
user	0m0.093s
sys	0m0.261s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.091 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.132 I main: llama backend init
0.00.000.134 I main: load the model and apply lora adapter, if any
0.00.010.408 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.998 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.000 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.000 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.004 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.007 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.007 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.008 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.008 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.008 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.009 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.010 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.011 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.011 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.900 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.993 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.799 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.800 I llama_model_loader: - type  f32:  194 tensors
0.00.039.800 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.801 I print_info: file format = GGUF V3 (latest)
0.00.039.801 I print_info: file type   = Q8_0
0.00.039.802 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.802 I load: special tokens cache size = 25
0.00.055.485 I load: token to piece cache size = 0.2984 MB
0.00.055.489 I print_info: arch             = gptneox
0.00.055.489 I print_info: vocab_only       = 0
0.00.055.490 I print_info: n_ctx_train      = 2048
0.00.055.490 I print_info: n_embd           = 2048
0.00.055.492 I print_info: n_layer          = 24
0.00.055.497 I print_info: n_head           = 16
0.00.055.498 I print_info: n_head_kv        = 16
0.00.055.501 I print_info: n_rot            = 32
0.00.055.501 I print_info: n_swa            = 0
0.00.055.501 I print_info: n_embd_head_k    = 128
0.00.055.501 I print_info: n_embd_head_v    = 128
0.00.055.502 I print_info: n_gqa            = 1
0.00.055.503 I print_info: n_embd_k_gqa     = 2048
0.00.055.503 I print_info: n_embd_v_gqa     = 2048
0.00.055.504 I print_info: f_norm_eps       = 1.0e-05
0.00.055.504 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.504 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.505 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.505 I print_info: f_logit_scale    = 0.0e+00
0.00.055.506 I print_info: n_ff             = 8192
0.00.055.506 I print_info: n_expert         = 0
0.00.055.507 I print_info: n_expert_used    = 0
0.00.055.507 I print_info: causal attn      = 1
0.00.055.508 I print_info: pooling type     = 0
0.00.055.508 I print_info: rope type        = 2
0.00.055.508 I print_info: rope scaling     = linear
0.00.055.508 I print_info: freq_base_train  = 10000.0
0.00.055.509 I print_info: freq_scale_train = 1
0.00.055.509 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.509 I print_info: rope_finetuned   = unknown
0.00.055.509 I print_info: ssm_d_conv       = 0
0.00.055.509 I print_info: ssm_d_inner      = 0
0.00.055.509 I print_info: ssm_d_state      = 0
0.00.055.510 I print_info: ssm_dt_rank      = 0
0.00.055.510 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.510 I print_info: model type       = 1.4B
0.00.055.510 I print_info: model params     = 1.41 B
0.00.055.512 I print_info: general.name     = 1.4B
0.00.055.513 I print_info: vocab type       = BPE
0.00.055.513 I print_info: n_vocab          = 50304
0.00.055.513 I print_info: n_merges         = 50009
0.00.055.513 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.513 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.514 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.514 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.514 I print_info: LF token         = 187 ''
0.00.055.514 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.515 I print_info: max token length = 1024
0.00.055.515 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.220.033 I load_tensors: offloading 24 repeating layers to GPU
0.01.220.039 I load_tensors: offloading output layer to GPU
0.01.220.040 I load_tensors: offloaded 25/25 layers to GPU
0.01.220.066 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.220.068 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.221.266 I llama_init_from_model: n_seq_max     = 1
0.01.221.268 I llama_init_from_model: n_ctx         = 2048
0.01.221.269 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.221.269 I llama_init_from_model: n_batch       = 2048
0.01.221.270 I llama_init_from_model: n_ubatch      = 512
0.01.221.270 I llama_init_from_model: flash_attn    = 0
0.01.221.271 I llama_init_from_model: freq_base     = 10000.0
0.01.221.271 I llama_init_from_model: freq_scale    = 1
0.01.221.272 I ggml_metal_init: allocating
0.01.221.287 I ggml_metal_init: found device: Apple M4
0.01.221.296 I ggml_metal_init: picking default device: Apple M4
0.01.222.657 I ggml_metal_init: using embedded metal library
0.01.228.158 I ggml_metal_init: GPU name:   Apple M4
0.01.228.161 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.228.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.228.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.228.163 I ggml_metal_init: simdgroup reduction   = true
0.01.228.163 I ggml_metal_init: simdgroup matrix mul. = true
0.01.228.164 I ggml_metal_init: has residency sets    = true
0.01.228.164 I ggml_metal_init: has bfloat            = true
0.01.228.164 I ggml_metal_init: use bfloat            = true
0.01.228.165 I ggml_metal_init: hasUnifiedMemory      = true
0.01.228.166 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.244.731 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.299.432 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.299.442 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.299.478 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.303.773 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.303.775 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.303.775 I llama_init_from_model: graph nodes  = 967
0.01.303.776 I llama_init_from_model: graph splits = 2
0.01.303.780 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.303.905 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.303.905 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.361.222 I main: llama threadpool init, n_threads = 4
0.01.361.266 I 
0.01.361.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.361.291 I 
0.01.361.469 I sampler seed: 1234
0.01.361.474 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.361.511 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.361.515 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.361.515 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.457.775 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.02.457.775 I llama_perf_context_print:        load time =    1350.08 ms
0.02.457.777 I llama_perf_context_print: prompt eval time =      49.45 ms /     7 tokens (    7.06 ms per token,   141.57 tokens per second)
0.02.457.778 I llama_perf_context_print:        eval time =    1043.84 ms /    63 runs   (   16.57 ms per token,    60.35 tokens per second)
0.02.457.779 I llama_perf_context_print:       total time =    1097.29 ms /    70 tokens
0.02.458.043 I ggml_metal_free: deallocating

real	0m2.477s
user	0m0.110s
sys	0m0.290s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.440 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.743 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.851 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.862 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.863 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.863 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.864 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.867 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.868 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.868 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.869 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.806 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.597 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.598 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.598 I llama_model_loader: - type  f32:  194 tensors
0.00.027.599 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.599 I print_info: file format = GGUF V3 (latest)
0.00.027.600 I print_info: file type   = Q8_0
0.00.027.601 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.990 I load: special tokens cache size = 25
0.00.042.086 I load: token to piece cache size = 0.2984 MB
0.00.042.094 I print_info: arch             = gptneox
0.00.042.095 I print_info: vocab_only       = 0
0.00.042.095 I print_info: n_ctx_train      = 2048
0.00.042.095 I print_info: n_embd           = 2048
0.00.042.095 I print_info: n_layer          = 24
0.00.042.100 I print_info: n_head           = 16
0.00.042.101 I print_info: n_head_kv        = 16
0.00.042.101 I print_info: n_rot            = 32
0.00.042.101 I print_info: n_swa            = 0
0.00.042.101 I print_info: n_embd_head_k    = 128
0.00.042.103 I print_info: n_embd_head_v    = 128
0.00.042.103 I print_info: n_gqa            = 1
0.00.042.104 I print_info: n_embd_k_gqa     = 2048
0.00.042.107 I print_info: n_embd_v_gqa     = 2048
0.00.042.107 I print_info: f_norm_eps       = 1.0e-05
0.00.042.108 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.108 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.108 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.108 I print_info: f_logit_scale    = 0.0e+00
0.00.042.109 I print_info: n_ff             = 8192
0.00.042.109 I print_info: n_expert         = 0
0.00.042.109 I print_info: n_expert_used    = 0
0.00.042.111 I print_info: causal attn      = 1
0.00.042.111 I print_info: pooling type     = 0
0.00.042.111 I print_info: rope type        = 2
0.00.042.111 I print_info: rope scaling     = linear
0.00.042.112 I print_info: freq_base_train  = 10000.0
0.00.042.112 I print_info: freq_scale_train = 1
0.00.042.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.113 I print_info: rope_finetuned   = unknown
0.00.042.113 I print_info: ssm_d_conv       = 0
0.00.042.113 I print_info: ssm_d_inner      = 0
0.00.042.113 I print_info: ssm_d_state      = 0
0.00.042.113 I print_info: ssm_dt_rank      = 0
0.00.042.113 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.114 I print_info: model type       = 1.4B
0.00.042.114 I print_info: model params     = 1.41 B
0.00.042.114 I print_info: general.name     = 1.4B
0.00.042.115 I print_info: vocab type       = BPE
0.00.042.115 I print_info: n_vocab          = 50304
0.00.042.115 I print_info: n_merges         = 50009
0.00.042.115 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.116 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.116 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.116 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.116 I print_info: LF token         = 187 ''
0.00.042.116 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.117 I print_info: max token length = 1024
0.00.042.117 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.945.077 I load_tensors: offloading 24 repeating layers to GPU
0.00.945.086 I load_tensors: offloading output layer to GPU
0.00.945.086 I load_tensors: offloaded 25/25 layers to GPU
0.00.945.106 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.945.107 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.946.056 I llama_init_from_model: n_seq_max     = 1
0.00.946.060 I llama_init_from_model: n_ctx         = 128
0.00.946.060 I llama_init_from_model: n_ctx_per_seq = 128
0.00.946.061 I llama_init_from_model: n_batch       = 128
0.00.946.061 I llama_init_from_model: n_ubatch      = 128
0.00.946.062 I llama_init_from_model: flash_attn    = 0
0.00.946.063 I llama_init_from_model: freq_base     = 10000.0
0.00.946.063 I llama_init_from_model: freq_scale    = 1
0.00.946.064 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.946.065 I ggml_metal_init: allocating
0.00.946.102 I ggml_metal_init: found device: Apple M4
0.00.946.112 I ggml_metal_init: picking default device: Apple M4
0.00.947.130 I ggml_metal_init: using embedded metal library
0.00.951.333 I ggml_metal_init: GPU name:   Apple M4
0.00.951.340 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.951.340 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.951.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.951.341 I ggml_metal_init: simdgroup reduction   = true
0.00.951.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.951.342 I ggml_metal_init: has residency sets    = true
0.00.951.342 I ggml_metal_init: has bfloat            = true
0.00.951.343 I ggml_metal_init: use bfloat            = true
0.00.951.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.951.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.964.962 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.966.559 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.966.562 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.966.599 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.968.133 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.968.134 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.968.134 I llama_init_from_model: graph nodes  = 967
0.00.968.135 I llama_init_from_model: graph splits = 2
0.00.968.136 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.968.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.992.649 I 
0.00.992.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.992.688 I perplexity: tokenizing the input ..
0.00.996.496 I perplexity: tokenization took 3.806 ms
0.00.996.499 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.134.218 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.135.643 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.135.666 I llama_perf_context_print:        load time =     980.90 ms
0.01.135.667 I llama_perf_context_print: prompt eval time =     137.49 ms /   128 tokens (    1.07 ms per token,   930.98 tokens per second)
0.01.135.668 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.135.668 I llama_perf_context_print:       total time =     143.02 ms /   129 tokens
0.01.136.051 I ggml_metal_free: deallocating

real	0m1.154s
user	0m0.069s
sys	0m0.142s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.016.595 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.590 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.597 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.599 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.599 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.599 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.600 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.605 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.607 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.607 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.616 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.562 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.548 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.047.550 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.550 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.551 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.552 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.552 I llama_model_loader: - type  f32:  194 tensors
0.00.047.553 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.553 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.554 I print_info: file format = GGUF V3 (latest)
0.00.047.555 I print_info: file type   = Q4_0
0.00.047.556 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.060.423 I load: special tokens cache size = 25
0.00.074.064 I load: token to piece cache size = 0.2984 MB
0.00.074.070 I print_info: arch             = gptneox
0.00.074.071 I print_info: vocab_only       = 0
0.00.074.071 I print_info: n_ctx_train      = 2048
0.00.074.072 I print_info: n_embd           = 2048
0.00.074.075 I print_info: n_layer          = 24
0.00.074.082 I print_info: n_head           = 16
0.00.074.083 I print_info: n_head_kv        = 16
0.00.074.084 I print_info: n_rot            = 32
0.00.074.084 I print_info: n_swa            = 0
0.00.074.084 I print_info: n_embd_head_k    = 128
0.00.074.085 I print_info: n_embd_head_v    = 128
0.00.074.086 I print_info: n_gqa            = 1
0.00.074.087 I print_info: n_embd_k_gqa     = 2048
0.00.074.089 I print_info: n_embd_v_gqa     = 2048
0.00.074.090 I print_info: f_norm_eps       = 1.0e-05
0.00.074.090 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.090 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.091 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.093 I print_info: f_logit_scale    = 0.0e+00
0.00.074.094 I print_info: n_ff             = 8192
0.00.074.094 I print_info: n_expert         = 0
0.00.074.095 I print_info: n_expert_used    = 0
0.00.074.095 I print_info: causal attn      = 1
0.00.074.095 I print_info: pooling type     = 0
0.00.074.098 I print_info: rope type        = 2
0.00.074.100 I print_info: rope scaling     = linear
0.00.074.101 I print_info: freq_base_train  = 10000.0
0.00.074.101 I print_info: freq_scale_train = 1
0.00.074.102 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.102 I print_info: rope_finetuned   = unknown
0.00.074.103 I print_info: ssm_d_conv       = 0
0.00.074.103 I print_info: ssm_d_inner      = 0
0.00.074.103 I print_info: ssm_d_state      = 0
0.00.074.103 I print_info: ssm_dt_rank      = 0
0.00.074.104 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.110 I print_info: model type       = 1.4B
0.00.074.111 I print_info: model params     = 1.41 B
0.00.074.111 I print_info: general.name     = 1.4B
0.00.074.112 I print_info: vocab type       = BPE
0.00.074.112 I print_info: n_vocab          = 50304
0.00.074.113 I print_info: n_merges         = 50009
0.00.074.113 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.114 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.114 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.114 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.115 I print_info: LF token         = 187 ''
0.00.074.115 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.116 I print_info: max token length = 1024
0.00.074.117 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.655.203 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.221 I load_tensors: offloading output layer to GPU
0.00.655.222 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.267 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.655.268 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.657.073 I llama_init_from_model: n_seq_max     = 1
0.00.657.075 I llama_init_from_model: n_ctx         = 2048
0.00.657.076 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.657.077 I llama_init_from_model: n_batch       = 2048
0.00.657.077 I llama_init_from_model: n_ubatch      = 512
0.00.657.078 I llama_init_from_model: flash_attn    = 0
0.00.657.079 I llama_init_from_model: freq_base     = 10000.0
0.00.657.080 I llama_init_from_model: freq_scale    = 1
0.00.657.084 I ggml_metal_init: allocating
0.00.657.162 I ggml_metal_init: found device: Apple M4
0.00.657.175 I ggml_metal_init: picking default device: Apple M4
0.00.659.110 I ggml_metal_init: using embedded metal library
0.00.665.848 I ggml_metal_init: GPU name:   Apple M4
0.00.665.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.665.854 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.665.855 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.665.856 I ggml_metal_init: simdgroup reduction   = true
0.00.665.856 I ggml_metal_init: simdgroup matrix mul. = true
0.00.665.857 I ggml_metal_init: has residency sets    = true
0.00.665.857 I ggml_metal_init: has bfloat            = true
0.00.665.857 I ggml_metal_init: use bfloat            = true
0.00.665.858 I ggml_metal_init: hasUnifiedMemory      = true
0.00.665.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.684.777 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.200 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.747.209 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.747.246 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.751.199 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.751.201 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.751.202 I llama_init_from_model: graph nodes  = 967
0.00.751.202 I llama_init_from_model: graph splits = 2
0.00.751.207 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.751.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.751.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.506 I main: llama threadpool init, n_threads = 4
0.00.804.564 I 
0.00.804.588 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.590 I 
0.00.804.754 I sampler seed: 1234
0.00.804.759 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.770 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.770 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.774 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.487.629 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50035.24 tokens per second)
0.01.487.630 I llama_perf_context_print:        load time =     787.18 ms
0.01.487.631 I llama_perf_context_print: prompt eval time =      49.18 ms /     7 tokens (    7.03 ms per token,   142.33 tokens per second)
0.01.487.631 I llama_perf_context_print:        eval time =     630.75 ms /    63 runs   (   10.01 ms per token,    99.88 tokens per second)
0.01.487.632 I llama_perf_context_print:       total time =     683.85 ms /    70 tokens
0.01.487.908 I ggml_metal_free: deallocating

real	0m1.525s
user	0m0.129s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.280 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.211 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.671 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.679 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.683 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.684 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.685 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.687 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.687 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.687 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.605 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.564 I llama_model_loader: - type  f32:  194 tensors
0.00.026.564 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.564 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.565 I print_info: file format = GGUF V3 (latest)
0.00.026.565 I print_info: file type   = Q4_0
0.00.026.566 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.867 I load: special tokens cache size = 25
0.00.041.007 I load: token to piece cache size = 0.2984 MB
0.00.041.012 I print_info: arch             = gptneox
0.00.041.012 I print_info: vocab_only       = 0
0.00.041.013 I print_info: n_ctx_train      = 2048
0.00.041.013 I print_info: n_embd           = 2048
0.00.041.013 I print_info: n_layer          = 24
0.00.041.018 I print_info: n_head           = 16
0.00.041.019 I print_info: n_head_kv        = 16
0.00.041.021 I print_info: n_rot            = 32
0.00.041.022 I print_info: n_swa            = 0
0.00.041.022 I print_info: n_embd_head_k    = 128
0.00.041.022 I print_info: n_embd_head_v    = 128
0.00.041.022 I print_info: n_gqa            = 1
0.00.041.023 I print_info: n_embd_k_gqa     = 2048
0.00.041.023 I print_info: n_embd_v_gqa     = 2048
0.00.041.024 I print_info: f_norm_eps       = 1.0e-05
0.00.041.024 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.024 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.025 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.025 I print_info: f_logit_scale    = 0.0e+00
0.00.041.025 I print_info: n_ff             = 8192
0.00.041.025 I print_info: n_expert         = 0
0.00.041.026 I print_info: n_expert_used    = 0
0.00.041.026 I print_info: causal attn      = 1
0.00.041.026 I print_info: pooling type     = 0
0.00.041.026 I print_info: rope type        = 2
0.00.041.026 I print_info: rope scaling     = linear
0.00.041.027 I print_info: freq_base_train  = 10000.0
0.00.041.027 I print_info: freq_scale_train = 1
0.00.041.027 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.027 I print_info: rope_finetuned   = unknown
0.00.041.027 I print_info: ssm_d_conv       = 0
0.00.041.027 I print_info: ssm_d_inner      = 0
0.00.041.028 I print_info: ssm_d_state      = 0
0.00.041.028 I print_info: ssm_dt_rank      = 0
0.00.041.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.028 I print_info: model type       = 1.4B
0.00.041.028 I print_info: model params     = 1.41 B
0.00.041.028 I print_info: general.name     = 1.4B
0.00.041.029 I print_info: vocab type       = BPE
0.00.041.029 I print_info: n_vocab          = 50304
0.00.041.029 I print_info: n_merges         = 50009
0.00.041.029 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.029 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.029 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.030 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.030 I print_info: LF token         = 187 ''
0.00.041.030 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.030 I print_info: max token length = 1024
0.00.041.031 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.656.037 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.047 I load_tensors: offloading output layer to GPU
0.00.656.048 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.079 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.656.080 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.657.234 I llama_init_from_model: n_seq_max     = 1
0.00.657.238 I llama_init_from_model: n_ctx         = 128
0.00.657.238 I llama_init_from_model: n_ctx_per_seq = 128
0.00.657.238 I llama_init_from_model: n_batch       = 128
0.00.657.239 I llama_init_from_model: n_ubatch      = 128
0.00.657.239 I llama_init_from_model: flash_attn    = 0
0.00.657.240 I llama_init_from_model: freq_base     = 10000.0
0.00.657.241 I llama_init_from_model: freq_scale    = 1
0.00.657.241 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.657.242 I ggml_metal_init: allocating
0.00.657.310 I ggml_metal_init: found device: Apple M4
0.00.657.321 I ggml_metal_init: picking default device: Apple M4
0.00.659.358 I ggml_metal_init: using embedded metal library
0.00.664.365 I ggml_metal_init: GPU name:   Apple M4
0.00.664.378 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.378 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.379 I ggml_metal_init: simdgroup reduction   = true
0.00.664.379 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.382 I ggml_metal_init: has residency sets    = true
0.00.664.382 I ggml_metal_init: has bfloat            = true
0.00.664.382 I ggml_metal_init: use bfloat            = true
0.00.664.384 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.386 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.627 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.362 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.677.366 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.677.396 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.048 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.679.049 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.679.049 I llama_init_from_model: graph nodes  = 967
0.00.679.050 I llama_init_from_model: graph splits = 2
0.00.679.051 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.679.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.599 I 
0.00.704.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.639 I perplexity: tokenizing the input ..
0.00.708.763 I perplexity: tokenization took 4.122 ms
0.00.708.767 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.520 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.838.852 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.838.873 I llama_perf_context_print:        load time =     694.38 ms
0.00.838.874 I llama_perf_context_print: prompt eval time =     128.52 ms /   128 tokens (    1.00 ms per token,   995.97 tokens per second)
0.00.838.876 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.877 I llama_perf_context_print:       total time =     134.28 ms /   129 tokens
0.00.839.225 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.068s
sys	0m0.105s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.731 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.580 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.580 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.582 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.583 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.583 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.584 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.585 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.367 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.448 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.177 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.179 I llama_model_loader: - type  f32:  194 tensors
0.00.034.179 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.180 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.180 I print_info: file format = GGUF V3 (latest)
0.00.034.180 I print_info: file type   = Q4_1
0.00.034.181 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.007 I load: special tokens cache size = 25
0.00.049.726 I load: token to piece cache size = 0.2984 MB
0.00.049.729 I print_info: arch             = gptneox
0.00.049.729 I print_info: vocab_only       = 0
0.00.049.729 I print_info: n_ctx_train      = 2048
0.00.049.729 I print_info: n_embd           = 2048
0.00.049.729 I print_info: n_layer          = 24
0.00.049.732 I print_info: n_head           = 16
0.00.049.733 I print_info: n_head_kv        = 16
0.00.049.733 I print_info: n_rot            = 32
0.00.049.733 I print_info: n_swa            = 0
0.00.049.733 I print_info: n_embd_head_k    = 128
0.00.049.734 I print_info: n_embd_head_v    = 128
0.00.049.734 I print_info: n_gqa            = 1
0.00.049.735 I print_info: n_embd_k_gqa     = 2048
0.00.049.736 I print_info: n_embd_v_gqa     = 2048
0.00.049.736 I print_info: f_norm_eps       = 1.0e-05
0.00.049.737 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.737 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.737 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.737 I print_info: f_logit_scale    = 0.0e+00
0.00.049.738 I print_info: n_ff             = 8192
0.00.049.738 I print_info: n_expert         = 0
0.00.049.738 I print_info: n_expert_used    = 0
0.00.049.738 I print_info: causal attn      = 1
0.00.049.738 I print_info: pooling type     = 0
0.00.049.738 I print_info: rope type        = 2
0.00.049.739 I print_info: rope scaling     = linear
0.00.049.740 I print_info: freq_base_train  = 10000.0
0.00.049.741 I print_info: freq_scale_train = 1
0.00.049.741 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.741 I print_info: rope_finetuned   = unknown
0.00.049.741 I print_info: ssm_d_conv       = 0
0.00.049.741 I print_info: ssm_d_inner      = 0
0.00.049.741 I print_info: ssm_d_state      = 0
0.00.049.741 I print_info: ssm_dt_rank      = 0
0.00.049.742 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.742 I print_info: model type       = 1.4B
0.00.049.742 I print_info: model params     = 1.41 B
0.00.049.742 I print_info: general.name     = 1.4B
0.00.049.743 I print_info: vocab type       = BPE
0.00.049.743 I print_info: n_vocab          = 50304
0.00.049.743 I print_info: n_merges         = 50009
0.00.049.743 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.743 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.743 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.744 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.744 I print_info: LF token         = 187 ''
0.00.049.744 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.744 I print_info: max token length = 1024
0.00.049.745 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.716.666 I load_tensors: offloading 24 repeating layers to GPU
0.00.716.681 I load_tensors: offloading output layer to GPU
0.00.716.682 I load_tensors: offloaded 25/25 layers to GPU
0.00.716.723 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.716.724 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.718.170 I llama_init_from_model: n_seq_max     = 1
0.00.718.173 I llama_init_from_model: n_ctx         = 2048
0.00.718.174 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.718.174 I llama_init_from_model: n_batch       = 2048
0.00.718.175 I llama_init_from_model: n_ubatch      = 512
0.00.718.175 I llama_init_from_model: flash_attn    = 0
0.00.718.178 I llama_init_from_model: freq_base     = 10000.0
0.00.718.178 I llama_init_from_model: freq_scale    = 1
0.00.718.180 I ggml_metal_init: allocating
0.00.718.306 I ggml_metal_init: found device: Apple M4
0.00.718.321 I ggml_metal_init: picking default device: Apple M4
0.00.720.310 I ggml_metal_init: using embedded metal library
0.00.727.251 I ggml_metal_init: GPU name:   Apple M4
0.00.727.255 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.727.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.727.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.727.257 I ggml_metal_init: simdgroup reduction   = true
0.00.727.258 I ggml_metal_init: simdgroup matrix mul. = true
0.00.727.258 I ggml_metal_init: has residency sets    = true
0.00.727.258 I ggml_metal_init: has bfloat            = true
0.00.727.258 I ggml_metal_init: use bfloat            = true
0.00.727.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.727.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.746.208 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.797.546 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.797.553 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.797.589 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.801.473 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.801.475 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.801.475 I llama_init_from_model: graph nodes  = 967
0.00.801.475 I llama_init_from_model: graph splits = 2
0.00.801.482 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.801.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.801.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.858.966 I main: llama threadpool init, n_threads = 4
0.00.859.010 I 
0.00.859.034 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.859.034 I 
0.00.859.180 I sampler seed: 1234
0.00.859.184 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.859.195 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.859.197 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.859.197 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.586.429 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.586.429 I llama_perf_context_print:        load time =     849.52 ms
0.01.586.430 I llama_perf_context_print: prompt eval time =      49.16 ms /     7 tokens (    7.02 ms per token,   142.40 tokens per second)
0.01.586.431 I llama_perf_context_print:        eval time =     675.43 ms /    63 runs   (   10.72 ms per token,    93.27 tokens per second)
0.01.586.431 I llama_perf_context_print:       total time =     728.18 ms /    70 tokens
0.01.586.732 I ggml_metal_free: deallocating

real	0m1.603s
user	0m0.113s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.811 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.804 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.810 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.812 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.816 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.817 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.817 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.581 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.701 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.438 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.439 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.439 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.440 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.440 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.440 I llama_model_loader: - type  f32:  194 tensors
0.00.026.441 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.441 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.441 I print_info: file format = GGUF V3 (latest)
0.00.026.442 I print_info: file type   = Q4_1
0.00.026.443 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.234 I load: special tokens cache size = 25
0.00.040.184 I load: token to piece cache size = 0.2984 MB
0.00.040.189 I print_info: arch             = gptneox
0.00.040.189 I print_info: vocab_only       = 0
0.00.040.190 I print_info: n_ctx_train      = 2048
0.00.040.190 I print_info: n_embd           = 2048
0.00.040.190 I print_info: n_layer          = 24
0.00.040.194 I print_info: n_head           = 16
0.00.040.195 I print_info: n_head_kv        = 16
0.00.040.195 I print_info: n_rot            = 32
0.00.040.195 I print_info: n_swa            = 0
0.00.040.197 I print_info: n_embd_head_k    = 128
0.00.040.197 I print_info: n_embd_head_v    = 128
0.00.040.198 I print_info: n_gqa            = 1
0.00.040.199 I print_info: n_embd_k_gqa     = 2048
0.00.040.199 I print_info: n_embd_v_gqa     = 2048
0.00.040.200 I print_info: f_norm_eps       = 1.0e-05
0.00.040.200 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.200 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.200 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.201 I print_info: f_logit_scale    = 0.0e+00
0.00.040.201 I print_info: n_ff             = 8192
0.00.040.201 I print_info: n_expert         = 0
0.00.040.201 I print_info: n_expert_used    = 0
0.00.040.202 I print_info: causal attn      = 1
0.00.040.202 I print_info: pooling type     = 0
0.00.040.202 I print_info: rope type        = 2
0.00.040.202 I print_info: rope scaling     = linear
0.00.040.202 I print_info: freq_base_train  = 10000.0
0.00.040.203 I print_info: freq_scale_train = 1
0.00.040.203 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.203 I print_info: rope_finetuned   = unknown
0.00.040.203 I print_info: ssm_d_conv       = 0
0.00.040.203 I print_info: ssm_d_inner      = 0
0.00.040.203 I print_info: ssm_d_state      = 0
0.00.040.203 I print_info: ssm_dt_rank      = 0
0.00.040.204 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.204 I print_info: model type       = 1.4B
0.00.040.204 I print_info: model params     = 1.41 B
0.00.040.204 I print_info: general.name     = 1.4B
0.00.040.205 I print_info: vocab type       = BPE
0.00.040.205 I print_info: n_vocab          = 50304
0.00.040.205 I print_info: n_merges         = 50009
0.00.040.205 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.206 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.206 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.206 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.207 I print_info: LF token         = 187 ''
0.00.040.207 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.207 I print_info: max token length = 1024
0.00.040.207 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.025 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.040 I load_tensors: offloading output layer to GPU
0.00.649.041 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.076 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.649.078 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.650.712 I llama_init_from_model: n_seq_max     = 1
0.00.650.715 I llama_init_from_model: n_ctx         = 128
0.00.650.716 I llama_init_from_model: n_ctx_per_seq = 128
0.00.650.716 I llama_init_from_model: n_batch       = 128
0.00.650.717 I llama_init_from_model: n_ubatch      = 128
0.00.650.717 I llama_init_from_model: flash_attn    = 0
0.00.650.720 I llama_init_from_model: freq_base     = 10000.0
0.00.650.721 I llama_init_from_model: freq_scale    = 1
0.00.650.721 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.650.723 I ggml_metal_init: allocating
0.00.650.833 I ggml_metal_init: found device: Apple M4
0.00.650.847 I ggml_metal_init: picking default device: Apple M4
0.00.652.775 I ggml_metal_init: using embedded metal library
0.00.659.779 I ggml_metal_init: GPU name:   Apple M4
0.00.659.786 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.788 I ggml_metal_init: simdgroup reduction   = true
0.00.659.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.789 I ggml_metal_init: has residency sets    = true
0.00.659.789 I ggml_metal_init: has bfloat            = true
0.00.659.789 I ggml_metal_init: use bfloat            = true
0.00.659.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.179 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.697 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.681.701 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.681.750 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.224 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.685.226 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.685.227 I llama_init_from_model: graph nodes  = 967
0.00.685.227 I llama_init_from_model: graph splits = 2
0.00.685.230 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.685.230 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.257 I 
0.00.709.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.337 I perplexity: tokenizing the input ..
0.00.716.481 I perplexity: tokenization took 7.142 ms
0.00.716.488 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.532 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.851.869 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.851.893 I llama_perf_context_print:        load time =     700.44 ms
0.00.851.894 I llama_perf_context_print: prompt eval time =     133.19 ms /   128 tokens (    1.04 ms per token,   961.04 tokens per second)
0.00.851.895 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.895 I llama_perf_context_print:       total time =     142.64 ms /   129 tokens
0.00.852.272 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.080s
sys	0m0.120s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.283 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.721 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.725 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.728 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.729 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.729 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.730 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.731 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.731 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.736 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.736 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.736 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.387 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.452 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.111 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.111 I llama_model_loader: - type  f32:  194 tensors
0.00.027.111 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.112 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.112 I print_info: file format = GGUF V3 (latest)
0.00.027.113 I print_info: file type   = Q5_0
0.00.027.114 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.162 I load: special tokens cache size = 25
0.00.040.958 I load: token to piece cache size = 0.2984 MB
0.00.040.961 I print_info: arch             = gptneox
0.00.040.961 I print_info: vocab_only       = 0
0.00.040.961 I print_info: n_ctx_train      = 2048
0.00.040.961 I print_info: n_embd           = 2048
0.00.040.962 I print_info: n_layer          = 24
0.00.040.964 I print_info: n_head           = 16
0.00.040.965 I print_info: n_head_kv        = 16
0.00.040.965 I print_info: n_rot            = 32
0.00.040.966 I print_info: n_swa            = 0
0.00.040.966 I print_info: n_embd_head_k    = 128
0.00.040.966 I print_info: n_embd_head_v    = 128
0.00.040.967 I print_info: n_gqa            = 1
0.00.040.969 I print_info: n_embd_k_gqa     = 2048
0.00.040.970 I print_info: n_embd_v_gqa     = 2048
0.00.040.971 I print_info: f_norm_eps       = 1.0e-05
0.00.040.971 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.971 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.972 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.972 I print_info: f_logit_scale    = 0.0e+00
0.00.040.972 I print_info: n_ff             = 8192
0.00.040.973 I print_info: n_expert         = 0
0.00.040.973 I print_info: n_expert_used    = 0
0.00.040.973 I print_info: causal attn      = 1
0.00.040.973 I print_info: pooling type     = 0
0.00.040.975 I print_info: rope type        = 2
0.00.040.977 I print_info: rope scaling     = linear
0.00.040.977 I print_info: freq_base_train  = 10000.0
0.00.040.977 I print_info: freq_scale_train = 1
0.00.040.978 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.978 I print_info: rope_finetuned   = unknown
0.00.040.978 I print_info: ssm_d_conv       = 0
0.00.040.978 I print_info: ssm_d_inner      = 0
0.00.040.978 I print_info: ssm_d_state      = 0
0.00.040.978 I print_info: ssm_dt_rank      = 0
0.00.040.978 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.979 I print_info: model type       = 1.4B
0.00.040.979 I print_info: model params     = 1.41 B
0.00.040.979 I print_info: general.name     = 1.4B
0.00.040.980 I print_info: vocab type       = BPE
0.00.040.980 I print_info: n_vocab          = 50304
0.00.040.982 I print_info: n_merges         = 50009
0.00.040.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.982 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.982 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.983 I print_info: LF token         = 187 ''
0.00.040.987 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.987 I print_info: max token length = 1024
0.00.040.988 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.711.813 I load_tensors: offloading 24 repeating layers to GPU
0.00.711.825 I load_tensors: offloading output layer to GPU
0.00.711.826 I load_tensors: offloaded 25/25 layers to GPU
0.00.711.857 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.711.858 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.713.567 I llama_init_from_model: n_seq_max     = 1
0.00.713.570 I llama_init_from_model: n_ctx         = 2048
0.00.713.571 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.713.571 I llama_init_from_model: n_batch       = 2048
0.00.713.572 I llama_init_from_model: n_ubatch      = 512
0.00.713.572 I llama_init_from_model: flash_attn    = 0
0.00.713.574 I llama_init_from_model: freq_base     = 10000.0
0.00.713.575 I llama_init_from_model: freq_scale    = 1
0.00.713.577 I ggml_metal_init: allocating
0.00.713.627 I ggml_metal_init: found device: Apple M4
0.00.713.639 I ggml_metal_init: picking default device: Apple M4
0.00.715.492 I ggml_metal_init: using embedded metal library
0.00.722.293 I ggml_metal_init: GPU name:   Apple M4
0.00.722.297 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.722.298 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.722.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.722.299 I ggml_metal_init: simdgroup reduction   = true
0.00.722.300 I ggml_metal_init: simdgroup matrix mul. = true
0.00.722.300 I ggml_metal_init: has residency sets    = true
0.00.722.300 I ggml_metal_init: has bfloat            = true
0.00.722.300 I ggml_metal_init: use bfloat            = true
0.00.722.301 I ggml_metal_init: hasUnifiedMemory      = true
0.00.722.303 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.740.253 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.784.357 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.784.362 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.784.396 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.788.849 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.788.851 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.788.851 I llama_init_from_model: graph nodes  = 967
0.00.788.851 I llama_init_from_model: graph splits = 2
0.00.788.857 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.788.980 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.788.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.846.924 I main: llama threadpool init, n_threads = 4
0.00.846.968 I 
0.00.846.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.846.991 I 
0.00.847.155 I sampler seed: 1234
0.00.847.159 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.847.170 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.847.170 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.847.172 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.630.543 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.630.544 I llama_perf_context_print:        load time =     834.92 ms
0.01.630.545 I llama_perf_context_print: prompt eval time =      42.82 ms /     7 tokens (    6.12 ms per token,   163.49 tokens per second)
0.01.630.547 I llama_perf_context_print:        eval time =     737.72 ms /    63 runs   (   11.71 ms per token,    85.40 tokens per second)
0.01.630.547 I llama_perf_context_print:       total time =     784.34 ms /    70 tokens
0.01.630.799 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.109s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.756 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.756 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.756 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.757 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.757 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.759 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.760 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.760 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.762 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.763 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.763 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.763 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.765 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.766 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.610 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.757 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.614 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.616 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.616 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.617 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.617 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.618 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.618 I llama_model_loader: - type  f32:  194 tensors
0.00.026.618 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.619 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.619 I print_info: file format = GGUF V3 (latest)
0.00.026.620 I print_info: file type   = Q5_0
0.00.026.621 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.686 I load: special tokens cache size = 25
0.00.040.488 I load: token to piece cache size = 0.2984 MB
0.00.040.492 I print_info: arch             = gptneox
0.00.040.492 I print_info: vocab_only       = 0
0.00.040.492 I print_info: n_ctx_train      = 2048
0.00.040.492 I print_info: n_embd           = 2048
0.00.040.493 I print_info: n_layer          = 24
0.00.040.497 I print_info: n_head           = 16
0.00.040.497 I print_info: n_head_kv        = 16
0.00.040.497 I print_info: n_rot            = 32
0.00.040.498 I print_info: n_swa            = 0
0.00.040.498 I print_info: n_embd_head_k    = 128
0.00.040.498 I print_info: n_embd_head_v    = 128
0.00.040.499 I print_info: n_gqa            = 1
0.00.040.503 I print_info: n_embd_k_gqa     = 2048
0.00.040.503 I print_info: n_embd_v_gqa     = 2048
0.00.040.504 I print_info: f_norm_eps       = 1.0e-05
0.00.040.504 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.504 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.504 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.504 I print_info: f_logit_scale    = 0.0e+00
0.00.040.505 I print_info: n_ff             = 8192
0.00.040.505 I print_info: n_expert         = 0
0.00.040.505 I print_info: n_expert_used    = 0
0.00.040.505 I print_info: causal attn      = 1
0.00.040.506 I print_info: pooling type     = 0
0.00.040.507 I print_info: rope type        = 2
0.00.040.507 I print_info: rope scaling     = linear
0.00.040.507 I print_info: freq_base_train  = 10000.0
0.00.040.508 I print_info: freq_scale_train = 1
0.00.040.508 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.508 I print_info: rope_finetuned   = unknown
0.00.040.509 I print_info: ssm_d_conv       = 0
0.00.040.509 I print_info: ssm_d_inner      = 0
0.00.040.510 I print_info: ssm_d_state      = 0
0.00.040.510 I print_info: ssm_dt_rank      = 0
0.00.040.511 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.511 I print_info: model type       = 1.4B
0.00.040.511 I print_info: model params     = 1.41 B
0.00.040.511 I print_info: general.name     = 1.4B
0.00.040.512 I print_info: vocab type       = BPE
0.00.040.512 I print_info: n_vocab          = 50304
0.00.040.512 I print_info: n_merges         = 50009
0.00.040.512 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.512 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.513 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.513 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.517 I print_info: LF token         = 187 ''
0.00.040.518 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.519 I print_info: max token length = 1024
0.00.040.519 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.725.515 I load_tensors: offloading 24 repeating layers to GPU
0.00.725.527 I load_tensors: offloading output layer to GPU
0.00.725.527 I load_tensors: offloaded 25/25 layers to GPU
0.00.725.563 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.725.564 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.726.999 I llama_init_from_model: n_seq_max     = 1
0.00.727.004 I llama_init_from_model: n_ctx         = 128
0.00.727.005 I llama_init_from_model: n_ctx_per_seq = 128
0.00.727.005 I llama_init_from_model: n_batch       = 128
0.00.727.005 I llama_init_from_model: n_ubatch      = 128
0.00.727.006 I llama_init_from_model: flash_attn    = 0
0.00.727.007 I llama_init_from_model: freq_base     = 10000.0
0.00.727.008 I llama_init_from_model: freq_scale    = 1
0.00.727.008 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.727.010 I ggml_metal_init: allocating
0.00.727.087 I ggml_metal_init: found device: Apple M4
0.00.727.100 I ggml_metal_init: picking default device: Apple M4
0.00.729.014 I ggml_metal_init: using embedded metal library
0.00.736.208 I ggml_metal_init: GPU name:   Apple M4
0.00.736.219 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.736.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.736.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.736.221 I ggml_metal_init: simdgroup reduction   = true
0.00.736.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.736.222 I ggml_metal_init: has residency sets    = true
0.00.736.222 I ggml_metal_init: has bfloat            = true
0.00.736.222 I ggml_metal_init: use bfloat            = true
0.00.736.227 I ggml_metal_init: hasUnifiedMemory      = true
0.00.736.230 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.754.423 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.758.168 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.758.173 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.758.222 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.761.647 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.761.649 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.761.650 I llama_init_from_model: graph nodes  = 967
0.00.761.650 I llama_init_from_model: graph splits = 2
0.00.761.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.761.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.824 I 
0.00.790.910 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.917 I perplexity: tokenizing the input ..
0.00.797.938 I perplexity: tokenization took 7.018 ms
0.00.797.945 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.933.045 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.934.387 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.934.408 I llama_perf_context_print:        load time =     780.86 ms
0.00.934.409 I llama_perf_context_print: prompt eval time =     134.17 ms /   128 tokens (    1.05 ms per token,   953.99 tokens per second)
0.00.934.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.934.410 I llama_perf_context_print:       total time =     143.59 ms /   129 tokens
0.00.934.817 I ggml_metal_free: deallocating

real	0m0.950s
user	0m0.080s
sys	0m0.144s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.844 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.902 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.906 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.908 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.910 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.910 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.910 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.911 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.911 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.912 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.916 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.916 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.916 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.610 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.409 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.410 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.410 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.410 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.411 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.411 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.411 I llama_model_loader: - type  f32:  194 tensors
0.00.025.412 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.412 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.412 I print_info: file format = GGUF V3 (latest)
0.00.025.413 I print_info: file type   = Q5_1
0.00.025.413 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.179 I load: special tokens cache size = 25
0.00.039.038 I load: token to piece cache size = 0.2984 MB
0.00.039.041 I print_info: arch             = gptneox
0.00.039.041 I print_info: vocab_only       = 0
0.00.039.042 I print_info: n_ctx_train      = 2048
0.00.039.042 I print_info: n_embd           = 2048
0.00.039.042 I print_info: n_layer          = 24
0.00.039.045 I print_info: n_head           = 16
0.00.039.046 I print_info: n_head_kv        = 16
0.00.039.046 I print_info: n_rot            = 32
0.00.039.046 I print_info: n_swa            = 0
0.00.039.047 I print_info: n_embd_head_k    = 128
0.00.039.047 I print_info: n_embd_head_v    = 128
0.00.039.048 I print_info: n_gqa            = 1
0.00.039.048 I print_info: n_embd_k_gqa     = 2048
0.00.039.049 I print_info: n_embd_v_gqa     = 2048
0.00.039.050 I print_info: f_norm_eps       = 1.0e-05
0.00.039.050 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.050 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.050 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.050 I print_info: f_logit_scale    = 0.0e+00
0.00.039.053 I print_info: n_ff             = 8192
0.00.039.053 I print_info: n_expert         = 0
0.00.039.053 I print_info: n_expert_used    = 0
0.00.039.053 I print_info: causal attn      = 1
0.00.039.053 I print_info: pooling type     = 0
0.00.039.055 I print_info: rope type        = 2
0.00.039.057 I print_info: rope scaling     = linear
0.00.039.057 I print_info: freq_base_train  = 10000.0
0.00.039.057 I print_info: freq_scale_train = 1
0.00.039.058 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.058 I print_info: rope_finetuned   = unknown
0.00.039.058 I print_info: ssm_d_conv       = 0
0.00.039.058 I print_info: ssm_d_inner      = 0
0.00.039.058 I print_info: ssm_d_state      = 0
0.00.039.058 I print_info: ssm_dt_rank      = 0
0.00.039.058 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.059 I print_info: model type       = 1.4B
0.00.039.059 I print_info: model params     = 1.41 B
0.00.039.059 I print_info: general.name     = 1.4B
0.00.039.059 I print_info: vocab type       = BPE
0.00.039.060 I print_info: n_vocab          = 50304
0.00.039.060 I print_info: n_merges         = 50009
0.00.039.060 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.060 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.060 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.061 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.065 I print_info: LF token         = 187 ''
0.00.039.065 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.065 I print_info: max token length = 1024
0.00.039.065 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.039 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.055 I load_tensors: offloading output layer to GPU
0.00.613.056 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.093 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.613.095 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.614.817 I llama_init_from_model: n_seq_max     = 1
0.00.614.820 I llama_init_from_model: n_ctx         = 2048
0.00.614.820 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.614.821 I llama_init_from_model: n_batch       = 2048
0.00.614.822 I llama_init_from_model: n_ubatch      = 512
0.00.614.822 I llama_init_from_model: flash_attn    = 0
0.00.614.824 I llama_init_from_model: freq_base     = 10000.0
0.00.614.825 I llama_init_from_model: freq_scale    = 1
0.00.614.827 I ggml_metal_init: allocating
0.00.614.904 I ggml_metal_init: found device: Apple M4
0.00.614.918 I ggml_metal_init: picking default device: Apple M4
0.00.616.842 I ggml_metal_init: using embedded metal library
0.00.623.445 I ggml_metal_init: GPU name:   Apple M4
0.00.623.448 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.449 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.450 I ggml_metal_init: simdgroup reduction   = true
0.00.623.451 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.451 I ggml_metal_init: has residency sets    = true
0.00.623.451 I ggml_metal_init: has bfloat            = true
0.00.623.451 I ggml_metal_init: use bfloat            = true
0.00.623.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.461 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.769 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.508 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.695.515 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.695.551 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.361 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.700.364 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.700.364 I llama_init_from_model: graph nodes  = 967
0.00.700.364 I llama_init_from_model: graph splits = 2
0.00.700.369 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.700.494 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.700.495 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.144 I main: llama threadpool init, n_threads = 4
0.00.760.189 I 
0.00.760.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.213 I 
0.00.760.372 I sampler seed: 1234
0.00.760.377 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.397 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.397 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.397 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.611.647 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53625.38 tokens per second)
0.01.611.647 I llama_perf_context_print:        load time =     750.58 ms
0.01.611.648 I llama_perf_context_print: prompt eval time =      51.95 ms /     7 tokens (    7.42 ms per token,   134.75 tokens per second)
0.01.611.649 I llama_perf_context_print:        eval time =     796.46 ms /    63 runs   (   12.64 ms per token,    79.10 tokens per second)
0.01.611.649 I llama_perf_context_print:       total time =     852.22 ms /    70 tokens
0.01.611.866 I ggml_metal_free: deallocating

real	0m1.630s
user	0m0.108s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.910 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.056 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.057 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.058 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.059 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.063 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.063 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.825 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.545 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.546 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.547 I llama_model_loader: - type  f32:  194 tensors
0.00.024.547 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.547 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.548 I print_info: file format = GGUF V3 (latest)
0.00.024.548 I print_info: file type   = Q5_1
0.00.024.550 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.716 I load: special tokens cache size = 25
0.00.038.778 I load: token to piece cache size = 0.2984 MB
0.00.038.783 I print_info: arch             = gptneox
0.00.038.783 I print_info: vocab_only       = 0
0.00.038.783 I print_info: n_ctx_train      = 2048
0.00.038.783 I print_info: n_embd           = 2048
0.00.038.784 I print_info: n_layer          = 24
0.00.038.788 I print_info: n_head           = 16
0.00.038.789 I print_info: n_head_kv        = 16
0.00.038.789 I print_info: n_rot            = 32
0.00.038.789 I print_info: n_swa            = 0
0.00.038.789 I print_info: n_embd_head_k    = 128
0.00.038.790 I print_info: n_embd_head_v    = 128
0.00.038.790 I print_info: n_gqa            = 1
0.00.038.791 I print_info: n_embd_k_gqa     = 2048
0.00.038.792 I print_info: n_embd_v_gqa     = 2048
0.00.038.792 I print_info: f_norm_eps       = 1.0e-05
0.00.038.793 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.795 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.796 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.796 I print_info: f_logit_scale    = 0.0e+00
0.00.038.796 I print_info: n_ff             = 8192
0.00.038.796 I print_info: n_expert         = 0
0.00.038.798 I print_info: n_expert_used    = 0
0.00.038.798 I print_info: causal attn      = 1
0.00.038.798 I print_info: pooling type     = 0
0.00.038.798 I print_info: rope type        = 2
0.00.038.798 I print_info: rope scaling     = linear
0.00.038.799 I print_info: freq_base_train  = 10000.0
0.00.038.799 I print_info: freq_scale_train = 1
0.00.038.799 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.799 I print_info: rope_finetuned   = unknown
0.00.038.799 I print_info: ssm_d_conv       = 0
0.00.038.799 I print_info: ssm_d_inner      = 0
0.00.038.799 I print_info: ssm_d_state      = 0
0.00.038.800 I print_info: ssm_dt_rank      = 0
0.00.038.800 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.800 I print_info: model type       = 1.4B
0.00.038.801 I print_info: model params     = 1.41 B
0.00.038.801 I print_info: general.name     = 1.4B
0.00.038.801 I print_info: vocab type       = BPE
0.00.038.801 I print_info: n_vocab          = 50304
0.00.038.801 I print_info: n_merges         = 50009
0.00.038.802 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.803 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.803 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.803 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.804 I print_info: LF token         = 187 ''
0.00.038.804 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.804 I print_info: max token length = 1024
0.00.038.804 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.445 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.462 I load_tensors: offloading output layer to GPU
0.00.608.463 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.496 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.608.498 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.610.136 I llama_init_from_model: n_seq_max     = 1
0.00.610.139 I llama_init_from_model: n_ctx         = 128
0.00.610.139 I llama_init_from_model: n_ctx_per_seq = 128
0.00.610.140 I llama_init_from_model: n_batch       = 128
0.00.610.140 I llama_init_from_model: n_ubatch      = 128
0.00.610.141 I llama_init_from_model: flash_attn    = 0
0.00.610.143 I llama_init_from_model: freq_base     = 10000.0
0.00.610.144 I llama_init_from_model: freq_scale    = 1
0.00.610.144 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.146 I ggml_metal_init: allocating
0.00.610.236 I ggml_metal_init: found device: Apple M4
0.00.610.250 I ggml_metal_init: picking default device: Apple M4
0.00.612.084 I ggml_metal_init: using embedded metal library
0.00.618.714 I ggml_metal_init: GPU name:   Apple M4
0.00.618.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.722 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.722 I ggml_metal_init: simdgroup reduction   = true
0.00.618.722 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.723 I ggml_metal_init: has residency sets    = true
0.00.618.723 I ggml_metal_init: has bfloat            = true
0.00.618.723 I ggml_metal_init: use bfloat            = true
0.00.618.724 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.927 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.552 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.640.556 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.597 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.644.007 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.644.009 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.644.009 I llama_init_from_model: graph nodes  = 967
0.00.644.010 I llama_init_from_model: graph splits = 2
0.00.644.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.644.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.096 I 
0.00.673.186 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.193 I perplexity: tokenizing the input ..
0.00.680.818 I perplexity: tokenization took 7.621 ms
0.00.680.832 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.101 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.827.429 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.827.452 I llama_perf_context_print:        load time =     664.18 ms
0.00.827.456 I llama_perf_context_print: prompt eval time =     144.40 ms /   128 tokens (    1.13 ms per token,   886.46 tokens per second)
0.00.827.457 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.458 I llama_perf_context_print:       total time =     154.36 ms /   129 tokens
0.00.827.799 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.080s
sys	0m0.139s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.838 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.473 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.478 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.480 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.483 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.483 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.484 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.484 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.484 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.485 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.487 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.207 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.378 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.129 I llama_model_loader: - type  f32:  194 tensors
0.00.025.129 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.129 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.130 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.130 I print_info: file format = GGUF V3 (latest)
0.00.025.131 I print_info: file type   = Q2_K - Medium
0.00.025.131 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.277 I load: special tokens cache size = 25
0.00.039.328 I load: token to piece cache size = 0.2984 MB
0.00.039.331 I print_info: arch             = gptneox
0.00.039.332 I print_info: vocab_only       = 0
0.00.039.332 I print_info: n_ctx_train      = 2048
0.00.039.332 I print_info: n_embd           = 2048
0.00.039.332 I print_info: n_layer          = 24
0.00.039.335 I print_info: n_head           = 16
0.00.039.336 I print_info: n_head_kv        = 16
0.00.039.336 I print_info: n_rot            = 32
0.00.039.336 I print_info: n_swa            = 0
0.00.039.337 I print_info: n_embd_head_k    = 128
0.00.039.337 I print_info: n_embd_head_v    = 128
0.00.039.338 I print_info: n_gqa            = 1
0.00.039.338 I print_info: n_embd_k_gqa     = 2048
0.00.039.339 I print_info: n_embd_v_gqa     = 2048
0.00.039.340 I print_info: f_norm_eps       = 1.0e-05
0.00.039.340 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.340 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.343 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.343 I print_info: f_logit_scale    = 0.0e+00
0.00.039.343 I print_info: n_ff             = 8192
0.00.039.344 I print_info: n_expert         = 0
0.00.039.344 I print_info: n_expert_used    = 0
0.00.039.344 I print_info: causal attn      = 1
0.00.039.350 I print_info: pooling type     = 0
0.00.039.351 I print_info: rope type        = 2
0.00.039.352 I print_info: rope scaling     = linear
0.00.039.352 I print_info: freq_base_train  = 10000.0
0.00.039.353 I print_info: freq_scale_train = 1
0.00.039.353 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.353 I print_info: rope_finetuned   = unknown
0.00.039.353 I print_info: ssm_d_conv       = 0
0.00.039.354 I print_info: ssm_d_inner      = 0
0.00.039.354 I print_info: ssm_d_state      = 0
0.00.039.354 I print_info: ssm_dt_rank      = 0
0.00.039.354 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.354 I print_info: model type       = 1.4B
0.00.039.355 I print_info: model params     = 1.41 B
0.00.039.356 I print_info: general.name     = 1.4B
0.00.039.356 I print_info: vocab type       = BPE
0.00.039.356 I print_info: n_vocab          = 50304
0.00.039.356 I print_info: n_merges         = 50009
0.00.039.357 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.357 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.357 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.357 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.357 I print_info: LF token         = 187 ''
0.00.039.357 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.358 I print_info: max token length = 1024
0.00.039.358 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.351.018 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.036 I load_tensors: offloading output layer to GPU
0.00.351.037 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.071 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.072 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.352.695 I llama_init_from_model: n_seq_max     = 1
0.00.352.699 I llama_init_from_model: n_ctx         = 2048
0.00.352.700 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.352.700 I llama_init_from_model: n_batch       = 2048
0.00.352.701 I llama_init_from_model: n_ubatch      = 512
0.00.352.701 I llama_init_from_model: flash_attn    = 0
0.00.352.703 I llama_init_from_model: freq_base     = 10000.0
0.00.352.703 I llama_init_from_model: freq_scale    = 1
0.00.352.705 I ggml_metal_init: allocating
0.00.352.789 I ggml_metal_init: found device: Apple M4
0.00.352.803 I ggml_metal_init: picking default device: Apple M4
0.00.354.647 I ggml_metal_init: using embedded metal library
0.00.360.130 I ggml_metal_init: GPU name:   Apple M4
0.00.360.140 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.360.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.360.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.360.142 I ggml_metal_init: simdgroup reduction   = true
0.00.360.143 I ggml_metal_init: simdgroup matrix mul. = true
0.00.360.143 I ggml_metal_init: has residency sets    = true
0.00.360.143 I ggml_metal_init: has bfloat            = true
0.00.360.144 I ggml_metal_init: use bfloat            = true
0.00.360.146 I ggml_metal_init: hasUnifiedMemory      = true
0.00.360.150 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.381.971 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.446.198 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.446.207 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.446.243 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.450.503 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.450.504 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.450.505 I llama_init_from_model: graph nodes  = 967
0.00.450.505 I llama_init_from_model: graph splits = 2
0.00.450.510 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.450.627 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.450.628 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.512.113 I main: llama threadpool init, n_threads = 4
0.00.512.157 I 
0.00.512.180 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.512.181 I 
0.00.512.350 I sampler seed: 1234
0.00.512.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.512.375 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.512.375 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.512.375 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.188.848 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.188.848 I llama_perf_context_print:        load time =     501.55 ms
0.01.188.849 I llama_perf_context_print: prompt eval time =      39.24 ms /     7 tokens (    5.61 ms per token,   178.39 tokens per second)
0.01.188.850 I llama_perf_context_print:        eval time =     634.43 ms /    63 runs   (   10.07 ms per token,    99.30 tokens per second)
0.01.188.850 I llama_perf_context_print:       total time =     677.45 ms /    70 tokens
0.01.189.087 I ggml_metal_free: deallocating

real	0m1.207s
user	0m0.112s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.795 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.804 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.804 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.805 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.805 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.805 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.806 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.806 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.807 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.807 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.807 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.808 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.808 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.810 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.810 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.811 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.712 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.499 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.500 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.501 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.501 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.502 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.502 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.502 I llama_model_loader: - type  f32:  194 tensors
0.00.025.503 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.503 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.503 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.504 I print_info: file format = GGUF V3 (latest)
0.00.025.505 I print_info: file type   = Q2_K - Medium
0.00.025.506 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.097 I load: special tokens cache size = 25
0.00.040.155 I load: token to piece cache size = 0.2984 MB
0.00.040.159 I print_info: arch             = gptneox
0.00.040.159 I print_info: vocab_only       = 0
0.00.040.160 I print_info: n_ctx_train      = 2048
0.00.040.160 I print_info: n_embd           = 2048
0.00.040.160 I print_info: n_layer          = 24
0.00.040.164 I print_info: n_head           = 16
0.00.040.165 I print_info: n_head_kv        = 16
0.00.040.165 I print_info: n_rot            = 32
0.00.040.165 I print_info: n_swa            = 0
0.00.040.166 I print_info: n_embd_head_k    = 128
0.00.040.166 I print_info: n_embd_head_v    = 128
0.00.040.166 I print_info: n_gqa            = 1
0.00.040.167 I print_info: n_embd_k_gqa     = 2048
0.00.040.168 I print_info: n_embd_v_gqa     = 2048
0.00.040.168 I print_info: f_norm_eps       = 1.0e-05
0.00.040.169 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.169 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.169 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.169 I print_info: f_logit_scale    = 0.0e+00
0.00.040.170 I print_info: n_ff             = 8192
0.00.040.170 I print_info: n_expert         = 0
0.00.040.170 I print_info: n_expert_used    = 0
0.00.040.170 I print_info: causal attn      = 1
0.00.040.171 I print_info: pooling type     = 0
0.00.040.171 I print_info: rope type        = 2
0.00.040.171 I print_info: rope scaling     = linear
0.00.040.171 I print_info: freq_base_train  = 10000.0
0.00.040.172 I print_info: freq_scale_train = 1
0.00.040.172 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.172 I print_info: rope_finetuned   = unknown
0.00.040.172 I print_info: ssm_d_conv       = 0
0.00.040.172 I print_info: ssm_d_inner      = 0
0.00.040.172 I print_info: ssm_d_state      = 0
0.00.040.175 I print_info: ssm_dt_rank      = 0
0.00.040.176 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.176 I print_info: model type       = 1.4B
0.00.040.176 I print_info: model params     = 1.41 B
0.00.040.176 I print_info: general.name     = 1.4B
0.00.040.177 I print_info: vocab type       = BPE
0.00.040.177 I print_info: n_vocab          = 50304
0.00.040.177 I print_info: n_merges         = 50009
0.00.040.177 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: LF token         = 187 ''
0.00.040.180 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.180 I print_info: max token length = 1024
0.00.040.180 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.350.580 I load_tensors: offloading 24 repeating layers to GPU
0.00.350.590 I load_tensors: offloading output layer to GPU
0.00.350.590 I load_tensors: offloaded 25/25 layers to GPU
0.00.350.621 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.350.627 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.352.247 I llama_init_from_model: n_seq_max     = 1
0.00.352.253 I llama_init_from_model: n_ctx         = 128
0.00.352.254 I llama_init_from_model: n_ctx_per_seq = 128
0.00.352.255 I llama_init_from_model: n_batch       = 128
0.00.352.255 I llama_init_from_model: n_ubatch      = 128
0.00.352.255 I llama_init_from_model: flash_attn    = 0
0.00.352.257 I llama_init_from_model: freq_base     = 10000.0
0.00.352.257 I llama_init_from_model: freq_scale    = 1
0.00.352.258 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.352.260 I ggml_metal_init: allocating
0.00.352.333 I ggml_metal_init: found device: Apple M4
0.00.352.347 I ggml_metal_init: picking default device: Apple M4
0.00.354.172 I ggml_metal_init: using embedded metal library
0.00.359.566 I ggml_metal_init: GPU name:   Apple M4
0.00.359.578 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.359.579 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.359.579 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.359.580 I ggml_metal_init: simdgroup reduction   = true
0.00.359.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.359.580 I ggml_metal_init: has residency sets    = true
0.00.359.581 I ggml_metal_init: has bfloat            = true
0.00.359.581 I ggml_metal_init: use bfloat            = true
0.00.359.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.359.587 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.381.721 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.385.373 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.385.381 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.385.428 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.388.755 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.388.757 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.388.758 I llama_init_from_model: graph nodes  = 967
0.00.388.759 I llama_init_from_model: graph splits = 2
0.00.388.761 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.388.762 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.415.022 I 
0.00.415.107 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.415.116 I perplexity: tokenizing the input ..
0.00.421.099 I perplexity: tokenization took 5.98 ms
0.00.421.105 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.552.457 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.553.784 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.553.805 I llama_perf_context_print:        load time =     405.08 ms
0.00.553.806 I llama_perf_context_print: prompt eval time =     130.78 ms /   128 tokens (    1.02 ms per token,   978.71 tokens per second)
0.00.553.806 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.553.807 I llama_perf_context_print:       total time =     138.79 ms /   129 tokens
0.00.554.180 I ggml_metal_free: deallocating

real	0m0.570s
user	0m0.081s
sys	0m0.098s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.120 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.609 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.620 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.622 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.622 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.624 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.624 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.625 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.625 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.626 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.627 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.628 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.054 I llama_model_loader: - type  f32:  194 tensors
0.00.025.054 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.054 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.054 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.054 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.055 I print_info: file format = GGUF V3 (latest)
0.00.025.055 I print_info: file type   = Q3_K - Medium
0.00.025.056 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.784 I load: special tokens cache size = 25
0.00.038.782 I load: token to piece cache size = 0.2984 MB
0.00.038.784 I print_info: arch             = gptneox
0.00.038.785 I print_info: vocab_only       = 0
0.00.038.785 I print_info: n_ctx_train      = 2048
0.00.038.785 I print_info: n_embd           = 2048
0.00.038.785 I print_info: n_layer          = 24
0.00.038.788 I print_info: n_head           = 16
0.00.038.788 I print_info: n_head_kv        = 16
0.00.038.789 I print_info: n_rot            = 32
0.00.038.789 I print_info: n_swa            = 0
0.00.038.789 I print_info: n_embd_head_k    = 128
0.00.038.789 I print_info: n_embd_head_v    = 128
0.00.038.790 I print_info: n_gqa            = 1
0.00.038.791 I print_info: n_embd_k_gqa     = 2048
0.00.038.791 I print_info: n_embd_v_gqa     = 2048
0.00.038.792 I print_info: f_norm_eps       = 1.0e-05
0.00.038.792 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.793 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.793 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.793 I print_info: f_logit_scale    = 0.0e+00
0.00.038.794 I print_info: n_ff             = 8192
0.00.038.794 I print_info: n_expert         = 0
0.00.038.794 I print_info: n_expert_used    = 0
0.00.038.794 I print_info: causal attn      = 1
0.00.038.794 I print_info: pooling type     = 0
0.00.038.794 I print_info: rope type        = 2
0.00.038.795 I print_info: rope scaling     = linear
0.00.038.795 I print_info: freq_base_train  = 10000.0
0.00.038.795 I print_info: freq_scale_train = 1
0.00.038.796 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.796 I print_info: rope_finetuned   = unknown
0.00.038.796 I print_info: ssm_d_conv       = 0
0.00.038.796 I print_info: ssm_d_inner      = 0
0.00.038.796 I print_info: ssm_d_state      = 0
0.00.038.798 I print_info: ssm_dt_rank      = 0
0.00.038.798 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.799 I print_info: model type       = 1.4B
0.00.038.799 I print_info: model params     = 1.41 B
0.00.038.799 I print_info: general.name     = 1.4B
0.00.038.799 I print_info: vocab type       = BPE
0.00.038.800 I print_info: n_vocab          = 50304
0.00.038.800 I print_info: n_merges         = 50009
0.00.038.800 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.800 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.801 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.801 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.801 I print_info: LF token         = 187 ''
0.00.038.801 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.802 I print_info: max token length = 1024
0.00.038.802 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.451.293 I load_tensors: offloading 24 repeating layers to GPU
0.00.451.311 I load_tensors: offloading output layer to GPU
0.00.451.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.451.348 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.451.349 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.452.489 I llama_init_from_model: n_seq_max     = 1
0.00.452.492 I llama_init_from_model: n_ctx         = 2048
0.00.452.493 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.452.494 I llama_init_from_model: n_batch       = 2048
0.00.452.494 I llama_init_from_model: n_ubatch      = 512
0.00.452.494 I llama_init_from_model: flash_attn    = 0
0.00.452.497 I llama_init_from_model: freq_base     = 10000.0
0.00.452.498 I llama_init_from_model: freq_scale    = 1
0.00.452.500 I ggml_metal_init: allocating
0.00.452.583 I ggml_metal_init: found device: Apple M4
0.00.452.596 I ggml_metal_init: picking default device: Apple M4
0.00.454.524 I ggml_metal_init: using embedded metal library
0.00.460.261 I ggml_metal_init: GPU name:   Apple M4
0.00.460.272 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.460.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.460.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.460.274 I ggml_metal_init: simdgroup reduction   = true
0.00.460.274 I ggml_metal_init: simdgroup matrix mul. = true
0.00.460.274 I ggml_metal_init: has residency sets    = true
0.00.460.275 I ggml_metal_init: has bfloat            = true
0.00.460.275 I ggml_metal_init: use bfloat            = true
0.00.460.280 I ggml_metal_init: hasUnifiedMemory      = true
0.00.460.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.484.245 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.034 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.543.039 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.543.074 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.547.806 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.547.808 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.547.808 I llama_init_from_model: graph nodes  = 967
0.00.547.809 I llama_init_from_model: graph splits = 2
0.00.547.814 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.547.944 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.547.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.605.100 I main: llama threadpool init, n_threads = 4
0.00.605.142 I 
0.00.605.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.605.166 I 
0.00.605.332 I sampler seed: 1234
0.00.605.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.605.347 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.605.347 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.605.348 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.345.012 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.345.012 I llama_perf_context_print:        load time =     595.26 ms
0.01.345.013 I llama_perf_context_print: prompt eval time =      46.46 ms /     7 tokens (    6.64 ms per token,   150.67 tokens per second)
0.01.345.015 I llama_perf_context_print:        eval time =     690.40 ms /    63 runs   (   10.96 ms per token,    91.25 tokens per second)
0.01.345.015 I llama_perf_context_print:       total time =     740.63 ms /    70 tokens
0.01.345.215 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.112s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.988 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.999 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.001 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.003 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.003 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.004 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.005 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.005 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.005 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.006 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.006 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.006 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.007 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.009 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.009 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.009 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.686 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.469 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.470 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.470 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.471 I llama_model_loader: - type  f32:  194 tensors
0.00.024.471 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.471 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.471 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.472 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.472 I print_info: file format = GGUF V3 (latest)
0.00.024.473 I print_info: file type   = Q3_K - Medium
0.00.024.474 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.443 I load: special tokens cache size = 25
0.00.038.489 I load: token to piece cache size = 0.2984 MB
0.00.038.493 I print_info: arch             = gptneox
0.00.038.494 I print_info: vocab_only       = 0
0.00.038.494 I print_info: n_ctx_train      = 2048
0.00.038.494 I print_info: n_embd           = 2048
0.00.038.494 I print_info: n_layer          = 24
0.00.038.499 I print_info: n_head           = 16
0.00.038.500 I print_info: n_head_kv        = 16
0.00.038.500 I print_info: n_rot            = 32
0.00.038.500 I print_info: n_swa            = 0
0.00.038.500 I print_info: n_embd_head_k    = 128
0.00.038.500 I print_info: n_embd_head_v    = 128
0.00.038.502 I print_info: n_gqa            = 1
0.00.038.503 I print_info: n_embd_k_gqa     = 2048
0.00.038.504 I print_info: n_embd_v_gqa     = 2048
0.00.038.504 I print_info: f_norm_eps       = 1.0e-05
0.00.038.505 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.505 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.505 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.505 I print_info: f_logit_scale    = 0.0e+00
0.00.038.507 I print_info: n_ff             = 8192
0.00.038.507 I print_info: n_expert         = 0
0.00.038.508 I print_info: n_expert_used    = 0
0.00.038.508 I print_info: causal attn      = 1
0.00.038.508 I print_info: pooling type     = 0
0.00.038.508 I print_info: rope type        = 2
0.00.038.508 I print_info: rope scaling     = linear
0.00.038.509 I print_info: freq_base_train  = 10000.0
0.00.038.509 I print_info: freq_scale_train = 1
0.00.038.509 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.509 I print_info: rope_finetuned   = unknown
0.00.038.510 I print_info: ssm_d_conv       = 0
0.00.038.510 I print_info: ssm_d_inner      = 0
0.00.038.510 I print_info: ssm_d_state      = 0
0.00.038.510 I print_info: ssm_dt_rank      = 0
0.00.038.510 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.511 I print_info: model type       = 1.4B
0.00.038.511 I print_info: model params     = 1.41 B
0.00.038.511 I print_info: general.name     = 1.4B
0.00.038.512 I print_info: vocab type       = BPE
0.00.038.512 I print_info: n_vocab          = 50304
0.00.038.512 I print_info: n_merges         = 50009
0.00.038.514 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.514 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.514 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.514 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: LF token         = 187 ''
0.00.038.515 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.515 I print_info: max token length = 1024
0.00.038.516 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.453.628 I load_tensors: offloading 24 repeating layers to GPU
0.00.453.638 I load_tensors: offloading output layer to GPU
0.00.453.639 I load_tensors: offloaded 25/25 layers to GPU
0.00.453.673 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.453.674 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.455.342 I llama_init_from_model: n_seq_max     = 1
0.00.455.346 I llama_init_from_model: n_ctx         = 128
0.00.455.347 I llama_init_from_model: n_ctx_per_seq = 128
0.00.455.347 I llama_init_from_model: n_batch       = 128
0.00.455.347 I llama_init_from_model: n_ubatch      = 128
0.00.455.348 I llama_init_from_model: flash_attn    = 0
0.00.455.349 I llama_init_from_model: freq_base     = 10000.0
0.00.455.349 I llama_init_from_model: freq_scale    = 1
0.00.455.350 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.455.353 I ggml_metal_init: allocating
0.00.455.410 I ggml_metal_init: found device: Apple M4
0.00.455.426 I ggml_metal_init: picking default device: Apple M4
0.00.457.392 I ggml_metal_init: using embedded metal library
0.00.463.292 I ggml_metal_init: GPU name:   Apple M4
0.00.463.302 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.463.303 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.463.304 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.463.304 I ggml_metal_init: simdgroup reduction   = true
0.00.463.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.463.305 I ggml_metal_init: has residency sets    = true
0.00.463.305 I ggml_metal_init: has bfloat            = true
0.00.463.306 I ggml_metal_init: use bfloat            = true
0.00.463.308 I ggml_metal_init: hasUnifiedMemory      = true
0.00.463.311 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.484.370 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.488.223 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.488.231 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.488.304 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.491.781 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.491.783 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.491.783 I llama_init_from_model: graph nodes  = 967
0.00.491.784 I llama_init_from_model: graph splits = 2
0.00.491.787 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.491.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.522.173 I 
0.00.522.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.522.254 I perplexity: tokenizing the input ..
0.00.529.304 I perplexity: tokenization took 7.048 ms
0.00.529.310 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.671.085 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.672.656 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.672.683 I llama_perf_context_print:        load time =     513.24 ms
0.00.672.684 I llama_perf_context_print: prompt eval time =     141.18 ms /   128 tokens (    1.10 ms per token,   906.66 tokens per second)
0.00.672.685 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.672.685 I llama_perf_context_print:       total time =     150.52 ms /   129 tokens
0.00.673.063 I ggml_metal_free: deallocating

real	0m0.687s
user	0m0.079s
sys	0m0.113s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.212 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.646 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.653 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.654 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.654 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.654 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.655 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.656 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.656 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.656 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.657 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.657 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.658 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.662 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.662 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.382 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.431 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.170 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.171 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.172 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.172 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.173 I llama_model_loader: - type  f32:  194 tensors
0.00.026.173 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.173 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.174 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.174 I print_info: file format = GGUF V3 (latest)
0.00.026.175 I print_info: file type   = Q4_K - Medium
0.00.026.175 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.138 I load: special tokens cache size = 25
0.00.040.160 I load: token to piece cache size = 0.2984 MB
0.00.040.163 I print_info: arch             = gptneox
0.00.040.164 I print_info: vocab_only       = 0
0.00.040.164 I print_info: n_ctx_train      = 2048
0.00.040.164 I print_info: n_embd           = 2048
0.00.040.164 I print_info: n_layer          = 24
0.00.040.167 I print_info: n_head           = 16
0.00.040.168 I print_info: n_head_kv        = 16
0.00.040.168 I print_info: n_rot            = 32
0.00.040.168 I print_info: n_swa            = 0
0.00.040.168 I print_info: n_embd_head_k    = 128
0.00.040.168 I print_info: n_embd_head_v    = 128
0.00.040.169 I print_info: n_gqa            = 1
0.00.040.170 I print_info: n_embd_k_gqa     = 2048
0.00.040.171 I print_info: n_embd_v_gqa     = 2048
0.00.040.171 I print_info: f_norm_eps       = 1.0e-05
0.00.040.172 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.172 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.172 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.172 I print_info: f_logit_scale    = 0.0e+00
0.00.040.173 I print_info: n_ff             = 8192
0.00.040.173 I print_info: n_expert         = 0
0.00.040.173 I print_info: n_expert_used    = 0
0.00.040.173 I print_info: causal attn      = 1
0.00.040.174 I print_info: pooling type     = 0
0.00.040.176 I print_info: rope type        = 2
0.00.040.176 I print_info: rope scaling     = linear
0.00.040.177 I print_info: freq_base_train  = 10000.0
0.00.040.177 I print_info: freq_scale_train = 1
0.00.040.177 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.177 I print_info: rope_finetuned   = unknown
0.00.040.177 I print_info: ssm_d_conv       = 0
0.00.040.178 I print_info: ssm_d_inner      = 0
0.00.040.179 I print_info: ssm_d_state      = 0
0.00.040.179 I print_info: ssm_dt_rank      = 0
0.00.040.179 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.179 I print_info: model type       = 1.4B
0.00.040.180 I print_info: model params     = 1.41 B
0.00.040.180 I print_info: general.name     = 1.4B
0.00.040.181 I print_info: vocab type       = BPE
0.00.040.181 I print_info: n_vocab          = 50304
0.00.040.181 I print_info: n_merges         = 50009
0.00.040.181 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.181 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.181 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.182 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.186 I print_info: LF token         = 187 ''
0.00.040.186 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.186 I print_info: max token length = 1024
0.00.040.187 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.548.950 I load_tensors: offloading 24 repeating layers to GPU
0.00.548.966 I load_tensors: offloading output layer to GPU
0.00.548.967 I load_tensors: offloaded 25/25 layers to GPU
0.00.549.002 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.549.004 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.550.684 I llama_init_from_model: n_seq_max     = 1
0.00.550.687 I llama_init_from_model: n_ctx         = 2048
0.00.550.687 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.550.688 I llama_init_from_model: n_batch       = 2048
0.00.550.688 I llama_init_from_model: n_ubatch      = 512
0.00.550.689 I llama_init_from_model: flash_attn    = 0
0.00.550.690 I llama_init_from_model: freq_base     = 10000.0
0.00.550.690 I llama_init_from_model: freq_scale    = 1
0.00.550.691 I ggml_metal_init: allocating
0.00.550.701 I ggml_metal_init: found device: Apple M4
0.00.550.709 I ggml_metal_init: picking default device: Apple M4
0.00.552.282 I ggml_metal_init: using embedded metal library
0.00.559.467 I ggml_metal_init: GPU name:   Apple M4
0.00.559.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.559.473 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.559.474 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.559.475 I ggml_metal_init: simdgroup reduction   = true
0.00.559.475 I ggml_metal_init: simdgroup matrix mul. = true
0.00.559.475 I ggml_metal_init: has residency sets    = true
0.00.559.476 I ggml_metal_init: has bfloat            = true
0.00.559.476 I ggml_metal_init: use bfloat            = true
0.00.559.477 I ggml_metal_init: hasUnifiedMemory      = true
0.00.559.479 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.578.176 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.638.922 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.638.929 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.638.963 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.643.408 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.643.410 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.643.410 I llama_init_from_model: graph nodes  = 967
0.00.643.411 I llama_init_from_model: graph splits = 2
0.00.643.416 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.643.559 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.643.560 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.535 I main: llama threadpool init, n_threads = 4
0.00.698.578 I 
0.00.698.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.601 I 
0.00.698.754 I sampler seed: 1234
0.00.698.758 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.698.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.698.769 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.698.770 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.448.875 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50176.68 tokens per second)
0.01.448.875 I llama_perf_context_print:        load time =     687.58 ms
0.01.448.877 I llama_perf_context_print: prompt eval time =      47.19 ms /     7 tokens (    6.74 ms per token,   148.33 tokens per second)
0.01.448.878 I llama_perf_context_print:        eval time =     700.01 ms /    63 runs   (   11.11 ms per token,    90.00 tokens per second)
0.01.448.880 I llama_perf_context_print:       total time =     751.08 ms /    70 tokens
0.01.449.106 I ggml_metal_free: deallocating

real	0m1.467s
user	0m0.111s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.783 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.759 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.765 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.766 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.770 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.577 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.515 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.516 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.517 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.518 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.518 I llama_model_loader: - type  f32:  194 tensors
0.00.024.519 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.519 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.519 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.520 I print_info: file format = GGUF V3 (latest)
0.00.024.520 I print_info: file type   = Q4_K - Medium
0.00.024.522 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.567 I load: special tokens cache size = 25
0.00.038.358 I load: token to piece cache size = 0.2984 MB
0.00.038.363 I print_info: arch             = gptneox
0.00.038.363 I print_info: vocab_only       = 0
0.00.038.363 I print_info: n_ctx_train      = 2048
0.00.038.363 I print_info: n_embd           = 2048
0.00.038.364 I print_info: n_layer          = 24
0.00.038.368 I print_info: n_head           = 16
0.00.038.369 I print_info: n_head_kv        = 16
0.00.038.369 I print_info: n_rot            = 32
0.00.038.369 I print_info: n_swa            = 0
0.00.038.369 I print_info: n_embd_head_k    = 128
0.00.038.370 I print_info: n_embd_head_v    = 128
0.00.038.370 I print_info: n_gqa            = 1
0.00.038.371 I print_info: n_embd_k_gqa     = 2048
0.00.038.372 I print_info: n_embd_v_gqa     = 2048
0.00.038.372 I print_info: f_norm_eps       = 1.0e-05
0.00.038.373 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.373 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.373 I print_info: f_logit_scale    = 0.0e+00
0.00.038.374 I print_info: n_ff             = 8192
0.00.038.374 I print_info: n_expert         = 0
0.00.038.374 I print_info: n_expert_used    = 0
0.00.038.374 I print_info: causal attn      = 1
0.00.038.374 I print_info: pooling type     = 0
0.00.038.375 I print_info: rope type        = 2
0.00.038.376 I print_info: rope scaling     = linear
0.00.038.376 I print_info: freq_base_train  = 10000.0
0.00.038.376 I print_info: freq_scale_train = 1
0.00.038.376 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.377 I print_info: rope_finetuned   = unknown
0.00.038.377 I print_info: ssm_d_conv       = 0
0.00.038.378 I print_info: ssm_d_inner      = 0
0.00.038.378 I print_info: ssm_d_state      = 0
0.00.038.379 I print_info: ssm_dt_rank      = 0
0.00.038.379 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.379 I print_info: model type       = 1.4B
0.00.038.379 I print_info: model params     = 1.41 B
0.00.038.379 I print_info: general.name     = 1.4B
0.00.038.380 I print_info: vocab type       = BPE
0.00.038.380 I print_info: n_vocab          = 50304
0.00.038.380 I print_info: n_merges         = 50009
0.00.038.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.381 I print_info: LF token         = 187 ''
0.00.038.381 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.382 I print_info: max token length = 1024
0.00.038.382 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.521.114 I load_tensors: offloading 24 repeating layers to GPU
0.00.521.132 I load_tensors: offloading output layer to GPU
0.00.521.133 I load_tensors: offloaded 25/25 layers to GPU
0.00.521.168 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.521.170 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.522.864 I llama_init_from_model: n_seq_max     = 1
0.00.522.866 I llama_init_from_model: n_ctx         = 128
0.00.522.867 I llama_init_from_model: n_ctx_per_seq = 128
0.00.522.867 I llama_init_from_model: n_batch       = 128
0.00.522.868 I llama_init_from_model: n_ubatch      = 128
0.00.522.868 I llama_init_from_model: flash_attn    = 0
0.00.522.870 I llama_init_from_model: freq_base     = 10000.0
0.00.522.871 I llama_init_from_model: freq_scale    = 1
0.00.522.871 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.522.876 I ggml_metal_init: allocating
0.00.522.958 I ggml_metal_init: found device: Apple M4
0.00.522.973 I ggml_metal_init: picking default device: Apple M4
0.00.524.783 I ggml_metal_init: using embedded metal library
0.00.531.563 I ggml_metal_init: GPU name:   Apple M4
0.00.531.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.572 I ggml_metal_init: simdgroup reduction   = true
0.00.531.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.573 I ggml_metal_init: has residency sets    = true
0.00.531.573 I ggml_metal_init: has bfloat            = true
0.00.531.573 I ggml_metal_init: use bfloat            = true
0.00.531.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.585 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.482 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.552.915 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.552.921 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.552.963 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.556.043 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.556.045 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.556.046 I llama_init_from_model: graph nodes  = 967
0.00.556.046 I llama_init_from_model: graph splits = 2
0.00.556.049 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.556.049 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.384 I 
0.00.585.456 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.464 I perplexity: tokenizing the input ..
0.00.593.143 I perplexity: tokenization took 7.676 ms
0.00.593.158 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.735.663 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.737.002 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.737.027 I llama_perf_context_print:        load time =     576.59 ms
0.00.737.028 I llama_perf_context_print: prompt eval time =     141.55 ms /   128 tokens (    1.11 ms per token,   904.26 tokens per second)
0.00.737.029 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.029 I llama_perf_context_print:       total time =     151.65 ms /   129 tokens
0.00.737.411 I ggml_metal_free: deallocating

real	0m0.752s
user	0m0.080s
sys	0m0.120s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.250 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.594 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.595 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.597 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.597 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.598 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.598 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.599 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.599 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.601 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.601 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.199 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.200 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.200 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.200 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.201 I llama_model_loader: - type  f32:  194 tensors
0.00.027.201 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.201 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.202 I print_info: file format = GGUF V3 (latest)
0.00.027.202 I print_info: file type   = Q5_K - Medium
0.00.027.203 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.034 I load: special tokens cache size = 25
0.00.040.848 I load: token to piece cache size = 0.2984 MB
0.00.040.851 I print_info: arch             = gptneox
0.00.040.851 I print_info: vocab_only       = 0
0.00.040.851 I print_info: n_ctx_train      = 2048
0.00.040.851 I print_info: n_embd           = 2048
0.00.040.852 I print_info: n_layer          = 24
0.00.040.855 I print_info: n_head           = 16
0.00.040.855 I print_info: n_head_kv        = 16
0.00.040.855 I print_info: n_rot            = 32
0.00.040.856 I print_info: n_swa            = 0
0.00.040.856 I print_info: n_embd_head_k    = 128
0.00.040.856 I print_info: n_embd_head_v    = 128
0.00.040.857 I print_info: n_gqa            = 1
0.00.040.857 I print_info: n_embd_k_gqa     = 2048
0.00.040.858 I print_info: n_embd_v_gqa     = 2048
0.00.040.859 I print_info: f_norm_eps       = 1.0e-05
0.00.040.859 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.859 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.859 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.860 I print_info: f_logit_scale    = 0.0e+00
0.00.040.860 I print_info: n_ff             = 8192
0.00.040.860 I print_info: n_expert         = 0
0.00.040.860 I print_info: n_expert_used    = 0
0.00.040.861 I print_info: causal attn      = 1
0.00.040.861 I print_info: pooling type     = 0
0.00.040.861 I print_info: rope type        = 2
0.00.040.861 I print_info: rope scaling     = linear
0.00.040.862 I print_info: freq_base_train  = 10000.0
0.00.040.862 I print_info: freq_scale_train = 1
0.00.040.862 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.862 I print_info: rope_finetuned   = unknown
0.00.040.862 I print_info: ssm_d_conv       = 0
0.00.040.862 I print_info: ssm_d_inner      = 0
0.00.040.863 I print_info: ssm_d_state      = 0
0.00.040.863 I print_info: ssm_dt_rank      = 0
0.00.040.865 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.865 I print_info: model type       = 1.4B
0.00.040.866 I print_info: model params     = 1.41 B
0.00.040.866 I print_info: general.name     = 1.4B
0.00.040.866 I print_info: vocab type       = BPE
0.00.040.867 I print_info: n_vocab          = 50304
0.00.040.867 I print_info: n_merges         = 50009
0.00.040.867 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.867 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.867 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.868 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.868 I print_info: LF token         = 187 ''
0.00.040.868 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.868 I print_info: max token length = 1024
0.00.040.869 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.153 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.170 I load_tensors: offloading output layer to GPU
0.00.588.171 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.204 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.588.205 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.589.644 I llama_init_from_model: n_seq_max     = 1
0.00.589.646 I llama_init_from_model: n_ctx         = 2048
0.00.589.646 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.589.647 I llama_init_from_model: n_batch       = 2048
0.00.589.647 I llama_init_from_model: n_ubatch      = 512
0.00.589.648 I llama_init_from_model: flash_attn    = 0
0.00.589.649 I llama_init_from_model: freq_base     = 10000.0
0.00.589.649 I llama_init_from_model: freq_scale    = 1
0.00.589.651 I ggml_metal_init: allocating
0.00.589.669 I ggml_metal_init: found device: Apple M4
0.00.589.679 I ggml_metal_init: picking default device: Apple M4
0.00.591.220 I ggml_metal_init: using embedded metal library
0.00.597.497 I ggml_metal_init: GPU name:   Apple M4
0.00.597.500 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.501 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.502 I ggml_metal_init: simdgroup reduction   = true
0.00.597.502 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.503 I ggml_metal_init: has residency sets    = true
0.00.597.503 I ggml_metal_init: has bfloat            = true
0.00.597.503 I ggml_metal_init: use bfloat            = true
0.00.597.504 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.706 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.487 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.673.493 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.673.525 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.392 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.677.394 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.677.394 I llama_init_from_model: graph nodes  = 967
0.00.677.394 I llama_init_from_model: graph splits = 2
0.00.677.399 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.677.513 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.677.514 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.097 I main: llama threadpool init, n_threads = 4
0.00.738.150 I 
0.00.738.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.174 I 
0.00.738.326 I sampler seed: 1234
0.00.738.330 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.375 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.379 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.379 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.588.750 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.588.751 I llama_perf_context_print:        load time =     726.13 ms
0.01.588.752 I llama_perf_context_print: prompt eval time =      52.93 ms /     7 tokens (    7.56 ms per token,   132.25 tokens per second)
0.01.588.753 I llama_perf_context_print:        eval time =     794.62 ms /    63 runs   (   12.61 ms per token,    79.28 tokens per second)
0.01.588.753 I llama_perf_context_print:       total time =     851.37 ms /    70 tokens
0.01.589.027 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.108s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.612 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.767 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.774 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.776 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.777 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.777 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.778 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.779 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.779 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.779 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.780 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.780 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.782 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.782 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.783 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.542 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.537 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.539 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.539 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.540 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.540 I llama_model_loader: - type  f32:  194 tensors
0.00.025.541 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.541 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.542 I print_info: file format = GGUF V3 (latest)
0.00.025.542 I print_info: file type   = Q5_K - Medium
0.00.025.543 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.906 I load: special tokens cache size = 25
0.00.040.039 I load: token to piece cache size = 0.2984 MB
0.00.040.044 I print_info: arch             = gptneox
0.00.040.044 I print_info: vocab_only       = 0
0.00.040.044 I print_info: n_ctx_train      = 2048
0.00.040.044 I print_info: n_embd           = 2048
0.00.040.045 I print_info: n_layer          = 24
0.00.040.050 I print_info: n_head           = 16
0.00.040.051 I print_info: n_head_kv        = 16
0.00.040.051 I print_info: n_rot            = 32
0.00.040.051 I print_info: n_swa            = 0
0.00.040.054 I print_info: n_embd_head_k    = 128
0.00.040.054 I print_info: n_embd_head_v    = 128
0.00.040.055 I print_info: n_gqa            = 1
0.00.040.056 I print_info: n_embd_k_gqa     = 2048
0.00.040.056 I print_info: n_embd_v_gqa     = 2048
0.00.040.057 I print_info: f_norm_eps       = 1.0e-05
0.00.040.057 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.057 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.057 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.061 I print_info: f_logit_scale    = 0.0e+00
0.00.040.063 I print_info: n_ff             = 8192
0.00.040.063 I print_info: n_expert         = 0
0.00.040.063 I print_info: n_expert_used    = 0
0.00.040.063 I print_info: causal attn      = 1
0.00.040.064 I print_info: pooling type     = 0
0.00.040.065 I print_info: rope type        = 2
0.00.040.065 I print_info: rope scaling     = linear
0.00.040.065 I print_info: freq_base_train  = 10000.0
0.00.040.066 I print_info: freq_scale_train = 1
0.00.040.066 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.066 I print_info: rope_finetuned   = unknown
0.00.040.066 I print_info: ssm_d_conv       = 0
0.00.040.066 I print_info: ssm_d_inner      = 0
0.00.040.067 I print_info: ssm_d_state      = 0
0.00.040.067 I print_info: ssm_dt_rank      = 0
0.00.040.067 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.067 I print_info: model type       = 1.4B
0.00.040.067 I print_info: model params     = 1.41 B
0.00.040.070 I print_info: general.name     = 1.4B
0.00.040.070 I print_info: vocab type       = BPE
0.00.040.071 I print_info: n_vocab          = 50304
0.00.040.071 I print_info: n_merges         = 50009
0.00.040.071 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.071 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.071 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.071 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.072 I print_info: LF token         = 187 ''
0.00.040.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.072 I print_info: max token length = 1024
0.00.040.073 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.584.348 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.364 I load_tensors: offloading output layer to GPU
0.00.584.365 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.398 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.584.400 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.586.222 I llama_init_from_model: n_seq_max     = 1
0.00.586.224 I llama_init_from_model: n_ctx         = 128
0.00.586.225 I llama_init_from_model: n_ctx_per_seq = 128
0.00.586.226 I llama_init_from_model: n_batch       = 128
0.00.586.226 I llama_init_from_model: n_ubatch      = 128
0.00.586.226 I llama_init_from_model: flash_attn    = 0
0.00.586.228 I llama_init_from_model: freq_base     = 10000.0
0.00.586.229 I llama_init_from_model: freq_scale    = 1
0.00.586.230 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.232 I ggml_metal_init: allocating
0.00.586.307 I ggml_metal_init: found device: Apple M4
0.00.586.320 I ggml_metal_init: picking default device: Apple M4
0.00.587.907 I ggml_metal_init: using embedded metal library
0.00.594.402 I ggml_metal_init: GPU name:   Apple M4
0.00.594.407 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.407 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.408 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.409 I ggml_metal_init: simdgroup reduction   = true
0.00.594.409 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.409 I ggml_metal_init: has residency sets    = true
0.00.594.409 I ggml_metal_init: has bfloat            = true
0.00.594.410 I ggml_metal_init: use bfloat            = true
0.00.594.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.414 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.584 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.971 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.614.982 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.039 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.618.352 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.618.353 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.618.354 I llama_init_from_model: graph nodes  = 967
0.00.618.354 I llama_init_from_model: graph splits = 2
0.00.618.357 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.618.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.412 I 
0.00.652.499 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.507 I perplexity: tokenizing the input ..
0.00.658.538 I perplexity: tokenization took 6.029 ms
0.00.658.543 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.820 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.164 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.191 I llama_perf_context_print:        load time =     642.79 ms
0.00.796.192 I llama_perf_context_print: prompt eval time =     136.04 ms /   128 tokens (    1.06 ms per token,   940.89 tokens per second)
0.00.796.193 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.193 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.00.796.591 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.078s
sys	0m0.132s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.858 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.753 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.754 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.756 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.756 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.761 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.761 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.761 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.601 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.303 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.304 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.305 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.305 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.305 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.306 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.306 I llama_model_loader: - type  f32:  194 tensors
0.00.025.307 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.307 I print_info: file format = GGUF V3 (latest)
0.00.025.308 I print_info: file type   = Q6_K
0.00.025.312 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.516 I load: special tokens cache size = 25
0.00.039.556 I load: token to piece cache size = 0.2984 MB
0.00.039.559 I print_info: arch             = gptneox
0.00.039.559 I print_info: vocab_only       = 0
0.00.039.560 I print_info: n_ctx_train      = 2048
0.00.039.560 I print_info: n_embd           = 2048
0.00.039.560 I print_info: n_layer          = 24
0.00.039.563 I print_info: n_head           = 16
0.00.039.564 I print_info: n_head_kv        = 16
0.00.039.564 I print_info: n_rot            = 32
0.00.039.564 I print_info: n_swa            = 0
0.00.039.564 I print_info: n_embd_head_k    = 128
0.00.039.564 I print_info: n_embd_head_v    = 128
0.00.039.565 I print_info: n_gqa            = 1
0.00.039.568 I print_info: n_embd_k_gqa     = 2048
0.00.039.569 I print_info: n_embd_v_gqa     = 2048
0.00.039.569 I print_info: f_norm_eps       = 1.0e-05
0.00.039.570 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.570 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.570 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.570 I print_info: f_logit_scale    = 0.0e+00
0.00.039.571 I print_info: n_ff             = 8192
0.00.039.571 I print_info: n_expert         = 0
0.00.039.571 I print_info: n_expert_used    = 0
0.00.039.571 I print_info: causal attn      = 1
0.00.039.572 I print_info: pooling type     = 0
0.00.039.572 I print_info: rope type        = 2
0.00.039.572 I print_info: rope scaling     = linear
0.00.039.572 I print_info: freq_base_train  = 10000.0
0.00.039.573 I print_info: freq_scale_train = 1
0.00.039.573 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.573 I print_info: rope_finetuned   = unknown
0.00.039.579 I print_info: ssm_d_conv       = 0
0.00.039.581 I print_info: ssm_d_inner      = 0
0.00.039.581 I print_info: ssm_d_state      = 0
0.00.039.581 I print_info: ssm_dt_rank      = 0
0.00.039.581 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.582 I print_info: model type       = 1.4B
0.00.039.582 I print_info: model params     = 1.41 B
0.00.039.582 I print_info: general.name     = 1.4B
0.00.039.583 I print_info: vocab type       = BPE
0.00.039.583 I print_info: n_vocab          = 50304
0.00.039.583 I print_info: n_merges         = 50009
0.00.039.583 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.583 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: LF token         = 187 ''
0.00.039.584 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: max token length = 1024
0.00.039.585 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.556 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.562 I load_tensors: offloading output layer to GPU
0.00.637.563 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.587 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.637.590 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.638.884 I llama_init_from_model: n_seq_max     = 1
0.00.638.886 I llama_init_from_model: n_ctx         = 2048
0.00.638.886 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.638.887 I llama_init_from_model: n_batch       = 2048
0.00.638.887 I llama_init_from_model: n_ubatch      = 512
0.00.638.887 I llama_init_from_model: flash_attn    = 0
0.00.638.888 I llama_init_from_model: freq_base     = 10000.0
0.00.638.889 I llama_init_from_model: freq_scale    = 1
0.00.638.890 I ggml_metal_init: allocating
0.00.638.901 I ggml_metal_init: found device: Apple M4
0.00.638.909 I ggml_metal_init: picking default device: Apple M4
0.00.640.378 I ggml_metal_init: using embedded metal library
0.00.646.574 I ggml_metal_init: GPU name:   Apple M4
0.00.646.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.578 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.578 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.579 I ggml_metal_init: simdgroup reduction   = true
0.00.646.579 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.580 I ggml_metal_init: has residency sets    = true
0.00.646.580 I ggml_metal_init: has bfloat            = true
0.00.646.580 I ggml_metal_init: use bfloat            = true
0.00.646.581 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.584 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.375 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.558 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.715.567 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.715.602 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.640 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.719.642 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.719.642 I llama_init_from_model: graph nodes  = 967
0.00.719.642 I llama_init_from_model: graph splits = 2
0.00.719.647 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.719.772 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.772 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.916 I main: llama threadpool init, n_threads = 4
0.00.788.962 I 
0.00.788.985 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.988 I 
0.00.789.140 I sampler seed: 1234
0.00.789.145 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.165 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.166 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.166 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.675.419 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.675.421 I llama_perf_context_print:        load time =     779.34 ms
0.01.675.422 I llama_perf_context_print: prompt eval time =      57.54 ms /     7 tokens (    8.22 ms per token,   121.65 tokens per second)
0.01.675.422 I llama_perf_context_print:        eval time =     825.69 ms /    63 runs   (   13.11 ms per token,    76.30 tokens per second)
0.01.675.423 I llama_perf_context_print:       total time =     887.22 ms /    70 tokens
0.01.675.668 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.108s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4779 (d7cfe1ff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.815 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.716 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.721 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.723 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.724 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.724 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.724 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.725 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.726 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.726 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.727 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.727 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.727 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.728 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.728 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.730 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.730 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.731 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.387 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.392 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.394 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.395 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.396 I llama_model_loader: - type  f32:  194 tensors
0.00.024.396 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.397 I print_info: file format = GGUF V3 (latest)
0.00.024.398 I print_info: file type   = Q6_K
0.00.024.399 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.706 I load: special tokens cache size = 25
0.00.038.796 I load: token to piece cache size = 0.2984 MB
0.00.038.801 I print_info: arch             = gptneox
0.00.038.801 I print_info: vocab_only       = 0
0.00.038.801 I print_info: n_ctx_train      = 2048
0.00.038.802 I print_info: n_embd           = 2048
0.00.038.802 I print_info: n_layer          = 24
0.00.038.806 I print_info: n_head           = 16
0.00.038.806 I print_info: n_head_kv        = 16
0.00.038.807 I print_info: n_rot            = 32
0.00.038.807 I print_info: n_swa            = 0
0.00.038.807 I print_info: n_embd_head_k    = 128
0.00.038.807 I print_info: n_embd_head_v    = 128
0.00.038.808 I print_info: n_gqa            = 1
0.00.038.808 I print_info: n_embd_k_gqa     = 2048
0.00.038.809 I print_info: n_embd_v_gqa     = 2048
0.00.038.811 I print_info: f_norm_eps       = 1.0e-05
0.00.038.811 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.812 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.812 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.812 I print_info: f_logit_scale    = 0.0e+00
0.00.038.812 I print_info: n_ff             = 8192
0.00.038.813 I print_info: n_expert         = 0
0.00.038.813 I print_info: n_expert_used    = 0
0.00.038.813 I print_info: causal attn      = 1
0.00.038.813 I print_info: pooling type     = 0
0.00.038.813 I print_info: rope type        = 2
0.00.038.813 I print_info: rope scaling     = linear
0.00.038.814 I print_info: freq_base_train  = 10000.0
0.00.038.814 I print_info: freq_scale_train = 1
0.00.038.814 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.814 I print_info: rope_finetuned   = unknown
0.00.038.815 I print_info: ssm_d_conv       = 0
0.00.038.815 I print_info: ssm_d_inner      = 0
0.00.038.815 I print_info: ssm_d_state      = 0
0.00.038.815 I print_info: ssm_dt_rank      = 0
0.00.038.815 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.817 I print_info: model type       = 1.4B
0.00.038.817 I print_info: model params     = 1.41 B
0.00.038.817 I print_info: general.name     = 1.4B
0.00.038.818 I print_info: vocab type       = BPE
0.00.038.818 I print_info: n_vocab          = 50304
0.00.038.818 I print_info: n_merges         = 50009
0.00.038.818 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.818 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.818 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.819 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.819 I print_info: LF token         = 187 ''
0.00.038.819 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.819 I print_info: max token length = 1024
0.00.038.820 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.620.521 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.537 I load_tensors: offloading output layer to GPU
0.00.620.538 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.594 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.620.596 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.621.731 I llama_init_from_model: n_seq_max     = 1
0.00.621.735 I llama_init_from_model: n_ctx         = 128
0.00.621.736 I llama_init_from_model: n_ctx_per_seq = 128
0.00.621.737 I llama_init_from_model: n_batch       = 128
0.00.621.737 I llama_init_from_model: n_ubatch      = 128
0.00.621.737 I llama_init_from_model: flash_attn    = 0
0.00.621.738 I llama_init_from_model: freq_base     = 10000.0
0.00.621.739 I llama_init_from_model: freq_scale    = 1
0.00.621.739 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.621.742 I ggml_metal_init: allocating
0.00.621.825 I ggml_metal_init: found device: Apple M4
0.00.621.840 I ggml_metal_init: picking default device: Apple M4
0.00.623.489 I ggml_metal_init: using embedded metal library
0.00.630.009 I ggml_metal_init: GPU name:   Apple M4
0.00.630.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.015 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.015 I ggml_metal_init: simdgroup reduction   = true
0.00.630.015 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.016 I ggml_metal_init: has residency sets    = true
0.00.630.016 I ggml_metal_init: has bfloat            = true
0.00.630.016 I ggml_metal_init: use bfloat            = true
0.00.630.017 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.019 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.069 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.650.553 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.650.558 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.650.605 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.653.905 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.653.907 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.653.907 I llama_init_from_model: graph nodes  = 967
0.00.653.908 I llama_init_from_model: graph splits = 2
0.00.653.910 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.653.910 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.070 I 
0.00.691.154 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.182 I perplexity: tokenizing the input ..
0.00.697.576 I perplexity: tokenization took 6.393 ms
0.00.697.581 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.283 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.829.551 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.829.578 I llama_perf_context_print:        load time =     682.25 ms
0.00.829.578 I llama_perf_context_print: prompt eval time =     130.45 ms /   128 tokens (    1.02 ms per token,   981.20 tokens per second)
0.00.829.579 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.579 I llama_perf_context_print:       total time =     138.51 ms /   129 tokens
0.00.829.999 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.078s
sys	0m0.144s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4779 (d7cfe1ff)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b404c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b4052a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b405710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b405b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b406150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b406700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b406cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b407260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b407810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b407d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b408210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b408710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b409230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b4099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b40a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b40a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b40b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b40b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b40be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b40c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b40cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b40d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b40dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b40e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b40eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b40ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b40f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b4100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b4105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b4108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b410d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b411000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b411890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b411dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b412090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b412530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b4129d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b412e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b413310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b4137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b413c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b4140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b414590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b414a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b414cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b415300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b415910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b416230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b416840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b416e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b417460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b417a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b418080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b418690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b418e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b419320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b4197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b419a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b41a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b41a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b41ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b41afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b41b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b41b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b41bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b41c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b41c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b41cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b41d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b41d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b41d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b41de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b41e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b41e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b41ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b41f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b41f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b41fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b4202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b4207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b420d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b421290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b4217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b421d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b422280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b4227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b422d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b423270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b4237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b423d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b424260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b4247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b424d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b425250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b4257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b425cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b426240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b415f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b4266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b426e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b4273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b427900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b427e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b4283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b4288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b428e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b429390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b4298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b429e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b42a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b42a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b42ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b42b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b42b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b42bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b42c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b42c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b42ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b42cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b42d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b42d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b42dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b42e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b42e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b42eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b42ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b42f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b42f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b42fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b430210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b4306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b430b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b430ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b431490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b431930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b431dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b432270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b432710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b432bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b433050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b4334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b433990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b433e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b4342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b434770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b434c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b4350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b435550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b4359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b435e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b436330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b4367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b436c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b437110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b4375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b437a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b437ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b438390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b438830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b438cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b439170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b439610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b439ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b439f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b43a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b43a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b43ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b43b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b43b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b43bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b43bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b43c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b43c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b43cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b43d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b43d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b43db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b43e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b43e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b43e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b43edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b43f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b43f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b43fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b440070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b440510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b4409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b440e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b4412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b441790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b441c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b4420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b442570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b442ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b443010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b443560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b443ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b443d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b444380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b444990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b444fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b445790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b445c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b445ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b446500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b446b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b447300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b4477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b447c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b4480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b448890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b448de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b449330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b449880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b449dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b44a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b44a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b44adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b44b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b44b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b44bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b44c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b44c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b44cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b44d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b44d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b44dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b44e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b44e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b44ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b44f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b44f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b44fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b4502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b450810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b450d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b4512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b451800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b451d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b4522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b4527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b452d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b453290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b4537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b453d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b454280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b4547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b454d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b455270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b4557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b455d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b456260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b4567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b456d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b457250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b4577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b457cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b458240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b458790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b458ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b459230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b459780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b459cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b45a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b45a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b45acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b45b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b45b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b45bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b45bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b45c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b45c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b45cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b45d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b45d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b45dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b45e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b45e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b45e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b45ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b45f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b45f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15b45fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15b4600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15b460550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15b4609f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15b460e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15b461330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15b4617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15b461c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15b462110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15b4625b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b462b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b463220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b463940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b464060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b464780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b464a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b465230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b4654f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b465b00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.749.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b444640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b4461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b4657b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b444030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b444c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b417d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b417720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b419d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b40f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b415bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b4164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b416b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b414fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b417110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b40e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b446f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b418950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b41a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b426970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b464d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b4112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b411580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b445260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b40f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b40f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b40fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b465f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b466220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b4664e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b4667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b466a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b466d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b466fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b4672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b467560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b467820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b467ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b467da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b468060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b468320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b4685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b4688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b468b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b468e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b4690e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b4693a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b469660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b469920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b469be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b469ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b46a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b46a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b46a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b46a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b46ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b46af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b46b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b46b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b46b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b46ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b46bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b46bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b46c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b46c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b46c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b46caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b46cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b46d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b46d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b46d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b46d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b46db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b46dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b46e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b46e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b46e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b46e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b46eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b46ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b46f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b46f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b46f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b46f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b46fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b46fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b4701a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b470460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b470720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b4709e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b470ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b470f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b471220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b4714e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b4717a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b471a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b471d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b471fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b4722a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b472560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b472820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b472ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b472da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b473060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b473320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b4735e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b4738a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b473b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b473e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b4740e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b4743a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b474660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b474920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b474be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b474ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b475160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b475420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b4756e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b4759a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b475c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b475f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b4761e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b4764a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b476760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b476a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b476ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b476fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b477260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b477520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b4777e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b477aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b477d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b478020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b4782e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b4785a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b478860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b478b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b478de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b4790a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b479360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b479620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b4798e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b479ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b479e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b47a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b47a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b47a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b47a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b47ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b47aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b47b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b47b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b47b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b47b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b47bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b47bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b47c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b47c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b47c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b47ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b47cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b47cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b47d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b47d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b47d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b47dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b47dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b47e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b47e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b47e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b47e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b47eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b47ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b47f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b47f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b47f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b47f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b47fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b47fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b480160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b480420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b4806e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b4809a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b480c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b480f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b4811e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b4814a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b481760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b481a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b481ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b481fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b482260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b482520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b4827e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b482aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b482d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b483020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b4832e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b4835a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b483860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b483b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b483de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b4840a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b484360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b484620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b4848e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b484ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b484e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b485120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b4853e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b4856a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b485c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b485f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b4861f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b4864b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b486770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b486a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b486cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b486fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b487270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b487530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b4877f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b487ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b487d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b488030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b4882f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b4885b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b488870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b488b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b488df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b4890b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b489370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b489630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b4898f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b489bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b489e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b48a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b48a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b48a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b48a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b48ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b48aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b48b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b48b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b48b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b48b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b48bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b48bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b48c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b48c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b48ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b48cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b48d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b48da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b48df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b48e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b48ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b48ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b48f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b48fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b48ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b4904b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b490a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b490f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b4914a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b4919f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b491f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b492490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b492750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b492a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b492cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b493140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b4935b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b493a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b493e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b494300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b494770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b494be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b495050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b4954c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b495930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b495da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b496210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15b496680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15b496af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15b496f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15b4973d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15b497840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15b497cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15b498120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15b498590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15b498a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15b498e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b4992e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b499d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b49a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b49ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b49b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b49b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b49b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b49bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b49c5e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b308cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b306ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b3092e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b309750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b309bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b30a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b30a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b30acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b30b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b30b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b30bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b30c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b30cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b30d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b30dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b30e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b30eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b30f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b30f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b3100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b3107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b310ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b311610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b311d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b312450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b312710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b312d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b313330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b313940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b314130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b3145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b314890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b315120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b315660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b315920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b315dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b316260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b316700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b316ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b317040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b3174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b317980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b317e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b3182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b318580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b318b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b3191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b3197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b319dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b31a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b31a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b31aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b31b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b31bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b31c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b31c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b31cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b31d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b31d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b31de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b31e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b31e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b31ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b31f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b31f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b31f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b31fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b320300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b3207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b320c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b3210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b321580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b321a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b321f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b3224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b322a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b322f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b3234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b323a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b323f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b3244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b3249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b324f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b325490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b3259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b325f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b326480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b3269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b326f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b327470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b3279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b327f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b328460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b3289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b328f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b329450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b3299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b329ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b32a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b32a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b32aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b32b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b32b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b32bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b32c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b32c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b32cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b32d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b32d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b32deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b32e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b32e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b32eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b32f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b32f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b32fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b330120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b3305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b330a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b330f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b3313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b331840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b331ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b332180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b332620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b332ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b332f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b333400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b3338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b333d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b3341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b334680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b334b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b334fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b335460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b335900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b335da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b336240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b3366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b336b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b337020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b3374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b337960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b337e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b3382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b338740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b338be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b339080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b339520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b3399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b339e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b33a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b33a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b33ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b33b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b33b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b33ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b33bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b33c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b33c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b33cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b33d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b33d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b33da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b33df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b33e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b33e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b33ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b33f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b33f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b33fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b33ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b340420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b3408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b340d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b341200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b3416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b341b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b341fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b342480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b342920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b342dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b343260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b343700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b343ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b344040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b3444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b344980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b344e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b3452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b345760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b345c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b3460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b3465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b346b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b347090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b3475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b3478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b347eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b3484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b348ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b3492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b349760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b349a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b34a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b34a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b34ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b34b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b34b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b34bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b34c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b34c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b34ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b34d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b34d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b34de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b34e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b34e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b34ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b34f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b34f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b34fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b350380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b3508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b350e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b351370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b3518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b351e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b352360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b3528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b352e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b353350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b3538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b353df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b354340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b354890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b354de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b355330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b355880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b355dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b356320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b356870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b356dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b357310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b357860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b357db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b358300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b358850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b358da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b3592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b359840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b359d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b35a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b35a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b35ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b35b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b35b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b35bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b35c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b35c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b35cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b35d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b35d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b35dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b35e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b35e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b35ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b35f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b35f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b35fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b35ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b360460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b360900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b360da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b361240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b3616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b361b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b362020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b3624c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b362960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b362e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b3632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15b363740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15b363be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15b364080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15b364520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15b3649c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15b364e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15b365300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15b3657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15b365c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15b3660e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b366630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b366d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b367470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b367b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b3682b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b368570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b368d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b369020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b369630 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.794s
user	0m0.279s
sys	0m0.319s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4779 (d7cfe1ff)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a70f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a70f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a70fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a710470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a710a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a710fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a711580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a711b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a7120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a7125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a712ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a713b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a7142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a714ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a7151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a715900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a716020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a716f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a717630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a717d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a718470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a718d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a719430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a7196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a719d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a71a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a71aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a71b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a71b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a71b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a71c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a71c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a71c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a71ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a71d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a71d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a71dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a71e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a71e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a71e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a71ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a71f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a71f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a71fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a7201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a720b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a721110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a721720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a721d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a722340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a722950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a722f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a723750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a723bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a724090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a724350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a724960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a725150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a7258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a725d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a7261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a726690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a726b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a726fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a727470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a727910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a727db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a728250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a7286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a728b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a7290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a729630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a729b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a72a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a72a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a72ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a72b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a72b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a72bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a72c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a72c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a72cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a72d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a72d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a72db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a72e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a72e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a72eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a72f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a72f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a72fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a730070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a7305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a730b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a7207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a730f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a731730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a731c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a7321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a732720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a732c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a7331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a733710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a733c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a7341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a734700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a734c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a7351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a7356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a735c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a7360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a736580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a736a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a736ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a737360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a737800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a737ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a738140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a7385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a738a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a738f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a7393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a739860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a739d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a73a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a73a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a73aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a73af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a73b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a73b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a73bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a73c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a73c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a73cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a73cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a73d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a73d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a73ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a73e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a73e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a73eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a73f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a73f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a73f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a73fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a7402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a740760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a740c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a7410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a741540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a7419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a741e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a742320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a7427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a742c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a743100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a7435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a743a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a743ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a744380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a744820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a744cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a745160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a745600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a745aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a745f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a7463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a746880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a746d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a7471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a747660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a747b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a747fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a748440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a7488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a748d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a749220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a7496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a749b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a74a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a74a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a74a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a74ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a74b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a74b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a74bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a74c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a74c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a74c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a74ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a74d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a74d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a74de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a74e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a74e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a74ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a74f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a74f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a750060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a750500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a7507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a750dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a7513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a751bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a752070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a752510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a7529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a753160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a7536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a753c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a754150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a7546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a754bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a755140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a755690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a755be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a756130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a756680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a756bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a757120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a757670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a757bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a758110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a758660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a758bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a759100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a759650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a759ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a75a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a75a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a75ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a75b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a75b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a75bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a75c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a75c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a75cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a75d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a75d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a75db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a75e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a75e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a75eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a75f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a75f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a75fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a760090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a7605e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a760b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a761080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a7615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a761b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a762070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a7625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a762b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a763060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a7635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a763b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a764050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a7645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a764af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a765040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a765590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a765ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a765f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a766420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a7668c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a766d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a767200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a7676a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a767b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a767fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a768480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a768920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a768dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a769260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a769700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a769ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a76a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14a76a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14a76a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14a76ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14a76b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14a76b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14a76bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14a76c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14a76c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14a76c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14a76ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a76d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a76daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a76e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a76e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a76f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a76f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a76fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a76fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a7703d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.742 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f604ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f604f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f6053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f605830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f605ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f606110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f606580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f6069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f606e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f6073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f607850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f607ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f6089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f6091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f6099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f60a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f60a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f60af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f60b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f60be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f60c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f60cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f60d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f60da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f60e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f60e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f60e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f60eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f60f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f60f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f60f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f60fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f610280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f610540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f6109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f610e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f611290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f611700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f611b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f611fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f612450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f6128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f612d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f6131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f613610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f613a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f613ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f614360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f6147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f614c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f6150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f615520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f615990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f615e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f616270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f6166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f616c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f617150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f6175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f617a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f617ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f618310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f618780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f6194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f619940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f619db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f61a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f61a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f61ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f61af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f61b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f61b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14f61bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14f61c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14f61c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14f61ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14f61ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14f61d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14f61d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14f61dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14f61e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14f61e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14f61e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14f61ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14f61f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14f61f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14f61fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14f61ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14f6203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14f620830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14f620ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f621110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f621580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f6219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f621e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14f6222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f622740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14f622bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14f623020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14f623490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14f623900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14f623d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14f6241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f624650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f624ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14f624f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14f6253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14f625810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14f625c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14f6260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f626560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f6269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f626e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f6272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f627720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f627b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f628000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f628470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f6288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f628d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f6291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f629630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f629aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f629f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f62a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f62a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f62ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f62b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f62b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f62b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f62be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f62c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f62c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f62cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f62cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f62d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f62d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f62dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f62e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f62e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f62ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f62eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f62f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f62f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f62fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f6300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f630520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f630990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f630e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f631270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f6316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f631fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f632430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f6328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f632d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f633180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f6335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f633a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f633ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f634340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f6347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f634c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f635090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f635f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f636240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f6366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f636b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f636f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f637400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f637870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f637ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f638150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f6385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f638a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f638ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f639310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f639780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f639bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f63a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f63a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f63a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f63adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f63b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f63b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f63bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14f63bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14f63c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14f63c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14f63ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14f63d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14f63d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14f63da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14f63de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14f63e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14f63e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14f63ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14f63f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14f63f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14f63fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14f63ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14f640390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14f640800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14f640c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14f6410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14f641600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14f641b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14f642680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14f642940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14f642f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14f6434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f643a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14f644040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14f644600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14f644bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14f645180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14f645740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14f645d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14f6462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14f646880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f646e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14f647400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14f6479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14f647f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14f648540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14f648b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14f6490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14f649680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14f649c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f64a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14f64a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14f64ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14f64b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14f64b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f64bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14f64c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f64ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14f64d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14f64d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14f64db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14f64e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14f64e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f64ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f64f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f64f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f64fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f6503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14f650980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14f650f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14f651500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f651ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14f652080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14f652640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14f652c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14f6531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14f653780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14f653d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14f654300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14f6548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14f654e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14f655440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14f655a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14f655fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14f656580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14f656b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14f657040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14f657540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14f657a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14f657f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14f658440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14f658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14f658e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14f659340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14f659840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14f659d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14f65a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14f65a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14f65ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14f65b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14f65b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14f65bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14f65c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14f65c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14f65ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14f65cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14f65d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14f65d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14f65de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14f65e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14f65e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14f65f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14f65f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14f660090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14f6607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14f660a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14f661260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f661520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f661b30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c005e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c006270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c0066e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c006b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c006fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c007430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c0078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c007d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c008180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c0085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c008a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c009100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c009c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c00a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c00abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c00b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c00ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c00c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c00c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c00d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c00d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c00de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c00e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c00ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c00f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c00f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c00f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c00fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c010230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c0106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c010b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c0114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c011770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c011be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c012050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c0124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c012930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c012da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c013210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c013680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c013af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c013f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c0143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c014840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c014cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c015120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c015590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c015a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c015e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c0162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c016750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c016bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c017030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c0174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c017910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c017e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c018380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c0187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c018c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c0190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c019540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c0199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c019e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c01a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c01a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c01ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c01afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c01b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c01b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c01bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c01c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c01c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c01ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c01cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c01d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c01d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c01dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c01e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c01e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c01e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c01ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c01f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c01f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c01fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c01ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c020430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c0208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c020d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c021180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c0215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c021a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c021ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c022340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c0227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c022c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c023090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c023500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c023d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c024280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c024830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c024de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c025390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c025940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c025ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c0264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c026a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c027000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c0275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c027b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c028110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c0286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c028c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c029220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c029720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c029c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c02a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c02a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c02ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c02b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c02b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c02ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c02bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c02c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c02c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c02ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c02d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c02d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c02dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c02e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c02e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c02ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c02f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c02f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c02fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c030020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c030520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c030a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c030f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c031420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c031920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c031e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c032320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c032820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c032d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c033220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c033720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c033c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c034120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c034620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c034b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c035020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c035520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c035a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c035f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c036420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c036920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c036e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c037320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c037820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c037d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c038220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c038720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c038c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c039120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c039620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c039b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c03a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c03a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c03aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c03af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c03b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c03b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c03be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c03c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c03c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c03cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c03d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c03d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c03dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c03e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c03e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c03eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c03f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c03f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c03fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c03ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c040420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c040920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c040e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c041320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c041820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c041d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c042220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c0427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c042d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c043330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c0438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c043ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c044500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c045300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c0457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c045a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c046070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c046680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c046e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c047310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c0477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c047c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c048400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c048950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c048ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c0493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c049940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c049e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c04a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c04a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c04ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c04b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c04b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c04be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c04c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c04c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c04ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c04d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c04d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c04de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c04e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c04e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c04ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c04f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c04f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c04fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c050380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c0508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c050e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c051370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c0518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c051e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c052360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c0528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c052e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c053350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c0538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c053df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c054340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c054890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c054de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c055330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c055880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c055dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c056320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c056870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c056dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c057310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c057860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c057db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c058300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c058850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c058da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c0592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c059840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c059d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c05a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c05a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c05ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c05b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c05b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c05bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c05c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c05c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c05c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c05cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c05d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c05d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c05dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c05e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c05e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c05e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c05ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c05f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14c05f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14c05fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14c0600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14c060560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14c060a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14c060ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14c061340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14c0617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14c061c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14c062120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c062670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c062d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c0634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c063bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c0642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c0645b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c064da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c065060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c065670 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.952s
user	0m0.230s
sys	0m0.186s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.38 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.81 sec*proc (2 tests)

Total Test time (real) =   1.82 sec
        1.85 real         0.51 user         0.22 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.09 sys
```
