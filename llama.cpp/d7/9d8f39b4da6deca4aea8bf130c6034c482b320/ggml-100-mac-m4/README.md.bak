### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.51 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.61 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.83 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.23 sec*proc (28 tests)

Total Test time (real) = 221.24 sec

real	3m41.339s
user	7m41.288s
sys	0m6.246s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.32 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   27.95 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.27 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.11 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  49.83 sec*proc (28 tests)

Total Test time (real) =  49.84 sec

real	0m49.849s
user	1m12.112s
sys	0m5.021s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.115 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.444 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.626 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.633 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.636 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.636 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.637 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.638 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.639 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.640 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.641 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.641 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.642 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.643 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.646 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.647 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.647 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.648 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.648 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.653 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.654 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.752 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.030 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.032 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.033 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.033 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.034 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.034 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.034 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.035 I llama_model_loader: - type  f32:  124 tensors
0.00.032.035 I llama_model_loader: - type  f16:   73 tensors
0.00.036.599 I llm_load_vocab: special tokens cache size = 5
0.00.038.776 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.038.804 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.038.805 I llm_load_print_meta: arch             = bert
0.00.038.805 I llm_load_print_meta: vocab type       = WPM
0.00.038.806 I llm_load_print_meta: n_vocab          = 30522
0.00.038.806 I llm_load_print_meta: n_merges         = 0
0.00.038.806 I llm_load_print_meta: vocab_only       = 0
0.00.038.807 I llm_load_print_meta: n_ctx_train      = 512
0.00.038.807 I llm_load_print_meta: n_embd           = 384
0.00.038.807 I llm_load_print_meta: n_layer          = 12
0.00.038.810 I llm_load_print_meta: n_head           = 12
0.00.038.811 I llm_load_print_meta: n_head_kv        = 12
0.00.038.812 I llm_load_print_meta: n_rot            = 32
0.00.038.812 I llm_load_print_meta: n_swa            = 0
0.00.038.812 I llm_load_print_meta: n_embd_head_k    = 32
0.00.038.812 I llm_load_print_meta: n_embd_head_v    = 32
0.00.038.815 I llm_load_print_meta: n_gqa            = 1
0.00.038.816 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.038.817 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.038.818 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.038.825 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.038.825 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.038.826 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.038.826 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.038.827 I llm_load_print_meta: n_ff             = 1536
0.00.038.827 I llm_load_print_meta: n_expert         = 0
0.00.038.827 I llm_load_print_meta: n_expert_used    = 0
0.00.038.828 I llm_load_print_meta: causal attn      = 0
0.00.038.828 I llm_load_print_meta: pooling type     = 2
0.00.038.828 I llm_load_print_meta: rope type        = 2
0.00.038.828 I llm_load_print_meta: rope scaling     = linear
0.00.038.829 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.038.829 I llm_load_print_meta: freq_scale_train = 1
0.00.038.829 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.038.830 I llm_load_print_meta: rope_finetuned   = unknown
0.00.038.830 I llm_load_print_meta: ssm_d_conv       = 0
0.00.038.830 I llm_load_print_meta: ssm_d_inner      = 0
0.00.038.830 I llm_load_print_meta: ssm_d_state      = 0
0.00.038.831 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.038.831 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.038.831 I llm_load_print_meta: model type       = 33M
0.00.038.832 I llm_load_print_meta: model ftype      = F16
0.00.038.832 I llm_load_print_meta: model params     = 33.21 M
0.00.038.833 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.038.834 I llm_load_print_meta: general.name     = Bge Small
0.00.038.835 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.038.836 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.038.836 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.038.836 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.038.836 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.038.837 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.038.837 I llm_load_print_meta: max token length = 21
0.00.040.889 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.040.889 I llm_load_tensors: offloading output layer to GPU
0.00.040.894 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.040.921 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.923 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.469 I llama_new_context_with_model: n_seq_max     = 1
0.00.041.470 I llama_new_context_with_model: n_ctx         = 512
0.00.041.471 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.041.471 I llama_new_context_with_model: n_batch       = 2048
0.00.041.471 I llama_new_context_with_model: n_ubatch      = 2048
0.00.041.471 I llama_new_context_with_model: flash_attn    = 0
0.00.041.472 I llama_new_context_with_model: freq_base     = 10000.0
0.00.041.472 I llama_new_context_with_model: freq_scale    = 1
0.00.041.473 I ggml_metal_init: allocating
0.00.041.477 I ggml_metal_init: found device: Apple M4
0.00.041.480 I ggml_metal_init: picking default device: Apple M4
0.00.042.323 I ggml_metal_init: using embedded metal library
0.00.046.630 I ggml_metal_init: GPU name:   Apple M4
0.00.046.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.046.633 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.046.634 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.046.634 I ggml_metal_init: simdgroup reduction   = true
0.00.046.634 I ggml_metal_init: simdgroup matrix mul. = true
0.00.046.634 I ggml_metal_init: has bfloat            = true
0.00.046.634 I ggml_metal_init: use bfloat            = true
0.00.046.635 I ggml_metal_init: hasUnifiedMemory      = true
0.00.046.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.905 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.059.524 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.526 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.528 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.060.287 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.060.289 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.060.289 I llama_new_context_with_model: graph nodes  = 429
0.00.060.290 I llama_new_context_with_model: graph splits = 2
0.00.060.310 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.069.782 I 
0.00.069.799 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.070.457 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.075.080 I llama_perf_context_print:        load time =      48.33 ms
0.00.075.081 I llama_perf_context_print: prompt eval time =       4.47 ms /     9 tokens (    0.50 ms per token,  2014.32 tokens per second)
0.00.075.081 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.075.083 I llama_perf_context_print:       total time =       5.30 ms /    10 tokens
0.00.075.226 I ggml_metal_free: deallocating

real	0m0.258s
user	0m0.063s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.077 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.190 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.194 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.195 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.196 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.196 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.196 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.196 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.197 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.198 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.198 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.198 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.199 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.201 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.201 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.012.201 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.012.202 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.012.202 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.202 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.012.202 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.751 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.433 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.435 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.435 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.436 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.436 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.436 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.436 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.437 I llama_model_loader: - type  f32:  124 tensors
0.00.015.437 I llama_model_loader: - type q8_0:   73 tensors
0.00.018.118 I llm_load_vocab: special tokens cache size = 5
0.00.019.436 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.445 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.446 I llm_load_print_meta: arch             = bert
0.00.019.447 I llm_load_print_meta: vocab type       = WPM
0.00.019.447 I llm_load_print_meta: n_vocab          = 30522
0.00.019.447 I llm_load_print_meta: n_merges         = 0
0.00.019.447 I llm_load_print_meta: vocab_only       = 0
0.00.019.448 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.448 I llm_load_print_meta: n_embd           = 384
0.00.019.448 I llm_load_print_meta: n_layer          = 12
0.00.019.451 I llm_load_print_meta: n_head           = 12
0.00.019.453 I llm_load_print_meta: n_head_kv        = 12
0.00.019.453 I llm_load_print_meta: n_rot            = 32
0.00.019.453 I llm_load_print_meta: n_swa            = 0
0.00.019.453 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.453 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.482 I llm_load_print_meta: n_gqa            = 1
0.00.019.485 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.485 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.486 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.486 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.487 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.487 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.487 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.489 I llm_load_print_meta: n_ff             = 1536
0.00.019.489 I llm_load_print_meta: n_expert         = 0
0.00.019.489 I llm_load_print_meta: n_expert_used    = 0
0.00.019.489 I llm_load_print_meta: causal attn      = 0
0.00.019.489 I llm_load_print_meta: pooling type     = 2
0.00.019.489 I llm_load_print_meta: rope type        = 2
0.00.019.490 I llm_load_print_meta: rope scaling     = linear
0.00.019.490 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.490 I llm_load_print_meta: freq_scale_train = 1
0.00.019.491 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.496 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.496 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.496 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.496 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.496 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.496 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.497 I llm_load_print_meta: model type       = 33M
0.00.019.497 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.497 I llm_load_print_meta: model params     = 33.21 M
0.00.019.498 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.500 I llm_load_print_meta: general.name     = Bge Small
0.00.019.500 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.500 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.500 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.501 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.501 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.501 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.501 I llm_load_print_meta: max token length = 21
0.00.020.793 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.793 I llm_load_tensors: offloading output layer to GPU
0.00.020.793 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.800 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.801 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.157 I llama_new_context_with_model: n_seq_max     = 1
0.00.021.158 I llama_new_context_with_model: n_ctx         = 512
0.00.021.158 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.021.158 I llama_new_context_with_model: n_batch       = 2048
0.00.021.158 I llama_new_context_with_model: n_ubatch      = 2048
0.00.021.158 I llama_new_context_with_model: flash_attn    = 0
0.00.021.159 I llama_new_context_with_model: freq_base     = 10000.0
0.00.021.159 I llama_new_context_with_model: freq_scale    = 1
0.00.021.159 I ggml_metal_init: allocating
0.00.021.162 I ggml_metal_init: found device: Apple M4
0.00.021.164 I ggml_metal_init: picking default device: Apple M4
0.00.021.796 I ggml_metal_init: using embedded metal library
0.00.024.355 I ggml_metal_init: GPU name:   Apple M4
0.00.024.358 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.358 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.358 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.359 I ggml_metal_init: simdgroup reduction   = true
0.00.024.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.359 I ggml_metal_init: has bfloat            = true
0.00.024.359 I ggml_metal_init: use bfloat            = true
0.00.024.360 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.361 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.565 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.035.045 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.047 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.048 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.632 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.633 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.633 I llama_new_context_with_model: graph nodes  = 429
0.00.035.634 I llama_new_context_with_model: graph splits = 2
0.00.035.647 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.182 I 
0.00.040.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.715 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.139 I llama_perf_context_print:        load time =      30.10 ms
0.00.045.140 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2098.88 tokens per second)
0.00.045.141 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.142 I llama_perf_context_print:       total time =       4.96 ms /    10 tokens
0.00.045.321 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.146 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.125 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.057 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.062 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.064 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.066 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.066 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.067 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.068 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.069 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.070 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.070 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.071 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.074 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.074 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.075 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.076 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.076 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.039.841 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.030 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.046.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.046.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.046.554 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.046.555 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.046.555 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.046.555 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.046.556 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.046.556 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.046.556 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.046.557 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.046.557 I llama_model_loader: - type  f32:   40 tensors
0.00.046.558 I llama_model_loader: - type  f16:   30 tensors
0.00.065.072 W llm_load_vocab: empty token at index 5
0.00.069.725 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.083 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.114 I llm_load_vocab: special tokens cache size = 5
0.00.332.357 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.332.363 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.332.364 I llm_load_print_meta: arch             = jina-bert-v2
0.00.332.364 I llm_load_print_meta: vocab type       = BPE
0.00.332.365 I llm_load_print_meta: n_vocab          = 61056
0.00.332.365 I llm_load_print_meta: n_merges         = 39382
0.00.332.365 I llm_load_print_meta: vocab_only       = 0
0.00.332.365 I llm_load_print_meta: n_ctx_train      = 8192
0.00.332.365 I llm_load_print_meta: n_embd           = 384
0.00.332.365 I llm_load_print_meta: n_layer          = 4
0.00.332.372 I llm_load_print_meta: n_head           = 12
0.00.332.373 I llm_load_print_meta: n_head_kv        = 12
0.00.332.373 I llm_load_print_meta: n_rot            = 32
0.00.332.373 I llm_load_print_meta: n_swa            = 0
0.00.332.375 I llm_load_print_meta: n_embd_head_k    = 32
0.00.332.375 I llm_load_print_meta: n_embd_head_v    = 32
0.00.332.376 I llm_load_print_meta: n_gqa            = 1
0.00.332.377 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.332.377 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.332.378 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.332.381 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.332.381 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.332.381 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.332.381 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.332.382 I llm_load_print_meta: n_ff             = 1536
0.00.332.382 I llm_load_print_meta: n_expert         = 0
0.00.332.382 I llm_load_print_meta: n_expert_used    = 0
0.00.332.382 I llm_load_print_meta: causal attn      = 0
0.00.332.382 I llm_load_print_meta: pooling type     = -1
0.00.332.383 I llm_load_print_meta: rope type        = -1
0.00.332.383 I llm_load_print_meta: rope scaling     = linear
0.00.332.383 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.332.384 I llm_load_print_meta: freq_scale_train = 1
0.00.332.384 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.332.384 I llm_load_print_meta: rope_finetuned   = unknown
0.00.332.384 I llm_load_print_meta: ssm_d_conv       = 0
0.00.332.385 I llm_load_print_meta: ssm_d_inner      = 0
0.00.332.385 I llm_load_print_meta: ssm_d_state      = 0
0.00.332.385 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.332.387 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.332.387 I llm_load_print_meta: model type       = 33M
0.00.332.387 I llm_load_print_meta: model ftype      = F16
0.00.332.388 I llm_load_print_meta: model params     = 32.90 M
0.00.332.392 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.332.393 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.332.393 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.332.393 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.332.393 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.332.394 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.332.394 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.332.394 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.332.394 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.332.395 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.332.395 I llm_load_print_meta: max token length = 45
0.00.333.580 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.333.581 I llm_load_tensors: offloading output layer to GPU
0.00.333.581 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.333.604 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.605 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.334.390 I llama_new_context_with_model: n_seq_max     = 1
0.00.334.391 I llama_new_context_with_model: n_ctx         = 8192
0.00.334.392 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.334.392 I llama_new_context_with_model: n_batch       = 2048
0.00.334.392 I llama_new_context_with_model: n_ubatch      = 2048
0.00.334.392 I llama_new_context_with_model: flash_attn    = 0
0.00.334.393 I llama_new_context_with_model: freq_base     = 10000.0
0.00.334.393 I llama_new_context_with_model: freq_scale    = 1
0.00.334.393 I ggml_metal_init: allocating
0.00.334.396 I ggml_metal_init: found device: Apple M4
0.00.334.398 I ggml_metal_init: picking default device: Apple M4
0.00.335.346 I ggml_metal_init: using embedded metal library
0.00.338.065 I ggml_metal_init: GPU name:   Apple M4
0.00.338.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.338.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.338.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.338.067 I ggml_metal_init: simdgroup reduction   = true
0.00.338.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.338.068 I ggml_metal_init: has bfloat            = true
0.00.338.068 I ggml_metal_init: use bfloat            = true
0.00.338.068 I ggml_metal_init: hasUnifiedMemory      = true
0.00.338.069 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.588 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.350.056 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.350.058 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.350.059 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.590 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.591 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.591 I llama_new_context_with_model: graph nodes  = 154
0.00.350.591 I llama_new_context_with_model: graph splits = 2
0.00.350.609 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.350.610 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.627 I 
0.00.361.646 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.826 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.827 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.831 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.833 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.840 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.840 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.362.344 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.366.101 I llama_perf_context_print:        load time =     340.50 ms
0.00.366.102 I llama_perf_context_print: prompt eval time =       3.75 ms /    62 tokens (    0.06 ms per token, 16537.74 tokens per second)
0.00.366.106 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.366.106 I llama_perf_context_print:       total time =       4.47 ms /    63 tokens
0.00.366.282 I ggml_metal_free: deallocating

real	0m1.077s
user	0m0.344s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.113 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.235 I main: llama backend init
0.00.000.241 I main: load the model and apply lora adapter, if any
0.00.049.259 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.062.672 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.062.684 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.062.688 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.062.688 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.062.689 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.062.690 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.062.690 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.062.692 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.062.693 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.062.693 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.062.694 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.062.695 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.062.695 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.062.696 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.062.701 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.062.701 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.062.702 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.072.278 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.074.580 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.082.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.082.674 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.082.674 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.082.675 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.082.675 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.082.676 I llama_model_loader: - type  f32:  194 tensors
0.00.082.676 I llama_model_loader: - type  f16:   98 tensors
0.00.113.405 I llm_load_vocab: special tokens cache size = 25
0.00.120.158 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.120.162 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.120.162 I llm_load_print_meta: arch             = gptneox
0.00.120.162 I llm_load_print_meta: vocab type       = BPE
0.00.120.162 I llm_load_print_meta: n_vocab          = 50304
0.00.120.163 I llm_load_print_meta: n_merges         = 50009
0.00.120.163 I llm_load_print_meta: vocab_only       = 0
0.00.120.163 I llm_load_print_meta: n_ctx_train      = 2048
0.00.120.163 I llm_load_print_meta: n_embd           = 2048
0.00.120.163 I llm_load_print_meta: n_layer          = 24
0.00.120.166 I llm_load_print_meta: n_head           = 16
0.00.120.166 I llm_load_print_meta: n_head_kv        = 16
0.00.120.167 I llm_load_print_meta: n_rot            = 32
0.00.120.167 I llm_load_print_meta: n_swa            = 0
0.00.120.167 I llm_load_print_meta: n_embd_head_k    = 128
0.00.120.167 I llm_load_print_meta: n_embd_head_v    = 128
0.00.120.168 I llm_load_print_meta: n_gqa            = 1
0.00.120.169 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.120.171 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.120.171 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.120.172 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.120.172 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.120.172 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.120.172 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.120.173 I llm_load_print_meta: n_ff             = 8192
0.00.120.173 I llm_load_print_meta: n_expert         = 0
0.00.120.173 I llm_load_print_meta: n_expert_used    = 0
0.00.120.178 I llm_load_print_meta: causal attn      = 1
0.00.120.178 I llm_load_print_meta: pooling type     = 0
0.00.120.178 I llm_load_print_meta: rope type        = 2
0.00.120.178 I llm_load_print_meta: rope scaling     = linear
0.00.120.180 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.120.180 I llm_load_print_meta: freq_scale_train = 1
0.00.120.180 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.120.181 I llm_load_print_meta: rope_finetuned   = unknown
0.00.120.181 I llm_load_print_meta: ssm_d_conv       = 0
0.00.120.181 I llm_load_print_meta: ssm_d_inner      = 0
0.00.120.181 I llm_load_print_meta: ssm_d_state      = 0
0.00.120.181 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.120.181 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.120.181 I llm_load_print_meta: model type       = 1.4B
0.00.120.182 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.120.182 I llm_load_print_meta: model params     = 1.41 B
0.00.120.185 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.120.185 I llm_load_print_meta: general.name     = 1.4B
0.00.120.186 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.120.186 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.120.186 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.120.186 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.120.186 I llm_load_print_meta: LF token         = 128 ''
0.00.120.187 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.120.187 I llm_load_print_meta: max token length = 1024
0.00.122.207 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.122.207 I llm_load_tensors: offloading output layer to GPU
0.00.122.208 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.122.226 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.122.227 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.123.149 I llama_new_context_with_model: n_seq_max     = 1
0.00.123.150 I llama_new_context_with_model: n_ctx         = 2048
0.00.123.150 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.123.150 I llama_new_context_with_model: n_batch       = 2048
0.00.123.150 I llama_new_context_with_model: n_ubatch      = 512
0.00.123.151 I llama_new_context_with_model: flash_attn    = 0
0.00.123.151 I llama_new_context_with_model: freq_base     = 10000.0
0.00.123.151 I llama_new_context_with_model: freq_scale    = 1
0.00.123.152 I ggml_metal_init: allocating
0.00.123.160 I ggml_metal_init: found device: Apple M4
0.00.123.164 I ggml_metal_init: picking default device: Apple M4
0.00.123.892 I ggml_metal_init: using embedded metal library
0.00.143.582 I ggml_metal_init: GPU name:   Apple M4
0.00.143.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.143.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.143.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.143.585 I ggml_metal_init: simdgroup reduction   = true
0.00.143.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.143.585 I ggml_metal_init: has bfloat            = true
0.00.143.585 I ggml_metal_init: use bfloat            = true
0.00.143.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.143.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.235.610 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.255.896 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.255.903 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.255.928 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.256.981 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.256.984 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.256.984 I llama_new_context_with_model: graph nodes  = 967
0.00.256.985 I llama_new_context_with_model: graph splits = 2
0.00.257.010 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.257.139 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.257.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.336.329 I main: llama threadpool init, n_threads = 4
0.00.336.363 I 
0.00.336.385 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.336.385 I 
0.00.336.460 I sampler seed: 1234
0.00.336.465 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.336.488 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.336.490 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.336.490 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.137.280 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55642.63 tokens per second)
0.02.137.282 I llama_perf_context_print:        load time =     287.06 ms
0.02.137.282 I llama_perf_context_print: prompt eval time =      44.09 ms /     7 tokens (    6.30 ms per token,   158.78 tokens per second)
0.02.137.283 I llama_perf_context_print:        eval time =    1753.72 ms /    63 runs   (   27.84 ms per token,    35.92 tokens per second)
0.02.137.284 I llama_perf_context_print:       total time =    1800.95 ms /    70 tokens
0.02.137.450 I ggml_metal_free: deallocating

real	0m2.451s
user	0m0.154s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.572 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.922 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.174 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.183 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.185 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.186 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.187 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.188 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.188 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.191 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.195 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.196 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.197 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.200 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.201 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.201 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.672 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.672 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.673 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.674 I llama_model_loader: - type  f32:  194 tensors
0.00.051.675 I llama_model_loader: - type  f16:   98 tensors
0.00.080.013 I llm_load_vocab: special tokens cache size = 25
0.00.086.505 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.508 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.508 I llm_load_print_meta: arch             = gptneox
0.00.086.508 I llm_load_print_meta: vocab type       = BPE
0.00.086.508 I llm_load_print_meta: n_vocab          = 50304
0.00.086.509 I llm_load_print_meta: n_merges         = 50009
0.00.086.509 I llm_load_print_meta: vocab_only       = 0
0.00.086.509 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.509 I llm_load_print_meta: n_embd           = 2048
0.00.086.509 I llm_load_print_meta: n_layer          = 24
0.00.086.512 I llm_load_print_meta: n_head           = 16
0.00.086.512 I llm_load_print_meta: n_head_kv        = 16
0.00.086.512 I llm_load_print_meta: n_rot            = 32
0.00.086.512 I llm_load_print_meta: n_swa            = 0
0.00.086.513 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.513 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.514 I llm_load_print_meta: n_gqa            = 1
0.00.086.514 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.515 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.516 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.516 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.516 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.516 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.519 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.519 I llm_load_print_meta: n_ff             = 8192
0.00.086.519 I llm_load_print_meta: n_expert         = 0
0.00.086.519 I llm_load_print_meta: n_expert_used    = 0
0.00.086.520 I llm_load_print_meta: causal attn      = 1
0.00.086.520 I llm_load_print_meta: pooling type     = 0
0.00.086.520 I llm_load_print_meta: rope type        = 2
0.00.086.520 I llm_load_print_meta: rope scaling     = linear
0.00.086.520 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.521 I llm_load_print_meta: freq_scale_train = 1
0.00.086.521 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.521 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.521 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.521 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.521 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.521 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.522 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.522 I llm_load_print_meta: model type       = 1.4B
0.00.086.522 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.523 I llm_load_print_meta: model params     = 1.41 B
0.00.086.533 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.533 I llm_load_print_meta: general.name     = 1.4B
0.00.086.534 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.534 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.534 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.534 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.535 I llm_load_print_meta: LF token         = 128 ''
0.00.086.536 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.536 I llm_load_print_meta: max token length = 1024
0.00.088.593 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.593 I llm_load_tensors: offloading output layer to GPU
0.00.088.593 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.598 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.599 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.496 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.497 I llama_new_context_with_model: n_ctx         = 128
0.00.089.497 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.497 I llama_new_context_with_model: n_batch       = 128
0.00.089.498 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.498 I llama_new_context_with_model: flash_attn    = 0
0.00.089.498 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.498 I llama_new_context_with_model: freq_scale    = 1
0.00.089.499 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.499 I ggml_metal_init: allocating
0.00.089.502 I ggml_metal_init: found device: Apple M4
0.00.089.504 I ggml_metal_init: picking default device: Apple M4
0.00.090.128 I ggml_metal_init: using embedded metal library
0.00.092.680 I ggml_metal_init: GPU name:   Apple M4
0.00.092.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.683 I ggml_metal_init: simdgroup reduction   = true
0.00.092.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.683 I ggml_metal_init: has bfloat            = true
0.00.092.683 I ggml_metal_init: use bfloat            = true
0.00.092.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.754 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.102.971 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.977 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.990 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.815 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.816 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.816 I llama_new_context_with_model: graph nodes  = 967
0.00.103.816 I llama_new_context_with_model: graph splits = 2
0.00.103.823 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.823 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.993.551 I 
0.00.993.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.993.613 I perplexity: tokenizing the input ..
0.01.004.035 I perplexity: tokenization took 10.42 ms
0.01.004.044 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.124.504 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.126.886 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.126.904 I llama_perf_context_print:        load time =     971.62 ms
0.01.126.906 I llama_perf_context_print: prompt eval time =     120.17 ms /   128 tokens (    0.94 ms per token,  1065.18 tokens per second)
0.01.126.910 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.126.911 I llama_perf_context_print:       total time =     133.35 ms /   129 tokens
0.01.127.488 I ggml_metal_free: deallocating

real	0m1.350s
user	0m0.124s
sys	0m0.244s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.014.364 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.356 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.356 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.357 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.357 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.358 I llama_model_loader: - type  f32:  194 tensors
0.00.041.358 I llama_model_loader: - type q8_0:   98 tensors
0.00.069.397 I llm_load_vocab: special tokens cache size = 25
0.00.077.233 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.239 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.239 I llm_load_print_meta: arch             = gptneox
0.00.077.239 I llm_load_print_meta: vocab type       = BPE
0.00.077.240 I llm_load_print_meta: n_vocab          = 50304
0.00.077.240 I llm_load_print_meta: n_merges         = 50009
0.00.077.240 I llm_load_print_meta: vocab_only       = 0
0.00.077.240 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.240 I llm_load_print_meta: n_embd           = 2048
0.00.077.240 I llm_load_print_meta: n_layer          = 24
0.00.077.245 I llm_load_print_meta: n_head           = 16
0.00.077.246 I llm_load_print_meta: n_head_kv        = 16
0.00.077.246 I llm_load_print_meta: n_rot            = 32
0.00.077.246 I llm_load_print_meta: n_swa            = 0
0.00.077.246 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.247 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.249 I llm_load_print_meta: n_gqa            = 1
0.00.077.250 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.251 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.251 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.252 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.252 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.252 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.252 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.253 I llm_load_print_meta: n_ff             = 8192
0.00.077.256 I llm_load_print_meta: n_expert         = 0
0.00.077.256 I llm_load_print_meta: n_expert_used    = 0
0.00.077.256 I llm_load_print_meta: causal attn      = 1
0.00.077.256 I llm_load_print_meta: pooling type     = 0
0.00.077.257 I llm_load_print_meta: rope type        = 2
0.00.077.257 I llm_load_print_meta: rope scaling     = linear
0.00.077.258 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.258 I llm_load_print_meta: freq_scale_train = 1
0.00.077.258 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.259 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.259 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.259 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.260 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.261 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.261 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.261 I llm_load_print_meta: model type       = 1.4B
0.00.077.261 I llm_load_print_meta: model ftype      = Q8_0
0.00.077.262 I llm_load_print_meta: model params     = 1.41 B
0.00.077.262 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.077.262 I llm_load_print_meta: general.name     = 1.4B
0.00.077.262 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.263 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.263 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.263 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.263 I llm_load_print_meta: LF token         = 128 ''
0.00.077.263 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.264 I llm_load_print_meta: max token length = 1024
0.00.079.656 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.656 I llm_load_tensors: offloading output layer to GPU
0.00.079.656 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.668 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.079.669 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.080.747 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.748 I llama_new_context_with_model: n_ctx         = 2048
0.00.080.748 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.080.749 I llama_new_context_with_model: n_batch       = 2048
0.00.080.749 I llama_new_context_with_model: n_ubatch      = 512
0.00.080.749 I llama_new_context_with_model: flash_attn    = 0
0.00.080.749 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.750 I llama_new_context_with_model: freq_scale    = 1
0.00.080.750 I ggml_metal_init: allocating
0.00.080.753 I ggml_metal_init: found device: Apple M4
0.00.080.755 I ggml_metal_init: picking default device: Apple M4
0.00.081.613 I ggml_metal_init: using embedded metal library
0.00.084.825 I ggml_metal_init: GPU name:   Apple M4
0.00.084.827 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.084.827 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.084.827 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.084.828 I ggml_metal_init: simdgroup reduction   = true
0.00.084.828 I ggml_metal_init: simdgroup matrix mul. = true
0.00.084.828 I ggml_metal_init: has bfloat            = true
0.00.084.828 I ggml_metal_init: use bfloat            = true
0.00.084.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.084.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.053 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.120.681 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.690 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.715 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.847 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.121.851 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.121.851 I llama_new_context_with_model: graph nodes  = 967
0.00.121.851 I llama_new_context_with_model: graph splits = 2
0.00.121.870 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.121.999 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.122.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.326.077 I main: llama threadpool init, n_threads = 4
0.01.326.116 I 
0.01.326.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.326.144 I 
0.01.326.304 I sampler seed: 1234
0.01.326.310 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.326.319 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.326.319 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.326.319 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.413.047 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.02.413.051 I llama_perf_context_print:        load time =    1311.71 ms
0.02.413.052 I llama_perf_context_print: prompt eval time =      40.86 ms /     7 tokens (    5.84 ms per token,   171.31 tokens per second)
0.02.413.053 I llama_perf_context_print:        eval time =    1042.84 ms /    63 runs   (   16.55 ms per token,    60.41 tokens per second)
0.02.413.053 I llama_perf_context_print:       total time =    1086.97 ms /    70 tokens
0.02.413.260 I ggml_metal_free: deallocating

real	0m2.434s
user	0m0.125s
sys	0m0.274s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.177 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.364 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.433 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.433 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.433 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.435 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.436 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.437 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.438 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.440 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.361 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.823 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.101 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.102 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.103 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.103 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.104 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.104 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.105 I llama_model_loader: - type  f32:  194 tensors
0.00.033.105 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.216 I llm_load_vocab: special tokens cache size = 25
0.00.063.000 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.003 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.003 I llm_load_print_meta: arch             = gptneox
0.00.063.004 I llm_load_print_meta: vocab type       = BPE
0.00.063.004 I llm_load_print_meta: n_vocab          = 50304
0.00.063.004 I llm_load_print_meta: n_merges         = 50009
0.00.063.004 I llm_load_print_meta: vocab_only       = 0
0.00.063.004 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.004 I llm_load_print_meta: n_embd           = 2048
0.00.063.004 I llm_load_print_meta: n_layer          = 24
0.00.063.007 I llm_load_print_meta: n_head           = 16
0.00.063.008 I llm_load_print_meta: n_head_kv        = 16
0.00.063.008 I llm_load_print_meta: n_rot            = 32
0.00.063.009 I llm_load_print_meta: n_swa            = 0
0.00.063.009 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.010 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.011 I llm_load_print_meta: n_gqa            = 1
0.00.063.012 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.012 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.013 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.013 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.013 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.014 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.014 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.015 I llm_load_print_meta: n_ff             = 8192
0.00.063.015 I llm_load_print_meta: n_expert         = 0
0.00.063.015 I llm_load_print_meta: n_expert_used    = 0
0.00.063.015 I llm_load_print_meta: causal attn      = 1
0.00.063.015 I llm_load_print_meta: pooling type     = 0
0.00.063.015 I llm_load_print_meta: rope type        = 2
0.00.063.015 I llm_load_print_meta: rope scaling     = linear
0.00.063.016 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.016 I llm_load_print_meta: freq_scale_train = 1
0.00.063.016 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.017 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.017 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.019 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.019 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.019 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.020 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.020 I llm_load_print_meta: model type       = 1.4B
0.00.063.020 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.020 I llm_load_print_meta: model params     = 1.41 B
0.00.063.021 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.021 I llm_load_print_meta: general.name     = 1.4B
0.00.063.021 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.022 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.022 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.022 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.022 I llm_load_print_meta: LF token         = 128 ''
0.00.063.022 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.023 I llm_load_print_meta: max token length = 1024
0.00.064.779 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.779 I llm_load_tensors: offloading output layer to GPU
0.00.064.779 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.790 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.791 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.625 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.626 I llama_new_context_with_model: n_ctx         = 128
0.00.065.626 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.627 I llama_new_context_with_model: n_batch       = 128
0.00.065.627 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.627 I llama_new_context_with_model: flash_attn    = 0
0.00.065.627 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.628 I llama_new_context_with_model: freq_scale    = 1
0.00.065.628 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.628 I ggml_metal_init: allocating
0.00.065.634 I ggml_metal_init: found device: Apple M4
0.00.065.639 I ggml_metal_init: picking default device: Apple M4
0.00.066.257 I ggml_metal_init: using embedded metal library
0.00.068.728 I ggml_metal_init: GPU name:   Apple M4
0.00.068.730 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.730 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.731 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.731 I ggml_metal_init: simdgroup reduction   = true
0.00.068.731 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.731 I ggml_metal_init: has bfloat            = true
0.00.068.731 I ggml_metal_init: use bfloat            = true
0.00.068.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.732 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.530 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.078.904 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.907 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.922 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.822 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.823 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.823 I llama_new_context_with_model: graph nodes  = 967
0.00.079.824 I llama_new_context_with_model: graph splits = 2
0.00.079.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.079.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.068.401 I 
0.01.068.435 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.068.469 I perplexity: tokenizing the input ..
0.01.075.986 I perplexity: tokenization took 7.515 ms
0.01.075.990 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.200.409 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.201.539 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.201.552 I llama_perf_context_print:        load time =    1057.03 ms
0.01.201.553 I llama_perf_context_print: prompt eval time =     124.20 ms /   128 tokens (    0.97 ms per token,  1030.60 tokens per second)
0.01.201.554 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.201.555 I llama_perf_context_print:       total time =     133.15 ms /   129 tokens
0.01.202.078 I ggml_metal_free: deallocating

real	0m1.221s
user	0m0.092s
sys	0m0.217s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.641 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.489 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.494 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.497 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.498 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.498 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.498 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.499 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.500 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.500 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.502 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.502 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.504 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.505 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.505 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.263 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.123 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.125 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.125 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.126 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.126 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.126 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.127 I llama_model_loader: - type  f32:  194 tensors
0.00.033.127 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.128 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.814 I llm_load_vocab: special tokens cache size = 25
0.00.060.670 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.674 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.674 I llm_load_print_meta: arch             = gptneox
0.00.060.675 I llm_load_print_meta: vocab type       = BPE
0.00.060.675 I llm_load_print_meta: n_vocab          = 50304
0.00.060.675 I llm_load_print_meta: n_merges         = 50009
0.00.060.675 I llm_load_print_meta: vocab_only       = 0
0.00.060.675 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.676 I llm_load_print_meta: n_embd           = 2048
0.00.060.676 I llm_load_print_meta: n_layer          = 24
0.00.060.680 I llm_load_print_meta: n_head           = 16
0.00.060.681 I llm_load_print_meta: n_head_kv        = 16
0.00.060.681 I llm_load_print_meta: n_rot            = 32
0.00.060.681 I llm_load_print_meta: n_swa            = 0
0.00.060.681 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.683 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.683 I llm_load_print_meta: n_gqa            = 1
0.00.060.684 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.685 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.686 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.686 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.686 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.686 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.686 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.689 I llm_load_print_meta: n_ff             = 8192
0.00.060.689 I llm_load_print_meta: n_expert         = 0
0.00.060.690 I llm_load_print_meta: n_expert_used    = 0
0.00.060.690 I llm_load_print_meta: causal attn      = 1
0.00.060.690 I llm_load_print_meta: pooling type     = 0
0.00.060.690 I llm_load_print_meta: rope type        = 2
0.00.060.690 I llm_load_print_meta: rope scaling     = linear
0.00.060.691 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.691 I llm_load_print_meta: freq_scale_train = 1
0.00.060.691 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.692 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.692 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.692 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.692 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.692 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.692 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.693 I llm_load_print_meta: model type       = 1.4B
0.00.060.693 I llm_load_print_meta: model ftype      = Q4_0
0.00.060.693 I llm_load_print_meta: model params     = 1.41 B
0.00.060.694 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.060.694 I llm_load_print_meta: general.name     = 1.4B
0.00.060.695 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.695 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.695 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.695 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.695 I llm_load_print_meta: LF token         = 128 ''
0.00.060.695 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.696 I llm_load_print_meta: max token length = 1024
0.00.062.718 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.718 I llm_load_tensors: offloading output layer to GPU
0.00.062.718 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.729 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.062.730 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.063.609 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.609 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.610 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.610 I llama_new_context_with_model: n_batch       = 2048
0.00.063.610 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.610 I llama_new_context_with_model: flash_attn    = 0
0.00.063.611 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.611 I llama_new_context_with_model: freq_scale    = 1
0.00.063.611 I ggml_metal_init: allocating
0.00.063.615 I ggml_metal_init: found device: Apple M4
0.00.063.617 I ggml_metal_init: picking default device: Apple M4
0.00.064.376 I ggml_metal_init: using embedded metal library
0.00.066.966 I ggml_metal_init: GPU name:   Apple M4
0.00.066.968 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.968 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.969 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.969 I ggml_metal_init: simdgroup reduction   = true
0.00.066.969 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.969 I ggml_metal_init: has bfloat            = true
0.00.066.969 I ggml_metal_init: use bfloat            = true
0.00.066.970 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.971 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.588 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.451 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.458 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.482 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.707 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.710 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.710 I llama_new_context_with_model: graph nodes  = 967
0.00.105.711 I llama_new_context_with_model: graph splits = 2
0.00.105.729 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.056 I main: llama threadpool init, n_threads = 4
0.00.811.093 I 
0.00.811.115 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.115 I 
0.00.811.307 I sampler seed: 1234
0.00.811.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.329 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.330 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.330 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.490.416 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.490.416 I llama_perf_context_print:        load time =     800.41 ms
0.01.490.417 I llama_perf_context_print: prompt eval time =      40.11 ms /     7 tokens (    5.73 ms per token,   174.50 tokens per second)
0.01.490.418 I llama_perf_context_print:        eval time =     635.96 ms /    63 runs   (   10.09 ms per token,    99.06 tokens per second)
0.01.490.418 I llama_perf_context_print:       total time =     679.36 ms /    70 tokens
0.01.490.617 I ggml_metal_free: deallocating

real	0m1.510s
user	0m0.112s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.545 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.023 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.024 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.026 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.028 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.029 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.029 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.030 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.031 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.031 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.848 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.882 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.587 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.588 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.589 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.590 I llama_model_loader: - type  f32:  194 tensors
0.00.023.590 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.590 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.248 I llm_load_vocab: special tokens cache size = 25
0.00.050.146 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.149 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.149 I llm_load_print_meta: arch             = gptneox
0.00.050.149 I llm_load_print_meta: vocab type       = BPE
0.00.050.149 I llm_load_print_meta: n_vocab          = 50304
0.00.050.150 I llm_load_print_meta: n_merges         = 50009
0.00.050.150 I llm_load_print_meta: vocab_only       = 0
0.00.050.150 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.150 I llm_load_print_meta: n_embd           = 2048
0.00.050.150 I llm_load_print_meta: n_layer          = 24
0.00.050.153 I llm_load_print_meta: n_head           = 16
0.00.050.154 I llm_load_print_meta: n_head_kv        = 16
0.00.050.154 I llm_load_print_meta: n_rot            = 32
0.00.050.154 I llm_load_print_meta: n_swa            = 0
0.00.050.154 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.154 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.155 I llm_load_print_meta: n_gqa            = 1
0.00.050.157 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.158 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.158 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.158 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.159 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.159 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.159 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.160 I llm_load_print_meta: n_ff             = 8192
0.00.050.160 I llm_load_print_meta: n_expert         = 0
0.00.050.160 I llm_load_print_meta: n_expert_used    = 0
0.00.050.160 I llm_load_print_meta: causal attn      = 1
0.00.050.160 I llm_load_print_meta: pooling type     = 0
0.00.050.161 I llm_load_print_meta: rope type        = 2
0.00.050.161 I llm_load_print_meta: rope scaling     = linear
0.00.050.161 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.162 I llm_load_print_meta: freq_scale_train = 1
0.00.050.162 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.162 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.162 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.162 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.162 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.162 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.163 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.163 I llm_load_print_meta: model type       = 1.4B
0.00.050.163 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.164 I llm_load_print_meta: model params     = 1.41 B
0.00.050.164 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.166 I llm_load_print_meta: general.name     = 1.4B
0.00.050.167 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.167 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.167 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.167 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.167 I llm_load_print_meta: LF token         = 128 ''
0.00.050.168 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.170 I llm_load_print_meta: max token length = 1024
0.00.051.981 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.982 I llm_load_tensors: offloading output layer to GPU
0.00.051.982 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.992 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.993 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.876 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.877 I llama_new_context_with_model: n_ctx         = 128
0.00.052.877 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.877 I llama_new_context_with_model: n_batch       = 128
0.00.052.877 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.878 I llama_new_context_with_model: flash_attn    = 0
0.00.052.878 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.878 I llama_new_context_with_model: freq_scale    = 1
0.00.052.879 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.879 I ggml_metal_init: allocating
0.00.052.883 I ggml_metal_init: found device: Apple M4
0.00.052.886 I ggml_metal_init: picking default device: Apple M4
0.00.053.481 I ggml_metal_init: using embedded metal library
0.00.055.792 I ggml_metal_init: GPU name:   Apple M4
0.00.055.794 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.794 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.795 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.795 I ggml_metal_init: simdgroup reduction   = true
0.00.055.795 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.795 I ggml_metal_init: has bfloat            = true
0.00.055.795 I ggml_metal_init: use bfloat            = true
0.00.055.796 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.796 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.487 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.801 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.804 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.819 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.669 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.670 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.671 I llama_new_context_with_model: graph nodes  = 967
0.00.067.671 I llama_new_context_with_model: graph splits = 2
0.00.067.683 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.683 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.441 I 
0.00.677.467 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.480 I perplexity: tokenizing the input ..
0.00.685.082 I perplexity: tokenization took 7.6 ms
0.00.685.086 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.199 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.809.293 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.809.308 I llama_perf_context_print:        load time =     667.89 ms
0.00.809.309 I llama_perf_context_print: prompt eval time =     122.89 ms /   128 tokens (    0.96 ms per token,  1041.55 tokens per second)
0.00.809.310 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.310 I llama_perf_context_print:       total time =     131.87 ms /   129 tokens
0.00.809.725 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.079s
sys	0m0.142s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.058 I main: llama backend init
0.00.000.060 I main: load the model and apply lora adapter, if any
0.00.010.320 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.759 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.761 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.761 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.762 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.762 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.763 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.764 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.764 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.766 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.767 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.769 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.769 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.770 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.782 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.784 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.784 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.784 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.784 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.785 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.785 I llama_model_loader: - type  f32:  194 tensors
0.00.033.785 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.786 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.113 I llm_load_vocab: special tokens cache size = 25
0.00.065.934 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.937 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.938 I llm_load_print_meta: arch             = gptneox
0.00.065.938 I llm_load_print_meta: vocab type       = BPE
0.00.065.938 I llm_load_print_meta: n_vocab          = 50304
0.00.065.938 I llm_load_print_meta: n_merges         = 50009
0.00.065.939 I llm_load_print_meta: vocab_only       = 0
0.00.065.939 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.939 I llm_load_print_meta: n_embd           = 2048
0.00.065.939 I llm_load_print_meta: n_layer          = 24
0.00.065.941 I llm_load_print_meta: n_head           = 16
0.00.065.942 I llm_load_print_meta: n_head_kv        = 16
0.00.065.943 I llm_load_print_meta: n_rot            = 32
0.00.065.943 I llm_load_print_meta: n_swa            = 0
0.00.065.947 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.947 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.950 I llm_load_print_meta: n_gqa            = 1
0.00.065.950 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.951 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.952 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.952 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.952 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.952 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.952 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.953 I llm_load_print_meta: n_ff             = 8192
0.00.065.953 I llm_load_print_meta: n_expert         = 0
0.00.065.953 I llm_load_print_meta: n_expert_used    = 0
0.00.065.953 I llm_load_print_meta: causal attn      = 1
0.00.065.953 I llm_load_print_meta: pooling type     = 0
0.00.065.954 I llm_load_print_meta: rope type        = 2
0.00.065.954 I llm_load_print_meta: rope scaling     = linear
0.00.065.954 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.954 I llm_load_print_meta: freq_scale_train = 1
0.00.065.954 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.955 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.955 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.955 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.955 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.955 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.955 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.958 I llm_load_print_meta: model type       = 1.4B
0.00.065.958 I llm_load_print_meta: model ftype      = Q4_1
0.00.065.959 I llm_load_print_meta: model params     = 1.41 B
0.00.065.959 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.065.959 I llm_load_print_meta: general.name     = 1.4B
0.00.065.960 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.960 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.960 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.961 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.961 I llm_load_print_meta: LF token         = 128 ''
0.00.065.962 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.962 I llm_load_print_meta: max token length = 1024
0.00.067.911 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.912 I llm_load_tensors: offloading output layer to GPU
0.00.067.912 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.922 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.067.923 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.068.894 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.896 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.896 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.896 I llama_new_context_with_model: n_batch       = 2048
0.00.068.896 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.897 I llama_new_context_with_model: flash_attn    = 0
0.00.068.897 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.898 I llama_new_context_with_model: freq_scale    = 1
0.00.068.898 I ggml_metal_init: allocating
0.00.068.905 I ggml_metal_init: found device: Apple M4
0.00.068.908 I ggml_metal_init: picking default device: Apple M4
0.00.069.572 I ggml_metal_init: using embedded metal library
0.00.072.307 I ggml_metal_init: GPU name:   Apple M4
0.00.072.309 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.310 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.310 I ggml_metal_init: simdgroup reduction   = true
0.00.072.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.311 I ggml_metal_init: has bfloat            = true
0.00.072.311 I ggml_metal_init: use bfloat            = true
0.00.072.311 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.312 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.311 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.105.618 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.623 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.643 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.763 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.764 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.765 I llama_new_context_with_model: graph nodes  = 967
0.00.106.765 I llama_new_context_with_model: graph splits = 2
0.00.106.780 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.912 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.912 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.858.107 I main: llama threadpool init, n_threads = 4
0.00.858.142 I 
0.00.858.162 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.858.162 I 
0.00.858.326 I sampler seed: 1234
0.00.858.330 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.341 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.342 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.342 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.583.588 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62171.63 tokens per second)
0.01.583.590 I llama_perf_context_print:        load time =     847.78 ms
0.01.583.590 I llama_perf_context_print: prompt eval time =      40.00 ms /     7 tokens (    5.71 ms per token,   175.00 tokens per second)
0.01.583.591 I llama_perf_context_print:        eval time =     682.22 ms /    63 runs   (   10.83 ms per token,    92.35 tokens per second)
0.01.583.591 I llama_perf_context_print:       total time =     725.48 ms /    70 tokens
0.01.583.774 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.118s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.325 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.734 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.738 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.747 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.254 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.254 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.255 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.255 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.255 I llama_model_loader: - type  f32:  194 tensors
0.00.024.256 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.256 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.572 I llm_load_vocab: special tokens cache size = 25
0.00.050.543 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.546 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.546 I llm_load_print_meta: arch             = gptneox
0.00.050.546 I llm_load_print_meta: vocab type       = BPE
0.00.050.546 I llm_load_print_meta: n_vocab          = 50304
0.00.050.547 I llm_load_print_meta: n_merges         = 50009
0.00.050.547 I llm_load_print_meta: vocab_only       = 0
0.00.050.547 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.547 I llm_load_print_meta: n_embd           = 2048
0.00.050.547 I llm_load_print_meta: n_layer          = 24
0.00.050.550 I llm_load_print_meta: n_head           = 16
0.00.050.551 I llm_load_print_meta: n_head_kv        = 16
0.00.050.551 I llm_load_print_meta: n_rot            = 32
0.00.050.551 I llm_load_print_meta: n_swa            = 0
0.00.050.554 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.554 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.554 I llm_load_print_meta: n_gqa            = 1
0.00.050.555 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.556 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.556 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.557 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.557 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.557 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.557 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.558 I llm_load_print_meta: n_ff             = 8192
0.00.050.558 I llm_load_print_meta: n_expert         = 0
0.00.050.558 I llm_load_print_meta: n_expert_used    = 0
0.00.050.558 I llm_load_print_meta: causal attn      = 1
0.00.050.558 I llm_load_print_meta: pooling type     = 0
0.00.050.559 I llm_load_print_meta: rope type        = 2
0.00.050.559 I llm_load_print_meta: rope scaling     = linear
0.00.050.563 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.564 I llm_load_print_meta: freq_scale_train = 1
0.00.050.564 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.564 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.564 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.565 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.565 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.565 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.566 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.566 I llm_load_print_meta: model type       = 1.4B
0.00.050.567 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.567 I llm_load_print_meta: model params     = 1.41 B
0.00.050.568 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.569 I llm_load_print_meta: general.name     = 1.4B
0.00.050.569 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.569 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.569 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.569 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.570 I llm_load_print_meta: LF token         = 128 ''
0.00.050.570 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.570 I llm_load_print_meta: max token length = 1024
0.00.052.280 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.280 I llm_load_tensors: offloading output layer to GPU
0.00.052.280 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.290 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.291 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.121 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.121 I llama_new_context_with_model: n_ctx         = 128
0.00.053.122 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.122 I llama_new_context_with_model: n_batch       = 128
0.00.053.122 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.122 I llama_new_context_with_model: flash_attn    = 0
0.00.053.123 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.123 I llama_new_context_with_model: freq_scale    = 1
0.00.053.123 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.124 I ggml_metal_init: allocating
0.00.053.127 I ggml_metal_init: found device: Apple M4
0.00.053.129 I ggml_metal_init: picking default device: Apple M4
0.00.053.703 I ggml_metal_init: using embedded metal library
0.00.056.330 I ggml_metal_init: GPU name:   Apple M4
0.00.056.332 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.333 I ggml_metal_init: simdgroup reduction   = true
0.00.056.333 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.333 I ggml_metal_init: has bfloat            = true
0.00.056.333 I ggml_metal_init: use bfloat            = true
0.00.056.334 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.334 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.775 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.011 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.013 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.033 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.882 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.883 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.883 I llama_new_context_with_model: graph nodes  = 967
0.00.067.884 I llama_new_context_with_model: graph splits = 2
0.00.067.895 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.995 I 
0.00.735.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.041 I perplexity: tokenizing the input ..
0.00.742.602 I perplexity: tokenization took 7.56 ms
0.00.742.606 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.865.759 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.866.856 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.866.872 I llama_perf_context_print:        load time =     724.66 ms
0.00.866.875 I llama_perf_context_print: prompt eval time =     122.93 ms /   128 tokens (    0.96 ms per token,  1041.21 tokens per second)
0.00.866.875 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.876 I llama_perf_context_print:       total time =     131.88 ms /   129 tokens
0.00.867.313 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.078s
sys	0m0.147s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.020.599 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.290 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.290 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.291 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.291 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.296 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.296 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.967 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.227 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.883 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.883 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.884 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.884 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.038.884 I llama_model_loader: - type  f32:  194 tensors
0.00.038.885 I llama_model_loader: - type q5_0:   97 tensors
0.00.038.885 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.913 I llm_load_vocab: special tokens cache size = 25
0.00.079.540 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.544 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.545 I llm_load_print_meta: arch             = gptneox
0.00.079.545 I llm_load_print_meta: vocab type       = BPE
0.00.079.545 I llm_load_print_meta: n_vocab          = 50304
0.00.079.546 I llm_load_print_meta: n_merges         = 50009
0.00.079.546 I llm_load_print_meta: vocab_only       = 0
0.00.079.546 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.546 I llm_load_print_meta: n_embd           = 2048
0.00.079.546 I llm_load_print_meta: n_layer          = 24
0.00.079.550 I llm_load_print_meta: n_head           = 16
0.00.079.551 I llm_load_print_meta: n_head_kv        = 16
0.00.079.551 I llm_load_print_meta: n_rot            = 32
0.00.079.552 I llm_load_print_meta: n_swa            = 0
0.00.079.552 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.552 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.553 I llm_load_print_meta: n_gqa            = 1
0.00.079.556 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.557 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.558 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.559 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.560 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.560 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.560 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.561 I llm_load_print_meta: n_ff             = 8192
0.00.079.561 I llm_load_print_meta: n_expert         = 0
0.00.079.562 I llm_load_print_meta: n_expert_used    = 0
0.00.079.562 I llm_load_print_meta: causal attn      = 1
0.00.079.562 I llm_load_print_meta: pooling type     = 0
0.00.079.562 I llm_load_print_meta: rope type        = 2
0.00.079.562 I llm_load_print_meta: rope scaling     = linear
0.00.079.563 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.563 I llm_load_print_meta: freq_scale_train = 1
0.00.079.564 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.564 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.564 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.564 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.565 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.565 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.565 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.566 I llm_load_print_meta: model type       = 1.4B
0.00.079.566 I llm_load_print_meta: model ftype      = Q5_0
0.00.079.567 I llm_load_print_meta: model params     = 1.41 B
0.00.079.567 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.079.568 I llm_load_print_meta: general.name     = 1.4B
0.00.079.568 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.568 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.569 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.569 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.569 I llm_load_print_meta: LF token         = 128 ''
0.00.079.570 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.570 I llm_load_print_meta: max token length = 1024
0.00.082.445 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.445 I llm_load_tensors: offloading output layer to GPU
0.00.082.445 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.456 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.082.458 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.083.880 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.882 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.882 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.882 I llama_new_context_with_model: n_batch       = 2048
0.00.083.883 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.883 I llama_new_context_with_model: flash_attn    = 0
0.00.083.884 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.884 I llama_new_context_with_model: freq_scale    = 1
0.00.083.885 I ggml_metal_init: allocating
0.00.083.893 I ggml_metal_init: found device: Apple M4
0.00.083.896 I ggml_metal_init: picking default device: Apple M4
0.00.084.748 I ggml_metal_init: using embedded metal library
0.00.088.515 I ggml_metal_init: GPU name:   Apple M4
0.00.088.518 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.518 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.519 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.519 I ggml_metal_init: simdgroup reduction   = true
0.00.088.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.519 I ggml_metal_init: has bfloat            = true
0.00.088.519 I ggml_metal_init: use bfloat            = true
0.00.088.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.527 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.174 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.125.172 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.186 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.205 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.208 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.209 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.209 I llama_new_context_with_model: graph nodes  = 967
0.00.126.209 I llama_new_context_with_model: graph splits = 2
0.00.126.226 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.126.357 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.126.358 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.915.908 I main: llama threadpool init, n_threads = 4
0.00.915.970 I 
0.00.916.012 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.916.014 I 
0.00.916.328 I sampler seed: 1234
0.00.916.334 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.916.352 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.916.352 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.916.352 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.703.171 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.703.171 I llama_perf_context_print:        load time =     895.30 ms
0.01.703.173 I llama_perf_context_print: prompt eval time =      44.12 ms /     7 tokens (    6.30 ms per token,   158.68 tokens per second)
0.01.703.173 I llama_perf_context_print:        eval time =     739.77 ms /    63 runs   (   11.74 ms per token,    85.16 tokens per second)
0.01.703.173 I llama_perf_context_print:       total time =     787.27 ms /    70 tokens
0.01.703.389 I ggml_metal_free: deallocating

real	0m1.732s
user	0m0.137s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.390 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.795 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.464 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.120 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.121 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.122 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.122 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.122 I llama_model_loader: - type  f32:  194 tensors
0.00.025.123 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.123 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.802 I llm_load_vocab: special tokens cache size = 25
0.00.050.626 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.629 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.629 I llm_load_print_meta: arch             = gptneox
0.00.050.630 I llm_load_print_meta: vocab type       = BPE
0.00.050.630 I llm_load_print_meta: n_vocab          = 50304
0.00.050.630 I llm_load_print_meta: n_merges         = 50009
0.00.050.630 I llm_load_print_meta: vocab_only       = 0
0.00.050.630 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.631 I llm_load_print_meta: n_embd           = 2048
0.00.050.631 I llm_load_print_meta: n_layer          = 24
0.00.050.633 I llm_load_print_meta: n_head           = 16
0.00.050.634 I llm_load_print_meta: n_head_kv        = 16
0.00.050.634 I llm_load_print_meta: n_rot            = 32
0.00.050.634 I llm_load_print_meta: n_swa            = 0
0.00.050.635 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.637 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.638 I llm_load_print_meta: n_gqa            = 1
0.00.050.639 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.639 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.640 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.640 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.641 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.641 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.641 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.641 I llm_load_print_meta: n_ff             = 8192
0.00.050.642 I llm_load_print_meta: n_expert         = 0
0.00.050.642 I llm_load_print_meta: n_expert_used    = 0
0.00.050.642 I llm_load_print_meta: causal attn      = 1
0.00.050.642 I llm_load_print_meta: pooling type     = 0
0.00.050.642 I llm_load_print_meta: rope type        = 2
0.00.050.642 I llm_load_print_meta: rope scaling     = linear
0.00.050.643 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.647 I llm_load_print_meta: freq_scale_train = 1
0.00.050.647 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.647 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.648 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.648 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.648 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.648 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.648 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.648 I llm_load_print_meta: model type       = 1.4B
0.00.050.649 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.649 I llm_load_print_meta: model params     = 1.41 B
0.00.050.650 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.651 I llm_load_print_meta: general.name     = 1.4B
0.00.050.652 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.652 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.652 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.652 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.652 I llm_load_print_meta: LF token         = 128 ''
0.00.050.653 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.653 I llm_load_print_meta: max token length = 1024
0.00.052.398 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.398 I llm_load_tensors: offloading output layer to GPU
0.00.052.398 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.408 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.409 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.253 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.253 I llama_new_context_with_model: n_ctx         = 128
0.00.053.254 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.254 I llama_new_context_with_model: n_batch       = 128
0.00.053.254 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.254 I llama_new_context_with_model: flash_attn    = 0
0.00.053.254 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.255 I llama_new_context_with_model: freq_scale    = 1
0.00.053.255 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.255 I ggml_metal_init: allocating
0.00.053.260 I ggml_metal_init: found device: Apple M4
0.00.053.263 I ggml_metal_init: picking default device: Apple M4
0.00.053.823 I ggml_metal_init: using embedded metal library
0.00.056.139 I ggml_metal_init: GPU name:   Apple M4
0.00.056.140 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.140 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.141 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.141 I ggml_metal_init: simdgroup reduction   = true
0.00.056.141 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.141 I ggml_metal_init: has bfloat            = true
0.00.056.141 I ggml_metal_init: use bfloat            = true
0.00.056.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.142 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.657 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.884 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.888 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.901 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.750 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.751 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.751 I llama_new_context_with_model: graph nodes  = 967
0.00.067.752 I llama_new_context_with_model: graph splits = 2
0.00.067.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.374 I 
0.00.757.403 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.416 I perplexity: tokenizing the input ..
0.00.765.099 I perplexity: tokenization took 7.681 ms
0.00.765.105 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.900.722 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.901.830 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.901.843 I llama_perf_context_print:        load time =     745.98 ms
0.00.901.844 I llama_perf_context_print: prompt eval time =     135.40 ms /   128 tokens (    1.06 ms per token,   945.37 tokens per second)
0.00.901.845 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.845 I llama_perf_context_print:       total time =     144.47 ms /   129 tokens
0.00.902.197 I ggml_metal_free: deallocating

real	0m0.917s
user	0m0.077s
sys	0m0.151s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.017.988 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.694 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.034.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.701 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.701 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.701 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.707 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.710 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.713 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.714 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.715 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.715 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.249 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.354 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.357 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.357 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.358 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.046.358 I llama_model_loader: - type  f32:  194 tensors
0.00.046.358 I llama_model_loader: - type q5_1:   97 tensors
0.00.046.359 I llama_model_loader: - type q6_K:    1 tensors
0.00.077.600 I llm_load_vocab: special tokens cache size = 25
0.00.088.639 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.643 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.643 I llm_load_print_meta: arch             = gptneox
0.00.088.644 I llm_load_print_meta: vocab type       = BPE
0.00.088.644 I llm_load_print_meta: n_vocab          = 50304
0.00.088.644 I llm_load_print_meta: n_merges         = 50009
0.00.088.645 I llm_load_print_meta: vocab_only       = 0
0.00.088.645 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.645 I llm_load_print_meta: n_embd           = 2048
0.00.088.645 I llm_load_print_meta: n_layer          = 24
0.00.088.648 I llm_load_print_meta: n_head           = 16
0.00.088.650 I llm_load_print_meta: n_head_kv        = 16
0.00.088.650 I llm_load_print_meta: n_rot            = 32
0.00.088.650 I llm_load_print_meta: n_swa            = 0
0.00.088.651 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.651 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.652 I llm_load_print_meta: n_gqa            = 1
0.00.088.656 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.657 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.659 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.659 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.660 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.660 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.660 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.661 I llm_load_print_meta: n_ff             = 8192
0.00.088.661 I llm_load_print_meta: n_expert         = 0
0.00.088.662 I llm_load_print_meta: n_expert_used    = 0
0.00.088.662 I llm_load_print_meta: causal attn      = 1
0.00.088.664 I llm_load_print_meta: pooling type     = 0
0.00.088.664 I llm_load_print_meta: rope type        = 2
0.00.088.664 I llm_load_print_meta: rope scaling     = linear
0.00.088.665 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.665 I llm_load_print_meta: freq_scale_train = 1
0.00.088.666 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.666 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.666 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.667 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.667 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.667 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.667 I llm_load_print_meta: model type       = 1.4B
0.00.088.668 I llm_load_print_meta: model ftype      = Q5_1
0.00.088.668 I llm_load_print_meta: model params     = 1.41 B
0.00.088.676 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.088.679 I llm_load_print_meta: general.name     = 1.4B
0.00.088.679 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.680 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.680 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.680 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.680 I llm_load_print_meta: LF token         = 128 ''
0.00.088.681 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.681 I llm_load_print_meta: max token length = 1024
0.00.091.198 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.199 I llm_load_tensors: offloading output layer to GPU
0.00.091.199 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.210 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.091.212 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.092.525 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.526 I llama_new_context_with_model: n_ctx         = 2048
0.00.092.527 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.092.527 I llama_new_context_with_model: n_batch       = 2048
0.00.092.527 I llama_new_context_with_model: n_ubatch      = 512
0.00.092.527 I llama_new_context_with_model: flash_attn    = 0
0.00.092.528 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.528 I llama_new_context_with_model: freq_scale    = 1
0.00.092.529 I ggml_metal_init: allocating
0.00.092.533 I ggml_metal_init: found device: Apple M4
0.00.092.535 I ggml_metal_init: picking default device: Apple M4
0.00.093.356 I ggml_metal_init: using embedded metal library
0.00.097.145 I ggml_metal_init: GPU name:   Apple M4
0.00.097.148 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.148 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.149 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.149 I ggml_metal_init: simdgroup reduction   = true
0.00.097.149 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.149 I ggml_metal_init: has bfloat            = true
0.00.097.149 I ggml_metal_init: use bfloat            = true
0.00.097.150 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.153 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.246 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.133.903 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.133.909 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.133.938 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.134.998 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.135.000 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.135.000 I llama_new_context_with_model: graph nodes  = 967
0.00.135.000 I llama_new_context_with_model: graph splits = 2
0.00.135.018 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.135.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.135.159 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.904.041 I main: llama threadpool init, n_threads = 4
0.00.904.127 I 
0.00.904.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.904.186 I 
0.00.904.555 I sampler seed: 1234
0.00.904.568 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.904.588 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.904.590 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.904.590 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.744.146 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.01.744.146 I llama_perf_context_print:        load time =     886.04 ms
0.01.744.147 I llama_perf_context_print: prompt eval time =      43.23 ms /     7 tokens (    6.18 ms per token,   161.91 tokens per second)
0.01.744.148 I llama_perf_context_print:        eval time =     793.24 ms /    63 runs   (   12.59 ms per token,    79.42 tokens per second)
0.01.744.148 I llama_perf_context_print:       total time =     840.11 ms /    70 tokens
0.01.744.343 I ggml_metal_free: deallocating

real	0m1.772s
user	0m0.146s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.495 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.082 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.086 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.088 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.088 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.089 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.092 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.093 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.093 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.093 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.094 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.094 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.094 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.095 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.099 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.800 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.468 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.469 I llama_model_loader: - type  f32:  194 tensors
0.00.022.469 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.469 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.818 I llm_load_vocab: special tokens cache size = 25
0.00.048.768 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.771 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.771 I llm_load_print_meta: arch             = gptneox
0.00.048.772 I llm_load_print_meta: vocab type       = BPE
0.00.048.772 I llm_load_print_meta: n_vocab          = 50304
0.00.048.772 I llm_load_print_meta: n_merges         = 50009
0.00.048.772 I llm_load_print_meta: vocab_only       = 0
0.00.048.772 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.772 I llm_load_print_meta: n_embd           = 2048
0.00.048.773 I llm_load_print_meta: n_layer          = 24
0.00.048.775 I llm_load_print_meta: n_head           = 16
0.00.048.776 I llm_load_print_meta: n_head_kv        = 16
0.00.048.776 I llm_load_print_meta: n_rot            = 32
0.00.048.776 I llm_load_print_meta: n_swa            = 0
0.00.048.776 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.776 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.777 I llm_load_print_meta: n_gqa            = 1
0.00.048.778 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.779 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.779 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.780 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.782 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.782 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.782 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.783 I llm_load_print_meta: n_ff             = 8192
0.00.048.783 I llm_load_print_meta: n_expert         = 0
0.00.048.783 I llm_load_print_meta: n_expert_used    = 0
0.00.048.783 I llm_load_print_meta: causal attn      = 1
0.00.048.783 I llm_load_print_meta: pooling type     = 0
0.00.048.783 I llm_load_print_meta: rope type        = 2
0.00.048.784 I llm_load_print_meta: rope scaling     = linear
0.00.048.784 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.786 I llm_load_print_meta: freq_scale_train = 1
0.00.048.786 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.787 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.787 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.787 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.787 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.787 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.787 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.788 I llm_load_print_meta: model type       = 1.4B
0.00.048.788 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.788 I llm_load_print_meta: model params     = 1.41 B
0.00.048.789 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.789 I llm_load_print_meta: general.name     = 1.4B
0.00.048.789 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.791 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.791 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.791 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.791 I llm_load_print_meta: LF token         = 128 ''
0.00.048.792 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.792 I llm_load_print_meta: max token length = 1024
0.00.050.534 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.534 I llm_load_tensors: offloading output layer to GPU
0.00.050.534 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.544 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.545 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.376 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.376 I llama_new_context_with_model: n_ctx         = 128
0.00.051.377 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.377 I llama_new_context_with_model: n_batch       = 128
0.00.051.377 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.377 I llama_new_context_with_model: flash_attn    = 0
0.00.051.378 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.378 I llama_new_context_with_model: freq_scale    = 1
0.00.051.378 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.379 I ggml_metal_init: allocating
0.00.051.384 I ggml_metal_init: found device: Apple M4
0.00.051.386 I ggml_metal_init: picking default device: Apple M4
0.00.051.960 I ggml_metal_init: using embedded metal library
0.00.054.301 I ggml_metal_init: GPU name:   Apple M4
0.00.054.302 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.303 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.303 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.303 I ggml_metal_init: simdgroup reduction   = true
0.00.054.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.303 I ggml_metal_init: has bfloat            = true
0.00.054.303 I ggml_metal_init: use bfloat            = true
0.00.054.304 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.304 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.675 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.064.938 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.940 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.953 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.733 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.734 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.734 I llama_new_context_with_model: graph nodes  = 967
0.00.065.734 I llama_new_context_with_model: graph splits = 2
0.00.065.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.331 I 
0.00.811.362 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.374 I perplexity: tokenizing the input ..
0.00.818.950 I perplexity: tokenization took 7.575 ms
0.00.818.954 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.954.106 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.955.256 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.955.267 I llama_perf_context_print:        load time =     802.83 ms
0.00.955.268 I llama_perf_context_print: prompt eval time =     134.93 ms /   128 tokens (    1.05 ms per token,   948.62 tokens per second)
0.00.955.269 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.955.269 I llama_perf_context_print:       total time =     143.94 ms /   129 tokens
0.00.955.674 I ggml_metal_free: deallocating

real	0m0.969s
user	0m0.078s
sys	0m0.193s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.424 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.778 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.783 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.784 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.785 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.785 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.785 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.786 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.787 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.787 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.787 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.790 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.790 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.791 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.791 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.792 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.793 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.793 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.627 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.514 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.515 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.515 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.515 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.516 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.516 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.516 I llama_model_loader: - type  f32:  194 tensors
0.00.023.517 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.517 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.517 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.990 I llm_load_vocab: special tokens cache size = 25
0.00.049.962 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.965 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.965 I llm_load_print_meta: arch             = gptneox
0.00.049.966 I llm_load_print_meta: vocab type       = BPE
0.00.049.966 I llm_load_print_meta: n_vocab          = 50304
0.00.049.966 I llm_load_print_meta: n_merges         = 50009
0.00.049.966 I llm_load_print_meta: vocab_only       = 0
0.00.049.967 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.967 I llm_load_print_meta: n_embd           = 2048
0.00.049.967 I llm_load_print_meta: n_layer          = 24
0.00.049.970 I llm_load_print_meta: n_head           = 16
0.00.049.970 I llm_load_print_meta: n_head_kv        = 16
0.00.049.970 I llm_load_print_meta: n_rot            = 32
0.00.049.971 I llm_load_print_meta: n_swa            = 0
0.00.049.971 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.971 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.972 I llm_load_print_meta: n_gqa            = 1
0.00.049.977 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.978 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.978 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.979 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.979 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.979 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.979 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.980 I llm_load_print_meta: n_ff             = 8192
0.00.049.980 I llm_load_print_meta: n_expert         = 0
0.00.049.980 I llm_load_print_meta: n_expert_used    = 0
0.00.049.980 I llm_load_print_meta: causal attn      = 1
0.00.049.980 I llm_load_print_meta: pooling type     = 0
0.00.049.980 I llm_load_print_meta: rope type        = 2
0.00.049.981 I llm_load_print_meta: rope scaling     = linear
0.00.049.981 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.982 I llm_load_print_meta: freq_scale_train = 1
0.00.049.982 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.983 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.983 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.983 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.983 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.983 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.983 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.983 I llm_load_print_meta: model type       = 1.4B
0.00.049.984 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.984 I llm_load_print_meta: model params     = 1.41 B
0.00.049.985 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.985 I llm_load_print_meta: general.name     = 1.4B
0.00.049.985 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.988 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.988 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.988 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.988 I llm_load_print_meta: LF token         = 128 ''
0.00.049.988 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.989 I llm_load_print_meta: max token length = 1024
0.00.051.741 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.741 I llm_load_tensors: offloading output layer to GPU
0.00.051.741 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.751 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.752 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.618 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.619 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.619 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.619 I llama_new_context_with_model: n_batch       = 2048
0.00.052.619 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.619 I llama_new_context_with_model: flash_attn    = 0
0.00.052.620 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.620 I llama_new_context_with_model: freq_scale    = 1
0.00.052.621 I ggml_metal_init: allocating
0.00.052.628 I ggml_metal_init: found device: Apple M4
0.00.052.630 I ggml_metal_init: picking default device: Apple M4
0.00.053.252 I ggml_metal_init: using embedded metal library
0.00.055.569 I ggml_metal_init: GPU name:   Apple M4
0.00.055.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.571 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.571 I ggml_metal_init: simdgroup reduction   = true
0.00.055.571 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.571 I ggml_metal_init: has bfloat            = true
0.00.055.571 I ggml_metal_init: use bfloat            = true
0.00.055.572 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.573 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.097 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.889 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.896 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.919 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.818 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.819 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.819 I llama_new_context_with_model: graph nodes  = 967
0.00.085.820 I llama_new_context_with_model: graph splits = 2
0.00.085.836 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.991 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.991 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.265 I main: llama threadpool init, n_threads = 4
0.00.445.315 I 
0.00.445.338 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.445.338 I 
0.00.445.498 I sampler seed: 1234
0.00.445.505 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.445.521 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.445.521 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.445.521 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.135.230 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.135.233 I llama_perf_context_print:        load time =     435.84 ms
0.01.135.233 I llama_perf_context_print: prompt eval time =      36.12 ms /     7 tokens (    5.16 ms per token,   193.81 tokens per second)
0.01.135.234 I llama_perf_context_print:        eval time =     650.58 ms /    63 runs   (   10.33 ms per token,    96.84 tokens per second)
0.01.135.234 I llama_perf_context_print:       total time =     689.97 ms /    70 tokens
0.01.135.404 I ggml_metal_free: deallocating

real	0m1.151s
user	0m0.109s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.958 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.368 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.372 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.377 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.377 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.378 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.378 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.378 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.379 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.380 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.380 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.380 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.382 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.383 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.383 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.385 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.385 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.385 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.060 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.033 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.669 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.669 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.670 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.670 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.670 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.671 I llama_model_loader: - type  f32:  194 tensors
0.00.023.671 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.671 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.671 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.267 I llm_load_vocab: special tokens cache size = 25
0.00.049.025 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.028 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.028 I llm_load_print_meta: arch             = gptneox
0.00.049.028 I llm_load_print_meta: vocab type       = BPE
0.00.049.029 I llm_load_print_meta: n_vocab          = 50304
0.00.049.029 I llm_load_print_meta: n_merges         = 50009
0.00.049.029 I llm_load_print_meta: vocab_only       = 0
0.00.049.029 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.029 I llm_load_print_meta: n_embd           = 2048
0.00.049.029 I llm_load_print_meta: n_layer          = 24
0.00.049.032 I llm_load_print_meta: n_head           = 16
0.00.049.032 I llm_load_print_meta: n_head_kv        = 16
0.00.049.032 I llm_load_print_meta: n_rot            = 32
0.00.049.033 I llm_load_print_meta: n_swa            = 0
0.00.049.033 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.033 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.034 I llm_load_print_meta: n_gqa            = 1
0.00.049.034 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.035 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.036 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.036 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.036 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.036 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.036 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.037 I llm_load_print_meta: n_ff             = 8192
0.00.049.038 I llm_load_print_meta: n_expert         = 0
0.00.049.038 I llm_load_print_meta: n_expert_used    = 0
0.00.049.038 I llm_load_print_meta: causal attn      = 1
0.00.049.038 I llm_load_print_meta: pooling type     = 0
0.00.049.039 I llm_load_print_meta: rope type        = 2
0.00.049.039 I llm_load_print_meta: rope scaling     = linear
0.00.049.039 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.040 I llm_load_print_meta: freq_scale_train = 1
0.00.049.041 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.041 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.041 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.042 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.042 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.042 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.042 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.042 I llm_load_print_meta: model type       = 1.4B
0.00.049.043 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.043 I llm_load_print_meta: model params     = 1.41 B
0.00.049.043 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.044 I llm_load_print_meta: general.name     = 1.4B
0.00.049.044 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.044 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.044 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.044 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.045 I llm_load_print_meta: LF token         = 128 ''
0.00.049.046 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.046 I llm_load_print_meta: max token length = 1024
0.00.050.743 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.743 I llm_load_tensors: offloading output layer to GPU
0.00.050.744 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.753 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.754 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.579 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.579 I llama_new_context_with_model: n_ctx         = 128
0.00.051.580 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.580 I llama_new_context_with_model: n_batch       = 128
0.00.051.580 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.580 I llama_new_context_with_model: flash_attn    = 0
0.00.051.581 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.581 I llama_new_context_with_model: freq_scale    = 1
0.00.051.581 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.582 I ggml_metal_init: allocating
0.00.051.587 I ggml_metal_init: found device: Apple M4
0.00.051.590 I ggml_metal_init: picking default device: Apple M4
0.00.052.160 I ggml_metal_init: using embedded metal library
0.00.054.547 I ggml_metal_init: GPU name:   Apple M4
0.00.054.549 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.550 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.550 I ggml_metal_init: simdgroup reduction   = true
0.00.054.550 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.550 I ggml_metal_init: has bfloat            = true
0.00.054.550 I ggml_metal_init: use bfloat            = true
0.00.054.551 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.551 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.967 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.426 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.430 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.443 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.327 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.328 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.328 I llama_new_context_with_model: graph nodes  = 967
0.00.066.329 I llama_new_context_with_model: graph splits = 2
0.00.066.341 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.454.699 I 
0.00.454.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.454.741 I perplexity: tokenizing the input ..
0.00.462.146 I perplexity: tokenization took 7.403 ms
0.00.462.152 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.594.427 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.595.568 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.595.584 I llama_perf_context_print:        load time =     444.74 ms
0.00.595.585 I llama_perf_context_print: prompt eval time =     132.06 ms /   128 tokens (    1.03 ms per token,   969.29 tokens per second)
0.00.595.586 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.595.586 I llama_perf_context_print:       total time =     140.89 ms /   129 tokens
0.00.596.171 I ggml_metal_free: deallocating

real	0m0.611s
user	0m0.077s
sys	0m0.099s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.065 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.847 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.852 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.855 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.856 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.856 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.856 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.857 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.858 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.858 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.858 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.861 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.861 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.862 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.862 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.864 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.866 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.655 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.660 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.457 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.458 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.459 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.459 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.459 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.460 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.460 I llama_model_loader: - type  f32:  194 tensors
0.00.024.461 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.461 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.461 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.461 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.188 I llm_load_vocab: special tokens cache size = 25
0.00.050.051 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.054 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.055 I llm_load_print_meta: arch             = gptneox
0.00.050.055 I llm_load_print_meta: vocab type       = BPE
0.00.050.055 I llm_load_print_meta: n_vocab          = 50304
0.00.050.055 I llm_load_print_meta: n_merges         = 50009
0.00.050.056 I llm_load_print_meta: vocab_only       = 0
0.00.050.056 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.056 I llm_load_print_meta: n_embd           = 2048
0.00.050.056 I llm_load_print_meta: n_layer          = 24
0.00.050.059 I llm_load_print_meta: n_head           = 16
0.00.050.059 I llm_load_print_meta: n_head_kv        = 16
0.00.050.059 I llm_load_print_meta: n_rot            = 32
0.00.050.060 I llm_load_print_meta: n_swa            = 0
0.00.050.060 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.060 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.061 I llm_load_print_meta: n_gqa            = 1
0.00.050.062 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.062 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.063 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.063 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.063 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.064 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.064 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.065 I llm_load_print_meta: n_ff             = 8192
0.00.050.065 I llm_load_print_meta: n_expert         = 0
0.00.050.065 I llm_load_print_meta: n_expert_used    = 0
0.00.050.065 I llm_load_print_meta: causal attn      = 1
0.00.050.065 I llm_load_print_meta: pooling type     = 0
0.00.050.065 I llm_load_print_meta: rope type        = 2
0.00.050.066 I llm_load_print_meta: rope scaling     = linear
0.00.050.066 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.066 I llm_load_print_meta: freq_scale_train = 1
0.00.050.067 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.067 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.067 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.069 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.070 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.070 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.070 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.070 I llm_load_print_meta: model type       = 1.4B
0.00.050.070 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.071 I llm_load_print_meta: model params     = 1.41 B
0.00.050.071 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.071 I llm_load_print_meta: general.name     = 1.4B
0.00.050.072 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.072 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.072 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.072 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.073 I llm_load_print_meta: LF token         = 128 ''
0.00.050.073 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.073 I llm_load_print_meta: max token length = 1024
0.00.051.774 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.774 I llm_load_tensors: offloading output layer to GPU
0.00.051.774 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.784 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.785 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.611 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.612 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.612 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.612 I llama_new_context_with_model: n_batch       = 2048
0.00.052.613 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.613 I llama_new_context_with_model: flash_attn    = 0
0.00.052.613 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.613 I llama_new_context_with_model: freq_scale    = 1
0.00.052.614 I ggml_metal_init: allocating
0.00.052.617 I ggml_metal_init: found device: Apple M4
0.00.052.619 I ggml_metal_init: picking default device: Apple M4
0.00.053.224 I ggml_metal_init: using embedded metal library
0.00.055.503 I ggml_metal_init: GPU name:   Apple M4
0.00.055.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.505 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.505 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.506 I ggml_metal_init: simdgroup reduction   = true
0.00.055.506 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.506 I ggml_metal_init: has bfloat            = true
0.00.055.506 I ggml_metal_init: use bfloat            = true
0.00.055.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.927 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.434 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.441 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.462 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.437 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.439 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.439 I llama_new_context_with_model: graph nodes  = 967
0.00.084.439 I llama_new_context_with_model: graph splits = 2
0.00.084.463 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.598 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.602 I main: llama threadpool init, n_threads = 4
0.00.571.647 I 
0.00.571.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.687 I 
0.00.571.833 I sampler seed: 1234
0.00.571.837 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.571.890 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.571.892 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.571.892 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.318.247 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.318.247 I llama_perf_context_print:        load time =     562.53 ms
0.01.318.248 I llama_perf_context_print: prompt eval time =      40.87 ms /     7 tokens (    5.84 ms per token,   171.28 tokens per second)
0.01.318.249 I llama_perf_context_print:        eval time =     702.43 ms /    63 runs   (   11.15 ms per token,    89.69 tokens per second)
0.01.318.249 I llama_perf_context_print:       total time =     746.65 ms /    70 tokens
0.01.318.412 I ggml_metal_free: deallocating

real	0m1.333s
user	0m0.108s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.269 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.018 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.028 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.028 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.029 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.029 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.030 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.031 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.032 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.033 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.034 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.034 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.708 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.446 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.446 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.447 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.448 I llama_model_loader: - type  f32:  194 tensors
0.00.023.448 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.448 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.449 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.449 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.083 I llm_load_vocab: special tokens cache size = 25
0.00.049.142 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.144 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.145 I llm_load_print_meta: arch             = gptneox
0.00.049.145 I llm_load_print_meta: vocab type       = BPE
0.00.049.145 I llm_load_print_meta: n_vocab          = 50304
0.00.049.145 I llm_load_print_meta: n_merges         = 50009
0.00.049.145 I llm_load_print_meta: vocab_only       = 0
0.00.049.146 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.146 I llm_load_print_meta: n_embd           = 2048
0.00.049.146 I llm_load_print_meta: n_layer          = 24
0.00.049.148 I llm_load_print_meta: n_head           = 16
0.00.049.148 I llm_load_print_meta: n_head_kv        = 16
0.00.049.148 I llm_load_print_meta: n_rot            = 32
0.00.049.148 I llm_load_print_meta: n_swa            = 0
0.00.049.149 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.149 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.149 I llm_load_print_meta: n_gqa            = 1
0.00.049.150 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.151 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.151 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.152 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.152 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.152 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.152 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.153 I llm_load_print_meta: n_ff             = 8192
0.00.049.153 I llm_load_print_meta: n_expert         = 0
0.00.049.153 I llm_load_print_meta: n_expert_used    = 0
0.00.049.153 I llm_load_print_meta: causal attn      = 1
0.00.049.154 I llm_load_print_meta: pooling type     = 0
0.00.049.154 I llm_load_print_meta: rope type        = 2
0.00.049.154 I llm_load_print_meta: rope scaling     = linear
0.00.049.154 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.155 I llm_load_print_meta: freq_scale_train = 1
0.00.049.155 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.155 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.155 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.155 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.155 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.158 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.158 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.158 I llm_load_print_meta: model type       = 1.4B
0.00.049.158 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.159 I llm_load_print_meta: model params     = 1.41 B
0.00.049.159 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.160 I llm_load_print_meta: general.name     = 1.4B
0.00.049.160 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.160 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.160 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.160 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.161 I llm_load_print_meta: LF token         = 128 ''
0.00.049.161 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.161 I llm_load_print_meta: max token length = 1024
0.00.050.815 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.815 I llm_load_tensors: offloading output layer to GPU
0.00.050.815 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.825 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.826 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.661 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.661 I llama_new_context_with_model: n_ctx         = 128
0.00.051.662 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.662 I llama_new_context_with_model: n_batch       = 128
0.00.051.662 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.662 I llama_new_context_with_model: flash_attn    = 0
0.00.051.663 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.663 I llama_new_context_with_model: freq_scale    = 1
0.00.051.663 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.664 I ggml_metal_init: allocating
0.00.051.668 I ggml_metal_init: found device: Apple M4
0.00.051.670 I ggml_metal_init: picking default device: Apple M4
0.00.052.232 I ggml_metal_init: using embedded metal library
0.00.054.747 I ggml_metal_init: GPU name:   Apple M4
0.00.054.749 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.749 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.750 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.750 I ggml_metal_init: simdgroup reduction   = true
0.00.054.750 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.750 I ggml_metal_init: has bfloat            = true
0.00.054.751 I ggml_metal_init: use bfloat            = true
0.00.054.751 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.752 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.153 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.403 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.410 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.429 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.366 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.367 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.367 I llama_new_context_with_model: graph nodes  = 967
0.00.066.368 I llama_new_context_with_model: graph splits = 2
0.00.066.380 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.380 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.243 I 
0.00.513.279 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.291 I perplexity: tokenizing the input ..
0.00.521.469 I perplexity: tokenization took 8.176 ms
0.00.521.474 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.652.523 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.653.674 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.653.689 I llama_perf_context_print:        load time =     503.97 ms
0.00.653.690 I llama_perf_context_print: prompt eval time =     130.82 ms /   128 tokens (    1.02 ms per token,   978.46 tokens per second)
0.00.653.690 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.653.691 I llama_perf_context_print:       total time =     140.45 ms /   129 tokens
0.00.654.198 I ggml_metal_free: deallocating

real	0m0.667s
user	0m0.078s
sys	0m0.096s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.759 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.986 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.991 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.993 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.993 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.993 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.994 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.994 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.995 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.995 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.998 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.998 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.998 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.999 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.999 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.001 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.002 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.002 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.816 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.581 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.582 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.582 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.583 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.583 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.583 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.584 I llama_model_loader: - type  f32:  194 tensors
0.00.023.584 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.584 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.584 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.565 I llm_load_vocab: special tokens cache size = 25
0.00.049.491 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.494 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.495 I llm_load_print_meta: arch             = gptneox
0.00.049.495 I llm_load_print_meta: vocab type       = BPE
0.00.049.495 I llm_load_print_meta: n_vocab          = 50304
0.00.049.495 I llm_load_print_meta: n_merges         = 50009
0.00.049.495 I llm_load_print_meta: vocab_only       = 0
0.00.049.496 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.496 I llm_load_print_meta: n_embd           = 2048
0.00.049.496 I llm_load_print_meta: n_layer          = 24
0.00.049.498 I llm_load_print_meta: n_head           = 16
0.00.049.499 I llm_load_print_meta: n_head_kv        = 16
0.00.049.499 I llm_load_print_meta: n_rot            = 32
0.00.049.499 I llm_load_print_meta: n_swa            = 0
0.00.049.500 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.500 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.501 I llm_load_print_meta: n_gqa            = 1
0.00.049.501 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.502 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.503 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.503 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.503 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.503 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.504 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.504 I llm_load_print_meta: n_ff             = 8192
0.00.049.504 I llm_load_print_meta: n_expert         = 0
0.00.049.506 I llm_load_print_meta: n_expert_used    = 0
0.00.049.508 I llm_load_print_meta: causal attn      = 1
0.00.049.508 I llm_load_print_meta: pooling type     = 0
0.00.049.508 I llm_load_print_meta: rope type        = 2
0.00.049.508 I llm_load_print_meta: rope scaling     = linear
0.00.049.509 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.509 I llm_load_print_meta: freq_scale_train = 1
0.00.049.509 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.510 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.510 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.510 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.510 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.510 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.510 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.511 I llm_load_print_meta: model type       = 1.4B
0.00.049.511 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.511 I llm_load_print_meta: model params     = 1.41 B
0.00.049.512 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.512 I llm_load_print_meta: general.name     = 1.4B
0.00.049.513 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: LF token         = 128 ''
0.00.049.514 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.514 I llm_load_print_meta: max token length = 1024
0.00.051.207 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.207 I llm_load_tensors: offloading output layer to GPU
0.00.051.208 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.218 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.219 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.058 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.059 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.059 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.059 I llama_new_context_with_model: n_batch       = 2048
0.00.052.059 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.060 I llama_new_context_with_model: flash_attn    = 0
0.00.052.060 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.060 I llama_new_context_with_model: freq_scale    = 1
0.00.052.061 I ggml_metal_init: allocating
0.00.052.068 I ggml_metal_init: found device: Apple M4
0.00.052.070 I ggml_metal_init: picking default device: Apple M4
0.00.052.660 I ggml_metal_init: using embedded metal library
0.00.055.015 I ggml_metal_init: GPU name:   Apple M4
0.00.055.017 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.017 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.018 I ggml_metal_init: simdgroup reduction   = true
0.00.055.019 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.019 I ggml_metal_init: has bfloat            = true
0.00.055.019 I ggml_metal_init: use bfloat            = true
0.00.055.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.020 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.395 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.436 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.442 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.473 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.410 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.411 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.412 I llama_new_context_with_model: graph nodes  = 967
0.00.084.412 I llama_new_context_with_model: graph splits = 2
0.00.084.436 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.983 I main: llama threadpool init, n_threads = 4
0.00.653.025 I 
0.00.653.044 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.044 I 
0.00.653.209 I sampler seed: 1234
0.00.653.214 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.653.243 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.653.245 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.653.245 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.409.744 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.409.744 I llama_perf_context_print:        load time =     644.22 ms
0.01.409.745 I llama_perf_context_print: prompt eval time =      47.50 ms /     7 tokens (    6.79 ms per token,   147.37 tokens per second)
0.01.409.746 I llama_perf_context_print:        eval time =     706.10 ms /    63 runs   (   11.21 ms per token,    89.22 tokens per second)
0.01.409.746 I llama_perf_context_print:       total time =     756.76 ms /    70 tokens
0.01.409.954 I ggml_metal_free: deallocating

real	0m1.428s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.626 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.231 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.237 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.237 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.238 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.238 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.239 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.239 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.240 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.240 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.240 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.241 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.243 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.953 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.738 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.739 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.739 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.740 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.740 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.740 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.741 I llama_model_loader: - type  f32:  194 tensors
0.00.024.741 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.741 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.741 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.939 I llm_load_vocab: special tokens cache size = 25
0.00.050.729 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.732 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.732 I llm_load_print_meta: arch             = gptneox
0.00.050.733 I llm_load_print_meta: vocab type       = BPE
0.00.050.733 I llm_load_print_meta: n_vocab          = 50304
0.00.050.733 I llm_load_print_meta: n_merges         = 50009
0.00.050.733 I llm_load_print_meta: vocab_only       = 0
0.00.050.734 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.734 I llm_load_print_meta: n_embd           = 2048
0.00.050.734 I llm_load_print_meta: n_layer          = 24
0.00.050.736 I llm_load_print_meta: n_head           = 16
0.00.050.737 I llm_load_print_meta: n_head_kv        = 16
0.00.050.737 I llm_load_print_meta: n_rot            = 32
0.00.050.738 I llm_load_print_meta: n_swa            = 0
0.00.050.738 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.738 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.739 I llm_load_print_meta: n_gqa            = 1
0.00.050.740 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.740 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.741 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.741 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.741 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.741 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.742 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.743 I llm_load_print_meta: n_ff             = 8192
0.00.050.744 I llm_load_print_meta: n_expert         = 0
0.00.050.744 I llm_load_print_meta: n_expert_used    = 0
0.00.050.744 I llm_load_print_meta: causal attn      = 1
0.00.050.744 I llm_load_print_meta: pooling type     = 0
0.00.050.744 I llm_load_print_meta: rope type        = 2
0.00.050.744 I llm_load_print_meta: rope scaling     = linear
0.00.050.745 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.745 I llm_load_print_meta: freq_scale_train = 1
0.00.050.745 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.746 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.746 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.746 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.746 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.746 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.746 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.747 I llm_load_print_meta: model type       = 1.4B
0.00.050.747 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.749 I llm_load_print_meta: model params     = 1.41 B
0.00.050.750 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.750 I llm_load_print_meta: general.name     = 1.4B
0.00.050.750 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.750 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.751 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.751 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.751 I llm_load_print_meta: LF token         = 128 ''
0.00.050.751 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.751 I llm_load_print_meta: max token length = 1024
0.00.052.493 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.493 I llm_load_tensors: offloading output layer to GPU
0.00.052.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.504 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.505 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.370 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.371 I llama_new_context_with_model: n_ctx         = 128
0.00.053.371 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.371 I llama_new_context_with_model: n_batch       = 128
0.00.053.371 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.372 I llama_new_context_with_model: flash_attn    = 0
0.00.053.372 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.372 I llama_new_context_with_model: freq_scale    = 1
0.00.053.373 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.373 I ggml_metal_init: allocating
0.00.053.379 I ggml_metal_init: found device: Apple M4
0.00.053.382 I ggml_metal_init: picking default device: Apple M4
0.00.053.957 I ggml_metal_init: using embedded metal library
0.00.056.320 I ggml_metal_init: GPU name:   Apple M4
0.00.056.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.323 I ggml_metal_init: simdgroup reduction   = true
0.00.056.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.323 I ggml_metal_init: has bfloat            = true
0.00.056.323 I ggml_metal_init: use bfloat            = true
0.00.056.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.770 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.073 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.080 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.099 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.975 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.976 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.976 I llama_new_context_with_model: graph nodes  = 967
0.00.067.977 I llama_new_context_with_model: graph splits = 2
0.00.067.989 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.061 I 
0.00.653.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.101 I perplexity: tokenizing the input ..
0.00.660.558 I perplexity: tokenization took 7.456 ms
0.00.660.561 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.135 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.796.220 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.796.234 I llama_perf_context_print:        load time =     642.43 ms
0.00.796.235 I llama_perf_context_print: prompt eval time =     134.35 ms /   128 tokens (    1.05 ms per token,   952.71 tokens per second)
0.00.796.236 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.236 I llama_perf_context_print:       total time =     143.17 ms /   129 tokens
0.00.796.679 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.078s
sys	0m0.136s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.552 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.364 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.366 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.366 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.367 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.367 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.367 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.368 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.368 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.369 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.369 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.370 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.370 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.370 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.375 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.326 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.292 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.993 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.993 I llama_model_loader: - type  f32:  194 tensors
0.00.024.994 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.994 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.855 I llm_load_vocab: special tokens cache size = 25
0.00.051.850 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.853 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.853 I llm_load_print_meta: arch             = gptneox
0.00.051.853 I llm_load_print_meta: vocab type       = BPE
0.00.051.853 I llm_load_print_meta: n_vocab          = 50304
0.00.051.854 I llm_load_print_meta: n_merges         = 50009
0.00.051.854 I llm_load_print_meta: vocab_only       = 0
0.00.051.854 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.854 I llm_load_print_meta: n_embd           = 2048
0.00.051.854 I llm_load_print_meta: n_layer          = 24
0.00.051.857 I llm_load_print_meta: n_head           = 16
0.00.051.858 I llm_load_print_meta: n_head_kv        = 16
0.00.051.858 I llm_load_print_meta: n_rot            = 32
0.00.051.858 I llm_load_print_meta: n_swa            = 0
0.00.051.858 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.858 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.859 I llm_load_print_meta: n_gqa            = 1
0.00.051.860 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.863 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.864 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.864 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.864 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.864 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.864 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.865 I llm_load_print_meta: n_ff             = 8192
0.00.051.865 I llm_load_print_meta: n_expert         = 0
0.00.051.865 I llm_load_print_meta: n_expert_used    = 0
0.00.051.867 I llm_load_print_meta: causal attn      = 1
0.00.051.867 I llm_load_print_meta: pooling type     = 0
0.00.051.867 I llm_load_print_meta: rope type        = 2
0.00.051.868 I llm_load_print_meta: rope scaling     = linear
0.00.051.869 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.870 I llm_load_print_meta: freq_scale_train = 1
0.00.051.870 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.870 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.870 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.870 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.871 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.871 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.871 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.871 I llm_load_print_meta: model type       = 1.4B
0.00.051.872 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.872 I llm_load_print_meta: model params     = 1.41 B
0.00.051.873 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.873 I llm_load_print_meta: general.name     = 1.4B
0.00.051.873 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.877 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.877 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.878 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.878 I llm_load_print_meta: LF token         = 128 ''
0.00.051.878 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.878 I llm_load_print_meta: max token length = 1024
0.00.053.687 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.687 I llm_load_tensors: offloading output layer to GPU
0.00.053.688 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.698 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.699 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.577 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.578 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.578 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.578 I llama_new_context_with_model: n_batch       = 2048
0.00.054.579 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.579 I llama_new_context_with_model: flash_attn    = 0
0.00.054.579 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.579 I llama_new_context_with_model: freq_scale    = 1
0.00.054.580 I ggml_metal_init: allocating
0.00.054.583 I ggml_metal_init: found device: Apple M4
0.00.054.585 I ggml_metal_init: picking default device: Apple M4
0.00.055.174 I ggml_metal_init: using embedded metal library
0.00.057.509 I ggml_metal_init: GPU name:   Apple M4
0.00.057.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.512 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.512 I ggml_metal_init: simdgroup reduction   = true
0.00.057.513 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.513 I ggml_metal_init: has bfloat            = true
0.00.057.513 I ggml_metal_init: use bfloat            = true
0.00.057.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.272 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.766 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.771 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.792 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.846 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.849 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.849 I llama_new_context_with_model: graph nodes  = 967
0.00.087.849 I llama_new_context_with_model: graph splits = 2
0.00.087.865 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.006 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.632 I main: llama threadpool init, n_threads = 4
0.00.758.669 I 
0.00.758.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.697 I 
0.00.758.848 I sampler seed: 1234
0.00.758.854 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.891 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.913 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.915 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.602.339 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.602.340 I llama_perf_context_print:        load time =     749.08 ms
0.01.602.341 I llama_perf_context_print: prompt eval time =      51.88 ms /     7 tokens (    7.41 ms per token,   134.93 tokens per second)
0.01.602.341 I llama_perf_context_print:        eval time =     788.48 ms /    63 runs   (   12.52 ms per token,    79.90 tokens per second)
0.01.602.342 I llama_perf_context_print:       total time =     843.71 ms /    70 tokens
0.01.602.554 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.111s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.079 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.663 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.668 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.670 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.671 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.671 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.672 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.673 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.674 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.675 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.676 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.677 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.865 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.866 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.866 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.867 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.867 I llama_model_loader: - type  f32:  194 tensors
0.00.023.867 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.867 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.381 I llm_load_vocab: special tokens cache size = 25
0.00.049.149 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.152 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.152 I llm_load_print_meta: arch             = gptneox
0.00.049.153 I llm_load_print_meta: vocab type       = BPE
0.00.049.153 I llm_load_print_meta: n_vocab          = 50304
0.00.049.153 I llm_load_print_meta: n_merges         = 50009
0.00.049.153 I llm_load_print_meta: vocab_only       = 0
0.00.049.153 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.153 I llm_load_print_meta: n_embd           = 2048
0.00.049.154 I llm_load_print_meta: n_layer          = 24
0.00.049.156 I llm_load_print_meta: n_head           = 16
0.00.049.157 I llm_load_print_meta: n_head_kv        = 16
0.00.049.157 I llm_load_print_meta: n_rot            = 32
0.00.049.157 I llm_load_print_meta: n_swa            = 0
0.00.049.158 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.158 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.161 I llm_load_print_meta: n_gqa            = 1
0.00.049.162 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.163 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.164 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.164 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.164 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.164 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.164 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.165 I llm_load_print_meta: n_ff             = 8192
0.00.049.165 I llm_load_print_meta: n_expert         = 0
0.00.049.166 I llm_load_print_meta: n_expert_used    = 0
0.00.049.166 I llm_load_print_meta: causal attn      = 1
0.00.049.166 I llm_load_print_meta: pooling type     = 0
0.00.049.166 I llm_load_print_meta: rope type        = 2
0.00.049.166 I llm_load_print_meta: rope scaling     = linear
0.00.049.167 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.167 I llm_load_print_meta: freq_scale_train = 1
0.00.049.167 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.168 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.168 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.168 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.168 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.172 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.172 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.173 I llm_load_print_meta: model type       = 1.4B
0.00.049.173 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.174 I llm_load_print_meta: model params     = 1.41 B
0.00.049.174 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.174 I llm_load_print_meta: general.name     = 1.4B
0.00.049.175 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.175 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.175 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.175 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.175 I llm_load_print_meta: LF token         = 128 ''
0.00.049.176 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.176 I llm_load_print_meta: max token length = 1024
0.00.050.825 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.826 I llm_load_tensors: offloading output layer to GPU
0.00.050.826 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.836 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.837 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.660 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.660 I llama_new_context_with_model: n_ctx         = 128
0.00.051.661 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.661 I llama_new_context_with_model: n_batch       = 128
0.00.051.661 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.661 I llama_new_context_with_model: flash_attn    = 0
0.00.051.661 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.662 I llama_new_context_with_model: freq_scale    = 1
0.00.051.662 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.663 I ggml_metal_init: allocating
0.00.051.666 I ggml_metal_init: found device: Apple M4
0.00.051.668 I ggml_metal_init: picking default device: Apple M4
0.00.052.231 I ggml_metal_init: using embedded metal library
0.00.054.516 I ggml_metal_init: GPU name:   Apple M4
0.00.054.518 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.518 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.518 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.519 I ggml_metal_init: simdgroup reduction   = true
0.00.054.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.519 I ggml_metal_init: has bfloat            = true
0.00.054.519 I ggml_metal_init: use bfloat            = true
0.00.054.519 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.737 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.064.979 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.982 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.997 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.838 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.839 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.839 I llama_new_context_with_model: graph nodes  = 967
0.00.065.840 I llama_new_context_with_model: graph splits = 2
0.00.065.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.828 I 
0.00.699.856 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.868 I perplexity: tokenizing the input ..
0.00.707.450 I perplexity: tokenization took 7.58 ms
0.00.707.455 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.976 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.849.081 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.849.091 I llama_perf_context_print:        load time =     689.74 ms
0.00.849.092 I llama_perf_context_print: prompt eval time =     140.30 ms /   128 tokens (    1.10 ms per token,   912.34 tokens per second)
0.00.849.093 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.093 I llama_perf_context_print:       total time =     149.27 ms /   129 tokens
0.00.849.603 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.077s
sys	0m0.157s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.009.051 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.150 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.152 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.152 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.152 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.117 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.186 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.190 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.192 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.192 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.192 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.193 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.193 I llama_model_loader: - type  f32:  194 tensors
0.00.024.194 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.980 I llm_load_vocab: special tokens cache size = 25
0.00.049.804 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.807 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.807 I llm_load_print_meta: arch             = gptneox
0.00.049.807 I llm_load_print_meta: vocab type       = BPE
0.00.049.808 I llm_load_print_meta: n_vocab          = 50304
0.00.049.808 I llm_load_print_meta: n_merges         = 50009
0.00.049.808 I llm_load_print_meta: vocab_only       = 0
0.00.049.808 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.809 I llm_load_print_meta: n_embd           = 2048
0.00.049.809 I llm_load_print_meta: n_layer          = 24
0.00.049.811 I llm_load_print_meta: n_head           = 16
0.00.049.812 I llm_load_print_meta: n_head_kv        = 16
0.00.049.812 I llm_load_print_meta: n_rot            = 32
0.00.049.812 I llm_load_print_meta: n_swa            = 0
0.00.049.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.812 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.813 I llm_load_print_meta: n_gqa            = 1
0.00.049.814 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.815 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.816 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.816 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.816 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.816 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.816 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.817 I llm_load_print_meta: n_ff             = 8192
0.00.049.817 I llm_load_print_meta: n_expert         = 0
0.00.049.817 I llm_load_print_meta: n_expert_used    = 0
0.00.049.818 I llm_load_print_meta: causal attn      = 1
0.00.049.818 I llm_load_print_meta: pooling type     = 0
0.00.049.818 I llm_load_print_meta: rope type        = 2
0.00.049.819 I llm_load_print_meta: rope scaling     = linear
0.00.049.820 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.820 I llm_load_print_meta: freq_scale_train = 1
0.00.049.820 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.820 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.820 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.821 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.822 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.822 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.823 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.823 I llm_load_print_meta: model type       = 1.4B
0.00.049.823 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.824 I llm_load_print_meta: model params     = 1.41 B
0.00.049.824 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.824 I llm_load_print_meta: general.name     = 1.4B
0.00.049.824 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.824 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.824 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.825 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.825 I llm_load_print_meta: LF token         = 128 ''
0.00.049.825 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.825 I llm_load_print_meta: max token length = 1024
0.00.051.572 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.572 I llm_load_tensors: offloading output layer to GPU
0.00.051.573 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.582 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.583 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.457 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.458 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.458 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.458 I llama_new_context_with_model: n_batch       = 2048
0.00.052.458 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.459 I llama_new_context_with_model: flash_attn    = 0
0.00.052.459 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.459 I llama_new_context_with_model: freq_scale    = 1
0.00.052.460 I ggml_metal_init: allocating
0.00.052.465 I ggml_metal_init: found device: Apple M4
0.00.052.467 I ggml_metal_init: picking default device: Apple M4
0.00.053.040 I ggml_metal_init: using embedded metal library
0.00.055.411 I ggml_metal_init: GPU name:   Apple M4
0.00.055.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.414 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.414 I ggml_metal_init: simdgroup reduction   = true
0.00.055.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.414 I ggml_metal_init: has bfloat            = true
0.00.055.414 I ggml_metal_init: use bfloat            = true
0.00.055.415 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.415 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.868 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.153 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.162 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.184 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.265 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.267 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.267 I llama_new_context_with_model: graph nodes  = 967
0.00.085.267 I llama_new_context_with_model: graph splits = 2
0.00.085.283 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.436 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.437 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.146 I main: llama threadpool init, n_threads = 4
0.00.814.184 I 
0.00.814.225 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.225 I 
0.00.814.374 I sampler seed: 1234
0.00.814.379 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.429 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.433 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.433 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.682.555 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.682.555 I llama_perf_context_print:        load time =     805.09 ms
0.01.682.556 I llama_perf_context_print: prompt eval time =      54.81 ms /     7 tokens (    7.83 ms per token,   127.72 tokens per second)
0.01.682.557 I llama_perf_context_print:        eval time =     810.29 ms /    63 runs   (   12.86 ms per token,    77.75 tokens per second)
0.01.682.557 I llama_perf_context_print:       total time =     868.41 ms /    70 tokens
0.01.682.726 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.108s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4393 (d79d8f39) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.560 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.076 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.080 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.081 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.085 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.086 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.086 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.086 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.087 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.088 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.088 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.091 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.091 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.091 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.094 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.094 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.094 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.815 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.522 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.523 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.524 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.524 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.524 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.525 I llama_model_loader: - type  f32:  194 tensors
0.00.024.525 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.990 I llm_load_vocab: special tokens cache size = 25
0.00.049.810 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.815 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.816 I llm_load_print_meta: arch             = gptneox
0.00.049.816 I llm_load_print_meta: vocab type       = BPE
0.00.049.816 I llm_load_print_meta: n_vocab          = 50304
0.00.049.817 I llm_load_print_meta: n_merges         = 50009
0.00.049.817 I llm_load_print_meta: vocab_only       = 0
0.00.049.817 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.817 I llm_load_print_meta: n_embd           = 2048
0.00.049.817 I llm_load_print_meta: n_layer          = 24
0.00.049.820 I llm_load_print_meta: n_head           = 16
0.00.049.820 I llm_load_print_meta: n_head_kv        = 16
0.00.049.821 I llm_load_print_meta: n_rot            = 32
0.00.049.821 I llm_load_print_meta: n_swa            = 0
0.00.049.821 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.821 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.822 I llm_load_print_meta: n_gqa            = 1
0.00.049.823 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.823 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.827 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.827 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.828 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.828 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.828 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.829 I llm_load_print_meta: n_ff             = 8192
0.00.049.829 I llm_load_print_meta: n_expert         = 0
0.00.049.829 I llm_load_print_meta: n_expert_used    = 0
0.00.049.829 I llm_load_print_meta: causal attn      = 1
0.00.049.830 I llm_load_print_meta: pooling type     = 0
0.00.049.830 I llm_load_print_meta: rope type        = 2
0.00.049.830 I llm_load_print_meta: rope scaling     = linear
0.00.049.830 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.831 I llm_load_print_meta: freq_scale_train = 1
0.00.049.831 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.831 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.831 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.832 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.832 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.833 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.833 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.833 I llm_load_print_meta: model type       = 1.4B
0.00.049.833 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.834 I llm_load_print_meta: model params     = 1.41 B
0.00.049.834 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.834 I llm_load_print_meta: general.name     = 1.4B
0.00.049.836 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.836 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.836 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.837 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.837 I llm_load_print_meta: LF token         = 128 ''
0.00.049.837 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.837 I llm_load_print_meta: max token length = 1024
0.00.051.546 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.547 I llm_load_tensors: offloading output layer to GPU
0.00.051.547 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.557 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.558 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.384 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.385 I llama_new_context_with_model: n_ctx         = 128
0.00.052.386 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.386 I llama_new_context_with_model: n_batch       = 128
0.00.052.386 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.386 I llama_new_context_with_model: flash_attn    = 0
0.00.052.387 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.387 I llama_new_context_with_model: freq_scale    = 1
0.00.052.387 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.388 I ggml_metal_init: allocating
0.00.052.391 I ggml_metal_init: found device: Apple M4
0.00.052.393 I ggml_metal_init: picking default device: Apple M4
0.00.052.957 I ggml_metal_init: using embedded metal library
0.00.055.265 I ggml_metal_init: GPU name:   Apple M4
0.00.055.266 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.266 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.267 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.267 I ggml_metal_init: simdgroup reduction   = true
0.00.055.267 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.267 I ggml_metal_init: has bfloat            = true
0.00.055.267 I ggml_metal_init: use bfloat            = true
0.00.055.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.268 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.805 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.294 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.296 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.309 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.174 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.175 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.176 I llama_new_context_with_model: graph nodes  = 967
0.00.066.176 I llama_new_context_with_model: graph splits = 2
0.00.066.188 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.180.585 I 
0.00.180.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.180.641 I perplexity: tokenizing the input ..
0.00.188.310 I perplexity: tokenization took 7.667 ms
0.00.188.316 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.328.415 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.329.529 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.329.544 I llama_perf_context_print:        load time =     170.02 ms
0.00.329.546 I llama_perf_context_print: prompt eval time =     139.80 ms /   128 tokens (    1.09 ms per token,   915.60 tokens per second)
0.00.329.546 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.329.547 I llama_perf_context_print:       total time =     148.96 ms /   129 tokens
0.00.330.035 I ggml_metal_free: deallocating

real	0m0.343s
user	0m0.076s
sys	0m0.043s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4393 (d79d8f39)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152a0b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152a0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152a0be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152a0c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152a0c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152a0cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152a0d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152a0daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152a0e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152a0e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152a0ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152a0ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152a0fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152a10220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152a10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152a11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152a11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152a11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152a126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152a12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152a135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152a13cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152a143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152a14c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152a153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152a15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152a15c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152a168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152a16e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152a170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152a17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152a17840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152a180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152a18610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152a188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152a18d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152a19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152a196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152a19b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152a19ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152a1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152a1a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152a1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152a1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152a1b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152a1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152a1c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152a1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152a1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152a1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152a1dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152a1e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152a1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152a1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152a1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152a1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152a20000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152a202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152a208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152a210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152a21380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152a21820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152a21cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152a22160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152a22600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152a22aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152a22f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152a233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152a23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152a23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152a241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152a24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152a24b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152a25050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152a255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152a25af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152a26040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152a26590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152a26ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152a27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152a27580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152a27ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152a28020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152a28570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152a28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152a29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152a29560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152a29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152a2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152a2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152a2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152a2aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152a2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152a2ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152a2bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152a2c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152a2ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152a1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152a2cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152a2d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152a2dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152a2e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152a2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152a2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152a2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152a2f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152a2fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152a30120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152a30670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152a30bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152a31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152a31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152a31bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152a32050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152a324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152a32990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152a32e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152a332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152a33770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152a33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152a340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152a34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152a349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152a34e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152a35330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152a357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152a35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152a36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152a365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152a36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152a36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152a37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152a37830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152a37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152a38170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152a38610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152a38ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152a38f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152a393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152a39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152a39d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152a3a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152a3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152a3ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152a3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152a3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152a3b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152a3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152a3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152a3c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152a3cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152a3d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152a3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152a3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152a3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152a3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152a3e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152a3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152a3f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152a3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152a3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152a3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152a402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152a40790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152a40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152a410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152a41570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152a41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152a41eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152a42350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152a427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152a42c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152a43130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152a435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152a43a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152a43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152a443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152a44850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152a44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152a45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152a45630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152a45ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152a45f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152a46410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152a468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152a46d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152a471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152a47690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152a47b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152a47fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152a48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152a48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152a48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152a49300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152a49850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152a49da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152a4a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152a4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152a4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152a4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152a4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152a4bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152a4c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152a4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152a4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152a4d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152a4db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152a4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152a4e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152a4e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152a4f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152a4f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152a4fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152a500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152a50610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152a50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152a510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152a51600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152a51b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152a520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152a525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152a52b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152a53090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152a535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152a53b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152a54080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152a545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152a54b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152a55070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152a555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152a55b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152a56060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152a565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152a56b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152a57050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152a575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152a57af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152a58040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152a58590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152a58ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152a59030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152a59580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152a59ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152a5a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152a5a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152a5aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152a5b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152a5b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152a5bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152a5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152a5c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152a5caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152a5cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152a5d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152a5da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152a5dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152a5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152a5ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152a5efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152a5f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152a5fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152a5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152a60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152a60a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152a60fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152a61500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152a61a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152a61ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152a62390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152a62830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152a62cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152a63170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152a63610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152a63ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152a63f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152a643f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152a64890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152a64d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152a651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152a65670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152a65b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152a65fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152a66500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152a66c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152a67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152a67a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152a68180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152a68440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152a68c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152a68ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152a69500 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.163.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.163.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152b06910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152b06d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152b071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152b07660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152b07ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152b07f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152b083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152b04b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152b04fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152b05410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152b08820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152b08f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152b09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152b0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152b0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152b0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152b0b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152b0bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152b0c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152b0cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152b0d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152b0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152b0e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152b0ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152b0f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152b0f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152b0f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152b0fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152b0ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152b10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152b108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152b10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152b11250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152b11510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152b11980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152b11df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152b12260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152b126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152b12b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152b12fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152b13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152b13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152b13d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152b14170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152b145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152b14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152b14ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152b15330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152b157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152b15c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152b16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152b164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152b16960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152b16dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152b17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152b176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152b17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152b18120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152b18590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152b18a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152b18e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152b192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152b19750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152b19bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152b1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152b1a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152b1a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152b1ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152b1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152b1b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152b1bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152b1bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152b1c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152b1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152b1cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152b1d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152b1d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152b1d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152b1de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152b1e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152b1e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152b1eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152b1f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152b1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152b1f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152b1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152b201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152b20640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152b20ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152b20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152b21390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152b21800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152b21c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152b220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152b22550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152b229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152b22e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152b232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152b23710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152b23b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152b23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152b24460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152b248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152b24d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152b251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152b25620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152b25a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152b25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152b26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152b267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152b26c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152b270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152b27530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152b279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152b27e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152b28280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152b286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152b28b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152b28fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152b29440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152b298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152b29d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152b2a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152b2a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152b2aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152b2aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152b2b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152b2b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152b2bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152b2c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152b2c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152b2c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152b2cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152b2d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152b2d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152b2db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152b2dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152b2e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152b2e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152b2ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152b2f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152b2f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152b2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152b2fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152b30330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152b307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152b30c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152b31080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152b314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152b31960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152b31dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152b32240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152b326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152b32b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152b32f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152b33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152b33870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152b33ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152b34150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152b345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152b34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152b34ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152b35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152b35780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152b35bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152b36060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152b364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152b36940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152b36db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152b37220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152b37690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152b37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152b37f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152b383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152b38850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152b38cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152b39130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152b395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152b39a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152b39e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152b3a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152b3a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152b3abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152b3b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152b3b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152b3b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152b3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152b3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152b3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152b3cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152b3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152b3d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152b3d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152b3dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152b3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152b3e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152b3e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152b3ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152b3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152b3f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152b3fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152b40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152b40490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152b40900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152b40d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152b411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152b41650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152b41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152b42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152b424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152b43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152b432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152b43590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152b43a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152b43e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152b442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152b44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152b44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152b45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152b454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152b45910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152b45d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152b461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152b46660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152b46ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152b46f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152b473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152b47820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152b47c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152b48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152b48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152b489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152b48e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152b492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152b49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152b49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152b4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152b4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152b4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152b4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152b4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152b4b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152b4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152b4bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152b4c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152b4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152b4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152b4d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152b4d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152b4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152b4de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152b4e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152b4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152b4eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152b4eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152b4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152b4f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152b4fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152b501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152b50620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152b50a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152b50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152b51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152b517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152b51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152b520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152b52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152b529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152b52e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152b53280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152b536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152b53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152b53fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152b54440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152b548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152b54d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152b55190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152b55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152b55a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152b55ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152b56350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152b567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152b56c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152b576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152b57dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152b584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152b58c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152b58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152b59330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152b59930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152b59f40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152a0f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x152a0f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152a0f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152a0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152a10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152a106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152a10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152a10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152a11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152a118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152a11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x152a12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x152a12a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152a13200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152a139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152a140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152a147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x152a14eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x152a155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x152a15f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x152a16610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152a16d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152a173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152a17ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152a181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x152a18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152a18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152a18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152a19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152a19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152a19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152a1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152a1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152a1a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152a1ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152a1b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152a1b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152a1b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152a1be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152a1c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152a1c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152a1cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152a1d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x152a1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152a1d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152a1dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152a1e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152a1e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152a1eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152a1ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152a1f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152a1f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x152a1fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152a200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152a20540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152a209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152a20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152a21290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152a21700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152a21b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x152a21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152a22450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152a228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x152a22d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152a231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152a23610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152a23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152a23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152a24360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152a247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152a24c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x152a250b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152a25520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152a25990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152a25e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152a26270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152a266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152a26b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152a26fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152a27430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x152a278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152a27d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152a28180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x152a285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152a28a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152a28ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152a29340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152a297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152a29c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152a2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152a2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152a2a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x152a2ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152a2b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152a2b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152a2bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152a2bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x152a2c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152a2c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152a2ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152a2d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152a2d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152a2da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152a2deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152a2e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152a2e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x152a2ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152a2f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152a2f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152a2f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152a2fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152a30230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152a306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152a30b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152a30f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152a313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152a31860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152a31cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152a32140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152a325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152a32a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152a32e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152a33300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152a33770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152a33be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152a34050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152a344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152a34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152a34da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152a35210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152a35680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152a35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152a35f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152a363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x152a36840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152a36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152a37120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152a37590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152a37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152a37e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152a382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152a38750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x152a38bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152a39030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152a394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152a39910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152a39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152a3a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152a3a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152a3aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152a3af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152a3b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152a3b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152a3bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152a3c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152a3c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152a3c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152a3ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x152a3d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152a3d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152a3dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152a3e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152a3e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152a3e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152a3ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152a3f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x152a3f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152a3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152a3ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152a40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152a40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152a40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152a410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152a41550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152a419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152a41e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152a422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152a42710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152a42b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152a42ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152a43460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152a438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152a43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152a441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152a44620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152a44a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152a44f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152a45370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152a457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152a45c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x152a460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152a46530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152a469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152a46e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152a47280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152a476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152a47b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152a47fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152a48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152a488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152a48d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152a49190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152a49600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152a49a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152a49ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152a4a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152a4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152a4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152a4b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152a4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152a4bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152a4c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152a4c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152a4c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152a4ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152a4d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152a4d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152a4dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152a4e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152a4e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152a4e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152a4ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152a4f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152a4f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152a4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x152a4ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152a50390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152a50800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152a50c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152a510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152a51550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152a519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152a51e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152a522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152a52710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152a52b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152a52ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152a53460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152a538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152a53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152a541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x152a54620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152a54a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152a54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x152a55370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152a557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152a55c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152a560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152a56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152a569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x152a56e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152a57280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152a576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x152a57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152a57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152a58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152a588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x152a58d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152a59190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152a59600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152a59a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152a59ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152a5a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152a5a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152a5ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152a5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152a5b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152a5b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152a5bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152a5c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152a5c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152a5cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152a5cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152a5d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152a5d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152a5dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152a5e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152a5e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152a5ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x152a5eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152a5f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152a5f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152a5fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152a60300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152a609f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152a610e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x152a617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152a61c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152a620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152a62520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152a62990 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.907s
user	0m0.312s
sys	0m0.307s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4393 (d79d8f39)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151f102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151f109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151f10fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151f11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151f11b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151f120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151f12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151f12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151f131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151f136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151f13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151f140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151f14be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151f15390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151f15ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151f162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151f169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151f17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151f17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151f17ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151f18710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151f18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151f19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151f19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151f1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151f1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151f1ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151f1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151f1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151f1c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151f1c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151f1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151f1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151f1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151f1da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151f1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151f1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151f1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151f1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151f1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151f1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151f1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151f203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151f206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151f20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151f212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151f21be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151f221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151f22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151f22e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151f23420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151f23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151f24040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151f24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151f24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151f25170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151f25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151f25a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151f26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151f264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151f26990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151f26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151f272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151f27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151f27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151f280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151f28550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151f289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151f28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151f29330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151f297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151f29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151f2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151f2a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151f2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151f2b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151f2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151f2bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151f2c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151f2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151f2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151f2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151f2d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151f2e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151f2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151f2ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151f2f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151f2f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151f2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151f30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151f306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151f30c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151f31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151f316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151f31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151f218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151f32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151f32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151f32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151f332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151f33800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151f33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151f342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151f347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151f34d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151f35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151f357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151f35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151f36280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151f367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151f371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151f37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151f37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151f37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151f38440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151f388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151f38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151f39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151f396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151f39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151f3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151f3a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151f3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151f3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151f3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151f3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151f3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151f3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151f3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151f3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151f3ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151f3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151f3dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151f3e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151f3e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151f3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151f3eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151f3f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151f3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151f3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151f40120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151f405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151f40a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151f40f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151f413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151f41840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151f41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151f42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151f42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151f42ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151f42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151f43400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151f438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151f43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151f441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151f44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151f44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151f44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151f45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151f45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151f45da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151f46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151f466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151f46b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151f47020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151f474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151f47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151f47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151f482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151f48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151f48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151f49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151f49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151f499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151f49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151f4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151f4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151f4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151f4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151f4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151f4ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151f4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151f4c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151f4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151f4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151f4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151f4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151f4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151f4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151f4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151f4ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151f4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151f4f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151f4fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151f50340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151f50950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151f51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151f515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151f518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151f51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151f524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151f52cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151f53150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151f535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151f53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151f54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151f54790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151f54ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151f55230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151f55780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151f55cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151f56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151f56770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151f56cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151f57210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151f57760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151f57cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151f58200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151f58750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151f58ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151f591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151f59c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151f5a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151f5a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151f5ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151f5b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151f5b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151f5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151f5c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151f5c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151f5cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151f5d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151f5d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151f5dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151f5e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151f5e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151f5ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151f5f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151f5f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151f5fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151f60180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151f606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151f60c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151f61170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151f616c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151f61c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151f62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151f626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151f62c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151f63150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151f636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151f63bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151f64140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151f64690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151f64be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151f65130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151f65680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151f65bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151f66120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151f66670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151f66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151f67060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151f67500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151f679a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151f67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151f682e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151f68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151f68c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151f690c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151f69560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151f69a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151f69ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151f6a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151f6a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151f6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151f6b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151f6b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151f6bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151f6c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151f6cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151f6d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151f6d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151f6dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151f6e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151f6e670 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.094.683 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153004ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153005150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1530055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153005a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153005ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153006310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153006780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153006bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153007060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1530074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153007940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153008020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153008b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1530092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153009b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15300a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15300a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15300b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15300b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15300bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15300c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15300cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15300d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15300dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15300e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15300e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15300e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15300ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15300f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15300f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15300fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15300ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1530103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153010690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153010b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153010f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1530113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153011850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153011cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153012130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1530125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153012a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153012e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1530132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153013760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153013bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153014040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1530144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153014920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153014d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153015200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153015670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153015ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153015f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1530163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153016830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153016da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1530172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153017710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153017b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153017ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153018460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1530188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153018d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1530191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153019620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153019a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153019f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15301a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15301a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15301ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15301b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15301b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15301b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15301be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15301c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15301c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15301cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15301cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15301d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15301d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15301dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15301e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15301e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15301ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15301eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15301f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15301f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15301fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1530200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153020510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153020980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153020df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153021260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1530216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153021b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153021fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153022420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153022890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153022d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153023170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1530235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153023a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153023ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153024330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1530247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153024c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153025080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1530254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153025960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153025dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153026240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1530266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153026b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153026f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153027400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153027870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153027ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153028150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1530285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153028a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153028ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153029310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153029780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153029bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15302a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15302a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15302a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15302adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15302b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15302b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15302bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15302bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15302c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15302c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15302ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15302d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15302d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15302da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15302de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15302e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15302e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15302ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15302f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15302f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15302f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15302fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153030200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153030670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153030ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153030f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1530313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153031830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153031ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153032110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153032580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1530329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153032e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1530332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153033740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153033bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153034020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153034490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153034900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153034d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1530351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153035650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153035ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153035f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1530363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153036810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153036c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1530370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153037560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1530379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153037e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1530382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153038720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153038b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153039000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153039470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1530398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153039d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15303a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15303a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15303aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15303af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15303b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15303b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15303bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15303c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15303c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15303c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15303ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15303d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15303d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15303db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15303dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15303e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15303e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15303ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15303f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15303f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15303fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15303fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153040360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1530407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153040d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1530411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153041640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153042190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153042450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153042710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153042b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153042ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153043460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1530438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153043d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1530441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153044620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153044a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153044f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153045370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1530457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153045c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1530460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153046530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1530469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153046e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153047280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1530476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153047b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153047fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153048440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1530488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153048d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153049190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153049600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153049a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153049ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15304a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15304a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15304ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15304b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15304b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15304b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15304bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15304c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15304c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15304cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15304cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15304d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15304d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15304dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15304e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15304e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15304ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15304eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15304f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15304f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15304fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153050080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1530504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153050960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153050dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153051240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1530516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153051b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x153051f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153052400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153052870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153052ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153053150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1530535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153053a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153053ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153054310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153054780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153054bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153055060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1530554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153055940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153055db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153056820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153056f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153057660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153057d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153058040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1530584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153058ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1530590c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149d044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149d04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149d04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149d05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149d056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149d05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149d05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149d063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149d06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149d06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149d07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149d07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149d08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149d08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149d09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149d09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149d0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149d0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149d0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149d0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149d0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149d0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149d0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149d0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149d0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149d0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149d0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149d0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149d0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149d0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149d0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149d0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149d0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149d0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149d102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149d10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149d10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149d10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149d11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149d118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149d11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149d121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149d12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149d12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149d12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149d13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149d137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149d13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149d140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149d14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149d149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149d14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149d15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149d156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149d15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149d15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149d16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149d16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149d16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149d17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149d17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149d17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149d18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149d184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149d18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149d18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149d19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149d196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149d19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149d19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149d1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149d1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149d1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149d1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149d1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149d1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149d1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149d1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149d1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149d1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149d1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149d1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149d1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149d1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149d1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149d1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149d1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149d1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149d1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149d1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149d1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149d20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149d20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149d20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149d20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149d212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149d21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149d21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149d22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149d224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149d22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149d22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149d231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149d23660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149d23ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149d23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149d243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149d24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149d24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149d25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149d25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149d259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149d25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149d262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149d26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149d26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149d27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149d27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149d278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149d27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149d281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149d28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149d28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149d28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149d29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149d29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149d29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149d2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149d2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149d2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149d2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149d2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149d2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149d2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149d2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149d2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149d2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149d2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149d2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149d2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149d2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149d2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149d2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149d2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149d2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149d2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149d2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149d2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149d2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149d30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149d306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149d30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149d30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149d31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149d318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149d31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149d32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149d32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149d32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149d32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149d33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149d337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149d33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149d340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149d34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149d34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149d34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149d35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149d356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149d35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149d35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149d36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149d36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149d36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149d37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149d375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149d37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149d37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149d38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149d387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149d38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149d39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149d394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149d39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149d39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149d3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149d3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149d3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149d3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149d3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149d3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149d3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149d3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149d3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149d3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149d3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149d3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149d3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149d3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149d3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149d3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149d3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149d3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149d3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149d3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149d3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149d3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149d40500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149d40970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149d40de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149d41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149d41bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149d41eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149d42320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149d42790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149d42c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149d43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149d434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149d43950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149d43dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149d44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149d446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149d44b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149d44f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149d453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149d45860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149d45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149d46140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149d465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149d46a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149d46e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149d47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149d47770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149d47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149d48050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149d484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149d48930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149d48da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149d49210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149d49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149d49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149d49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149d4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149d4a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149d4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149d4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149d4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149d4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149d4c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149d4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149d4caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149d4cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149d4d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149d4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149d4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149d4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149d4e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149d4e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149d4ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149d4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149d4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149d4fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149d4ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149d50450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149d508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149d50d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149d511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149d51610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149d51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149d51ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149d52360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149d527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149d52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149d530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149d53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149d53990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149d53e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149d54270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149d546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149d54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149d54fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149d55430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149d558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149d56310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149d56a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149d57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149d57870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149d57b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149d57fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149d585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149d58bb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.915s
user	0m0.244s
sys	0m0.133s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.71 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.26 sec*proc (2 tests)

Total Test time (real) =   1.28 sec
        1.30 real         0.74 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
