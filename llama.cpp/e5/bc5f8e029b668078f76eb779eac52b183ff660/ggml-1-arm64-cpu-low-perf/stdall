+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/e5/bc5f8e029b668078f76eb779eac52b183ff660/ggml-1-arm64-cpu-low-perf/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/e5/bc5f8e029b668078f76eb779eac52b183ff660/ggml-1-arm64-cpu-low-perf/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_noi8mm
-- Performing Test GGML_MACHINE_SUPPORTS_noi8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosme
-- Performing Test GGML_MACHINE_SUPPORTS_nosme - Failed
-- ARM feature DOTPROD enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=ares+crypto+noprofile+dotprod+noi8mm+nosve 
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m2.287s
user	0m1.450s
sys	0m0.636s
+ tee -a /home/ggml/results/llama.cpp/e5/bc5f8e029b668078f76eb779eac52b183ff660/ggml-1-arm64-cpu-low-perf/ctest_debug-make.log
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Built target xxhash
[  2%] Built target sha256
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Built target sha1
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  6%] Linking CXX shared library ../../bin/libggml-base.so
[  6%] Built target ggml-base
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../bin/libggml-cpu.so
[ 10%] Built target ggml-cpu
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml.so
[ 11%] Built target ggml
[ 12%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 12%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 13%] Linking CXX executable ../../bin/llama-gguf
[ 14%] Linking CXX executable ../../bin/llama-gguf-hash
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Built target llama-gguf
[ 18%] Built target llama-gguf-hash
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual size_t llama_context_enc_dec::state_get_size()’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:4986:8: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 4986 | size_t llama_context_enc_dec::state_get_size() {
      |        ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual size_t llama_context_enc_dec::state_get_data(uint8_t*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:4990:8: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 4990 | size_t llama_context_enc_dec::state_get_data(      uint8_t * dst, size_t size) {
      |        ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual size_t llama_context_enc_dec::state_set_data(const uint8_t*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:4996:8: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 4996 | size_t llama_context_enc_dec::state_set_data(const uint8_t * src, size_t size) {
      |        ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual size_t llama_context_enc_dec::state_seq_get_size(llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:5002:8: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 5002 | size_t llama_context_enc_dec::state_seq_get_size(llama_seq_id seq_id) {
      |        ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual size_t llama_context_enc_dec::state_seq_get_data(llama_seq_id, uint8_t*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:5007:8: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 5007 | size_t llama_context_enc_dec::state_seq_get_data(llama_seq_id seq_id,       uint8_t * dst, size_t size) {
      |        ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual size_t llama_context_enc_dec::state_seq_set_data(llama_seq_id, const uint8_t*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:5014:8: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 5014 | size_t llama_context_enc_dec::state_seq_set_data(llama_seq_id seq_id, const uint8_t * src, size_t size) {
      |        ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual bool llama_context_enc_dec::state_load_file(const char*, llama_token*, size_t, size_t*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:5021:6: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 5021 | bool llama_context_enc_dec::state_load_file(
      |      ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual bool llama_context_enc_dec::state_save_file(const char*, const llama_token*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:5033:6: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 5033 | bool llama_context_enc_dec::state_save_file(
      |      ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual size_t llama_context_enc_dec::state_seq_load_file(llama_seq_id, const char*, llama_token*, size_t, size_t*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:5043:8: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 5043 | size_t llama_context_enc_dec::state_seq_load_file(
      |        ^~~~~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual size_t llama_context_enc_dec::state_seq_save_file(llama_seq_id, const char*, const llama_token*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:5057:8: error: function might be candidate for attribute ‘noreturn’ [-Werror=suggest-attribute=noreturn]
 5057 | size_t llama_context_enc_dec::state_seq_save_file(
      |        ^~~~~~~~~~~~~~~~~~~~~
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/Makefile2:1771: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m4.856s
user	0m6.681s
sys	0m1.203s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/e5/bc5f8e029b668078f76eb779eac52b183ff660/ggml-1-arm64-cpu-low-perf/ctest_debug-ctest.log: No such file or directory
