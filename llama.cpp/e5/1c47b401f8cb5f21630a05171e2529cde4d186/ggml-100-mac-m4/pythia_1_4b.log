Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.640s
user	0m0.898s
sys	0m1.268s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Built target sha1
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Linking C executable ../bin/test-c
[ 34%] Built target llava
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target test-c
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-sampling
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-log
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Linking CXX executable ../bin/test-chat-template
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-chat-template
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-gguf
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-model-load-cancel
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-backend-ops
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-fns
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-quantize-perf
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Built target llama-batched-bench
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-embedding
[ 71%] Built target llama-batched
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-eval-callback
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-gbnf-validator
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-gguf-split
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-imatrix
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Built target llama-infill
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-cli
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-passkey
[ 85%] Generating index.html.gz.hpp
[ 85%] Built target llama-perplexity
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Built target llama-quantize
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-gen-docs
[ 95%] Built target llama-tts
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.039s
user	0m6.210s
sys	0m9.387s

main: quantize time =  4384.02 ms
main:    total time =  4384.02 ms

main: quantize time =  2199.21 ms
main:    total time =  2199.21 ms

main: quantize time =  2902.67 ms
main:    total time =  2902.67 ms

main: quantize time =  4036.22 ms
main:    total time =  4036.22 ms

main: quantize time =  2862.05 ms
main:    total time =  2862.05 ms

main: quantize time =  5830.75 ms
main:    total time =  5830.75 ms

main: quantize time =  5677.70 ms
main:    total time =  5677.70 ms

main: quantize time =  7156.03 ms
main:    total time =  7156.03 ms

main: quantize time =  5934.57 ms
main:    total time =  5934.57 ms

main: quantize time =  4536.38 ms
main:    total time =  4536.38 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.148 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.321 I main: llama backend init
0.00.000.328 I main: load the model and apply lora adapter, if any
0.00.031.368 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.713 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.751 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.756 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.757 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.757 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.758 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.758 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.761 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.762 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.764 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.764 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.773 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.774 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.028 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.063.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.032 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.033 I llama_model_loader: - type  f32:  194 tensors
0.00.063.034 I llama_model_loader: - type  f16:   98 tensors
0.00.063.035 I print_info: file format = GGUF V3 (latest)
0.00.063.038 I print_info: file type   = all F32 (guessed)
0.00.063.040 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.076.767 I load: special tokens cache size = 25
0.00.085.229 I load: token to piece cache size = 0.2984 MB
0.00.085.234 I print_info: arch             = gptneox
0.00.085.234 I print_info: vocab_only       = 0
0.00.085.234 I print_info: n_ctx_train      = 2048
0.00.085.234 I print_info: n_embd           = 2048
0.00.085.235 I print_info: n_layer          = 24
0.00.085.239 I print_info: n_head           = 16
0.00.085.243 I print_info: n_head_kv        = 16
0.00.085.243 I print_info: n_rot            = 32
0.00.085.243 I print_info: n_swa            = 0
0.00.085.244 I print_info: n_embd_head_k    = 128
0.00.085.244 I print_info: n_embd_head_v    = 128
0.00.085.246 I print_info: n_gqa            = 1
0.00.085.247 I print_info: n_embd_k_gqa     = 2048
0.00.085.248 I print_info: n_embd_v_gqa     = 2048
0.00.085.249 I print_info: f_norm_eps       = 1.0e-05
0.00.085.249 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.249 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.250 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.251 I print_info: f_logit_scale    = 0.0e+00
0.00.085.252 I print_info: n_ff             = 8192
0.00.085.253 I print_info: n_expert         = 0
0.00.085.253 I print_info: n_expert_used    = 0
0.00.085.253 I print_info: causal attn      = 1
0.00.085.253 I print_info: pooling type     = 0
0.00.085.253 I print_info: rope type        = 2
0.00.085.254 I print_info: rope scaling     = linear
0.00.085.254 I print_info: freq_base_train  = 10000.0
0.00.085.254 I print_info: freq_scale_train = 1
0.00.085.255 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.255 I print_info: rope_finetuned   = unknown
0.00.085.255 I print_info: ssm_d_conv       = 0
0.00.085.255 I print_info: ssm_d_inner      = 0
0.00.085.255 I print_info: ssm_d_state      = 0
0.00.085.255 I print_info: ssm_dt_rank      = 0
0.00.085.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.256 I print_info: model type       = 1.4B
0.00.085.256 I print_info: model params     = 1.41 B
0.00.085.257 I print_info: general.name     = 1.4B
0.00.085.259 I print_info: vocab type       = BPE
0.00.085.259 I print_info: n_vocab          = 50304
0.00.085.259 I print_info: n_merges         = 50009
0.00.085.259 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.260 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.260 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.266 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.268 I print_info: LF token         = 128 'Ä'
0.00.085.269 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.269 I print_info: max token length = 1024
0.00.126.520 I load_tensors: offloading 24 repeating layers to GPU
0.00.126.524 I load_tensors: offloading output layer to GPU
0.00.126.524 I load_tensors: offloaded 25/25 layers to GPU
0.00.126.549 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.126.550 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.126.820 I llama_init_from_model: n_seq_max     = 1
0.00.126.821 I llama_init_from_model: n_ctx         = 2048
0.00.126.822 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.126.822 I llama_init_from_model: n_batch       = 2048
0.00.126.822 I llama_init_from_model: n_ubatch      = 512
0.00.126.822 I llama_init_from_model: flash_attn    = 0
0.00.126.823 I llama_init_from_model: freq_base     = 10000.0
0.00.126.823 I llama_init_from_model: freq_scale    = 1
0.00.126.824 I ggml_metal_init: allocating
0.00.126.842 I ggml_metal_init: found device: Apple M4
0.00.126.848 I ggml_metal_init: picking default device: Apple M4
0.00.127.434 I ggml_metal_init: using embedded metal library
0.00.136.392 I ggml_metal_init: GPU name:   Apple M4
0.00.136.394 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.136.394 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.136.395 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.136.395 I ggml_metal_init: simdgroup reduction   = true
0.00.136.395 I ggml_metal_init: simdgroup matrix mul. = true
0.00.136.395 I ggml_metal_init: has residency sets    = true
0.00.136.395 I ggml_metal_init: has bfloat            = true
0.00.136.395 I ggml_metal_init: use bfloat            = true
0.00.136.396 I ggml_metal_init: hasUnifiedMemory      = true
0.00.136.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.159.121 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.186.828 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.186.836 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.186.856 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.191.341 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.191.344 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.191.344 I llama_init_from_model: graph nodes  = 967
0.00.191.344 I llama_init_from_model: graph splits = 2
0.00.191.348 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.191.476 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.191.477 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.257.156 I main: llama threadpool init, n_threads = 4
0.00.257.198 I 
0.00.257.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.257.230 I 
0.00.257.273 I sampler seed: 1234
0.00.257.278 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.257.302 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.257.304 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.257.304 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.080.241 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.02.080.242 I llama_perf_context_print:        load time =     224.74 ms
0.02.080.244 I llama_perf_context_print: prompt eval time =      43.64 ms /     7 tokens (    6.23 ms per token,   160.41 tokens per second)
0.02.080.245 I llama_perf_context_print:        eval time =    1776.44 ms /    63 runs   (   28.20 ms per token,    35.46 tokens per second)
0.02.080.246 I llama_perf_context_print:       total time =    1824.12 ms /    70 tokens
0.02.080.458 I ggml_metal_free: deallocating

real	0m2.407s
user	0m0.130s
sys	0m0.132s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.934 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.076 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.081 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.083 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.083 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.084 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.084 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.084 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.085 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.086 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.086 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.087 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.087 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.090 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.090 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.090 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.999 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.830 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.832 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.832 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.832 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.833 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.833 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.834 I llama_model_loader: - type  f32:  194 tensors
0.00.026.834 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.835 I print_info: file format = GGUF V3 (latest)
0.00.026.835 I print_info: file type   = Q8_0
0.00.026.836 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.367 I load: special tokens cache size = 25
0.00.041.377 I load: token to piece cache size = 0.2984 MB
0.00.041.383 I print_info: arch             = gptneox
0.00.041.384 I print_info: vocab_only       = 0
0.00.041.384 I print_info: n_ctx_train      = 2048
0.00.041.386 I print_info: n_embd           = 2048
0.00.041.386 I print_info: n_layer          = 24
0.00.041.392 I print_info: n_head           = 16
0.00.041.393 I print_info: n_head_kv        = 16
0.00.041.393 I print_info: n_rot            = 32
0.00.041.393 I print_info: n_swa            = 0
0.00.041.393 I print_info: n_embd_head_k    = 128
0.00.041.393 I print_info: n_embd_head_v    = 128
0.00.041.394 I print_info: n_gqa            = 1
0.00.041.395 I print_info: n_embd_k_gqa     = 2048
0.00.041.395 I print_info: n_embd_v_gqa     = 2048
0.00.041.396 I print_info: f_norm_eps       = 1.0e-05
0.00.041.396 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.397 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.397 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.397 I print_info: f_logit_scale    = 0.0e+00
0.00.041.398 I print_info: n_ff             = 8192
0.00.041.398 I print_info: n_expert         = 0
0.00.041.398 I print_info: n_expert_used    = 0
0.00.041.399 I print_info: causal attn      = 1
0.00.041.399 I print_info: pooling type     = 0
0.00.041.399 I print_info: rope type        = 2
0.00.041.399 I print_info: rope scaling     = linear
0.00.041.400 I print_info: freq_base_train  = 10000.0
0.00.041.400 I print_info: freq_scale_train = 1
0.00.041.400 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.400 I print_info: rope_finetuned   = unknown
0.00.041.400 I print_info: ssm_d_conv       = 0
0.00.041.401 I print_info: ssm_d_inner      = 0
0.00.041.401 I print_info: ssm_d_state      = 0
0.00.041.401 I print_info: ssm_dt_rank      = 0
0.00.041.401 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.401 I print_info: model type       = 1.4B
0.00.041.402 I print_info: model params     = 1.41 B
0.00.041.402 I print_info: general.name     = 1.4B
0.00.041.402 I print_info: vocab type       = BPE
0.00.041.403 I print_info: n_vocab          = 50304
0.00.041.403 I print_info: n_merges         = 50009
0.00.041.403 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.403 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.404 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.404 I print_info: LF token         = 128 'Ä'
0.00.041.404 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.404 I print_info: max token length = 1024
0.00.928.692 I load_tensors: offloading 24 repeating layers to GPU
0.00.928.697 I load_tensors: offloading output layer to GPU
0.00.928.699 I load_tensors: offloaded 25/25 layers to GPU
0.00.928.722 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.928.725 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.929.490 I llama_init_from_model: n_seq_max     = 1
0.00.929.492 I llama_init_from_model: n_ctx         = 2048
0.00.929.492 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.929.492 I llama_init_from_model: n_batch       = 2048
0.00.929.493 I llama_init_from_model: n_ubatch      = 512
0.00.929.493 I llama_init_from_model: flash_attn    = 0
0.00.929.494 I llama_init_from_model: freq_base     = 10000.0
0.00.929.495 I llama_init_from_model: freq_scale    = 1
0.00.929.496 I ggml_metal_init: allocating
0.00.929.511 I ggml_metal_init: found device: Apple M4
0.00.929.519 I ggml_metal_init: picking default device: Apple M4
0.00.930.752 I ggml_metal_init: using embedded metal library
0.00.935.937 I ggml_metal_init: GPU name:   Apple M4
0.00.935.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.935.941 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.935.942 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.935.942 I ggml_metal_init: simdgroup reduction   = true
0.00.935.942 I ggml_metal_init: simdgroup matrix mul. = true
0.00.935.942 I ggml_metal_init: has residency sets    = true
0.00.935.943 I ggml_metal_init: has bfloat            = true
0.00.935.943 I ggml_metal_init: use bfloat            = true
0.00.935.943 I ggml_metal_init: hasUnifiedMemory      = true
0.00.935.947 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.951.001 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.000.718 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.000.724 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.000.747 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.005.036 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.005.039 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.005.039 I llama_init_from_model: graph nodes  = 967
0.01.005.039 I llama_init_from_model: graph splits = 2
0.01.005.044 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.005.175 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.005.176 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.061.983 I main: llama threadpool init, n_threads = 4
0.01.062.027 I 
0.01.062.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.062.052 I 
0.01.062.153 I sampler seed: 1234
0.01.062.157 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.062.198 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.062.200 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.062.200 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.144.315 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.02.144.316 I llama_perf_context_print:        load time =    1051.13 ms
0.02.144.316 I llama_perf_context_print: prompt eval time =      49.26 ms /     7 tokens (    7.04 ms per token,   142.11 tokens per second)
0.02.144.317 I llama_perf_context_print:        eval time =    1029.94 ms /    63 runs   (   16.35 ms per token,    61.17 tokens per second)
0.02.144.318 I llama_perf_context_print:       total time =    1083.25 ms /    70 tokens
0.02.144.547 I ggml_metal_free: deallocating

real	0m2.163s
user	0m0.107s
sys	0m0.283s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.857 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.655 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.660 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.662 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.662 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.663 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.663 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.664 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.665 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.668 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.669 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.671 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.671 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.570 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.435 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.437 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.437 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.438 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.438 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.439 I llama_model_loader: - type  f32:  194 tensors
0.00.027.439 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.439 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.440 I print_info: file format = GGUF V3 (latest)
0.00.027.441 I print_info: file type   = Q4_0
0.00.027.442 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.528 I load: special tokens cache size = 25
0.00.041.439 I load: token to piece cache size = 0.2984 MB
0.00.041.442 I print_info: arch             = gptneox
0.00.041.443 I print_info: vocab_only       = 0
0.00.041.443 I print_info: n_ctx_train      = 2048
0.00.041.443 I print_info: n_embd           = 2048
0.00.041.443 I print_info: n_layer          = 24
0.00.041.448 I print_info: n_head           = 16
0.00.041.449 I print_info: n_head_kv        = 16
0.00.041.449 I print_info: n_rot            = 32
0.00.041.450 I print_info: n_swa            = 0
0.00.041.450 I print_info: n_embd_head_k    = 128
0.00.041.450 I print_info: n_embd_head_v    = 128
0.00.041.451 I print_info: n_gqa            = 1
0.00.041.452 I print_info: n_embd_k_gqa     = 2048
0.00.041.452 I print_info: n_embd_v_gqa     = 2048
0.00.041.453 I print_info: f_norm_eps       = 1.0e-05
0.00.041.453 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.453 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.454 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.454 I print_info: f_logit_scale    = 0.0e+00
0.00.041.455 I print_info: n_ff             = 8192
0.00.041.455 I print_info: n_expert         = 0
0.00.041.455 I print_info: n_expert_used    = 0
0.00.041.458 I print_info: causal attn      = 1
0.00.041.458 I print_info: pooling type     = 0
0.00.041.458 I print_info: rope type        = 2
0.00.041.459 I print_info: rope scaling     = linear
0.00.041.459 I print_info: freq_base_train  = 10000.0
0.00.041.460 I print_info: freq_scale_train = 1
0.00.041.460 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.460 I print_info: rope_finetuned   = unknown
0.00.041.460 I print_info: ssm_d_conv       = 0
0.00.041.460 I print_info: ssm_d_inner      = 0
0.00.041.460 I print_info: ssm_d_state      = 0
0.00.041.461 I print_info: ssm_dt_rank      = 0
0.00.041.461 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.465 I print_info: model type       = 1.4B
0.00.041.465 I print_info: model params     = 1.41 B
0.00.041.465 I print_info: general.name     = 1.4B
0.00.041.466 I print_info: vocab type       = BPE
0.00.041.466 I print_info: n_vocab          = 50304
0.00.041.466 I print_info: n_merges         = 50009
0.00.041.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.467 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.467 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.467 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.467 I print_info: LF token         = 128 'Ä'
0.00.041.467 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.468 I print_info: max token length = 1024
0.00.601.379 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.387 I load_tensors: offloading output layer to GPU
0.00.601.387 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.424 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.601.426 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.602.871 I llama_init_from_model: n_seq_max     = 1
0.00.602.885 I llama_init_from_model: n_ctx         = 2048
0.00.602.886 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.602.886 I llama_init_from_model: n_batch       = 2048
0.00.602.886 I llama_init_from_model: n_ubatch      = 512
0.00.602.887 I llama_init_from_model: flash_attn    = 0
0.00.602.888 I llama_init_from_model: freq_base     = 10000.0
0.00.602.889 I llama_init_from_model: freq_scale    = 1
0.00.602.891 I ggml_metal_init: allocating
0.00.602.961 I ggml_metal_init: found device: Apple M4
0.00.602.973 I ggml_metal_init: picking default device: Apple M4
0.00.604.666 I ggml_metal_init: using embedded metal library
0.00.610.781 I ggml_metal_init: GPU name:   Apple M4
0.00.610.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.789 I ggml_metal_init: simdgroup reduction   = true
0.00.610.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.789 I ggml_metal_init: has residency sets    = true
0.00.610.790 I ggml_metal_init: has bfloat            = true
0.00.610.790 I ggml_metal_init: use bfloat            = true
0.00.610.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.781 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.629 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.685.637 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.685.660 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.991 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.689.993 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.689.994 I llama_init_from_model: graph nodes  = 967
0.00.689.994 I llama_init_from_model: graph splits = 2
0.00.690.000 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.690.125 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.690.125 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.919 I main: llama threadpool init, n_threads = 4
0.00.745.960 I 
0.00.745.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.985 I 
0.00.746.159 I sampler seed: 1234
0.00.746.164 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.174 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.176 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.176 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.430.843 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49685.09 tokens per second)
0.01.430.843 I llama_perf_context_print:        load time =     734.14 ms
0.01.430.844 I llama_perf_context_print: prompt eval time =      49.26 ms /     7 tokens (    7.04 ms per token,   142.10 tokens per second)
0.01.430.845 I llama_perf_context_print:        eval time =     632.63 ms /    63 runs   (   10.04 ms per token,    99.58 tokens per second)
0.01.430.845 I llama_perf_context_print:       total time =     685.84 ms /    70 tokens
0.01.431.077 I ggml_metal_free: deallocating

real	0m1.452s
user	0m0.111s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.014.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.948 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.953 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.958 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.959 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.960 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.960 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.960 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.961 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.961 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.962 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.962 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.962 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.963 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.963 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.964 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.965 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.232 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.234 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.234 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.235 I llama_model_loader: - type  f32:  194 tensors
0.00.037.235 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.235 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.236 I print_info: file format = GGUF V3 (latest)
0.00.037.236 I print_info: file type   = Q4_1
0.00.037.237 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.446 I load: special tokens cache size = 25
0.00.053.696 I load: token to piece cache size = 0.2984 MB
0.00.053.700 I print_info: arch             = gptneox
0.00.053.700 I print_info: vocab_only       = 0
0.00.053.700 I print_info: n_ctx_train      = 2048
0.00.053.700 I print_info: n_embd           = 2048
0.00.053.700 I print_info: n_layer          = 24
0.00.053.703 I print_info: n_head           = 16
0.00.053.704 I print_info: n_head_kv        = 16
0.00.053.704 I print_info: n_rot            = 32
0.00.053.705 I print_info: n_swa            = 0
0.00.053.705 I print_info: n_embd_head_k    = 128
0.00.053.705 I print_info: n_embd_head_v    = 128
0.00.053.706 I print_info: n_gqa            = 1
0.00.053.706 I print_info: n_embd_k_gqa     = 2048
0.00.053.707 I print_info: n_embd_v_gqa     = 2048
0.00.053.708 I print_info: f_norm_eps       = 1.0e-05
0.00.053.708 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.708 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.708 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.709 I print_info: f_logit_scale    = 0.0e+00
0.00.053.709 I print_info: n_ff             = 8192
0.00.053.709 I print_info: n_expert         = 0
0.00.053.710 I print_info: n_expert_used    = 0
0.00.053.710 I print_info: causal attn      = 1
0.00.053.710 I print_info: pooling type     = 0
0.00.053.710 I print_info: rope type        = 2
0.00.053.712 I print_info: rope scaling     = linear
0.00.053.713 I print_info: freq_base_train  = 10000.0
0.00.053.713 I print_info: freq_scale_train = 1
0.00.053.713 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.713 I print_info: rope_finetuned   = unknown
0.00.053.714 I print_info: ssm_d_conv       = 0
0.00.053.714 I print_info: ssm_d_inner      = 0
0.00.053.715 I print_info: ssm_d_state      = 0
0.00.053.715 I print_info: ssm_dt_rank      = 0
0.00.053.715 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.715 I print_info: model type       = 1.4B
0.00.053.715 I print_info: model params     = 1.41 B
0.00.053.716 I print_info: general.name     = 1.4B
0.00.053.716 I print_info: vocab type       = BPE
0.00.053.716 I print_info: n_vocab          = 50304
0.00.053.717 I print_info: n_merges         = 50009
0.00.053.717 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.717 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.717 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.717 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.718 I print_info: LF token         = 128 'Ä'
0.00.053.718 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.718 I print_info: max token length = 1024
0.00.723.354 I load_tensors: offloading 24 repeating layers to GPU
0.00.723.372 I load_tensors: offloading output layer to GPU
0.00.723.372 I load_tensors: offloaded 25/25 layers to GPU
0.00.723.406 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.723.408 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.724.745 I llama_init_from_model: n_seq_max     = 1
0.00.724.750 I llama_init_from_model: n_ctx         = 2048
0.00.724.751 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.724.752 I llama_init_from_model: n_batch       = 2048
0.00.724.752 I llama_init_from_model: n_ubatch      = 512
0.00.724.752 I llama_init_from_model: flash_attn    = 0
0.00.724.755 I llama_init_from_model: freq_base     = 10000.0
0.00.724.755 I llama_init_from_model: freq_scale    = 1
0.00.724.757 I ggml_metal_init: allocating
0.00.724.837 I ggml_metal_init: found device: Apple M4
0.00.724.850 I ggml_metal_init: picking default device: Apple M4
0.00.726.628 I ggml_metal_init: using embedded metal library
0.00.732.169 I ggml_metal_init: GPU name:   Apple M4
0.00.732.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.732.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.732.175 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.732.176 I ggml_metal_init: simdgroup reduction   = true
0.00.732.176 I ggml_metal_init: simdgroup matrix mul. = true
0.00.732.177 I ggml_metal_init: has residency sets    = true
0.00.732.177 I ggml_metal_init: has bfloat            = true
0.00.732.177 I ggml_metal_init: use bfloat            = true
0.00.732.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.732.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.751.431 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.810.667 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.810.674 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.810.697 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.815.924 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.815.927 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.815.927 I llama_init_from_model: graph nodes  = 967
0.00.815.927 I llama_init_from_model: graph splits = 2
0.00.815.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.816.063 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.816.063 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.872.153 I main: llama threadpool init, n_threads = 4
0.00.872.203 I 
0.00.872.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.872.232 I 
0.00.872.388 I sampler seed: 1234
0.00.872.393 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.872.412 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.872.412 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.872.412 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.603.700 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.603.701 I llama_perf_context_print:        load time =     856.54 ms
0.01.603.702 I llama_perf_context_print: prompt eval time =      48.84 ms /     7 tokens (    6.98 ms per token,   143.31 tokens per second)
0.01.603.703 I llama_perf_context_print:        eval time =     679.60 ms /    63 runs   (   10.79 ms per token,    92.70 tokens per second)
0.01.603.703 I llama_perf_context_print:       total time =     732.52 ms /    70 tokens
0.01.603.941 I ggml_metal_free: deallocating

real	0m1.625s
user	0m0.113s
sys	0m0.237s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.251 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.023.255 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.257 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.258 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.259 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.260 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.261 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.261 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.261 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.262 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.262 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.262 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.263 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.264 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.265 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.265 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.146 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.032.085 I llama_model_loader: - type  f32:  194 tensors
0.00.032.085 I llama_model_loader: - type q5_0:   97 tensors
0.00.032.085 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.086 I print_info: file format = GGUF V3 (latest)
0.00.032.086 I print_info: file type   = Q5_0
0.00.032.091 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.040.045 I load: special tokens cache size = 25
0.00.046.024 I load: token to piece cache size = 0.2984 MB
0.00.046.026 I print_info: arch             = gptneox
0.00.046.027 I print_info: vocab_only       = 0
0.00.046.027 I print_info: n_ctx_train      = 2048
0.00.046.027 I print_info: n_embd           = 2048
0.00.046.027 I print_info: n_layer          = 24
0.00.046.030 I print_info: n_head           = 16
0.00.046.031 I print_info: n_head_kv        = 16
0.00.046.033 I print_info: n_rot            = 32
0.00.046.033 I print_info: n_swa            = 0
0.00.046.033 I print_info: n_embd_head_k    = 128
0.00.046.033 I print_info: n_embd_head_v    = 128
0.00.046.034 I print_info: n_gqa            = 1
0.00.046.035 I print_info: n_embd_k_gqa     = 2048
0.00.046.036 I print_info: n_embd_v_gqa     = 2048
0.00.046.036 I print_info: f_norm_eps       = 1.0e-05
0.00.046.037 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.037 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.037 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.037 I print_info: f_logit_scale    = 0.0e+00
0.00.046.038 I print_info: n_ff             = 8192
0.00.046.038 I print_info: n_expert         = 0
0.00.046.038 I print_info: n_expert_used    = 0
0.00.046.038 I print_info: causal attn      = 1
0.00.046.039 I print_info: pooling type     = 0
0.00.046.040 I print_info: rope type        = 2
0.00.046.041 I print_info: rope scaling     = linear
0.00.046.042 I print_info: freq_base_train  = 10000.0
0.00.046.042 I print_info: freq_scale_train = 1
0.00.046.042 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.042 I print_info: rope_finetuned   = unknown
0.00.046.042 I print_info: ssm_d_conv       = 0
0.00.046.043 I print_info: ssm_d_inner      = 0
0.00.046.043 I print_info: ssm_d_state      = 0
0.00.046.043 I print_info: ssm_dt_rank      = 0
0.00.046.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.043 I print_info: model type       = 1.4B
0.00.046.044 I print_info: model params     = 1.41 B
0.00.046.044 I print_info: general.name     = 1.4B
0.00.046.044 I print_info: vocab type       = BPE
0.00.046.045 I print_info: n_vocab          = 50304
0.00.046.045 I print_info: n_merges         = 50009
0.00.046.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.045 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.045 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.046 I print_info: LF token         = 128 'Ä'
0.00.046.046 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.046 I print_info: max token length = 1024
0.00.740.541 I load_tensors: offloading 24 repeating layers to GPU
0.00.740.557 I load_tensors: offloading output layer to GPU
0.00.740.558 I load_tensors: offloaded 25/25 layers to GPU
0.00.740.591 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.740.592 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.742.014 I llama_init_from_model: n_seq_max     = 1
0.00.742.018 I llama_init_from_model: n_ctx         = 2048
0.00.742.018 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.742.018 I llama_init_from_model: n_batch       = 2048
0.00.742.019 I llama_init_from_model: n_ubatch      = 512
0.00.742.020 I llama_init_from_model: flash_attn    = 0
0.00.742.021 I llama_init_from_model: freq_base     = 10000.0
0.00.742.022 I llama_init_from_model: freq_scale    = 1
0.00.742.027 I ggml_metal_init: allocating
0.00.742.045 I ggml_metal_init: found device: Apple M4
0.00.742.058 I ggml_metal_init: picking default device: Apple M4
0.00.743.472 I ggml_metal_init: using embedded metal library
0.00.749.804 I ggml_metal_init: GPU name:   Apple M4
0.00.749.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.749.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.749.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.749.811 I ggml_metal_init: simdgroup reduction   = true
0.00.749.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.749.811 I ggml_metal_init: has residency sets    = true
0.00.749.812 I ggml_metal_init: has bfloat            = true
0.00.749.812 I ggml_metal_init: use bfloat            = true
0.00.749.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.749.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.766.640 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.819.725 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.819.733 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.819.758 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.823.884 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.823.886 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.823.887 I llama_init_from_model: graph nodes  = 967
0.00.823.887 I llama_init_from_model: graph splits = 2
0.00.823.892 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.824.018 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.824.018 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.884.873 I main: llama threadpool init, n_threads = 4
0.00.884.913 I 
0.00.884.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.884.940 I 
0.00.885.094 I sampler seed: 1234
0.00.885.098 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.885.147 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.885.149 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.885.149 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.674.229 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.674.230 I llama_perf_context_print:        load time =     875.14 ms
0.01.674.231 I llama_perf_context_print: prompt eval time =      52.62 ms /     7 tokens (    7.52 ms per token,   133.03 tokens per second)
0.01.674.231 I llama_perf_context_print:        eval time =     733.52 ms /    63 runs   (   11.64 ms per token,    85.89 tokens per second)
0.01.674.232 I llama_perf_context_print:       total time =     790.23 ms /    70 tokens
0.01.674.467 I ggml_metal_free: deallocating

real	0m1.690s
user	0m0.108s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.012.247 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.860 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.870 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.872 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.874 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.875 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.875 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.875 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.878 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.879 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.880 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.881 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.882 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.668 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.334 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.336 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.337 I llama_model_loader: - type  f32:  194 tensors
0.00.028.337 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.337 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.338 I print_info: file format = GGUF V3 (latest)
0.00.028.339 I print_info: file type   = Q5_1
0.00.028.339 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.036.549 I load: special tokens cache size = 25
0.00.042.478 I load: token to piece cache size = 0.2984 MB
0.00.042.481 I print_info: arch             = gptneox
0.00.042.481 I print_info: vocab_only       = 0
0.00.042.481 I print_info: n_ctx_train      = 2048
0.00.042.481 I print_info: n_embd           = 2048
0.00.042.482 I print_info: n_layer          = 24
0.00.042.484 I print_info: n_head           = 16
0.00.042.485 I print_info: n_head_kv        = 16
0.00.042.486 I print_info: n_rot            = 32
0.00.042.486 I print_info: n_swa            = 0
0.00.042.486 I print_info: n_embd_head_k    = 128
0.00.042.486 I print_info: n_embd_head_v    = 128
0.00.042.487 I print_info: n_gqa            = 1
0.00.042.488 I print_info: n_embd_k_gqa     = 2048
0.00.042.488 I print_info: n_embd_v_gqa     = 2048
0.00.042.489 I print_info: f_norm_eps       = 1.0e-05
0.00.042.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.489 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.489 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.490 I print_info: f_logit_scale    = 0.0e+00
0.00.042.490 I print_info: n_ff             = 8192
0.00.042.491 I print_info: n_expert         = 0
0.00.042.491 I print_info: n_expert_used    = 0
0.00.042.491 I print_info: causal attn      = 1
0.00.042.491 I print_info: pooling type     = 0
0.00.042.491 I print_info: rope type        = 2
0.00.042.491 I print_info: rope scaling     = linear
0.00.042.492 I print_info: freq_base_train  = 10000.0
0.00.042.492 I print_info: freq_scale_train = 1
0.00.042.494 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.494 I print_info: rope_finetuned   = unknown
0.00.042.494 I print_info: ssm_d_conv       = 0
0.00.042.494 I print_info: ssm_d_inner      = 0
0.00.042.494 I print_info: ssm_d_state      = 0
0.00.042.494 I print_info: ssm_dt_rank      = 0
0.00.042.495 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.495 I print_info: model type       = 1.4B
0.00.042.495 I print_info: model params     = 1.41 B
0.00.042.495 I print_info: general.name     = 1.4B
0.00.042.496 I print_info: vocab type       = BPE
0.00.042.496 I print_info: n_vocab          = 50304
0.00.042.496 I print_info: n_merges         = 50009
0.00.042.496 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.497 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.497 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.497 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.498 I print_info: LF token         = 128 'Ä'
0.00.042.499 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.499 I print_info: max token length = 1024
0.00.686.779 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.795 I load_tensors: offloading output layer to GPU
0.00.686.796 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.831 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.686.832 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.688.264 I llama_init_from_model: n_seq_max     = 1
0.00.688.267 I llama_init_from_model: n_ctx         = 2048
0.00.688.268 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.688.268 I llama_init_from_model: n_batch       = 2048
0.00.688.269 I llama_init_from_model: n_ubatch      = 512
0.00.688.269 I llama_init_from_model: flash_attn    = 0
0.00.688.270 I llama_init_from_model: freq_base     = 10000.0
0.00.688.270 I llama_init_from_model: freq_scale    = 1
0.00.688.272 I ggml_metal_init: allocating
0.00.688.289 I ggml_metal_init: found device: Apple M4
0.00.688.298 I ggml_metal_init: picking default device: Apple M4
0.00.689.743 I ggml_metal_init: using embedded metal library
0.00.696.032 I ggml_metal_init: GPU name:   Apple M4
0.00.696.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.038 I ggml_metal_init: simdgroup reduction   = true
0.00.696.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.038 I ggml_metal_init: has residency sets    = true
0.00.696.038 I ggml_metal_init: has bfloat            = true
0.00.696.039 I ggml_metal_init: use bfloat            = true
0.00.696.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.718 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.764.062 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.764.068 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.764.092 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.769.743 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.769.744 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.769.745 I llama_init_from_model: graph nodes  = 967
0.00.769.745 I llama_init_from_model: graph splits = 2
0.00.769.751 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.769.879 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.769.880 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.823 I main: llama threadpool init, n_threads = 4
0.00.828.865 I 
0.00.828.889 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.889 I 
0.00.829.042 I sampler seed: 1234
0.00.829.047 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.058 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.059 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.059 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.667.287 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.01.667.288 I llama_perf_context_print:        load time =     815.68 ms
0.01.667.288 I llama_perf_context_print: prompt eval time =      51.74 ms /     7 tokens (    7.39 ms per token,   135.28 tokens per second)
0.01.667.289 I llama_perf_context_print:        eval time =     783.58 ms /    63 runs   (   12.44 ms per token,    80.40 tokens per second)
0.01.667.291 I llama_perf_context_print:       total time =     839.36 ms /    70 tokens
0.01.667.546 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.108s
sys	0m0.231s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.222 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.058 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.063 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.825 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.881 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.590 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.591 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.591 I llama_model_loader: - type  f32:  194 tensors
0.00.024.591 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.591 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.592 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.592 I print_info: file format = GGUF V3 (latest)
0.00.024.593 I print_info: file type   = Q2_K - Medium
0.00.024.594 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.926 I load: special tokens cache size = 25
0.00.038.967 I load: token to piece cache size = 0.2984 MB
0.00.038.970 I print_info: arch             = gptneox
0.00.038.970 I print_info: vocab_only       = 0
0.00.038.970 I print_info: n_ctx_train      = 2048
0.00.038.970 I print_info: n_embd           = 2048
0.00.038.970 I print_info: n_layer          = 24
0.00.038.973 I print_info: n_head           = 16
0.00.038.974 I print_info: n_head_kv        = 16
0.00.038.974 I print_info: n_rot            = 32
0.00.038.975 I print_info: n_swa            = 0
0.00.038.976 I print_info: n_embd_head_k    = 128
0.00.038.978 I print_info: n_embd_head_v    = 128
0.00.038.979 I print_info: n_gqa            = 1
0.00.038.980 I print_info: n_embd_k_gqa     = 2048
0.00.038.981 I print_info: n_embd_v_gqa     = 2048
0.00.038.981 I print_info: f_norm_eps       = 1.0e-05
0.00.038.982 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.982 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.982 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.982 I print_info: f_logit_scale    = 0.0e+00
0.00.038.983 I print_info: n_ff             = 8192
0.00.038.983 I print_info: n_expert         = 0
0.00.038.983 I print_info: n_expert_used    = 0
0.00.038.983 I print_info: causal attn      = 1
0.00.038.984 I print_info: pooling type     = 0
0.00.038.985 I print_info: rope type        = 2
0.00.038.985 I print_info: rope scaling     = linear
0.00.038.985 I print_info: freq_base_train  = 10000.0
0.00.038.986 I print_info: freq_scale_train = 1
0.00.038.986 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.988 I print_info: rope_finetuned   = unknown
0.00.038.988 I print_info: ssm_d_conv       = 0
0.00.038.988 I print_info: ssm_d_inner      = 0
0.00.038.988 I print_info: ssm_d_state      = 0
0.00.038.988 I print_info: ssm_dt_rank      = 0
0.00.038.988 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.988 I print_info: model type       = 1.4B
0.00.038.989 I print_info: model params     = 1.41 B
0.00.038.990 I print_info: general.name     = 1.4B
0.00.038.991 I print_info: vocab type       = BPE
0.00.038.991 I print_info: n_vocab          = 50304
0.00.038.991 I print_info: n_merges         = 50009
0.00.038.991 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.992 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.992 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.992 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.992 I print_info: LF token         = 128 'Ä'
0.00.038.992 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.992 I print_info: max token length = 1024
0.00.395.012 I load_tensors: offloading 24 repeating layers to GPU
0.00.395.022 I load_tensors: offloading output layer to GPU
0.00.395.023 I load_tensors: offloaded 25/25 layers to GPU
0.00.395.052 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.395.054 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.396.512 I llama_init_from_model: n_seq_max     = 1
0.00.396.517 I llama_init_from_model: n_ctx         = 2048
0.00.396.517 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.396.518 I llama_init_from_model: n_batch       = 2048
0.00.396.518 I llama_init_from_model: n_ubatch      = 512
0.00.396.519 I llama_init_from_model: flash_attn    = 0
0.00.396.520 I llama_init_from_model: freq_base     = 10000.0
0.00.396.521 I llama_init_from_model: freq_scale    = 1
0.00.396.526 I ggml_metal_init: allocating
0.00.396.630 I ggml_metal_init: found device: Apple M4
0.00.396.644 I ggml_metal_init: picking default device: Apple M4
0.00.398.499 I ggml_metal_init: using embedded metal library
0.00.404.403 I ggml_metal_init: GPU name:   Apple M4
0.00.404.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.404.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.404.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.404.422 I ggml_metal_init: simdgroup reduction   = true
0.00.404.422 I ggml_metal_init: simdgroup matrix mul. = true
0.00.404.422 I ggml_metal_init: has residency sets    = true
0.00.404.423 I ggml_metal_init: has bfloat            = true
0.00.404.423 I ggml_metal_init: use bfloat            = true
0.00.404.425 I ggml_metal_init: hasUnifiedMemory      = true
0.00.404.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.426.720 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.484.574 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.484.583 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.484.611 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.489.085 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.489.087 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.489.087 I llama_init_from_model: graph nodes  = 967
0.00.489.088 I llama_init_from_model: graph splits = 2
0.00.489.093 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.489.221 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.489.222 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.250 I main: llama threadpool init, n_threads = 4
0.00.545.294 I 
0.00.545.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.319 I 
0.00.545.473 I sampler seed: 1234
0.00.545.477 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.545.498 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.545.498 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.545.498 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.216.784 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.216.784 I llama_perf_context_print:        load time =     535.13 ms
0.01.216.786 I llama_perf_context_print: prompt eval time =      35.47 ms /     7 tokens (    5.07 ms per token,   197.38 tokens per second)
0.01.216.787 I llama_perf_context_print:        eval time =     632.90 ms /    63 runs   (   10.05 ms per token,    99.54 tokens per second)
0.01.216.788 I llama_perf_context_print:       total time =     672.43 ms /    70 tokens
0.01.217.011 I ggml_metal_free: deallocating

real	0m1.236s
user	0m0.113s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.804 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.527 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.533 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.534 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.535 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.535 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.536 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.536 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.537 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.537 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.538 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.538 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.538 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.539 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.539 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.543 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.543 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.543 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.340 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.386 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.143 I llama_model_loader: - type  f32:  194 tensors
0.00.025.144 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.144 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.144 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.144 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.145 I print_info: file format = GGUF V3 (latest)
0.00.025.145 I print_info: file type   = Q3_K - Medium
0.00.025.146 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.019 I load: special tokens cache size = 25
0.00.038.903 I load: token to piece cache size = 0.2984 MB
0.00.038.905 I print_info: arch             = gptneox
0.00.038.905 I print_info: vocab_only       = 0
0.00.038.906 I print_info: n_ctx_train      = 2048
0.00.038.906 I print_info: n_embd           = 2048
0.00.038.906 I print_info: n_layer          = 24
0.00.038.909 I print_info: n_head           = 16
0.00.038.909 I print_info: n_head_kv        = 16
0.00.038.910 I print_info: n_rot            = 32
0.00.038.910 I print_info: n_swa            = 0
0.00.038.910 I print_info: n_embd_head_k    = 128
0.00.038.910 I print_info: n_embd_head_v    = 128
0.00.038.911 I print_info: n_gqa            = 1
0.00.038.912 I print_info: n_embd_k_gqa     = 2048
0.00.038.912 I print_info: n_embd_v_gqa     = 2048
0.00.038.913 I print_info: f_norm_eps       = 1.0e-05
0.00.038.913 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.914 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.914 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.914 I print_info: f_logit_scale    = 0.0e+00
0.00.038.914 I print_info: n_ff             = 8192
0.00.038.915 I print_info: n_expert         = 0
0.00.038.915 I print_info: n_expert_used    = 0
0.00.038.916 I print_info: causal attn      = 1
0.00.038.918 I print_info: pooling type     = 0
0.00.038.918 I print_info: rope type        = 2
0.00.038.919 I print_info: rope scaling     = linear
0.00.038.919 I print_info: freq_base_train  = 10000.0
0.00.038.919 I print_info: freq_scale_train = 1
0.00.038.920 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.920 I print_info: rope_finetuned   = unknown
0.00.038.920 I print_info: ssm_d_conv       = 0
0.00.038.920 I print_info: ssm_d_inner      = 0
0.00.038.920 I print_info: ssm_d_state      = 0
0.00.038.920 I print_info: ssm_dt_rank      = 0
0.00.038.920 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.922 I print_info: model type       = 1.4B
0.00.038.923 I print_info: model params     = 1.41 B
0.00.038.923 I print_info: general.name     = 1.4B
0.00.038.924 I print_info: vocab type       = BPE
0.00.038.924 I print_info: n_vocab          = 50304
0.00.038.924 I print_info: n_merges         = 50009
0.00.038.924 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.925 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.925 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.925 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.925 I print_info: LF token         = 128 'Ä'
0.00.038.926 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: max token length = 1024
0.00.433.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.309 I load_tensors: offloading output layer to GPU
0.00.433.310 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.342 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.343 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.843 I llama_init_from_model: n_seq_max     = 1
0.00.434.849 I llama_init_from_model: n_ctx         = 2048
0.00.434.850 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.434.850 I llama_init_from_model: n_batch       = 2048
0.00.434.850 I llama_init_from_model: n_ubatch      = 512
0.00.434.851 I llama_init_from_model: flash_attn    = 0
0.00.434.856 I llama_init_from_model: freq_base     = 10000.0
0.00.434.857 I llama_init_from_model: freq_scale    = 1
0.00.434.860 I ggml_metal_init: allocating
0.00.434.911 I ggml_metal_init: found device: Apple M4
0.00.434.925 I ggml_metal_init: picking default device: Apple M4
0.00.436.730 I ggml_metal_init: using embedded metal library
0.00.442.234 I ggml_metal_init: GPU name:   Apple M4
0.00.442.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.253 I ggml_metal_init: simdgroup reduction   = true
0.00.442.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.254 I ggml_metal_init: has residency sets    = true
0.00.442.254 I ggml_metal_init: has bfloat            = true
0.00.442.255 I ggml_metal_init: use bfloat            = true
0.00.442.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.435 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.526.580 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.526.588 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.526.662 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.531.451 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.531.453 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.531.453 I llama_init_from_model: graph nodes  = 967
0.00.531.453 I llama_init_from_model: graph splits = 2
0.00.531.459 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.531.587 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.531.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.128 I main: llama threadpool init, n_threads = 4
0.00.589.169 I 
0.00.589.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.192 I 
0.00.589.367 I sampler seed: 1234
0.00.589.372 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.589.406 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.589.409 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.589.410 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.336.175 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51673.94 tokens per second)
0.01.336.176 I llama_perf_context_print:        load time =     579.42 ms
0.01.336.177 I llama_perf_context_print: prompt eval time =      48.76 ms /     7 tokens (    6.97 ms per token,   143.56 tokens per second)
0.01.336.177 I llama_perf_context_print:        eval time =     695.03 ms /    63 runs   (   11.03 ms per token,    90.64 tokens per second)
0.01.336.178 I llama_perf_context_print:       total time =     747.95 ms /    70 tokens
0.01.336.398 I ggml_metal_free: deallocating

real	0m1.352s
user	0m0.111s
sys	0m0.186s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.749 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.417 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.422 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.424 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.424 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.425 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.425 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.427 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.428 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.429 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.431 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.434 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.435 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.435 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.156 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.157 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.157 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.157 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.158 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.158 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.158 I llama_model_loader: - type  f32:  194 tensors
0.00.026.159 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.159 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.159 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.160 I print_info: file format = GGUF V3 (latest)
0.00.026.160 I print_info: file type   = Q4_K - Medium
0.00.026.161 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.121 I load: special tokens cache size = 25
0.00.040.055 I load: token to piece cache size = 0.2984 MB
0.00.040.058 I print_info: arch             = gptneox
0.00.040.058 I print_info: vocab_only       = 0
0.00.040.058 I print_info: n_ctx_train      = 2048
0.00.040.059 I print_info: n_embd           = 2048
0.00.040.059 I print_info: n_layer          = 24
0.00.040.061 I print_info: n_head           = 16
0.00.040.062 I print_info: n_head_kv        = 16
0.00.040.062 I print_info: n_rot            = 32
0.00.040.062 I print_info: n_swa            = 0
0.00.040.063 I print_info: n_embd_head_k    = 128
0.00.040.063 I print_info: n_embd_head_v    = 128
0.00.040.063 I print_info: n_gqa            = 1
0.00.040.064 I print_info: n_embd_k_gqa     = 2048
0.00.040.065 I print_info: n_embd_v_gqa     = 2048
0.00.040.065 I print_info: f_norm_eps       = 1.0e-05
0.00.040.067 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.068 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.068 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.068 I print_info: f_logit_scale    = 0.0e+00
0.00.040.069 I print_info: n_ff             = 8192
0.00.040.069 I print_info: n_expert         = 0
0.00.040.069 I print_info: n_expert_used    = 0
0.00.040.069 I print_info: causal attn      = 1
0.00.040.071 I print_info: pooling type     = 0
0.00.040.072 I print_info: rope type        = 2
0.00.040.073 I print_info: rope scaling     = linear
0.00.040.073 I print_info: freq_base_train  = 10000.0
0.00.040.073 I print_info: freq_scale_train = 1
0.00.040.073 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.074 I print_info: rope_finetuned   = unknown
0.00.040.074 I print_info: ssm_d_conv       = 0
0.00.040.074 I print_info: ssm_d_inner      = 0
0.00.040.074 I print_info: ssm_d_state      = 0
0.00.040.074 I print_info: ssm_dt_rank      = 0
0.00.040.074 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.075 I print_info: model type       = 1.4B
0.00.040.075 I print_info: model params     = 1.41 B
0.00.040.075 I print_info: general.name     = 1.4B
0.00.040.076 I print_info: vocab type       = BPE
0.00.040.076 I print_info: n_vocab          = 50304
0.00.040.076 I print_info: n_merges         = 50009
0.00.040.076 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.076 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.076 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.077 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.077 I print_info: LF token         = 128 'Ä'
0.00.040.081 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.081 I print_info: max token length = 1024
0.00.511.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.174 I load_tensors: offloading output layer to GPU
0.00.511.174 I load_tensors: offloaded 25/25 layers to GPU
0.00.511.207 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.511.208 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.512.741 I llama_init_from_model: n_seq_max     = 1
0.00.512.745 I llama_init_from_model: n_ctx         = 2048
0.00.512.746 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.512.747 I llama_init_from_model: n_batch       = 2048
0.00.512.747 I llama_init_from_model: n_ubatch      = 512
0.00.512.747 I llama_init_from_model: flash_attn    = 0
0.00.512.749 I llama_init_from_model: freq_base     = 10000.0
0.00.512.754 I llama_init_from_model: freq_scale    = 1
0.00.512.769 I ggml_metal_init: allocating
0.00.512.842 I ggml_metal_init: found device: Apple M4
0.00.512.856 I ggml_metal_init: picking default device: Apple M4
0.00.514.643 I ggml_metal_init: using embedded metal library
0.00.521.500 I ggml_metal_init: GPU name:   Apple M4
0.00.521.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.506 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.507 I ggml_metal_init: simdgroup reduction   = true
0.00.521.507 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.508 I ggml_metal_init: has residency sets    = true
0.00.521.508 I ggml_metal_init: has bfloat            = true
0.00.521.508 I ggml_metal_init: use bfloat            = true
0.00.521.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.510 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.538.673 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.593.572 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.593.580 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.593.603 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.598.613 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.598.616 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.598.616 I llama_init_from_model: graph nodes  = 967
0.00.598.616 I llama_init_from_model: graph splits = 2
0.00.598.623 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.598.752 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.598.752 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.724 I main: llama threadpool init, n_threads = 4
0.00.645.762 I 
0.00.645.785 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.786 I 
0.00.645.916 I sampler seed: 1234
0.00.645.920 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.645.937 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.645.937 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.645.938 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.406.551 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47619.05 tokens per second)
0.01.406.552 I llama_perf_context_print:        load time =     635.01 ms
0.01.406.553 I llama_perf_context_print: prompt eval time =      46.84 ms /     7 tokens (    6.69 ms per token,   149.44 tokens per second)
0.01.406.553 I llama_perf_context_print:        eval time =     711.17 ms /    63 runs   (   11.29 ms per token,    88.59 tokens per second)
0.01.406.554 I llama_perf_context_print:       total time =     761.79 ms /    70 tokens
0.01.406.849 I ggml_metal_free: deallocating

real	0m1.425s
user	0m0.110s
sys	0m0.181s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.975 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.050 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.063 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.072 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.072 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.944 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.945 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.946 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.946 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.947 I llama_model_loader: - type  f32:  194 tensors
0.00.025.947 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.947 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.948 I print_info: file format = GGUF V3 (latest)
0.00.025.949 I print_info: file type   = Q5_K - Medium
0.00.025.950 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.068 I load: special tokens cache size = 25
0.00.040.211 I load: token to piece cache size = 0.2984 MB
0.00.040.218 I print_info: arch             = gptneox
0.00.040.219 I print_info: vocab_only       = 0
0.00.040.219 I print_info: n_ctx_train      = 2048
0.00.040.219 I print_info: n_embd           = 2048
0.00.040.219 I print_info: n_layer          = 24
0.00.040.223 I print_info: n_head           = 16
0.00.040.224 I print_info: n_head_kv        = 16
0.00.040.224 I print_info: n_rot            = 32
0.00.040.225 I print_info: n_swa            = 0
0.00.040.225 I print_info: n_embd_head_k    = 128
0.00.040.225 I print_info: n_embd_head_v    = 128
0.00.040.226 I print_info: n_gqa            = 1
0.00.040.226 I print_info: n_embd_k_gqa     = 2048
0.00.040.227 I print_info: n_embd_v_gqa     = 2048
0.00.040.228 I print_info: f_norm_eps       = 1.0e-05
0.00.040.228 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.228 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.229 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.231 I print_info: f_logit_scale    = 0.0e+00
0.00.040.231 I print_info: n_ff             = 8192
0.00.040.232 I print_info: n_expert         = 0
0.00.040.232 I print_info: n_expert_used    = 0
0.00.040.232 I print_info: causal attn      = 1
0.00.040.232 I print_info: pooling type     = 0
0.00.040.232 I print_info: rope type        = 2
0.00.040.232 I print_info: rope scaling     = linear
0.00.040.233 I print_info: freq_base_train  = 10000.0
0.00.040.233 I print_info: freq_scale_train = 1
0.00.040.233 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.233 I print_info: rope_finetuned   = unknown
0.00.040.233 I print_info: ssm_d_conv       = 0
0.00.040.233 I print_info: ssm_d_inner      = 0
0.00.040.233 I print_info: ssm_d_state      = 0
0.00.040.234 I print_info: ssm_dt_rank      = 0
0.00.040.234 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.235 I print_info: model type       = 1.4B
0.00.040.236 I print_info: model params     = 1.41 B
0.00.040.236 I print_info: general.name     = 1.4B
0.00.040.236 I print_info: vocab type       = BPE
0.00.040.236 I print_info: n_vocab          = 50304
0.00.040.236 I print_info: n_merges         = 50009
0.00.040.237 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.237 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.237 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.237 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.237 I print_info: LF token         = 128 'Ä'
0.00.040.238 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.238 I print_info: max token length = 1024
0.00.586.615 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.630 I load_tensors: offloading output layer to GPU
0.00.586.630 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.661 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.586.662 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.587.649 I llama_init_from_model: n_seq_max     = 1
0.00.587.662 I llama_init_from_model: n_ctx         = 2048
0.00.587.662 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.587.663 I llama_init_from_model: n_batch       = 2048
0.00.587.663 I llama_init_from_model: n_ubatch      = 512
0.00.587.663 I llama_init_from_model: flash_attn    = 0
0.00.587.664 I llama_init_from_model: freq_base     = 10000.0
0.00.587.665 I llama_init_from_model: freq_scale    = 1
0.00.587.685 I ggml_metal_init: allocating
0.00.587.721 I ggml_metal_init: found device: Apple M4
0.00.587.735 I ggml_metal_init: picking default device: Apple M4
0.00.589.111 I ggml_metal_init: using embedded metal library
0.00.593.351 I ggml_metal_init: GPU name:   Apple M4
0.00.593.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.356 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.356 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.357 I ggml_metal_init: simdgroup reduction   = true
0.00.593.357 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.357 I ggml_metal_init: has residency sets    = true
0.00.593.357 I ggml_metal_init: has bfloat            = true
0.00.593.357 I ggml_metal_init: use bfloat            = true
0.00.593.358 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.360 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.603.415 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.635.439 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.635.446 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.635.514 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.639.842 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.639.844 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.639.844 I llama_init_from_model: graph nodes  = 967
0.00.639.844 I llama_init_from_model: graph splits = 2
0.00.639.849 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.639.980 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.639.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.007 I main: llama threadpool init, n_threads = 4
0.00.702.045 I 
0.00.702.066 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.066 I 
0.00.702.245 I sampler seed: 1234
0.00.702.249 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.267 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.267 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.267 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.542.179 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.542.179 I llama_perf_context_print:        load time =     692.09 ms
0.01.542.180 I llama_perf_context_print: prompt eval time =      51.32 ms /     7 tokens (    7.33 ms per token,   136.40 tokens per second)
0.01.542.181 I llama_perf_context_print:        eval time =     785.89 ms /    63 runs   (   12.47 ms per token,    80.16 tokens per second)
0.01.542.181 I llama_perf_context_print:       total time =     841.11 ms /    70 tokens
0.01.542.445 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.100s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.033 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.820 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.824 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.831 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.832 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.834 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.834 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.834 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.835 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.836 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.837 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.841 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.842 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.843 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.844 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.762 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.771 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.590 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.591 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.592 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.592 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.592 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.593 I llama_model_loader: - type  f32:  194 tensors
0.00.026.593 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.594 I print_info: file format = GGUF V3 (latest)
0.00.026.594 I print_info: file type   = Q6_K
0.00.026.595 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.498 I load: special tokens cache size = 25
0.00.040.342 I load: token to piece cache size = 0.2984 MB
0.00.040.345 I print_info: arch             = gptneox
0.00.040.345 I print_info: vocab_only       = 0
0.00.040.345 I print_info: n_ctx_train      = 2048
0.00.040.345 I print_info: n_embd           = 2048
0.00.040.345 I print_info: n_layer          = 24
0.00.040.348 I print_info: n_head           = 16
0.00.040.349 I print_info: n_head_kv        = 16
0.00.040.349 I print_info: n_rot            = 32
0.00.040.349 I print_info: n_swa            = 0
0.00.040.349 I print_info: n_embd_head_k    = 128
0.00.040.350 I print_info: n_embd_head_v    = 128
0.00.040.352 I print_info: n_gqa            = 1
0.00.040.353 I print_info: n_embd_k_gqa     = 2048
0.00.040.353 I print_info: n_embd_v_gqa     = 2048
0.00.040.354 I print_info: f_norm_eps       = 1.0e-05
0.00.040.354 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.359 I print_info: f_logit_scale    = 0.0e+00
0.00.040.360 I print_info: n_ff             = 8192
0.00.040.360 I print_info: n_expert         = 0
0.00.040.360 I print_info: n_expert_used    = 0
0.00.040.361 I print_info: causal attn      = 1
0.00.040.361 I print_info: pooling type     = 0
0.00.040.361 I print_info: rope type        = 2
0.00.040.361 I print_info: rope scaling     = linear
0.00.040.362 I print_info: freq_base_train  = 10000.0
0.00.040.363 I print_info: freq_scale_train = 1
0.00.040.363 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.363 I print_info: rope_finetuned   = unknown
0.00.040.363 I print_info: ssm_d_conv       = 0
0.00.040.363 I print_info: ssm_d_inner      = 0
0.00.040.364 I print_info: ssm_d_state      = 0
0.00.040.364 I print_info: ssm_dt_rank      = 0
0.00.040.365 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.365 I print_info: model type       = 1.4B
0.00.040.365 I print_info: model params     = 1.41 B
0.00.040.365 I print_info: general.name     = 1.4B
0.00.040.366 I print_info: vocab type       = BPE
0.00.040.366 I print_info: n_vocab          = 50304
0.00.040.366 I print_info: n_merges         = 50009
0.00.040.367 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.367 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.367 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.367 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.368 I print_info: LF token         = 128 'Ä'
0.00.040.368 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: max token length = 1024
0.00.664.714 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.719 I load_tensors: offloading output layer to GPU
0.00.664.720 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.745 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.664.747 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.665.963 I llama_init_from_model: n_seq_max     = 1
0.00.665.966 I llama_init_from_model: n_ctx         = 2048
0.00.665.966 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.665.967 I llama_init_from_model: n_batch       = 2048
0.00.665.967 I llama_init_from_model: n_ubatch      = 512
0.00.665.967 I llama_init_from_model: flash_attn    = 0
0.00.665.968 I llama_init_from_model: freq_base     = 10000.0
0.00.665.969 I llama_init_from_model: freq_scale    = 1
0.00.665.970 I ggml_metal_init: allocating
0.00.665.985 I ggml_metal_init: found device: Apple M4
0.00.665.994 I ggml_metal_init: picking default device: Apple M4
0.00.667.406 I ggml_metal_init: using embedded metal library
0.00.673.424 I ggml_metal_init: GPU name:   Apple M4
0.00.673.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.431 I ggml_metal_init: simdgroup reduction   = true
0.00.673.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.431 I ggml_metal_init: has residency sets    = true
0.00.673.431 I ggml_metal_init: has bfloat            = true
0.00.673.431 I ggml_metal_init: use bfloat            = true
0.00.673.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.690.626 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.749.015 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.749.024 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.749.051 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.753.581 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.753.583 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.753.583 I llama_init_from_model: graph nodes  = 967
0.00.753.584 I llama_init_from_model: graph splits = 2
0.00.753.589 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.753.721 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.753.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.513 I main: llama threadpool init, n_threads = 4
0.00.821.562 I 
0.00.821.587 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.821.589 I 
0.00.821.745 I sampler seed: 1234
0.00.821.750 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.821.792 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.821.796 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.821.796 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.692.797 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.692.798 I llama_perf_context_print:        load time =     810.59 ms
0.01.692.799 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.75 tokens per second)
0.01.692.799 I llama_perf_context_print:        eval time =     813.62 ms /    63 runs   (   12.91 ms per token,    77.43 tokens per second)
0.01.692.801 I llama_perf_context_print:       total time =     872.17 ms /    70 tokens
0.01.693.043 I ggml_metal_free: deallocating

real	0m1.710s
user	0m0.108s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.630 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.757 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.768 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.769 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.770 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.770 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.771 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.775 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.776 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.776 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.778 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.475 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.498 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.199 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.202 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.203 I llama_model_loader: - type  f32:  194 tensors
0.00.058.203 I llama_model_loader: - type  f16:   98 tensors
0.00.058.204 I print_info: file format = GGUF V3 (latest)
0.00.058.205 I print_info: file type   = all F32 (guessed)
0.00.058.208 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.113 I load: special tokens cache size = 25
0.00.077.927 I load: token to piece cache size = 0.2984 MB
0.00.077.930 I print_info: arch             = gptneox
0.00.077.930 I print_info: vocab_only       = 0
0.00.077.930 I print_info: n_ctx_train      = 2048
0.00.077.931 I print_info: n_embd           = 2048
0.00.077.931 I print_info: n_layer          = 24
0.00.077.934 I print_info: n_head           = 16
0.00.077.935 I print_info: n_head_kv        = 16
0.00.077.935 I print_info: n_rot            = 32
0.00.077.935 I print_info: n_swa            = 0
0.00.077.935 I print_info: n_embd_head_k    = 128
0.00.077.935 I print_info: n_embd_head_v    = 128
0.00.077.936 I print_info: n_gqa            = 1
0.00.077.937 I print_info: n_embd_k_gqa     = 2048
0.00.077.940 I print_info: n_embd_v_gqa     = 2048
0.00.077.940 I print_info: f_norm_eps       = 1.0e-05
0.00.077.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.942 I print_info: f_logit_scale    = 0.0e+00
0.00.077.942 I print_info: n_ff             = 8192
0.00.077.942 I print_info: n_expert         = 0
0.00.077.943 I print_info: n_expert_used    = 0
0.00.077.943 I print_info: causal attn      = 1
0.00.077.943 I print_info: pooling type     = 0
0.00.077.943 I print_info: rope type        = 2
0.00.077.943 I print_info: rope scaling     = linear
0.00.077.944 I print_info: freq_base_train  = 10000.0
0.00.077.944 I print_info: freq_scale_train = 1
0.00.077.944 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.945 I print_info: rope_finetuned   = unknown
0.00.077.945 I print_info: ssm_d_conv       = 0
0.00.077.945 I print_info: ssm_d_inner      = 0
0.00.077.945 I print_info: ssm_d_state      = 0
0.00.077.945 I print_info: ssm_dt_rank      = 0
0.00.077.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.946 I print_info: model type       = 1.4B
0.00.077.946 I print_info: model params     = 1.41 B
0.00.077.946 I print_info: general.name     = 1.4B
0.00.077.947 I print_info: vocab type       = BPE
0.00.077.947 I print_info: n_vocab          = 50304
0.00.077.947 I print_info: n_merges         = 50009
0.00.077.947 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.947 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.948 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.949 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.949 I print_info: LF token         = 128 'Ä'
0.00.077.949 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.950 I print_info: max token length = 1024
0.01.129.583 I load_tensors: offloading 24 repeating layers to GPU
0.01.129.587 I load_tensors: offloading output layer to GPU
0.01.129.587 I load_tensors: offloaded 25/25 layers to GPU
0.01.129.622 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.129.624 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.130.616 I llama_init_from_model: n_seq_max     = 1
0.01.130.618 I llama_init_from_model: n_ctx         = 128
0.01.130.618 I llama_init_from_model: n_ctx_per_seq = 128
0.01.130.618 I llama_init_from_model: n_batch       = 128
0.01.130.618 I llama_init_from_model: n_ubatch      = 128
0.01.130.621 I llama_init_from_model: flash_attn    = 0
0.01.130.622 I llama_init_from_model: freq_base     = 10000.0
0.01.130.622 I llama_init_from_model: freq_scale    = 1
0.01.130.622 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.130.627 I ggml_metal_init: allocating
0.01.130.708 I ggml_metal_init: found device: Apple M4
0.01.130.714 I ggml_metal_init: picking default device: Apple M4
0.01.131.789 I ggml_metal_init: using embedded metal library
0.01.135.529 I ggml_metal_init: GPU name:   Apple M4
0.01.135.531 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.135.532 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.135.532 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.135.532 I ggml_metal_init: simdgroup reduction   = true
0.01.135.533 I ggml_metal_init: simdgroup matrix mul. = true
0.01.135.533 I ggml_metal_init: has residency sets    = true
0.01.135.533 I ggml_metal_init: has bfloat            = true
0.01.135.533 I ggml_metal_init: use bfloat            = true
0.01.135.534 I ggml_metal_init: hasUnifiedMemory      = true
0.01.135.534 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.145.528 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.147.242 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.147.244 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.147.260 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.148.873 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.148.874 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.148.875 I llama_init_from_model: graph nodes  = 967
0.01.148.875 I llama_init_from_model: graph splits = 2
0.01.148.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.148.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.184.011 I 
0.01.184.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.184.071 I perplexity: tokenizing the input ..
0.01.189.224 I perplexity: tokenization took 5.151 ms
0.01.189.246 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.307.910 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.309.244 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.309.258 I llama_perf_context_print:        load time =    1159.11 ms
0.01.309.259 I llama_perf_context_print: prompt eval time =     118.40 ms /   128 tokens (    0.92 ms per token,  1081.11 tokens per second)
0.01.309.260 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.309.260 I llama_perf_context_print:       total time =     125.25 ms /   129 tokens
0.01.309.638 I ggml_metal_free: deallocating

real	0m1.523s
user	0m0.099s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.044 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.434 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.440 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.441 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.446 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.446 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.447 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.448 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.448 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.448 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.453 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.407 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.314 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.316 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.318 I llama_model_loader: - type  f32:  194 tensors
0.00.025.319 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.319 I print_info: file format = GGUF V3 (latest)
0.00.025.321 I print_info: file type   = Q8_0
0.00.025.322 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.774 I load: special tokens cache size = 25
0.00.040.596 I load: token to piece cache size = 0.2984 MB
0.00.040.600 I print_info: arch             = gptneox
0.00.040.601 I print_info: vocab_only       = 0
0.00.040.601 I print_info: n_ctx_train      = 2048
0.00.040.601 I print_info: n_embd           = 2048
0.00.040.601 I print_info: n_layer          = 24
0.00.040.606 I print_info: n_head           = 16
0.00.040.607 I print_info: n_head_kv        = 16
0.00.040.607 I print_info: n_rot            = 32
0.00.040.607 I print_info: n_swa            = 0
0.00.040.608 I print_info: n_embd_head_k    = 128
0.00.040.608 I print_info: n_embd_head_v    = 128
0.00.040.609 I print_info: n_gqa            = 1
0.00.040.609 I print_info: n_embd_k_gqa     = 2048
0.00.040.610 I print_info: n_embd_v_gqa     = 2048
0.00.040.611 I print_info: f_norm_eps       = 1.0e-05
0.00.040.611 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.611 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.611 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.612 I print_info: f_logit_scale    = 0.0e+00
0.00.040.612 I print_info: n_ff             = 8192
0.00.040.613 I print_info: n_expert         = 0
0.00.040.613 I print_info: n_expert_used    = 0
0.00.040.613 I print_info: causal attn      = 1
0.00.040.613 I print_info: pooling type     = 0
0.00.040.613 I print_info: rope type        = 2
0.00.040.614 I print_info: rope scaling     = linear
0.00.040.614 I print_info: freq_base_train  = 10000.0
0.00.040.614 I print_info: freq_scale_train = 1
0.00.040.614 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.615 I print_info: rope_finetuned   = unknown
0.00.040.615 I print_info: ssm_d_conv       = 0
0.00.040.615 I print_info: ssm_d_inner      = 0
0.00.040.615 I print_info: ssm_d_state      = 0
0.00.040.615 I print_info: ssm_dt_rank      = 0
0.00.040.615 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.616 I print_info: model type       = 1.4B
0.00.040.616 I print_info: model params     = 1.41 B
0.00.040.616 I print_info: general.name     = 1.4B
0.00.040.617 I print_info: vocab type       = BPE
0.00.040.617 I print_info: n_vocab          = 50304
0.00.040.617 I print_info: n_merges         = 50009
0.00.040.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.619 I print_info: LF token         = 128 'Ä'
0.00.040.619 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.619 I print_info: max token length = 1024
0.00.822.482 I load_tensors: offloading 24 repeating layers to GPU
0.00.822.486 I load_tensors: offloading output layer to GPU
0.00.822.488 I load_tensors: offloaded 25/25 layers to GPU
0.00.822.508 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.822.509 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.823.308 I llama_init_from_model: n_seq_max     = 1
0.00.823.309 I llama_init_from_model: n_ctx         = 128
0.00.823.310 I llama_init_from_model: n_ctx_per_seq = 128
0.00.823.310 I llama_init_from_model: n_batch       = 128
0.00.823.310 I llama_init_from_model: n_ubatch      = 128
0.00.823.311 I llama_init_from_model: flash_attn    = 0
0.00.823.312 I llama_init_from_model: freq_base     = 10000.0
0.00.823.312 I llama_init_from_model: freq_scale    = 1
0.00.823.313 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.823.314 I ggml_metal_init: allocating
0.00.823.335 I ggml_metal_init: found device: Apple M4
0.00.823.344 I ggml_metal_init: picking default device: Apple M4
0.00.824.541 I ggml_metal_init: using embedded metal library
0.00.830.165 I ggml_metal_init: GPU name:   Apple M4
0.00.830.169 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.830.170 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.830.171 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.830.171 I ggml_metal_init: simdgroup reduction   = true
0.00.830.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.830.172 I ggml_metal_init: has residency sets    = true
0.00.830.172 I ggml_metal_init: has bfloat            = true
0.00.830.172 I ggml_metal_init: use bfloat            = true
0.00.830.173 I ggml_metal_init: hasUnifiedMemory      = true
0.00.830.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.845.290 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.848.624 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.848.627 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.848.654 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.851.588 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.851.589 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.851.590 I llama_init_from_model: graph nodes  = 967
0.00.851.590 I llama_init_from_model: graph splits = 2
0.00.851.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.851.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.520 I 
0.00.879.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.614 I perplexity: tokenizing the input ..
0.00.886.450 I perplexity: tokenization took 6.834 ms
0.00.886.469 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.025.637 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.027.071 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.027.091 I llama_perf_context_print:        load time =     870.47 ms
0.01.027.092 I llama_perf_context_print: prompt eval time =     138.25 ms /   128 tokens (    1.08 ms per token,   925.87 tokens per second)
0.01.027.092 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.027.093 I llama_perf_context_print:       total time =     147.58 ms /   129 tokens
0.01.027.511 I ggml_metal_free: deallocating

real	0m1.042s
user	0m0.077s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.981 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.560 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.564 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.573 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.574 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.574 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.574 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.575 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.504 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.367 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.367 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.368 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.368 I llama_model_loader: - type  f32:  194 tensors
0.00.027.369 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.369 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.370 I print_info: file format = GGUF V3 (latest)
0.00.027.370 I print_info: file type   = Q4_0
0.00.027.371 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.372 I load: special tokens cache size = 25
0.00.041.305 I load: token to piece cache size = 0.2984 MB
0.00.041.308 I print_info: arch             = gptneox
0.00.041.309 I print_info: vocab_only       = 0
0.00.041.309 I print_info: n_ctx_train      = 2048
0.00.041.309 I print_info: n_embd           = 2048
0.00.041.309 I print_info: n_layer          = 24
0.00.041.312 I print_info: n_head           = 16
0.00.041.313 I print_info: n_head_kv        = 16
0.00.041.313 I print_info: n_rot            = 32
0.00.041.314 I print_info: n_swa            = 0
0.00.041.314 I print_info: n_embd_head_k    = 128
0.00.041.314 I print_info: n_embd_head_v    = 128
0.00.041.315 I print_info: n_gqa            = 1
0.00.041.315 I print_info: n_embd_k_gqa     = 2048
0.00.041.316 I print_info: n_embd_v_gqa     = 2048
0.00.041.317 I print_info: f_norm_eps       = 1.0e-05
0.00.041.317 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.317 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.317 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.318 I print_info: f_logit_scale    = 0.0e+00
0.00.041.318 I print_info: n_ff             = 8192
0.00.041.318 I print_info: n_expert         = 0
0.00.041.318 I print_info: n_expert_used    = 0
0.00.041.319 I print_info: causal attn      = 1
0.00.041.319 I print_info: pooling type     = 0
0.00.041.319 I print_info: rope type        = 2
0.00.041.319 I print_info: rope scaling     = linear
0.00.041.320 I print_info: freq_base_train  = 10000.0
0.00.041.320 I print_info: freq_scale_train = 1
0.00.041.320 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.320 I print_info: rope_finetuned   = unknown
0.00.041.320 I print_info: ssm_d_conv       = 0
0.00.041.321 I print_info: ssm_d_inner      = 0
0.00.041.321 I print_info: ssm_d_state      = 0
0.00.041.321 I print_info: ssm_dt_rank      = 0
0.00.041.321 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.321 I print_info: model type       = 1.4B
0.00.041.322 I print_info: model params     = 1.41 B
0.00.041.322 I print_info: general.name     = 1.4B
0.00.041.322 I print_info: vocab type       = BPE
0.00.041.322 I print_info: n_vocab          = 50304
0.00.041.323 I print_info: n_merges         = 50009
0.00.041.323 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.323 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.323 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.323 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.324 I print_info: LF token         = 128 'Ä'
0.00.041.324 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.324 I print_info: max token length = 1024
0.00.586.721 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.735 I load_tensors: offloading output layer to GPU
0.00.586.736 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.771 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.586.772 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.588.102 I llama_init_from_model: n_seq_max     = 1
0.00.588.107 I llama_init_from_model: n_ctx         = 128
0.00.588.108 I llama_init_from_model: n_ctx_per_seq = 128
0.00.588.112 I llama_init_from_model: n_batch       = 128
0.00.588.113 I llama_init_from_model: n_ubatch      = 128
0.00.588.113 I llama_init_from_model: flash_attn    = 0
0.00.588.115 I llama_init_from_model: freq_base     = 10000.0
0.00.588.116 I llama_init_from_model: freq_scale    = 1
0.00.588.116 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.119 I ggml_metal_init: allocating
0.00.588.205 I ggml_metal_init: found device: Apple M4
0.00.588.218 I ggml_metal_init: picking default device: Apple M4
0.00.590.028 I ggml_metal_init: using embedded metal library
0.00.595.496 I ggml_metal_init: GPU name:   Apple M4
0.00.595.501 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.503 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.503 I ggml_metal_init: simdgroup reduction   = true
0.00.595.504 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.504 I ggml_metal_init: has residency sets    = true
0.00.595.504 I ggml_metal_init: has bfloat            = true
0.00.595.504 I ggml_metal_init: use bfloat            = true
0.00.595.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.360 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.944 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.954 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.986 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.213 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.214 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.215 I llama_init_from_model: graph nodes  = 967
0.00.622.215 I llama_init_from_model: graph splits = 2
0.00.622.218 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.219 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.441 I 
0.00.651.533 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.552 I perplexity: tokenizing the input ..
0.00.658.799 I perplexity: tokenization took 7.244 ms
0.00.658.820 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.124 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.796.453 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.796.466 I llama_perf_context_print:        load time =     640.45 ms
0.00.796.467 I llama_perf_context_print: prompt eval time =     135.40 ms /   128 tokens (    1.06 ms per token,   945.37 tokens per second)
0.00.796.468 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.468 I llama_perf_context_print:       total time =     145.03 ms /   129 tokens
0.00.796.847 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.081s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.915 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.726 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.737 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.737 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.740 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.741 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.745 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.517 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.335 I llama_model_loader: - type  f32:  194 tensors
0.00.025.335 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.335 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.336 I print_info: file format = GGUF V3 (latest)
0.00.025.336 I print_info: file type   = Q4_1
0.00.025.337 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.563 I load: special tokens cache size = 25
0.00.039.443 I load: token to piece cache size = 0.2984 MB
0.00.039.446 I print_info: arch             = gptneox
0.00.039.446 I print_info: vocab_only       = 0
0.00.039.446 I print_info: n_ctx_train      = 2048
0.00.039.446 I print_info: n_embd           = 2048
0.00.039.447 I print_info: n_layer          = 24
0.00.039.449 I print_info: n_head           = 16
0.00.039.450 I print_info: n_head_kv        = 16
0.00.039.450 I print_info: n_rot            = 32
0.00.039.451 I print_info: n_swa            = 0
0.00.039.451 I print_info: n_embd_head_k    = 128
0.00.039.451 I print_info: n_embd_head_v    = 128
0.00.039.452 I print_info: n_gqa            = 1
0.00.039.453 I print_info: n_embd_k_gqa     = 2048
0.00.039.453 I print_info: n_embd_v_gqa     = 2048
0.00.039.454 I print_info: f_norm_eps       = 1.0e-05
0.00.039.454 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.455 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.455 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.455 I print_info: f_logit_scale    = 0.0e+00
0.00.039.456 I print_info: n_ff             = 8192
0.00.039.456 I print_info: n_expert         = 0
0.00.039.456 I print_info: n_expert_used    = 0
0.00.039.456 I print_info: causal attn      = 1
0.00.039.456 I print_info: pooling type     = 0
0.00.039.456 I print_info: rope type        = 2
0.00.039.457 I print_info: rope scaling     = linear
0.00.039.457 I print_info: freq_base_train  = 10000.0
0.00.039.457 I print_info: freq_scale_train = 1
0.00.039.458 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.458 I print_info: rope_finetuned   = unknown
0.00.039.458 I print_info: ssm_d_conv       = 0
0.00.039.458 I print_info: ssm_d_inner      = 0
0.00.039.458 I print_info: ssm_d_state      = 0
0.00.039.459 I print_info: ssm_dt_rank      = 0
0.00.039.459 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.459 I print_info: model type       = 1.4B
0.00.039.459 I print_info: model params     = 1.41 B
0.00.039.459 I print_info: general.name     = 1.4B
0.00.039.461 I print_info: vocab type       = BPE
0.00.039.461 I print_info: n_vocab          = 50304
0.00.039.461 I print_info: n_merges         = 50009
0.00.039.462 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: LF token         = 128 'Ä'
0.00.039.463 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.463 I print_info: max token length = 1024
0.00.627.919 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.931 I load_tensors: offloading output layer to GPU
0.00.627.932 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.964 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.627.965 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.629.334 I llama_init_from_model: n_seq_max     = 1
0.00.629.338 I llama_init_from_model: n_ctx         = 128
0.00.629.339 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.339 I llama_init_from_model: n_batch       = 128
0.00.629.340 I llama_init_from_model: n_ubatch      = 128
0.00.629.340 I llama_init_from_model: flash_attn    = 0
0.00.629.342 I llama_init_from_model: freq_base     = 10000.0
0.00.629.343 I llama_init_from_model: freq_scale    = 1
0.00.629.343 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.346 I ggml_metal_init: allocating
0.00.629.409 I ggml_metal_init: found device: Apple M4
0.00.629.423 I ggml_metal_init: picking default device: Apple M4
0.00.631.196 I ggml_metal_init: using embedded metal library
0.00.638.035 I ggml_metal_init: GPU name:   Apple M4
0.00.638.040 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.040 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.041 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.042 I ggml_metal_init: simdgroup reduction   = true
0.00.638.042 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.042 I ggml_metal_init: has residency sets    = true
0.00.638.042 I ggml_metal_init: has bfloat            = true
0.00.638.043 I ggml_metal_init: use bfloat            = true
0.00.638.043 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.045 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.387 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.658.881 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.658.885 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.658.911 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.662.101 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.662.103 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.662.103 I llama_init_from_model: graph nodes  = 967
0.00.662.104 I llama_init_from_model: graph splits = 2
0.00.662.107 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.662.107 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.678 I 
0.00.689.760 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.779 I perplexity: tokenizing the input ..
0.00.697.380 I perplexity: tokenization took 7.597 ms
0.00.697.399 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.989 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.835.325 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.835.338 I llama_perf_context_print:        load time =     680.75 ms
0.00.835.339 I llama_perf_context_print: prompt eval time =     135.67 ms /   128 tokens (    1.06 ms per token,   943.44 tokens per second)
0.00.835.339 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.340 I llama_perf_context_print:       total time =     145.66 ms /   129 tokens
0.00.835.718 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.080s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.774 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.969 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.980 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.980 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.981 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.981 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.981 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.982 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.983 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.983 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.984 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.984 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.984 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.985 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.986 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.987 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.987 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.913 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.813 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.813 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.814 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.814 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.815 I llama_model_loader: - type  f32:  194 tensors
0.00.026.815 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.815 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.816 I print_info: file format = GGUF V3 (latest)
0.00.026.816 I print_info: file type   = Q5_0
0.00.026.821 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.714 I load: special tokens cache size = 25
0.00.040.492 I load: token to piece cache size = 0.2984 MB
0.00.040.495 I print_info: arch             = gptneox
0.00.040.495 I print_info: vocab_only       = 0
0.00.040.495 I print_info: n_ctx_train      = 2048
0.00.040.496 I print_info: n_embd           = 2048
0.00.040.496 I print_info: n_layer          = 24
0.00.040.499 I print_info: n_head           = 16
0.00.040.500 I print_info: n_head_kv        = 16
0.00.040.500 I print_info: n_rot            = 32
0.00.040.500 I print_info: n_swa            = 0
0.00.040.500 I print_info: n_embd_head_k    = 128
0.00.040.501 I print_info: n_embd_head_v    = 128
0.00.040.503 I print_info: n_gqa            = 1
0.00.040.504 I print_info: n_embd_k_gqa     = 2048
0.00.040.505 I print_info: n_embd_v_gqa     = 2048
0.00.040.505 I print_info: f_norm_eps       = 1.0e-05
0.00.040.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.506 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.512 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.514 I print_info: f_logit_scale    = 0.0e+00
0.00.040.516 I print_info: n_ff             = 8192
0.00.040.516 I print_info: n_expert         = 0
0.00.040.517 I print_info: n_expert_used    = 0
0.00.040.517 I print_info: causal attn      = 1
0.00.040.517 I print_info: pooling type     = 0
0.00.040.517 I print_info: rope type        = 2
0.00.040.520 I print_info: rope scaling     = linear
0.00.040.521 I print_info: freq_base_train  = 10000.0
0.00.040.521 I print_info: freq_scale_train = 1
0.00.040.521 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.521 I print_info: rope_finetuned   = unknown
0.00.040.522 I print_info: ssm_d_conv       = 0
0.00.040.522 I print_info: ssm_d_inner      = 0
0.00.040.522 I print_info: ssm_d_state      = 0
0.00.040.522 I print_info: ssm_dt_rank      = 0
0.00.040.522 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.522 I print_info: model type       = 1.4B
0.00.040.523 I print_info: model params     = 1.41 B
0.00.040.523 I print_info: general.name     = 1.4B
0.00.040.523 I print_info: vocab type       = BPE
0.00.040.524 I print_info: n_vocab          = 50304
0.00.040.524 I print_info: n_merges         = 50009
0.00.040.525 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.525 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.525 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.525 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.526 I print_info: LF token         = 128 'Ä'
0.00.040.526 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.526 I print_info: max token length = 1024
0.00.691.620 I load_tensors: offloading 24 repeating layers to GPU
0.00.691.634 I load_tensors: offloading output layer to GPU
0.00.691.635 I load_tensors: offloaded 25/25 layers to GPU
0.00.691.670 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.691.671 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.693.225 I llama_init_from_model: n_seq_max     = 1
0.00.693.229 I llama_init_from_model: n_ctx         = 128
0.00.693.230 I llama_init_from_model: n_ctx_per_seq = 128
0.00.693.230 I llama_init_from_model: n_batch       = 128
0.00.693.231 I llama_init_from_model: n_ubatch      = 128
0.00.693.231 I llama_init_from_model: flash_attn    = 0
0.00.693.233 I llama_init_from_model: freq_base     = 10000.0
0.00.693.234 I llama_init_from_model: freq_scale    = 1
0.00.693.234 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.693.242 I ggml_metal_init: allocating
0.00.693.327 I ggml_metal_init: found device: Apple M4
0.00.693.341 I ggml_metal_init: picking default device: Apple M4
0.00.694.787 I ggml_metal_init: using embedded metal library
0.00.701.319 I ggml_metal_init: GPU name:   Apple M4
0.00.701.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.701.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.701.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.701.325 I ggml_metal_init: simdgroup reduction   = true
0.00.701.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.701.325 I ggml_metal_init: has residency sets    = true
0.00.701.325 I ggml_metal_init: has bfloat            = true
0.00.701.326 I ggml_metal_init: use bfloat            = true
0.00.701.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.701.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.718.191 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.634 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.721.637 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.721.664 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.166 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.725.168 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.725.168 I llama_init_from_model: graph nodes  = 967
0.00.725.169 I llama_init_from_model: graph splits = 2
0.00.725.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.725.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.961 I 
0.00.757.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.064 I perplexity: tokenizing the input ..
0.00.764.778 I perplexity: tokenization took 7.711 ms
0.00.764.802 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.912.400 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.913.737 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.913.752 I llama_perf_context_print:        load time =     746.18 ms
0.00.913.753 I llama_perf_context_print: prompt eval time =     146.69 ms /   128 tokens (    1.15 ms per token,   872.58 tokens per second)
0.00.913.754 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.913.754 I llama_perf_context_print:       total time =     156.80 ms /   129 tokens
0.00.914.144 I ggml_metal_free: deallocating

real	0m0.930s
user	0m0.079s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.115 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.389 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.394 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.396 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.396 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.396 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.397 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.279 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.285 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.123 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.124 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.125 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.125 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.125 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.126 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.126 I llama_model_loader: - type  f32:  194 tensors
0.00.025.126 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.127 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.127 I print_info: file format = GGUF V3 (latest)
0.00.025.128 I print_info: file type   = Q5_1
0.00.025.129 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.075 I load: special tokens cache size = 25
0.00.039.013 I load: token to piece cache size = 0.2984 MB
0.00.039.016 I print_info: arch             = gptneox
0.00.039.016 I print_info: vocab_only       = 0
0.00.039.016 I print_info: n_ctx_train      = 2048
0.00.039.016 I print_info: n_embd           = 2048
0.00.039.016 I print_info: n_layer          = 24
0.00.039.019 I print_info: n_head           = 16
0.00.039.020 I print_info: n_head_kv        = 16
0.00.039.020 I print_info: n_rot            = 32
0.00.039.020 I print_info: n_swa            = 0
0.00.039.020 I print_info: n_embd_head_k    = 128
0.00.039.023 I print_info: n_embd_head_v    = 128
0.00.039.023 I print_info: n_gqa            = 1
0.00.039.024 I print_info: n_embd_k_gqa     = 2048
0.00.039.025 I print_info: n_embd_v_gqa     = 2048
0.00.039.025 I print_info: f_norm_eps       = 1.0e-05
0.00.039.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.026 I print_info: f_logit_scale    = 0.0e+00
0.00.039.027 I print_info: n_ff             = 8192
0.00.039.027 I print_info: n_expert         = 0
0.00.039.027 I print_info: n_expert_used    = 0
0.00.039.027 I print_info: causal attn      = 1
0.00.039.027 I print_info: pooling type     = 0
0.00.039.027 I print_info: rope type        = 2
0.00.039.028 I print_info: rope scaling     = linear
0.00.039.028 I print_info: freq_base_train  = 10000.0
0.00.039.028 I print_info: freq_scale_train = 1
0.00.039.029 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.029 I print_info: rope_finetuned   = unknown
0.00.039.030 I print_info: ssm_d_conv       = 0
0.00.039.030 I print_info: ssm_d_inner      = 0
0.00.039.030 I print_info: ssm_d_state      = 0
0.00.039.031 I print_info: ssm_dt_rank      = 0
0.00.039.031 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.032 I print_info: model type       = 1.4B
0.00.039.032 I print_info: model params     = 1.41 B
0.00.039.032 I print_info: general.name     = 1.4B
0.00.039.037 I print_info: vocab type       = BPE
0.00.039.037 I print_info: n_vocab          = 50304
0.00.039.037 I print_info: n_merges         = 50009
0.00.039.037 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.039 I print_info: LF token         = 128 'Ä'
0.00.039.040 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.040 I print_info: max token length = 1024
0.00.660.825 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.840 I load_tensors: offloading output layer to GPU
0.00.660.841 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.875 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.660.876 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.662.289 I llama_init_from_model: n_seq_max     = 1
0.00.662.292 I llama_init_from_model: n_ctx         = 128
0.00.662.293 I llama_init_from_model: n_ctx_per_seq = 128
0.00.662.293 I llama_init_from_model: n_batch       = 128
0.00.662.293 I llama_init_from_model: n_ubatch      = 128
0.00.662.294 I llama_init_from_model: flash_attn    = 0
0.00.662.295 I llama_init_from_model: freq_base     = 10000.0
0.00.662.296 I llama_init_from_model: freq_scale    = 1
0.00.662.296 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.662.298 I ggml_metal_init: allocating
0.00.662.318 I ggml_metal_init: found device: Apple M4
0.00.662.328 I ggml_metal_init: picking default device: Apple M4
0.00.663.615 I ggml_metal_init: using embedded metal library
0.00.669.982 I ggml_metal_init: GPU name:   Apple M4
0.00.669.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.986 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.987 I ggml_metal_init: simdgroup reduction   = true
0.00.669.987 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.988 I ggml_metal_init: has residency sets    = true
0.00.669.988 I ggml_metal_init: has bfloat            = true
0.00.669.988 I ggml_metal_init: use bfloat            = true
0.00.669.989 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.990 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.686.779 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.690.265 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.690.271 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.690.296 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.693.464 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.693.466 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.693.466 I llama_init_from_model: graph nodes  = 967
0.00.693.466 I llama_init_from_model: graph splits = 2
0.00.693.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.693.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.725.688 I 
0.00.725.775 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.725.797 I perplexity: tokenizing the input ..
0.00.733.349 I perplexity: tokenization took 7.549 ms
0.00.733.374 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.876.351 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.877.684 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.877.699 I llama_perf_context_print:        load time =     716.57 ms
0.00.877.700 I llama_perf_context_print: prompt eval time =     142.02 ms /   128 tokens (    1.11 ms per token,   901.31 tokens per second)
0.00.877.700 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.877.701 I llama_perf_context_print:       total time =     152.01 ms /   129 tokens
0.00.878.095 I ggml_metal_free: deallocating

real	0m0.892s
user	0m0.079s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.094 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.953 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.959 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.960 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.961 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.961 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.963 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.963 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.964 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.964 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.965 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.965 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.966 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.967 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.835 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.862 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.672 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.673 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.674 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.674 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.675 I llama_model_loader: - type  f32:  194 tensors
0.00.025.675 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.675 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.675 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.676 I print_info: file format = GGUF V3 (latest)
0.00.025.676 I print_info: file type   = Q2_K - Medium
0.00.025.677 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.617 I load: special tokens cache size = 25
0.00.039.381 I load: token to piece cache size = 0.2984 MB
0.00.039.384 I print_info: arch             = gptneox
0.00.039.385 I print_info: vocab_only       = 0
0.00.039.385 I print_info: n_ctx_train      = 2048
0.00.039.385 I print_info: n_embd           = 2048
0.00.039.385 I print_info: n_layer          = 24
0.00.039.388 I print_info: n_head           = 16
0.00.039.388 I print_info: n_head_kv        = 16
0.00.039.389 I print_info: n_rot            = 32
0.00.039.389 I print_info: n_swa            = 0
0.00.039.389 I print_info: n_embd_head_k    = 128
0.00.039.389 I print_info: n_embd_head_v    = 128
0.00.039.390 I print_info: n_gqa            = 1
0.00.039.391 I print_info: n_embd_k_gqa     = 2048
0.00.039.392 I print_info: n_embd_v_gqa     = 2048
0.00.039.392 I print_info: f_norm_eps       = 1.0e-05
0.00.039.392 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.393 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.394 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.395 I print_info: f_logit_scale    = 0.0e+00
0.00.039.395 I print_info: n_ff             = 8192
0.00.039.396 I print_info: n_expert         = 0
0.00.039.396 I print_info: n_expert_used    = 0
0.00.039.396 I print_info: causal attn      = 1
0.00.039.396 I print_info: pooling type     = 0
0.00.039.396 I print_info: rope type        = 2
0.00.039.396 I print_info: rope scaling     = linear
0.00.039.397 I print_info: freq_base_train  = 10000.0
0.00.039.397 I print_info: freq_scale_train = 1
0.00.039.397 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.398 I print_info: rope_finetuned   = unknown
0.00.039.398 I print_info: ssm_d_conv       = 0
0.00.039.398 I print_info: ssm_d_inner      = 0
0.00.039.398 I print_info: ssm_d_state      = 0
0.00.039.399 I print_info: ssm_dt_rank      = 0
0.00.039.399 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.399 I print_info: model type       = 1.4B
0.00.039.400 I print_info: model params     = 1.41 B
0.00.039.400 I print_info: general.name     = 1.4B
0.00.039.401 I print_info: vocab type       = BPE
0.00.039.401 I print_info: n_vocab          = 50304
0.00.039.401 I print_info: n_merges         = 50009
0.00.039.401 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.401 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.401 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.402 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.402 I print_info: LF token         = 128 'Ä'
0.00.039.402 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.403 I print_info: max token length = 1024
0.00.375.680 I load_tensors: offloading 24 repeating layers to GPU
0.00.375.695 I load_tensors: offloading output layer to GPU
0.00.375.696 I load_tensors: offloaded 25/25 layers to GPU
0.00.375.727 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.375.733 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.377.243 I llama_init_from_model: n_seq_max     = 1
0.00.377.253 I llama_init_from_model: n_ctx         = 128
0.00.377.254 I llama_init_from_model: n_ctx_per_seq = 128
0.00.377.254 I llama_init_from_model: n_batch       = 128
0.00.377.255 I llama_init_from_model: n_ubatch      = 128
0.00.377.255 I llama_init_from_model: flash_attn    = 0
0.00.377.257 I llama_init_from_model: freq_base     = 10000.0
0.00.377.258 I llama_init_from_model: freq_scale    = 1
0.00.377.258 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.377.260 I ggml_metal_init: allocating
0.00.377.329 I ggml_metal_init: found device: Apple M4
0.00.377.342 I ggml_metal_init: picking default device: Apple M4
0.00.379.000 I ggml_metal_init: using embedded metal library
0.00.384.646 I ggml_metal_init: GPU name:   Apple M4
0.00.384.659 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.384.660 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.384.661 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.384.662 I ggml_metal_init: simdgroup reduction   = true
0.00.384.662 I ggml_metal_init: simdgroup matrix mul. = true
0.00.384.663 I ggml_metal_init: has residency sets    = true
0.00.384.663 I ggml_metal_init: has bfloat            = true
0.00.384.663 I ggml_metal_init: use bfloat            = true
0.00.384.668 I ggml_metal_init: hasUnifiedMemory      = true
0.00.384.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.406.070 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.409.858 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.409.866 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.409.900 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.413.412 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.413.414 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.413.415 I llama_init_from_model: graph nodes  = 967
0.00.413.415 I llama_init_from_model: graph splits = 2
0.00.413.419 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.413.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.348 I 
0.00.445.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.445.440 I perplexity: tokenizing the input ..
0.00.451.182 I perplexity: tokenization took 5.74 ms
0.00.451.199 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.585.647 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.587.169 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.587.182 I llama_perf_context_print:        load time =     435.25 ms
0.00.587.183 I llama_perf_context_print: prompt eval time =     133.72 ms /   128 tokens (    1.04 ms per token,   957.22 tokens per second)
0.00.587.183 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.587.184 I llama_perf_context_print:       total time =     141.84 ms /   129 tokens
0.00.587.550 I ggml_metal_free: deallocating

real	0m0.603s
user	0m0.078s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.773 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.910 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.915 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.917 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.918 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.918 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.918 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.920 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.922 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.922 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.926 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.716 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.693 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.458 I llama_model_loader: - type  f32:  194 tensors
0.00.024.458 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.459 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.459 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.460 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.461 I print_info: file format = GGUF V3 (latest)
0.00.024.461 I print_info: file type   = Q3_K - Medium
0.00.024.462 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.587 I load: special tokens cache size = 25
0.00.038.465 I load: token to piece cache size = 0.2984 MB
0.00.038.468 I print_info: arch             = gptneox
0.00.038.468 I print_info: vocab_only       = 0
0.00.038.468 I print_info: n_ctx_train      = 2048
0.00.038.468 I print_info: n_embd           = 2048
0.00.038.469 I print_info: n_layer          = 24
0.00.038.471 I print_info: n_head           = 16
0.00.038.472 I print_info: n_head_kv        = 16
0.00.038.472 I print_info: n_rot            = 32
0.00.038.473 I print_info: n_swa            = 0
0.00.038.473 I print_info: n_embd_head_k    = 128
0.00.038.473 I print_info: n_embd_head_v    = 128
0.00.038.474 I print_info: n_gqa            = 1
0.00.038.475 I print_info: n_embd_k_gqa     = 2048
0.00.038.475 I print_info: n_embd_v_gqa     = 2048
0.00.038.476 I print_info: f_norm_eps       = 1.0e-05
0.00.038.476 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.476 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.476 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.478 I print_info: f_logit_scale    = 0.0e+00
0.00.038.479 I print_info: n_ff             = 8192
0.00.038.479 I print_info: n_expert         = 0
0.00.038.479 I print_info: n_expert_used    = 0
0.00.038.479 I print_info: causal attn      = 1
0.00.038.480 I print_info: pooling type     = 0
0.00.038.480 I print_info: rope type        = 2
0.00.038.480 I print_info: rope scaling     = linear
0.00.038.481 I print_info: freq_base_train  = 10000.0
0.00.038.481 I print_info: freq_scale_train = 1
0.00.038.481 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.481 I print_info: rope_finetuned   = unknown
0.00.038.481 I print_info: ssm_d_conv       = 0
0.00.038.482 I print_info: ssm_d_inner      = 0
0.00.038.482 I print_info: ssm_d_state      = 0
0.00.038.482 I print_info: ssm_dt_rank      = 0
0.00.038.482 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.482 I print_info: model type       = 1.4B
0.00.038.483 I print_info: model params     = 1.41 B
0.00.038.483 I print_info: general.name     = 1.4B
0.00.038.483 I print_info: vocab type       = BPE
0.00.038.483 I print_info: n_vocab          = 50304
0.00.038.484 I print_info: n_merges         = 50009
0.00.038.484 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.485 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.485 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.486 I print_info: LF token         = 128 'Ä'
0.00.038.486 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.486 I print_info: max token length = 1024
0.00.437.867 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.886 I load_tensors: offloading output layer to GPU
0.00.437.887 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.923 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.924 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.439.302 I llama_init_from_model: n_seq_max     = 1
0.00.439.311 I llama_init_from_model: n_ctx         = 128
0.00.439.311 I llama_init_from_model: n_ctx_per_seq = 128
0.00.439.312 I llama_init_from_model: n_batch       = 128
0.00.439.312 I llama_init_from_model: n_ubatch      = 128
0.00.439.312 I llama_init_from_model: flash_attn    = 0
0.00.439.315 I llama_init_from_model: freq_base     = 10000.0
0.00.439.316 I llama_init_from_model: freq_scale    = 1
0.00.439.316 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.439.319 I ggml_metal_init: allocating
0.00.439.401 I ggml_metal_init: found device: Apple M4
0.00.439.414 I ggml_metal_init: picking default device: Apple M4
0.00.441.211 I ggml_metal_init: using embedded metal library
0.00.446.784 I ggml_metal_init: GPU name:   Apple M4
0.00.446.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.791 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.792 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.792 I ggml_metal_init: simdgroup reduction   = true
0.00.446.792 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.793 I ggml_metal_init: has residency sets    = true
0.00.446.793 I ggml_metal_init: has bfloat            = true
0.00.446.793 I ggml_metal_init: use bfloat            = true
0.00.446.795 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.786 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.470.422 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.470.433 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.470.485 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.473.884 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.473.886 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.473.887 I llama_init_from_model: graph nodes  = 967
0.00.473.887 I llama_init_from_model: graph splits = 2
0.00.473.890 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.473.890 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.217 I 
0.00.501.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.501.321 I perplexity: tokenizing the input ..
0.00.508.388 I perplexity: tokenization took 7.064 ms
0.00.508.410 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.641.555 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.642.903 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.642.918 I llama_perf_context_print:        load time =     492.43 ms
0.00.642.919 I llama_perf_context_print: prompt eval time =     132.24 ms /   128 tokens (    1.03 ms per token,   967.97 tokens per second)
0.00.642.920 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.642.920 I llama_perf_context_print:       total time =     141.71 ms /   129 tokens
0.00.643.308 I ggml_metal_free: deallocating

real	0m0.657s
user	0m0.080s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.793 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.810 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.811 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.815 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.817 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.818 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.605 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.459 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.459 I llama_model_loader: - type  f32:  194 tensors
0.00.025.460 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.460 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.460 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.461 I print_info: file format = GGUF V3 (latest)
0.00.025.461 I print_info: file type   = Q4_K - Medium
0.00.025.462 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.372 I load: special tokens cache size = 25
0.00.039.070 I load: token to piece cache size = 0.2984 MB
0.00.039.073 I print_info: arch             = gptneox
0.00.039.073 I print_info: vocab_only       = 0
0.00.039.073 I print_info: n_ctx_train      = 2048
0.00.039.074 I print_info: n_embd           = 2048
0.00.039.074 I print_info: n_layer          = 24
0.00.039.077 I print_info: n_head           = 16
0.00.039.077 I print_info: n_head_kv        = 16
0.00.039.078 I print_info: n_rot            = 32
0.00.039.078 I print_info: n_swa            = 0
0.00.039.078 I print_info: n_embd_head_k    = 128
0.00.039.078 I print_info: n_embd_head_v    = 128
0.00.039.079 I print_info: n_gqa            = 1
0.00.039.080 I print_info: n_embd_k_gqa     = 2048
0.00.039.080 I print_info: n_embd_v_gqa     = 2048
0.00.039.081 I print_info: f_norm_eps       = 1.0e-05
0.00.039.081 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.082 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.082 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.082 I print_info: f_logit_scale    = 0.0e+00
0.00.039.083 I print_info: n_ff             = 8192
0.00.039.083 I print_info: n_expert         = 0
0.00.039.083 I print_info: n_expert_used    = 0
0.00.039.083 I print_info: causal attn      = 1
0.00.039.083 I print_info: pooling type     = 0
0.00.039.083 I print_info: rope type        = 2
0.00.039.084 I print_info: rope scaling     = linear
0.00.039.086 I print_info: freq_base_train  = 10000.0
0.00.039.086 I print_info: freq_scale_train = 1
0.00.039.086 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.086 I print_info: rope_finetuned   = unknown
0.00.039.087 I print_info: ssm_d_conv       = 0
0.00.039.087 I print_info: ssm_d_inner      = 0
0.00.039.087 I print_info: ssm_d_state      = 0
0.00.039.087 I print_info: ssm_dt_rank      = 0
0.00.039.087 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.087 I print_info: model type       = 1.4B
0.00.039.088 I print_info: model params     = 1.41 B
0.00.039.088 I print_info: general.name     = 1.4B
0.00.039.088 I print_info: vocab type       = BPE
0.00.039.089 I print_info: n_vocab          = 50304
0.00.039.089 I print_info: n_merges         = 50009
0.00.039.089 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.089 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.089 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.089 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: LF token         = 128 'Ä'
0.00.039.090 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.093 I print_info: max token length = 1024
0.00.506.808 I load_tensors: offloading 24 repeating layers to GPU
0.00.506.822 I load_tensors: offloading output layer to GPU
0.00.506.823 I load_tensors: offloaded 25/25 layers to GPU
0.00.506.856 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.506.862 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.508.448 I llama_init_from_model: n_seq_max     = 1
0.00.508.452 I llama_init_from_model: n_ctx         = 128
0.00.508.453 I llama_init_from_model: n_ctx_per_seq = 128
0.00.508.453 I llama_init_from_model: n_batch       = 128
0.00.508.454 I llama_init_from_model: n_ubatch      = 128
0.00.508.454 I llama_init_from_model: flash_attn    = 0
0.00.508.456 I llama_init_from_model: freq_base     = 10000.0
0.00.508.456 I llama_init_from_model: freq_scale    = 1
0.00.508.457 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.508.459 I ggml_metal_init: allocating
0.00.508.534 I ggml_metal_init: found device: Apple M4
0.00.508.548 I ggml_metal_init: picking default device: Apple M4
0.00.510.229 I ggml_metal_init: using embedded metal library
0.00.516.622 I ggml_metal_init: GPU name:   Apple M4
0.00.516.627 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.516.628 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.516.629 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.516.630 I ggml_metal_init: simdgroup reduction   = true
0.00.516.630 I ggml_metal_init: simdgroup matrix mul. = true
0.00.516.630 I ggml_metal_init: has residency sets    = true
0.00.516.631 I ggml_metal_init: has bfloat            = true
0.00.516.631 I ggml_metal_init: use bfloat            = true
0.00.516.632 I ggml_metal_init: hasUnifiedMemory      = true
0.00.516.634 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.535.454 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.539.084 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.539.091 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.539.117 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.542.367 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.542.369 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.542.369 I llama_init_from_model: graph nodes  = 967
0.00.542.369 I llama_init_from_model: graph splits = 2
0.00.542.373 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.542.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.203 I 
0.00.570.288 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.570.304 I perplexity: tokenizing the input ..
0.00.575.798 I perplexity: tokenization took 5.491 ms
0.00.575.809 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.710.118 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.711.443 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.711.459 I llama_perf_context_print:        load time =     560.40 ms
0.00.711.461 I llama_perf_context_print: prompt eval time =     134.08 ms /   128 tokens (    1.05 ms per token,   954.64 tokens per second)
0.00.711.461 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.711.462 I llama_perf_context_print:       total time =     141.26 ms /   129 tokens
0.00.711.851 I ggml_metal_free: deallocating

real	0m0.728s
user	0m0.076s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.648 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.747 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.752 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.758 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.759 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.760 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.761 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.761 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.762 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.762 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.762 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.763 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.763 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.765 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.765 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.766 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.392 I llama_model_loader: - type  f32:  194 tensors
0.00.024.392 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.392 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.393 I print_info: file format = GGUF V3 (latest)
0.00.024.394 I print_info: file type   = Q5_K - Medium
0.00.024.395 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.501 I load: special tokens cache size = 25
0.00.038.348 I load: token to piece cache size = 0.2984 MB
0.00.038.351 I print_info: arch             = gptneox
0.00.038.351 I print_info: vocab_only       = 0
0.00.038.352 I print_info: n_ctx_train      = 2048
0.00.038.352 I print_info: n_embd           = 2048
0.00.038.352 I print_info: n_layer          = 24
0.00.038.355 I print_info: n_head           = 16
0.00.038.358 I print_info: n_head_kv        = 16
0.00.038.358 I print_info: n_rot            = 32
0.00.038.358 I print_info: n_swa            = 0
0.00.038.358 I print_info: n_embd_head_k    = 128
0.00.038.359 I print_info: n_embd_head_v    = 128
0.00.038.359 I print_info: n_gqa            = 1
0.00.038.360 I print_info: n_embd_k_gqa     = 2048
0.00.038.361 I print_info: n_embd_v_gqa     = 2048
0.00.038.361 I print_info: f_norm_eps       = 1.0e-05
0.00.038.362 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.362 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.362 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.362 I print_info: f_logit_scale    = 0.0e+00
0.00.038.363 I print_info: n_ff             = 8192
0.00.038.363 I print_info: n_expert         = 0
0.00.038.364 I print_info: n_expert_used    = 0
0.00.038.364 I print_info: causal attn      = 1
0.00.038.364 I print_info: pooling type     = 0
0.00.038.364 I print_info: rope type        = 2
0.00.038.364 I print_info: rope scaling     = linear
0.00.038.365 I print_info: freq_base_train  = 10000.0
0.00.038.365 I print_info: freq_scale_train = 1
0.00.038.365 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.365 I print_info: rope_finetuned   = unknown
0.00.038.366 I print_info: ssm_d_conv       = 0
0.00.038.366 I print_info: ssm_d_inner      = 0
0.00.038.366 I print_info: ssm_d_state      = 0
0.00.038.366 I print_info: ssm_dt_rank      = 0
0.00.038.366 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.366 I print_info: model type       = 1.4B
0.00.038.367 I print_info: model params     = 1.41 B
0.00.038.371 I print_info: general.name     = 1.4B
0.00.038.372 I print_info: vocab type       = BPE
0.00.038.372 I print_info: n_vocab          = 50304
0.00.038.372 I print_info: n_merges         = 50009
0.00.038.374 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.374 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.374 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.374 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.374 I print_info: LF token         = 128 'Ä'
0.00.038.375 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.375 I print_info: max token length = 1024
0.00.589.079 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.088 I load_tensors: offloading output layer to GPU
0.00.589.089 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.120 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.121 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.590.544 I llama_init_from_model: n_seq_max     = 1
0.00.590.549 I llama_init_from_model: n_ctx         = 128
0.00.590.549 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.550 I llama_init_from_model: n_batch       = 128
0.00.590.550 I llama_init_from_model: n_ubatch      = 128
0.00.590.551 I llama_init_from_model: flash_attn    = 0
0.00.590.552 I llama_init_from_model: freq_base     = 10000.0
0.00.590.552 I llama_init_from_model: freq_scale    = 1
0.00.590.553 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.555 I ggml_metal_init: allocating
0.00.590.616 I ggml_metal_init: found device: Apple M4
0.00.590.641 I ggml_metal_init: picking default device: Apple M4
0.00.592.201 I ggml_metal_init: using embedded metal library
0.00.598.584 I ggml_metal_init: GPU name:   Apple M4
0.00.598.588 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.589 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.591 I ggml_metal_init: simdgroup reduction   = true
0.00.598.591 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.592 I ggml_metal_init: has residency sets    = true
0.00.598.592 I ggml_metal_init: has bfloat            = true
0.00.598.592 I ggml_metal_init: use bfloat            = true
0.00.598.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.682 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.192 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.195 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.223 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.877 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.879 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.879 I llama_init_from_model: graph nodes  = 967
0.00.622.880 I llama_init_from_model: graph splits = 2
0.00.622.882 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.729 I 
0.00.660.809 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.833 I perplexity: tokenizing the input ..
0.00.666.870 I perplexity: tokenization took 6.036 ms
0.00.666.882 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.362 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.813.866 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.813.880 I llama_perf_context_print:        load time =     652.07 ms
0.00.813.881 I llama_perf_context_print: prompt eval time =     145.25 ms /   128 tokens (    1.13 ms per token,   881.25 tokens per second)
0.00.813.881 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.882 I llama_perf_context_print:       total time =     153.16 ms /   129 tokens
0.00.814.254 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.076s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.175 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.314 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.317 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.365 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.303 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.304 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.305 I llama_model_loader: - type  f32:  194 tensors
0.00.026.305 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.306 I print_info: file format = GGUF V3 (latest)
0.00.026.306 I print_info: file type   = Q6_K
0.00.026.307 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.445 I load: special tokens cache size = 25
0.00.040.443 I load: token to piece cache size = 0.2984 MB
0.00.040.447 I print_info: arch             = gptneox
0.00.040.448 I print_info: vocab_only       = 0
0.00.040.448 I print_info: n_ctx_train      = 2048
0.00.040.448 I print_info: n_embd           = 2048
0.00.040.448 I print_info: n_layer          = 24
0.00.040.453 I print_info: n_head           = 16
0.00.040.454 I print_info: n_head_kv        = 16
0.00.040.456 I print_info: n_rot            = 32
0.00.040.456 I print_info: n_swa            = 0
0.00.040.457 I print_info: n_embd_head_k    = 128
0.00.040.457 I print_info: n_embd_head_v    = 128
0.00.040.457 I print_info: n_gqa            = 1
0.00.040.458 I print_info: n_embd_k_gqa     = 2048
0.00.040.458 I print_info: n_embd_v_gqa     = 2048
0.00.040.459 I print_info: f_norm_eps       = 1.0e-05
0.00.040.459 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.459 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.459 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.460 I print_info: f_logit_scale    = 0.0e+00
0.00.040.461 I print_info: n_ff             = 8192
0.00.040.462 I print_info: n_expert         = 0
0.00.040.462 I print_info: n_expert_used    = 0
0.00.040.462 I print_info: causal attn      = 1
0.00.040.462 I print_info: pooling type     = 0
0.00.040.463 I print_info: rope type        = 2
0.00.040.463 I print_info: rope scaling     = linear
0.00.040.463 I print_info: freq_base_train  = 10000.0
0.00.040.463 I print_info: freq_scale_train = 1
0.00.040.464 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.464 I print_info: rope_finetuned   = unknown
0.00.040.465 I print_info: ssm_d_conv       = 0
0.00.040.465 I print_info: ssm_d_inner      = 0
0.00.040.465 I print_info: ssm_d_state      = 0
0.00.040.465 I print_info: ssm_dt_rank      = 0
0.00.040.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.468 I print_info: model type       = 1.4B
0.00.040.469 I print_info: model params     = 1.41 B
0.00.040.469 I print_info: general.name     = 1.4B
0.00.040.469 I print_info: vocab type       = BPE
0.00.040.469 I print_info: n_vocab          = 50304
0.00.040.469 I print_info: n_merges         = 50009
0.00.040.470 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.470 I print_info: LF token         = 128 'Ä'
0.00.040.471 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.471 I print_info: max token length = 1024
0.00.340.322 I load_tensors: offloading 24 repeating layers to GPU
0.00.340.329 I load_tensors: offloading output layer to GPU
0.00.340.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.340.349 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.340.350 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.341.169 I llama_init_from_model: n_seq_max     = 1
0.00.341.176 I llama_init_from_model: n_ctx         = 128
0.00.341.176 I llama_init_from_model: n_ctx_per_seq = 128
0.00.341.177 I llama_init_from_model: n_batch       = 128
0.00.341.180 I llama_init_from_model: n_ubatch      = 128
0.00.341.181 I llama_init_from_model: flash_attn    = 0
0.00.341.182 I llama_init_from_model: freq_base     = 10000.0
0.00.341.183 I llama_init_from_model: freq_scale    = 1
0.00.341.183 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.341.185 I ggml_metal_init: allocating
0.00.341.223 I ggml_metal_init: found device: Apple M4
0.00.341.236 I ggml_metal_init: picking default device: Apple M4
0.00.342.357 I ggml_metal_init: using embedded metal library
0.00.346.871 I ggml_metal_init: GPU name:   Apple M4
0.00.346.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.881 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.882 I ggml_metal_init: simdgroup reduction   = true
0.00.346.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.883 I ggml_metal_init: has residency sets    = true
0.00.346.883 I ggml_metal_init: has bfloat            = true
0.00.346.883 I ggml_metal_init: use bfloat            = true
0.00.346.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.357.701 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.359.288 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.359.291 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.359.309 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.360.859 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.360.860 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.360.861 I llama_init_from_model: graph nodes  = 967
0.00.360.861 I llama_init_from_model: graph splits = 2
0.00.360.862 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.360.863 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.395.835 I 
0.00.395.869 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.395.878 I perplexity: tokenizing the input ..
0.00.399.814 I perplexity: tokenization took 3.934 ms
0.00.399.823 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.548.893 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.550.227 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.550.240 I llama_perf_context_print:        load time =     385.66 ms
0.00.550.241 I llama_perf_context_print: prompt eval time =     148.84 ms /   128 tokens (    1.16 ms per token,   859.96 tokens per second)
0.00.550.242 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.550.242 I llama_perf_context_print:       total time =     154.41 ms /   129 tokens
0.00.550.605 I ggml_metal_free: deallocating

real	0m0.567s
user	0m0.066s
sys	0m0.076s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.260 I build: 4587 (e51c47b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.356 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.113 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.122 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.138 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.139 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.139 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.140 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.141 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.144 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.149 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.991 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.956 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.719 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.719 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.720 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.720 I llama_model_loader: - type  f32:  194 tensors
0.00.052.721 I llama_model_loader: - type  f16:   98 tensors
0.00.052.721 I print_info: file format = GGUF V3 (latest)
0.00.052.722 I print_info: file type   = all F32 (guessed)
0.00.052.723 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.048 I load: special tokens cache size = 25
0.00.072.849 I load: token to piece cache size = 0.2984 MB
0.00.072.852 I print_info: arch             = gptneox
0.00.072.852 I print_info: vocab_only       = 0
0.00.072.853 I print_info: n_ctx_train      = 2048
0.00.072.853 I print_info: n_embd           = 2048
0.00.072.853 I print_info: n_layer          = 24
0.00.072.856 I print_info: n_head           = 16
0.00.072.857 I print_info: n_head_kv        = 16
0.00.072.857 I print_info: n_rot            = 32
0.00.072.857 I print_info: n_swa            = 0
0.00.072.858 I print_info: n_embd_head_k    = 128
0.00.072.858 I print_info: n_embd_head_v    = 128
0.00.072.861 I print_info: n_gqa            = 1
0.00.072.862 I print_info: n_embd_k_gqa     = 2048
0.00.072.862 I print_info: n_embd_v_gqa     = 2048
0.00.072.863 I print_info: f_norm_eps       = 1.0e-05
0.00.072.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.864 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.864 I print_info: f_logit_scale    = 0.0e+00
0.00.072.865 I print_info: n_ff             = 8192
0.00.072.865 I print_info: n_expert         = 0
0.00.072.865 I print_info: n_expert_used    = 0
0.00.072.865 I print_info: causal attn      = 1
0.00.072.865 I print_info: pooling type     = 0
0.00.072.865 I print_info: rope type        = 2
0.00.072.866 I print_info: rope scaling     = linear
0.00.072.866 I print_info: freq_base_train  = 10000.0
0.00.072.866 I print_info: freq_scale_train = 1
0.00.072.866 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.867 I print_info: rope_finetuned   = unknown
0.00.072.867 I print_info: ssm_d_conv       = 0
0.00.072.867 I print_info: ssm_d_inner      = 0
0.00.072.867 I print_info: ssm_d_state      = 0
0.00.072.867 I print_info: ssm_dt_rank      = 0
0.00.072.867 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.868 I print_info: model type       = 1.4B
0.00.072.868 I print_info: model params     = 1.41 B
0.00.072.868 I print_info: general.name     = 1.4B
0.00.072.869 I print_info: vocab type       = BPE
0.00.072.869 I print_info: n_vocab          = 50304
0.00.072.870 I print_info: n_merges         = 50009
0.00.072.870 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.870 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.870 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.870 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.871 I print_info: LF token         = 128 'Ä'
0.00.072.871 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.871 I print_info: max token length = 1024
0.01.344.381 I load_tensors: offloading 24 repeating layers to GPU
0.01.344.385 I load_tensors: offloading output layer to GPU
0.01.344.386 I load_tensors: offloaded 25/25 layers to GPU
0.01.344.408 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.344.409 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.345.381 I llama_init_from_model: n_seq_max     = 1
0.01.345.382 I llama_init_from_model: n_ctx         = 128
0.01.345.382 I llama_init_from_model: n_ctx_per_seq = 128
0.01.345.383 I llama_init_from_model: n_batch       = 128
0.01.345.383 I llama_init_from_model: n_ubatch      = 128
0.01.345.383 I llama_init_from_model: flash_attn    = 0
0.01.345.384 I llama_init_from_model: freq_base     = 10000.0
0.01.345.384 I llama_init_from_model: freq_scale    = 1
0.01.345.384 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.345.385 I ggml_metal_init: allocating
0.01.345.426 I ggml_metal_init: found device: Apple M4
0.01.345.433 I ggml_metal_init: picking default device: Apple M4
0.01.346.424 I ggml_metal_init: using embedded metal library
0.01.350.230 I ggml_metal_init: GPU name:   Apple M4
0.01.350.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.350.233 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.350.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.350.233 I ggml_metal_init: simdgroup reduction   = true
0.01.350.234 I ggml_metal_init: simdgroup matrix mul. = true
0.01.350.234 I ggml_metal_init: has residency sets    = true
0.01.350.234 I ggml_metal_init: has bfloat            = true
0.01.350.234 I ggml_metal_init: use bfloat            = true
0.01.350.235 I ggml_metal_init: hasUnifiedMemory      = true
0.01.350.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.360.863 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.362.602 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.362.604 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.362.620 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.364.281 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.364.282 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.364.282 I llama_init_from_model: graph nodes  = 967
0.01.364.283 I llama_init_from_model: graph splits = 2
0.01.364.284 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.364.284 I 
0.01.364.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.364.321 I compute_imatrix: tokenizing the input ..
0.01.368.365 I compute_imatrix: tokenization took 4.043 ms
0.01.368.366 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.583.247 I compute_imatrix: 0.21 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.585.917 I llama_perf_context_print:        load time =    1561.89 ms
0.01.585.918 I llama_perf_context_print: prompt eval time =     213.14 ms /   128 tokens (    1.67 ms per token,   600.53 tokens per second)
0.01.585.919 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.585.919 I llama_perf_context_print:       total time =    1564.55 ms /   129 tokens
0.01.586.465 I ggml_metal_free: deallocating

real	0m1.805s
user	0m0.126s
sys	0m0.264s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4587 (e51c47b4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121e082b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121e08770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121e08d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121e092d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121e09880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121e09e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121e0a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121e0a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121e0af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121e0b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121e0b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121e0be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121e0c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121e0d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121e0d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121e0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121e0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121e0ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121e0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121e0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121e10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121e10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121e112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121e11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121e12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121e12550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121e12b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121e137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121e13d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121e13fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121e14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121e14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121e14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121e15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121e157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121e15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121e16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121e165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121e16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121e16ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121e17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121e17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121e17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121e18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121e18420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121e18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121e19040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121e19960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121e19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121e1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121e1ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121e1b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121e1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121e1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121e1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121e1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121e1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121e1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121e1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121e1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121e1e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121e1e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121e1ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121e1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121e1f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121e1f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121e1fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121e202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121e20770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121e20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121e210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121e21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121e219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121e21f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121e22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121e229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121e22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121e23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121e239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121e23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121e24470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121e249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121e24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121e25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121e259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121e25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121e26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121e269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121e26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121e27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121e27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121e27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121e28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121e28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121e28ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121e29420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121e29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121e19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121e29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121e2a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121e2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121e2b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121e2b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121e2bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121e2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121e2c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121e2cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121e2d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121e2d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121e2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121e2e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121e2e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121e2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121e2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121e2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121e2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121e2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121e301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121e30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121e30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121e30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121e31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121e318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121e31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121e32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121e326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121e32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121e33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121e334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121e33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121e33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121e34280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121e34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121e34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121e35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121e35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121e359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121e35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121e362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121e36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121e36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121e370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121e37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121e37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121e37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121e38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121e387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121e38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121e39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121e395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121e39a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121e39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121e3a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121e3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121e3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121e3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121e3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121e3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121e3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121e3c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121e3c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121e3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121e3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121e3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121e3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121e3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121e3e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121e3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121e3eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121e3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121e3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121e3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121e40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121e404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121e40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121e40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121e412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121e41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121e41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121e42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121e42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121e429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121e42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121e43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121e437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121e43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121e440e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121e44580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121e44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121e44ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121e45360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121e45800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121e45ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121e461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121e46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121e46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121e471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121e474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121e47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121e480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121e486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121e48ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121e49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121e49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121e49c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121e4a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121e4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121e4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121e4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121e4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121e4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121e4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121e4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121e4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121e4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121e4da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121e4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121e4e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121e4ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121e4ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121e4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121e4fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121e4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121e504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121e50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121e50f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121e514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121e51a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121e51f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121e524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121e52a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121e52f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121e534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121e539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121e53f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121e54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121e549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121e54f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121e55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121e559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121e55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121e56470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121e569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121e56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121e57460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121e579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121e57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121e58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121e589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121e58ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121e59440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121e59990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121e59ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121e5a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121e5a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121e5aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121e5b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121e5b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121e5bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121e5c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121e5c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121e5ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121e5d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121e5d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121e5dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121e5e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121e5e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121e5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121e5f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121e5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121e5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121e60060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121e60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121e609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121e60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121e612e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121e61780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121e61c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121e620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121e62560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121e62a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121e62ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121e633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121e63b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121e64230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121e64950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121e65070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121e65330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121e65b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121e65de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121e663f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.719.318 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.322 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121e498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121e47d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121e660a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121e47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121e48380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121e1b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121e1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121e1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121e49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121e12810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121e19300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121e19c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121e18cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121e1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121e1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121e11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121e1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121e2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121e655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121e149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121e14cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121e4a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121e48990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121e12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121e130e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121e133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121e66850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121e66b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121e66dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121e67090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121e67350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121e67610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121e678d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121e67b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121e67e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121e68110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121e683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121e68690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121e68950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121e68c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121e68ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121e69190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121e69450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121e69710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121e699d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121e69c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121e69f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121e6a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121e6a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121e6a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121e6aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121e6ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121e6afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121e6b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121e6b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121e6b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121e6bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121e6bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121e6c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121e6c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121e6c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121e6c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121e6cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121e6ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121e6d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121e6d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121e6d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121e6d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121e6dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121e6de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121e6e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121e6e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121e6e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121e6e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121e6ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121e6ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121e6f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121e6f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121e6f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121e6fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121e6fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121e6ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121e70250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121e70510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121e707d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121e70a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121e70d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121e71010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121e712d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121e71590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121e71850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121e71b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121e71dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121e72090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121e72350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121e72610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121e728d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121e72b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121e72e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121e73110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121e733d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121e73690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121e73950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121e73c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121e73ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121e74190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121e74450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121e74710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121e749d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121e74c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121e74f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121e75210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121e754d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121e75790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121e75a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121e75d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121e75fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121e76290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121e76550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121e76810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121e76ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121e76d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121e77050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121e77310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121e775d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121e77890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121e77b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121e77e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121e780d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121e78390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121e78650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121e78910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121e78bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121e78e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121e79150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121e79410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121e796d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121e79990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121e79c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121e79f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121e7a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121e7a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121e7a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121e7aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121e7acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121e7af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121e7b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121e7b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121e7b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121e7ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121e7bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121e7c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121e7c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121e7c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121e7c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121e7cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121e7cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121e7d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121e7d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121e7d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121e7d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121e7db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121e7de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121e7e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121e7e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121e7e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121e7e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121e7ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121e7eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121e7f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121e7f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121e7f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121e7f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121e7fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121e7ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121e80210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121e804d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121e80790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121e80a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121e80d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121e80fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121e81290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121e81550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121e81810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121e81ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121e81d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121e82050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121e82310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121e825d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121e82890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121e82b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121e82e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121e830d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121e83390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121e83650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121e83910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121e83bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121e83e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121e84150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121e84410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121e846d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121e84990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121e84c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121e84f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121e85450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121e85990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121e85c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121e860f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121e86590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121e86a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121e871e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121e874a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121e87760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121e87bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121e88040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121e884b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121e88920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121e88d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121e89200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121e89670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121e89ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121e89f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121e8a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121e8a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121e8aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121e8b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121e8b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121e8b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121e8be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121e8c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121e8c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121e8cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121e8d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121e8d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121e8d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121e8dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121e8e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121e8e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121e8eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121e8ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121e8f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121e8f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121e8fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121e900f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121e90560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121e909d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121e90e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121e912b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121e91720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121e91b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121e92000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121e92470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121e928e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121e92d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121e931c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121e93630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121e93aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121e93f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121e94380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121e947f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121e94c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121e950d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121e95540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121e959b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121e95e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121e96290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121e96700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121e96b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121e96fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121e97450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121e978c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121e97d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121e981a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121e98610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121e98a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121e98ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121e99360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121e997d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121e99c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121e9a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121e9a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121e9a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121e9ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121e9b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121e9bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121e9c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121e9cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121e9d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121e9d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121e9db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121e9e150 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107e046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107e04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107e04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107e05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107e058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107e05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107e06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107e065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107e06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107e06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107e07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107e07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107e08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107e08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107e09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107e09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107e0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107e0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107e0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107e0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107e0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107e0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107e0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107e0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107e0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107e0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107e0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107e0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107e0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107e0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107e0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107e0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107e0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107e10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107e104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107e10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107e10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107e111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107e11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107e11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107e11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107e123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107e12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107e12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107e13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107e13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107e139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107e13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107e142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107e14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107e14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107e15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107e15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107e158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107e15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107e161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107e16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107e16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107e170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107e17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107e17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107e17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107e18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107e186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107e18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107e18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107e19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107e198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107e19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107e1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107e1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107e1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107e1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107e1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107e1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107e1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107e1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107e1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107e1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107e1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107e1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107e1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107e1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107e1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107e1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107e1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107e1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107e1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107e1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107e1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107e1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107e20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107e20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107e20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107e21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107e214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107e21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107e21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107e22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107e226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107e22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107e22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107e233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107e23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107e23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107e243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107e24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107e24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107e25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107e25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107e259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107e25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107e262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107e26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107e26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107e27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107e27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107e278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107e27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107e281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107e28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107e28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107e28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107e29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107e29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107e29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107e2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107e2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107e2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107e2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107e2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107e2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107e2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107e2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107e2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107e2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107e2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107e2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107e2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107e2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107e2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107e2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107e2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107e2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107e2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107e2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107e2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107e2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107e30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107e306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107e30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107e30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107e31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107e318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107e31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107e32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107e32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107e32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107e32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107e33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107e337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107e33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107e340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107e34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107e34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107e34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107e35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107e356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107e35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107e35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107e36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107e36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107e36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107e37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107e375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107e37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107e37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107e38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107e387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107e38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107e39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107e394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107e39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107e39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107e3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107e3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107e3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107e3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107e3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107e3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107e3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107e3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107e3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107e3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107e3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107e3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107e3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107e3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107e3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107e3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x111f04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x111f046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x111f04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x111f04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x111f053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x111f05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x111f05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x111f06250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x111f066c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x111f06b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x111f07680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x111f07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x111f07c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x111f08070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x111f084e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x111f08950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x111f08dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x111f09230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x111f096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x111f09b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x111f09f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x111f0a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x111f0a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x111f0acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x111f0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x111f0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x111f0ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x111f0be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x111f0c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x111f0c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x111f0cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x111f0d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x111f0d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x111f0d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x111f0dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x111f0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x111f0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x111f0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x111f0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x111f0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x111f0f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x111f0fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x111f10120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x111f10590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x111f10a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x111f10e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x111f112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x111f11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x111f11bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x111f12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x111f124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x111f12910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x111f12d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x111f131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x111f13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x111f13ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x111f13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x111f143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x111f14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x111f14c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x111f15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x111f15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x111f159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x111f15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x111f162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x111f16730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x111f16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x111f17010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x111f17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x111f178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x111f17d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x111f181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x111f18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x111f18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x111f18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x111f19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x111f19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x111f19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x111f1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x111f1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x111f1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x111f1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x111f1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x111f1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x111f1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x111f1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x111f1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x111f1d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x111f1d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x111f1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x111f1e5b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.797s
user	0m0.284s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4587 (e51c47b4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a1077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a107ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a108490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a108a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a108ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a1095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a109b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a10a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a10a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a10abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a10b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a10b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a10c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a10c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a10d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a10d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a10ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a10e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a10ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a10f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a10fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a110320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a110a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a1112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a111a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a111cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a1122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a112f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a113480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a113740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a113be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a113ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a114730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a114c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a114f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a1153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a115870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a115d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a1161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a116650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a116af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a116f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a117430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a1178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a117b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a1181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a1187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a1190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a1196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a119cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a11a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a11a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a11af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a11b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a11bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a11c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a11c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a11c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a11cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a11d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a11d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a11de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a11e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a11e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a11ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a11f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a11f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a11fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a11fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a120380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a120820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a120cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a121160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a1216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a121c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a122150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a1226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a122bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a123140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a123690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a123be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a124130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a124680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a124bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a125120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a125670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a125bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a126110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a126660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a126bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a127100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a127650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a127ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a1280f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a128640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a128b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a1290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a118dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a129550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a129d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a12a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a12a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a12acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a12b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a12b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a12bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a12c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a12c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a12ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a12d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a12d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a12dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a12e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a12e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a12eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a12eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a12f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a12f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a12fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a130270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a130710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a130bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a131050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a1314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a131990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a131e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a1322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a132770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a132c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a1330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a133550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a1339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a133e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a134330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a1347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a134c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a135110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a1355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a135a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a135ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a136390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a136830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a136cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a137170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a137610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a137ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a137f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a1383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a138890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a138d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a1391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a139670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a139b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a139fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a13a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a13a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a13ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a13b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a13b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a13bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a13c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a13c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a13c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a13cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a13d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a13d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a13dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a13e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a13e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a13e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a13ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a13f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a13f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a13fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a1400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a140570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a140a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a140eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a141350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a1417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a141c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a142130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a1425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a142a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a142f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a1433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a143850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a143cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a144190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a144630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a144ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a144f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a145410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a145960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a145eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a146400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a146950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a146c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a147220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a147830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a147e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a148630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a148ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a148d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a1493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a1499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a14a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a14a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a14aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a14af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a14b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a14bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a14c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a14c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a14cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a14d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a14d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a14dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a14e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a14e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a14ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a14f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a14f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a14fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a150190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a1506e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a150c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a151180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a1516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a151c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a152170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a1526c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a152c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a153160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a1536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a153c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a154150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a1546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a154bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a155140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a155690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a155be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a156130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a156680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a156bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a157120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a157670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a157bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a158110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a158660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a158bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a159100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a159650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a159ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a15a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a15a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a15ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a15b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a15b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a15bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a15c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a15c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a15cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a15d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a15d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a15db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a15e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a15e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a15e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a15ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a15f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a15f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a15fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a160110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a1605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a160a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a160ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a161390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a161830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a161cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a162170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a162610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a162b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a163280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a1639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a1640c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a1647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a164aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a165290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a165550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a165b60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.113 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.117 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148f0b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148f0bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148f0c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148f0e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148f0eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148f0eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148f0f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148f0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148f0fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148f10270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148f106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148f10d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148f11880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148f12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148f12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148f12f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148f13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148f13da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148f144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148f14c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148f153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148f15ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148f161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148f16910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148f17030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148f172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148f175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148f17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148f17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148f18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148f18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148f18ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148f19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148f1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148f1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148f20450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148f208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148f20d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148f211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148f21610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148f21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148f21ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148f22360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148f227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148f22c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148f230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148f23520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148f23990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148f23e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148f24270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148f246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148f24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148f24fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148f25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148f258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148f25d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148f26180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148f265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148f26a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148f26ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148f27340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148f277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148f27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148f28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148f28500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148f28970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148f28de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148f29250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148f296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148f29b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148f29fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148f2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148f2a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148f2acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148f2b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148f2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148f2ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148f2beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148f2c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148f2c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148f2cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148f2d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148f2d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148f2d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148f2ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148f2e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148f2e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148f2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148f2ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148f2f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148f2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148f2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148f30140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148f305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148f30a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148f30e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148f31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148f31770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148f31be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148f32050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148f324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148f32930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148f32da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148f33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148f33680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148f33af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148f33f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148f343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148f34840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148f34cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148f35120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148f35590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148f35a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148f35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148f362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148f36750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x148f36bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148f37030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148f374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148f37910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148f37d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148f381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148f38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148f38ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148f38f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148f393b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148f39820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148f39c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148f3a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148f3a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148f3a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148f3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148f3b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148f3b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148f3bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148f3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148f3c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148f3c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148f3cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148f3d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148f3d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148f3dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148f3df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148f3eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148f3ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148f3f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148f3f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148f3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148f41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148f418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148f41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148f421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148f42610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148f42a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148f42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148f43360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148f437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148f43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148f440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148f44520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148f44990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148f44e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148f45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148f456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148f45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148f45fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148f46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148f468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148f46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148f47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148f475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148f47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148f47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148f48340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148f488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148f48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148f49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148f49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148f49b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148f49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148f4a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148f4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148f4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148f4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148f4bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148f4c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148f4c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148f4ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148f4d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148f4da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148f4e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148f4e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148f4eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148f4f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148f4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148f4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148f50290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148f50850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148f50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148f513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148f51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148f51f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148f52510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148f52ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148f53090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148f53650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148f53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148f541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148f54790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148f54d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148f55310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148f558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148f55e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148f56450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148f56a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148f56fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148f57590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148f57b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148f58110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148f586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148f58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148f59250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148f59810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148f59dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148f5a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148f5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148f5af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148f5b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148f5ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148f5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148f5c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148f5cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148f5d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148f5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148f5dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148f5e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148f5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148f5ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148f5f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148f5f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x148f5fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148f603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148f608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148f60dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148f612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148f617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148f61cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148f621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148f626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148f62bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148f630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148f635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148f63ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148f63fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148f644d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148f64ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148f65600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148f65d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148f66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148f66700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148f66ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148f671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148f677c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a0046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a004b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a004fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a005430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a0058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a005d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a006180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a0065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a006a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a006ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a007340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a007a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a008530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a008ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a0094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a009c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a00a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a00aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a00b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a00b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a00c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a00c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a00cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a00d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a00dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a00dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a00e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a00e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a00eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a00efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a00f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a00f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a00fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a010080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a0104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a010960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a010dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a011240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a0116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a011b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a011f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a012400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a012870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a012ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a013150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a0135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a013a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a013ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a014310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a014780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a014bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a015060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a0154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a015940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a015db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a016220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a016790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a016c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a017100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a017570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a0179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a017e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a0182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a018730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a018ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a019010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a019480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a0198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a019d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a01a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a01a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a01aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a01af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a01b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a01b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a01bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a01c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a01c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a01c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a01ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a01d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a01d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a01db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a01dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a01e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a01e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a01ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a01f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a01f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a01fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a01ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a020370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a0207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a020c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a0210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a021530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a0219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a021e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a022280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a0226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a022b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a022fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a023440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a023cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a023f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a024400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a024870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a024ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a025150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a0255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a025a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a025ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a026310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a026780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a026bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a027060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a0274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a027940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a027db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a028220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a028690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a028b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a028f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a0293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a029850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a029cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a02a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a02a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a02aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a02ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a02b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a02b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a02bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a02c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a02c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a02c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a02cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a02d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a02d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a02dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a02df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a02e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a02e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a02eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a02f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a02f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a02f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a02fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a0302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a030740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a030bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a031020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a031490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a031900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a031d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a0321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a032650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a032ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a032f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a0333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a033810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a033c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a0340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a034560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a0349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a034e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a0352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a035720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a035b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a036000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a036470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a0368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a036d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a0371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a037630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a037aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a037f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a038380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a0387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a038c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a0390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a039540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a0399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a039e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a03a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a03a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a03ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a03afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a03b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a03b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a03bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a03c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a03c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a03ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a03cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a03d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a03d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a03dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a03e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a03e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a03e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a03ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a03f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a03f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a03fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a03ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a040430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a0408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a040d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a041180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a041d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a041fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a042280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a0426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a042b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a042fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a043440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a0438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a043d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a044190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a044600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a044a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a044ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a045350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a0457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a045c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a0460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a046510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a046980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a046df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a047260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a0476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a047b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a047fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a048420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a048890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a048d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a049170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a0495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a049a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a049ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a04a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a04a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a04ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a04b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a04b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a04b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a04bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a04c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a04c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a04cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a04cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a04d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a04d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a04dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a04e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a04e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a04ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a04eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a04f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a04f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a04fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a050060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a0504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a050940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a050db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a051220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a051690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a051b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a051f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a0523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a052850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a052cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a053130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a0535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a053a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a053e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a0542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a054760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a054bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a055040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a0554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a055920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a056390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a056ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a0571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a0578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a057bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a058020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a058620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a058c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.966s
user	0m0.236s
sys	0m0.190s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
