Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.535s
user	0m0.861s
sys	0m1.259s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target sha256
[  4%] Built target build_info
[  4%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Built target llava
[ 31%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Built target llama-quantize-stats
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 48%] Built target test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-barrier
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-chat-template
[ 63%] Built target test-gguf
[ 63%] Built target test-arg-parser
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-autorelease
[ 63%] Built target test-backend-ops
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-rope
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-batched
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-embedding
[ 73%] Built target llama-infill
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-cli
[ 81%] Built target llama-parallel
[ 81%] Built target llama-passkey
[ 81%] Generating loading.html.hpp
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-perplexity
[ 81%] Built target llama-quantize
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-run
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tts
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-gen-docs
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.309s
user	0m6.520s
sys	0m10.257s

main: quantize time =  4722.23 ms
main:    total time =  4722.23 ms

main: quantize time =  1867.08 ms
main:    total time =  1867.08 ms

main: quantize time =  1866.08 ms
main:    total time =  1866.08 ms

main: quantize time =  2076.15 ms
main:    total time =  2076.15 ms

main: quantize time =  2007.84 ms
main:    total time =  2007.84 ms

main: quantize time =  5369.10 ms
main:    total time =  5369.10 ms

main: quantize time =  5789.13 ms
main:    total time =  5789.13 ms

main: quantize time =  7014.71 ms
main:    total time =  7014.71 ms

main: quantize time =  6120.06 ms
main:    total time =  6120.06 ms

main: quantize time =  4777.87 ms
main:    total time =  4777.87 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.217 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.376 I main: llama backend init
0.00.000.384 I main: load the model and apply lora adapter, if any
0.00.042.868 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.057.193 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.217 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.218 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.218 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.219 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.221 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.222 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.222 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.223 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.223 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.224 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.225 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.228 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.229 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.229 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.065.683 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.074.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.074.925 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.074.926 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.074.926 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.074.926 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.074.927 I llama_model_loader: - type  f32:  194 tensors
0.00.074.927 I llama_model_loader: - type  f16:   98 tensors
0.00.074.928 I print_info: file format = GGUF V3 (latest)
0.00.074.929 I print_info: file type   = all F32 (guessed)
0.00.074.931 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.087.921 I load: special tokens cache size = 25
0.00.096.321 I load: token to piece cache size = 0.2984 MB
0.00.096.324 I print_info: arch             = gptneox
0.00.096.325 I print_info: vocab_only       = 0
0.00.096.325 I print_info: n_ctx_train      = 2048
0.00.096.325 I print_info: n_embd           = 2048
0.00.096.325 I print_info: n_layer          = 24
0.00.096.328 I print_info: n_head           = 16
0.00.096.330 I print_info: n_head_kv        = 16
0.00.096.330 I print_info: n_rot            = 32
0.00.096.330 I print_info: n_swa            = 0
0.00.096.330 I print_info: n_embd_head_k    = 128
0.00.096.330 I print_info: n_embd_head_v    = 128
0.00.096.331 I print_info: n_gqa            = 1
0.00.096.332 I print_info: n_embd_k_gqa     = 2048
0.00.096.333 I print_info: n_embd_v_gqa     = 2048
0.00.096.333 I print_info: f_norm_eps       = 1.0e-05
0.00.096.336 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.336 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.336 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.336 I print_info: f_logit_scale    = 0.0e+00
0.00.096.337 I print_info: n_ff             = 8192
0.00.096.337 I print_info: n_expert         = 0
0.00.096.337 I print_info: n_expert_used    = 0
0.00.096.337 I print_info: causal attn      = 1
0.00.096.338 I print_info: pooling type     = 0
0.00.096.338 I print_info: rope type        = 2
0.00.096.338 I print_info: rope scaling     = linear
0.00.096.338 I print_info: freq_base_train  = 10000.0
0.00.096.339 I print_info: freq_scale_train = 1
0.00.096.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.339 I print_info: rope_finetuned   = unknown
0.00.096.339 I print_info: ssm_d_conv       = 0
0.00.096.340 I print_info: ssm_d_inner      = 0
0.00.096.340 I print_info: ssm_d_state      = 0
0.00.096.340 I print_info: ssm_dt_rank      = 0
0.00.096.340 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.340 I print_info: model type       = 1.4B
0.00.096.341 I print_info: model params     = 1.41 B
0.00.096.341 I print_info: general.name     = 1.4B
0.00.096.341 I print_info: vocab type       = BPE
0.00.096.342 I print_info: n_vocab          = 50304
0.00.096.342 I print_info: n_merges         = 50009
0.00.096.342 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.342 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.343 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.343 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.343 I print_info: LF token         = 187 'Ċ'
0.00.096.343 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.343 I print_info: max token length = 1024
0.00.096.344 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.132.125 I load_tensors: offloading 24 repeating layers to GPU
0.00.132.129 I load_tensors: offloading output layer to GPU
0.00.132.129 I load_tensors: offloaded 25/25 layers to GPU
0.00.132.152 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.132.154 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.132.482 I llama_init_from_model: n_seq_max     = 1
0.00.132.483 I llama_init_from_model: n_ctx         = 2048
0.00.132.484 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.132.484 I llama_init_from_model: n_batch       = 2048
0.00.132.484 I llama_init_from_model: n_ubatch      = 512
0.00.132.484 I llama_init_from_model: flash_attn    = 0
0.00.132.485 I llama_init_from_model: freq_base     = 10000.0
0.00.132.485 I llama_init_from_model: freq_scale    = 1
0.00.132.485 I ggml_metal_init: allocating
0.00.132.503 I ggml_metal_init: found device: Apple M4
0.00.132.509 I ggml_metal_init: picking default device: Apple M4
0.00.133.110 I ggml_metal_init: using embedded metal library
0.00.141.903 I ggml_metal_init: GPU name:   Apple M4
0.00.141.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.141.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.141.906 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.141.906 I ggml_metal_init: simdgroup reduction   = true
0.00.141.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.141.906 I ggml_metal_init: has residency sets    = true
0.00.141.906 I ggml_metal_init: has bfloat            = true
0.00.141.906 I ggml_metal_init: use bfloat            = true
0.00.141.907 I ggml_metal_init: hasUnifiedMemory      = true
0.00.141.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.194.560 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.223.388 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.223.394 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.223.442 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.227.081 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.227.083 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.227.083 I llama_init_from_model: graph nodes  = 967
0.00.227.084 I llama_init_from_model: graph splits = 2
0.00.227.087 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.227.207 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.227.208 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.293.009 I main: llama threadpool init, n_threads = 4
0.00.293.053 I 
0.00.293.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.293.071 I 
0.00.293.117 I sampler seed: 1234
0.00.293.121 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.293.146 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.293.148 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.293.148 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.124.759 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.02.124.760 I llama_perf_context_print:        load time =     249.25 ms
0.02.124.761 I llama_perf_context_print: prompt eval time =      43.61 ms /     7 tokens (    6.23 ms per token,   160.52 tokens per second)
0.02.124.761 I llama_perf_context_print:        eval time =    1785.05 ms /    63 runs   (   28.33 ms per token,    35.29 tokens per second)
0.02.124.762 I llama_perf_context_print:       total time =    1832.63 ms /    70 tokens
0.02.124.987 I ggml_metal_free: deallocating

real	0m2.434s
user	0m0.131s
sys	0m0.130s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.062 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.098 I main: llama backend init
0.00.000.100 I main: load the model and apply lora adapter, if any
0.00.009.873 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.308 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.313 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.315 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.317 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.319 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.319 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.319 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.320 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.321 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.323 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.323 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.323 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.208 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.378 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.380 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.380 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.380 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.381 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.381 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.381 I llama_model_loader: - type  f32:  194 tensors
0.00.035.382 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.382 I print_info: file format = GGUF V3 (latest)
0.00.035.383 I print_info: file type   = Q8_0
0.00.035.384 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.500 I load: special tokens cache size = 25
0.00.050.697 I load: token to piece cache size = 0.2984 MB
0.00.050.701 I print_info: arch             = gptneox
0.00.050.702 I print_info: vocab_only       = 0
0.00.050.702 I print_info: n_ctx_train      = 2048
0.00.050.705 I print_info: n_embd           = 2048
0.00.050.705 I print_info: n_layer          = 24
0.00.050.711 I print_info: n_head           = 16
0.00.050.712 I print_info: n_head_kv        = 16
0.00.050.712 I print_info: n_rot            = 32
0.00.050.712 I print_info: n_swa            = 0
0.00.050.712 I print_info: n_embd_head_k    = 128
0.00.050.713 I print_info: n_embd_head_v    = 128
0.00.050.713 I print_info: n_gqa            = 1
0.00.050.714 I print_info: n_embd_k_gqa     = 2048
0.00.050.715 I print_info: n_embd_v_gqa     = 2048
0.00.050.715 I print_info: f_norm_eps       = 1.0e-05
0.00.050.716 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.717 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.719 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.719 I print_info: f_logit_scale    = 0.0e+00
0.00.050.720 I print_info: n_ff             = 8192
0.00.050.720 I print_info: n_expert         = 0
0.00.050.721 I print_info: n_expert_used    = 0
0.00.050.721 I print_info: causal attn      = 1
0.00.050.721 I print_info: pooling type     = 0
0.00.050.721 I print_info: rope type        = 2
0.00.050.721 I print_info: rope scaling     = linear
0.00.050.722 I print_info: freq_base_train  = 10000.0
0.00.050.722 I print_info: freq_scale_train = 1
0.00.050.722 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.722 I print_info: rope_finetuned   = unknown
0.00.050.723 I print_info: ssm_d_conv       = 0
0.00.050.723 I print_info: ssm_d_inner      = 0
0.00.050.723 I print_info: ssm_d_state      = 0
0.00.050.723 I print_info: ssm_dt_rank      = 0
0.00.050.723 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.723 I print_info: model type       = 1.4B
0.00.050.724 I print_info: model params     = 1.41 B
0.00.050.724 I print_info: general.name     = 1.4B
0.00.050.724 I print_info: vocab type       = BPE
0.00.050.725 I print_info: n_vocab          = 50304
0.00.050.725 I print_info: n_merges         = 50009
0.00.050.725 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.725 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.725 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.725 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.727 I print_info: LF token         = 187 'Ċ'
0.00.050.727 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.727 I print_info: max token length = 1024
0.00.050.728 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.284.492 I load_tensors: offloading 24 repeating layers to GPU
0.01.284.497 I load_tensors: offloading output layer to GPU
0.01.284.499 I load_tensors: offloaded 25/25 layers to GPU
0.01.284.522 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.284.527 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.285.575 I llama_init_from_model: n_seq_max     = 1
0.01.285.577 I llama_init_from_model: n_ctx         = 2048
0.01.285.578 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.285.578 I llama_init_from_model: n_batch       = 2048
0.01.285.578 I llama_init_from_model: n_ubatch      = 512
0.01.285.579 I llama_init_from_model: flash_attn    = 0
0.01.285.580 I llama_init_from_model: freq_base     = 10000.0
0.01.285.580 I llama_init_from_model: freq_scale    = 1
0.01.285.581 I ggml_metal_init: allocating
0.01.285.593 I ggml_metal_init: found device: Apple M4
0.01.285.600 I ggml_metal_init: picking default device: Apple M4
0.01.286.916 I ggml_metal_init: using embedded metal library
0.01.294.474 I ggml_metal_init: GPU name:   Apple M4
0.01.294.478 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.294.479 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.294.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.294.480 I ggml_metal_init: simdgroup reduction   = true
0.01.294.480 I ggml_metal_init: simdgroup matrix mul. = true
0.01.294.481 I ggml_metal_init: has residency sets    = true
0.01.294.481 I ggml_metal_init: has bfloat            = true
0.01.294.481 I ggml_metal_init: use bfloat            = true
0.01.294.482 I ggml_metal_init: hasUnifiedMemory      = true
0.01.294.483 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.310.903 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.363.753 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.363.759 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.363.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.368.018 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.368.020 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.368.020 I llama_init_from_model: graph nodes  = 967
0.01.368.020 I llama_init_from_model: graph splits = 2
0.01.368.026 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.368.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.368.154 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.424.447 I main: llama threadpool init, n_threads = 4
0.01.424.488 I 
0.01.424.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.424.504 I 
0.01.424.679 I sampler seed: 1234
0.01.424.683 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.424.707 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.424.708 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.424.708 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.510.510 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.02.510.511 I llama_perf_context_print:        load time =    1413.88 ms
0.02.510.512 I llama_perf_context_print: prompt eval time =      49.07 ms /     7 tokens (    7.01 ms per token,   142.66 tokens per second)
0.02.510.512 I llama_perf_context_print:        eval time =    1033.69 ms /    63 runs   (   16.41 ms per token,    60.95 tokens per second)
0.02.510.513 I llama_perf_context_print:       total time =    1086.76 ms /    70 tokens
0.02.510.769 I ggml_metal_free: deallocating

real	0m2.529s
user	0m0.109s
sys	0m0.277s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.015.501 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.035 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.037 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.038 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.038 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.038 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.040 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.040 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.041 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.041 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.042 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.043 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.044 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.940 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.882 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.883 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.883 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.884 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.884 I llama_model_loader: - type  f32:  194 tensors
0.00.040.884 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.885 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.885 I print_info: file format = GGUF V3 (latest)
0.00.040.886 I print_info: file type   = Q4_0
0.00.040.886 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.430 I load: special tokens cache size = 25
0.00.057.954 I load: token to piece cache size = 0.2984 MB
0.00.057.957 I print_info: arch             = gptneox
0.00.057.957 I print_info: vocab_only       = 0
0.00.057.958 I print_info: n_ctx_train      = 2048
0.00.057.958 I print_info: n_embd           = 2048
0.00.057.958 I print_info: n_layer          = 24
0.00.057.963 I print_info: n_head           = 16
0.00.057.965 I print_info: n_head_kv        = 16
0.00.057.966 I print_info: n_rot            = 32
0.00.057.966 I print_info: n_swa            = 0
0.00.057.966 I print_info: n_embd_head_k    = 128
0.00.057.966 I print_info: n_embd_head_v    = 128
0.00.057.967 I print_info: n_gqa            = 1
0.00.057.968 I print_info: n_embd_k_gqa     = 2048
0.00.057.968 I print_info: n_embd_v_gqa     = 2048
0.00.057.969 I print_info: f_norm_eps       = 1.0e-05
0.00.057.969 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.970 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.970 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.970 I print_info: f_logit_scale    = 0.0e+00
0.00.057.971 I print_info: n_ff             = 8192
0.00.057.971 I print_info: n_expert         = 0
0.00.057.971 I print_info: n_expert_used    = 0
0.00.057.971 I print_info: causal attn      = 1
0.00.057.971 I print_info: pooling type     = 0
0.00.057.971 I print_info: rope type        = 2
0.00.057.972 I print_info: rope scaling     = linear
0.00.057.972 I print_info: freq_base_train  = 10000.0
0.00.057.973 I print_info: freq_scale_train = 1
0.00.057.973 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.977 I print_info: rope_finetuned   = unknown
0.00.057.977 I print_info: ssm_d_conv       = 0
0.00.057.977 I print_info: ssm_d_inner      = 0
0.00.057.978 I print_info: ssm_d_state      = 0
0.00.057.978 I print_info: ssm_dt_rank      = 0
0.00.057.978 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.978 I print_info: model type       = 1.4B
0.00.057.979 I print_info: model params     = 1.41 B
0.00.057.979 I print_info: general.name     = 1.4B
0.00.057.979 I print_info: vocab type       = BPE
0.00.057.980 I print_info: n_vocab          = 50304
0.00.057.980 I print_info: n_merges         = 50009
0.00.057.980 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.980 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.981 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.981 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.981 I print_info: LF token         = 187 'Ċ'
0.00.057.982 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.983 I print_info: max token length = 1024
0.00.057.985 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.660.520 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.535 I load_tensors: offloading output layer to GPU
0.00.660.536 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.571 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.660.573 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.662.041 I llama_init_from_model: n_seq_max     = 1
0.00.662.043 I llama_init_from_model: n_ctx         = 2048
0.00.662.044 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.662.045 I llama_init_from_model: n_batch       = 2048
0.00.662.045 I llama_init_from_model: n_ubatch      = 512
0.00.662.045 I llama_init_from_model: flash_attn    = 0
0.00.662.048 I llama_init_from_model: freq_base     = 10000.0
0.00.662.048 I llama_init_from_model: freq_scale    = 1
0.00.662.051 I ggml_metal_init: allocating
0.00.662.149 I ggml_metal_init: found device: Apple M4
0.00.662.162 I ggml_metal_init: picking default device: Apple M4
0.00.664.070 I ggml_metal_init: using embedded metal library
0.00.669.656 I ggml_metal_init: GPU name:   Apple M4
0.00.669.696 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.702 I ggml_metal_init: simdgroup reduction   = true
0.00.669.702 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.703 I ggml_metal_init: has residency sets    = true
0.00.669.703 I ggml_metal_init: has bfloat            = true
0.00.669.705 I ggml_metal_init: use bfloat            = true
0.00.669.709 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.509 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.745.430 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.745.438 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.745.463 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.749.565 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.749.567 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.749.568 I llama_init_from_model: graph nodes  = 967
0.00.749.568 I llama_init_from_model: graph splits = 2
0.00.749.573 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.749.691 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.692 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.027 I main: llama threadpool init, n_threads = 4
0.00.807.069 I 
0.00.807.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.084 I 
0.00.807.260 I sampler seed: 1234
0.00.807.264 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.276 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.276 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.276 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.490.162 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.490.163 I llama_perf_context_print:        load time =     790.83 ms
0.01.490.164 I llama_perf_context_print: prompt eval time =      45.76 ms /     7 tokens (    6.54 ms per token,   152.96 tokens per second)
0.01.490.164 I llama_perf_context_print:        eval time =     634.31 ms /    63 runs   (   10.07 ms per token,    99.32 tokens per second)
0.01.490.165 I llama_perf_context_print:       total time =     683.83 ms /    70 tokens
0.01.490.445 I ggml_metal_free: deallocating

real	0m1.509s
user	0m0.114s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.008.802 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.213 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.221 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.223 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.228 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.229 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.230 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.231 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.231 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.233 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.236 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.236 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.236 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.036 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.846 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.850 I llama_model_loader: - type  f32:  194 tensors
0.00.035.850 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.851 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.851 I print_info: file format = GGUF V3 (latest)
0.00.035.852 I print_info: file type   = Q4_1
0.00.035.854 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.849 I load: special tokens cache size = 25
0.00.050.043 I load: token to piece cache size = 0.2984 MB
0.00.050.047 I print_info: arch             = gptneox
0.00.050.047 I print_info: vocab_only       = 0
0.00.050.047 I print_info: n_ctx_train      = 2048
0.00.050.048 I print_info: n_embd           = 2048
0.00.050.048 I print_info: n_layer          = 24
0.00.050.052 I print_info: n_head           = 16
0.00.050.053 I print_info: n_head_kv        = 16
0.00.050.053 I print_info: n_rot            = 32
0.00.050.053 I print_info: n_swa            = 0
0.00.050.053 I print_info: n_embd_head_k    = 128
0.00.050.054 I print_info: n_embd_head_v    = 128
0.00.050.054 I print_info: n_gqa            = 1
0.00.050.058 I print_info: n_embd_k_gqa     = 2048
0.00.050.059 I print_info: n_embd_v_gqa     = 2048
0.00.050.059 I print_info: f_norm_eps       = 1.0e-05
0.00.050.060 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.060 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.062 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.063 I print_info: f_logit_scale    = 0.0e+00
0.00.050.064 I print_info: n_ff             = 8192
0.00.050.076 I print_info: n_expert         = 0
0.00.050.079 I print_info: n_expert_used    = 0
0.00.050.079 I print_info: causal attn      = 1
0.00.050.079 I print_info: pooling type     = 0
0.00.050.080 I print_info: rope type        = 2
0.00.050.081 I print_info: rope scaling     = linear
0.00.050.081 I print_info: freq_base_train  = 10000.0
0.00.050.082 I print_info: freq_scale_train = 1
0.00.050.082 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.083 I print_info: rope_finetuned   = unknown
0.00.050.083 I print_info: ssm_d_conv       = 0
0.00.050.083 I print_info: ssm_d_inner      = 0
0.00.050.083 I print_info: ssm_d_state      = 0
0.00.050.083 I print_info: ssm_dt_rank      = 0
0.00.050.084 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.084 I print_info: model type       = 1.4B
0.00.050.085 I print_info: model params     = 1.41 B
0.00.050.085 I print_info: general.name     = 1.4B
0.00.050.085 I print_info: vocab type       = BPE
0.00.050.087 I print_info: n_vocab          = 50304
0.00.050.087 I print_info: n_merges         = 50009
0.00.050.088 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.088 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.088 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.089 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.089 I print_info: LF token         = 187 'Ċ'
0.00.050.089 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.090 I print_info: max token length = 1024
0.00.050.090 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.692.571 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.588 I load_tensors: offloading output layer to GPU
0.00.692.590 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.624 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.692.625 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.693.651 I llama_init_from_model: n_seq_max     = 1
0.00.693.655 I llama_init_from_model: n_ctx         = 2048
0.00.693.656 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.693.657 I llama_init_from_model: n_batch       = 2048
0.00.693.657 I llama_init_from_model: n_ubatch      = 512
0.00.693.657 I llama_init_from_model: flash_attn    = 0
0.00.693.659 I llama_init_from_model: freq_base     = 10000.0
0.00.693.660 I llama_init_from_model: freq_scale    = 1
0.00.693.667 I ggml_metal_init: allocating
0.00.693.758 I ggml_metal_init: found device: Apple M4
0.00.693.773 I ggml_metal_init: picking default device: Apple M4
0.00.695.829 I ggml_metal_init: using embedded metal library
0.00.702.342 I ggml_metal_init: GPU name:   Apple M4
0.00.702.348 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.349 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.350 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.351 I ggml_metal_init: simdgroup reduction   = true
0.00.702.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.351 I ggml_metal_init: has residency sets    = true
0.00.702.352 I ggml_metal_init: has bfloat            = true
0.00.702.352 I ggml_metal_init: use bfloat            = true
0.00.702.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.365 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.721.035 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.777.142 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.777.149 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.777.171 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.782.014 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.782.016 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.782.016 I llama_init_from_model: graph nodes  = 967
0.00.782.017 I llama_init_from_model: graph splits = 2
0.00.782.022 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.782.145 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.782.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.837.191 I main: llama threadpool init, n_threads = 4
0.00.837.227 I 
0.00.837.240 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.837.240 I 
0.00.837.491 I sampler seed: 1234
0.00.837.499 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.837.515 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.837.516 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.837.516 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.566.529 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.566.530 I llama_perf_context_print:        load time =     827.65 ms
0.01.566.531 I llama_perf_context_print: prompt eval time =      49.10 ms /     7 tokens (    7.01 ms per token,   142.56 tokens per second)
0.01.566.532 I llama_perf_context_print:        eval time =     677.57 ms /    63 runs   (   10.76 ms per token,    92.98 tokens per second)
0.01.566.532 I llama_perf_context_print:       total time =     730.08 ms /    70 tokens
0.01.566.799 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.111s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.016.363 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.033 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.037.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.039 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.040 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.040 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.041 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.044 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.049 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.049 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.055 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.549 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.345 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.347 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.348 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.348 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.348 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.349 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.049.349 I llama_model_loader: - type  f32:  194 tensors
0.00.049.350 I llama_model_loader: - type q5_0:   97 tensors
0.00.049.350 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.351 I print_info: file format = GGUF V3 (latest)
0.00.049.351 I print_info: file type   = Q5_0
0.00.049.352 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.062.045 I load: special tokens cache size = 25
0.00.075.312 I load: token to piece cache size = 0.2984 MB
0.00.075.319 I print_info: arch             = gptneox
0.00.075.319 I print_info: vocab_only       = 0
0.00.075.320 I print_info: n_ctx_train      = 2048
0.00.075.320 I print_info: n_embd           = 2048
0.00.075.320 I print_info: n_layer          = 24
0.00.075.325 I print_info: n_head           = 16
0.00.075.327 I print_info: n_head_kv        = 16
0.00.075.327 I print_info: n_rot            = 32
0.00.075.327 I print_info: n_swa            = 0
0.00.075.331 I print_info: n_embd_head_k    = 128
0.00.075.331 I print_info: n_embd_head_v    = 128
0.00.075.333 I print_info: n_gqa            = 1
0.00.075.334 I print_info: n_embd_k_gqa     = 2048
0.00.075.335 I print_info: n_embd_v_gqa     = 2048
0.00.075.337 I print_info: f_norm_eps       = 1.0e-05
0.00.075.337 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.338 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.338 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.338 I print_info: f_logit_scale    = 0.0e+00
0.00.075.342 I print_info: n_ff             = 8192
0.00.075.342 I print_info: n_expert         = 0
0.00.075.342 I print_info: n_expert_used    = 0
0.00.075.342 I print_info: causal attn      = 1
0.00.075.343 I print_info: pooling type     = 0
0.00.075.345 I print_info: rope type        = 2
0.00.075.345 I print_info: rope scaling     = linear
0.00.075.346 I print_info: freq_base_train  = 10000.0
0.00.075.347 I print_info: freq_scale_train = 1
0.00.075.347 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.347 I print_info: rope_finetuned   = unknown
0.00.075.348 I print_info: ssm_d_conv       = 0
0.00.075.348 I print_info: ssm_d_inner      = 0
0.00.075.348 I print_info: ssm_d_state      = 0
0.00.075.348 I print_info: ssm_dt_rank      = 0
0.00.075.348 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.355 I print_info: model type       = 1.4B
0.00.075.355 I print_info: model params     = 1.41 B
0.00.075.356 I print_info: general.name     = 1.4B
0.00.075.357 I print_info: vocab type       = BPE
0.00.075.357 I print_info: n_vocab          = 50304
0.00.075.357 I print_info: n_merges         = 50009
0.00.075.359 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.361 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.361 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.362 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.362 I print_info: LF token         = 187 'Ċ'
0.00.075.363 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.363 I print_info: max token length = 1024
0.00.075.364 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.821.913 I load_tensors: offloading 24 repeating layers to GPU
0.00.821.927 I load_tensors: offloading output layer to GPU
0.00.821.928 I load_tensors: offloaded 25/25 layers to GPU
0.00.821.962 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.821.963 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.823.521 I llama_init_from_model: n_seq_max     = 1
0.00.823.524 I llama_init_from_model: n_ctx         = 2048
0.00.823.525 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.823.525 I llama_init_from_model: n_batch       = 2048
0.00.823.526 I llama_init_from_model: n_ubatch      = 512
0.00.823.526 I llama_init_from_model: flash_attn    = 0
0.00.823.529 I llama_init_from_model: freq_base     = 10000.0
0.00.823.529 I llama_init_from_model: freq_scale    = 1
0.00.823.532 I ggml_metal_init: allocating
0.00.823.585 I ggml_metal_init: found device: Apple M4
0.00.823.598 I ggml_metal_init: picking default device: Apple M4
0.00.825.338 I ggml_metal_init: using embedded metal library
0.00.831.731 I ggml_metal_init: GPU name:   Apple M4
0.00.831.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.831.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.831.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.831.740 I ggml_metal_init: simdgroup reduction   = true
0.00.831.740 I ggml_metal_init: simdgroup matrix mul. = true
0.00.831.740 I ggml_metal_init: has residency sets    = true
0.00.831.740 I ggml_metal_init: has bfloat            = true
0.00.831.741 I ggml_metal_init: use bfloat            = true
0.00.831.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.831.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.854.600 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.907.598 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.907.605 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.907.632 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.912.434 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.912.436 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.912.436 I llama_init_from_model: graph nodes  = 967
0.00.912.436 I llama_init_from_model: graph splits = 2
0.00.912.441 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.912.554 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.912.554 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.970.159 I main: llama threadpool init, n_threads = 4
0.00.970.201 I 
0.00.970.216 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.970.216 I 
0.00.970.375 I sampler seed: 1234
0.00.970.379 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.970.399 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.970.400 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.970.400 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.757.608 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.757.609 I llama_perf_context_print:        load time =     953.10 ms
0.01.757.610 I llama_perf_context_print: prompt eval time =      42.81 ms /     7 tokens (    6.12 ms per token,   163.52 tokens per second)
0.01.757.612 I llama_perf_context_print:        eval time =     741.56 ms /    63 runs   (   11.77 ms per token,    84.96 tokens per second)
0.01.757.612 I llama_perf_context_print:       total time =     788.15 ms /    70 tokens
0.01.757.885 I ggml_metal_free: deallocating

real	0m1.795s
user	0m0.132s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.835 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.810 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.811 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.811 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.812 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.815 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.657 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.514 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.516 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.516 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.516 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.517 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.517 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.517 I llama_model_loader: - type  f32:  194 tensors
0.00.026.518 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.518 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.518 I print_info: file format = GGUF V3 (latest)
0.00.026.519 I print_info: file type   = Q5_1
0.00.026.520 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.663 I load: special tokens cache size = 25
0.00.040.739 I load: token to piece cache size = 0.2984 MB
0.00.040.742 I print_info: arch             = gptneox
0.00.040.742 I print_info: vocab_only       = 0
0.00.040.743 I print_info: n_ctx_train      = 2048
0.00.040.743 I print_info: n_embd           = 2048
0.00.040.743 I print_info: n_layer          = 24
0.00.040.746 I print_info: n_head           = 16
0.00.040.747 I print_info: n_head_kv        = 16
0.00.040.747 I print_info: n_rot            = 32
0.00.040.747 I print_info: n_swa            = 0
0.00.040.747 I print_info: n_embd_head_k    = 128
0.00.040.748 I print_info: n_embd_head_v    = 128
0.00.040.748 I print_info: n_gqa            = 1
0.00.040.749 I print_info: n_embd_k_gqa     = 2048
0.00.040.750 I print_info: n_embd_v_gqa     = 2048
0.00.040.750 I print_info: f_norm_eps       = 1.0e-05
0.00.040.751 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.751 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.751 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.751 I print_info: f_logit_scale    = 0.0e+00
0.00.040.752 I print_info: n_ff             = 8192
0.00.040.752 I print_info: n_expert         = 0
0.00.040.752 I print_info: n_expert_used    = 0
0.00.040.752 I print_info: causal attn      = 1
0.00.040.752 I print_info: pooling type     = 0
0.00.040.755 I print_info: rope type        = 2
0.00.040.757 I print_info: rope scaling     = linear
0.00.040.757 I print_info: freq_base_train  = 10000.0
0.00.040.757 I print_info: freq_scale_train = 1
0.00.040.758 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.758 I print_info: rope_finetuned   = unknown
0.00.040.758 I print_info: ssm_d_conv       = 0
0.00.040.758 I print_info: ssm_d_inner      = 0
0.00.040.758 I print_info: ssm_d_state      = 0
0.00.040.758 I print_info: ssm_dt_rank      = 0
0.00.040.759 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.759 I print_info: model type       = 1.4B
0.00.040.759 I print_info: model params     = 1.41 B
0.00.040.759 I print_info: general.name     = 1.4B
0.00.040.760 I print_info: vocab type       = BPE
0.00.040.760 I print_info: n_vocab          = 50304
0.00.040.760 I print_info: n_merges         = 50009
0.00.040.761 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.761 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.766 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.768 I print_info: LF token         = 187 'Ċ'
0.00.040.768 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.768 I print_info: max token length = 1024
0.00.040.769 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.662.042 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.046 I load_tensors: offloading output layer to GPU
0.00.662.047 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.070 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.662.071 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.663.581 I llama_init_from_model: n_seq_max     = 1
0.00.663.583 I llama_init_from_model: n_ctx         = 2048
0.00.663.584 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.663.584 I llama_init_from_model: n_batch       = 2048
0.00.663.584 I llama_init_from_model: n_ubatch      = 512
0.00.663.585 I llama_init_from_model: flash_attn    = 0
0.00.663.586 I llama_init_from_model: freq_base     = 10000.0
0.00.663.587 I llama_init_from_model: freq_scale    = 1
0.00.663.588 I ggml_metal_init: allocating
0.00.663.604 I ggml_metal_init: found device: Apple M4
0.00.663.614 I ggml_metal_init: picking default device: Apple M4
0.00.665.118 I ggml_metal_init: using embedded metal library
0.00.671.023 I ggml_metal_init: GPU name:   Apple M4
0.00.671.027 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.028 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.029 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.029 I ggml_metal_init: simdgroup reduction   = true
0.00.671.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.030 I ggml_metal_init: has residency sets    = true
0.00.671.030 I ggml_metal_init: has bfloat            = true
0.00.671.030 I ggml_metal_init: use bfloat            = true
0.00.671.031 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.139 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.300 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.740.306 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.740.376 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.744.577 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.744.579 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.744.580 I llama_init_from_model: graph nodes  = 967
0.00.744.580 I llama_init_from_model: graph splits = 2
0.00.744.585 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.744.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.744.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.824 I main: llama threadpool init, n_threads = 4
0.00.796.867 I 
0.00.796.892 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.892 I 
0.00.797.011 I sampler seed: 1234
0.00.797.015 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.052 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.055 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.056 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.647.821 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50390.35 tokens per second)
0.01.647.822 I llama_perf_context_print:        load time =     787.31 ms
0.01.647.822 I llama_perf_context_print: prompt eval time =      52.91 ms /     7 tokens (    7.56 ms per token,   132.29 tokens per second)
0.01.647.823 I llama_perf_context_print:        eval time =     794.83 ms /    63 runs   (   12.62 ms per token,    79.26 tokens per second)
0.01.647.823 I llama_perf_context_print:       total time =     851.67 ms /    70 tokens
0.01.648.080 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.108s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.648 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.205 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.209 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.211 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.211 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.212 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.213 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.214 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.217 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.219 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.060 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.054 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.872 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.873 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.874 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.874 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.875 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.875 I llama_model_loader: - type  f32:  194 tensors
0.00.024.875 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.876 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.876 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.876 I print_info: file format = GGUF V3 (latest)
0.00.024.877 I print_info: file type   = Q2_K - Medium
0.00.024.878 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.710 I load: special tokens cache size = 25
0.00.038.707 I load: token to piece cache size = 0.2984 MB
0.00.038.710 I print_info: arch             = gptneox
0.00.038.710 I print_info: vocab_only       = 0
0.00.038.710 I print_info: n_ctx_train      = 2048
0.00.038.710 I print_info: n_embd           = 2048
0.00.038.710 I print_info: n_layer          = 24
0.00.038.714 I print_info: n_head           = 16
0.00.038.715 I print_info: n_head_kv        = 16
0.00.038.715 I print_info: n_rot            = 32
0.00.038.715 I print_info: n_swa            = 0
0.00.038.715 I print_info: n_embd_head_k    = 128
0.00.038.715 I print_info: n_embd_head_v    = 128
0.00.038.716 I print_info: n_gqa            = 1
0.00.038.717 I print_info: n_embd_k_gqa     = 2048
0.00.038.717 I print_info: n_embd_v_gqa     = 2048
0.00.038.718 I print_info: f_norm_eps       = 1.0e-05
0.00.038.718 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.719 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.719 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.719 I print_info: f_logit_scale    = 0.0e+00
0.00.038.719 I print_info: n_ff             = 8192
0.00.038.720 I print_info: n_expert         = 0
0.00.038.720 I print_info: n_expert_used    = 0
0.00.038.720 I print_info: causal attn      = 1
0.00.038.720 I print_info: pooling type     = 0
0.00.038.720 I print_info: rope type        = 2
0.00.038.720 I print_info: rope scaling     = linear
0.00.038.721 I print_info: freq_base_train  = 10000.0
0.00.038.721 I print_info: freq_scale_train = 1
0.00.038.721 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.721 I print_info: rope_finetuned   = unknown
0.00.038.722 I print_info: ssm_d_conv       = 0
0.00.038.722 I print_info: ssm_d_inner      = 0
0.00.038.722 I print_info: ssm_d_state      = 0
0.00.038.724 I print_info: ssm_dt_rank      = 0
0.00.038.724 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.724 I print_info: model type       = 1.4B
0.00.038.725 I print_info: model params     = 1.41 B
0.00.038.725 I print_info: general.name     = 1.4B
0.00.038.725 I print_info: vocab type       = BPE
0.00.038.725 I print_info: n_vocab          = 50304
0.00.038.726 I print_info: n_merges         = 50009
0.00.038.726 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.727 I print_info: LF token         = 187 'Ċ'
0.00.038.727 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.727 I print_info: max token length = 1024
0.00.038.727 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.348.027 I load_tensors: offloading 24 repeating layers to GPU
0.00.348.041 I load_tensors: offloading output layer to GPU
0.00.348.042 I load_tensors: offloaded 25/25 layers to GPU
0.00.348.074 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.348.079 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.349.924 I llama_init_from_model: n_seq_max     = 1
0.00.349.929 I llama_init_from_model: n_ctx         = 2048
0.00.349.930 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.349.931 I llama_init_from_model: n_batch       = 2048
0.00.349.931 I llama_init_from_model: n_ubatch      = 512
0.00.349.932 I llama_init_from_model: flash_attn    = 0
0.00.349.934 I llama_init_from_model: freq_base     = 10000.0
0.00.349.934 I llama_init_from_model: freq_scale    = 1
0.00.349.937 I ggml_metal_init: allocating
0.00.350.071 I ggml_metal_init: found device: Apple M4
0.00.350.085 I ggml_metal_init: picking default device: Apple M4
0.00.352.107 I ggml_metal_init: using embedded metal library
0.00.357.672 I ggml_metal_init: GPU name:   Apple M4
0.00.357.690 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.357.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.357.692 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.357.693 I ggml_metal_init: simdgroup reduction   = true
0.00.357.693 I ggml_metal_init: simdgroup matrix mul. = true
0.00.357.694 I ggml_metal_init: has residency sets    = true
0.00.357.694 I ggml_metal_init: has bfloat            = true
0.00.357.694 I ggml_metal_init: use bfloat            = true
0.00.357.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.357.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.378.179 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.432.850 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.432.859 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.432.888 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.437.128 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.437.130 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.437.130 I llama_init_from_model: graph nodes  = 967
0.00.437.131 I llama_init_from_model: graph splits = 2
0.00.437.136 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.437.265 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.437.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.461 I main: llama threadpool init, n_threads = 4
0.00.494.505 I 
0.00.494.522 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.522 I 
0.00.494.679 I sampler seed: 1234
0.00.494.683 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.494.705 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.494.705 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.494.705 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.169.411 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.169.411 I llama_perf_context_print:        load time =     484.12 ms
0.01.169.412 I llama_perf_context_print: prompt eval time =      35.49 ms /     7 tokens (    5.07 ms per token,   197.24 tokens per second)
0.01.169.413 I llama_perf_context_print:        eval time =     636.41 ms /    63 runs   (   10.10 ms per token,    98.99 tokens per second)
0.01.169.413 I llama_perf_context_print:       total time =     675.64 ms /    70 tokens
0.01.169.633 I ggml_metal_free: deallocating

real	0m1.189s
user	0m0.110s
sys	0m0.171s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.417 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.422 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.424 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.424 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.425 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.425 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.427 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.431 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.431 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.432 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.184 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.196 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.876 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.877 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.878 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.878 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.879 I llama_model_loader: - type  f32:  194 tensors
0.00.023.879 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.879 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.880 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.880 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.881 I print_info: file format = GGUF V3 (latest)
0.00.023.881 I print_info: file type   = Q3_K - Medium
0.00.023.882 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.713 I load: special tokens cache size = 25
0.00.037.894 I load: token to piece cache size = 0.2984 MB
0.00.037.897 I print_info: arch             = gptneox
0.00.037.897 I print_info: vocab_only       = 0
0.00.037.897 I print_info: n_ctx_train      = 2048
0.00.037.898 I print_info: n_embd           = 2048
0.00.037.898 I print_info: n_layer          = 24
0.00.037.900 I print_info: n_head           = 16
0.00.037.901 I print_info: n_head_kv        = 16
0.00.037.901 I print_info: n_rot            = 32
0.00.037.901 I print_info: n_swa            = 0
0.00.037.902 I print_info: n_embd_head_k    = 128
0.00.037.902 I print_info: n_embd_head_v    = 128
0.00.037.903 I print_info: n_gqa            = 1
0.00.037.903 I print_info: n_embd_k_gqa     = 2048
0.00.037.906 I print_info: n_embd_v_gqa     = 2048
0.00.037.907 I print_info: f_norm_eps       = 1.0e-05
0.00.037.907 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.907 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.908 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.908 I print_info: f_logit_scale    = 0.0e+00
0.00.037.908 I print_info: n_ff             = 8192
0.00.037.908 I print_info: n_expert         = 0
0.00.037.909 I print_info: n_expert_used    = 0
0.00.037.909 I print_info: causal attn      = 1
0.00.037.910 I print_info: pooling type     = 0
0.00.037.910 I print_info: rope type        = 2
0.00.037.911 I print_info: rope scaling     = linear
0.00.037.911 I print_info: freq_base_train  = 10000.0
0.00.037.911 I print_info: freq_scale_train = 1
0.00.037.912 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.912 I print_info: rope_finetuned   = unknown
0.00.037.912 I print_info: ssm_d_conv       = 0
0.00.037.912 I print_info: ssm_d_inner      = 0
0.00.037.912 I print_info: ssm_d_state      = 0
0.00.037.912 I print_info: ssm_dt_rank      = 0
0.00.037.912 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.913 I print_info: model type       = 1.4B
0.00.037.913 I print_info: model params     = 1.41 B
0.00.037.913 I print_info: general.name     = 1.4B
0.00.037.913 I print_info: vocab type       = BPE
0.00.037.914 I print_info: n_vocab          = 50304
0.00.037.914 I print_info: n_merges         = 50009
0.00.037.914 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.914 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.914 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.915 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.915 I print_info: LF token         = 187 'Ċ'
0.00.037.915 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.915 I print_info: max token length = 1024
0.00.037.920 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.436.265 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.279 I load_tensors: offloading output layer to GPU
0.00.436.280 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.313 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.315 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.886 I llama_init_from_model: n_seq_max     = 1
0.00.437.891 I llama_init_from_model: n_ctx         = 2048
0.00.437.892 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.437.893 I llama_init_from_model: n_batch       = 2048
0.00.437.893 I llama_init_from_model: n_ubatch      = 512
0.00.437.893 I llama_init_from_model: flash_attn    = 0
0.00.437.896 I llama_init_from_model: freq_base     = 10000.0
0.00.437.896 I llama_init_from_model: freq_scale    = 1
0.00.437.899 I ggml_metal_init: allocating
0.00.437.989 I ggml_metal_init: found device: Apple M4
0.00.438.003 I ggml_metal_init: picking default device: Apple M4
0.00.439.895 I ggml_metal_init: using embedded metal library
0.00.445.495 I ggml_metal_init: GPU name:   Apple M4
0.00.445.500 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.501 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.503 I ggml_metal_init: simdgroup reduction   = true
0.00.445.503 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.503 I ggml_metal_init: has residency sets    = true
0.00.445.504 I ggml_metal_init: has bfloat            = true
0.00.445.504 I ggml_metal_init: use bfloat            = true
0.00.445.505 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.161 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.522.689 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.522.696 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.522.730 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.527.387 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.527.389 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.527.389 I llama_init_from_model: graph nodes  = 967
0.00.527.389 I llama_init_from_model: graph splits = 2
0.00.527.395 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.527.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.527.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.465 I main: llama threadpool init, n_threads = 4
0.00.584.507 I 
0.00.584.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.525 I 
0.00.584.698 I sampler seed: 1234
0.00.584.703 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.584.727 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.584.728 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.584.729 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.336.011 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.336.012 I llama_perf_context_print:        load time =     574.92 ms
0.01.336.013 I llama_perf_context_print: prompt eval time =      50.14 ms /     7 tokens (    7.16 ms per token,   139.60 tokens per second)
0.01.336.013 I llama_perf_context_print:        eval time =     698.25 ms /    63 runs   (   11.08 ms per token,    90.23 tokens per second)
0.01.336.014 I llama_perf_context_print:       total time =     752.25 ms /    70 tokens
0.01.336.238 I ggml_metal_free: deallocating

real	0m1.352s
user	0m0.110s
sys	0m0.179s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.867 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.628 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.459 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.460 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.461 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.461 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.461 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.462 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.462 I llama_model_loader: - type  f32:  194 tensors
0.00.024.462 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.463 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.463 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.463 I print_info: file format = GGUF V3 (latest)
0.00.024.464 I print_info: file type   = Q4_K - Medium
0.00.024.465 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.677 I load: special tokens cache size = 25
0.00.038.626 I load: token to piece cache size = 0.2984 MB
0.00.038.629 I print_info: arch             = gptneox
0.00.038.629 I print_info: vocab_only       = 0
0.00.038.629 I print_info: n_ctx_train      = 2048
0.00.038.629 I print_info: n_embd           = 2048
0.00.038.630 I print_info: n_layer          = 24
0.00.038.632 I print_info: n_head           = 16
0.00.038.633 I print_info: n_head_kv        = 16
0.00.038.633 I print_info: n_rot            = 32
0.00.038.633 I print_info: n_swa            = 0
0.00.038.635 I print_info: n_embd_head_k    = 128
0.00.038.635 I print_info: n_embd_head_v    = 128
0.00.038.636 I print_info: n_gqa            = 1
0.00.038.637 I print_info: n_embd_k_gqa     = 2048
0.00.038.639 I print_info: n_embd_v_gqa     = 2048
0.00.038.639 I print_info: f_norm_eps       = 1.0e-05
0.00.038.640 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.640 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.640 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.640 I print_info: f_logit_scale    = 0.0e+00
0.00.038.641 I print_info: n_ff             = 8192
0.00.038.641 I print_info: n_expert         = 0
0.00.038.641 I print_info: n_expert_used    = 0
0.00.038.641 I print_info: causal attn      = 1
0.00.038.642 I print_info: pooling type     = 0
0.00.038.642 I print_info: rope type        = 2
0.00.038.642 I print_info: rope scaling     = linear
0.00.038.642 I print_info: freq_base_train  = 10000.0
0.00.038.643 I print_info: freq_scale_train = 1
0.00.038.643 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.643 I print_info: rope_finetuned   = unknown
0.00.038.643 I print_info: ssm_d_conv       = 0
0.00.038.643 I print_info: ssm_d_inner      = 0
0.00.038.643 I print_info: ssm_d_state      = 0
0.00.038.648 I print_info: ssm_dt_rank      = 0
0.00.038.648 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.648 I print_info: model type       = 1.4B
0.00.038.648 I print_info: model params     = 1.41 B
0.00.038.649 I print_info: general.name     = 1.4B
0.00.038.649 I print_info: vocab type       = BPE
0.00.038.649 I print_info: n_vocab          = 50304
0.00.038.649 I print_info: n_merges         = 50009
0.00.038.650 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.650 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.650 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.651 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: LF token         = 187 'Ċ'
0.00.038.652 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: max token length = 1024
0.00.038.652 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.534.797 I load_tensors: offloading 24 repeating layers to GPU
0.00.534.814 I load_tensors: offloading output layer to GPU
0.00.534.815 I load_tensors: offloaded 25/25 layers to GPU
0.00.534.848 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.534.849 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.536.328 I llama_init_from_model: n_seq_max     = 1
0.00.536.331 I llama_init_from_model: n_ctx         = 2048
0.00.536.331 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.536.332 I llama_init_from_model: n_batch       = 2048
0.00.536.332 I llama_init_from_model: n_ubatch      = 512
0.00.536.333 I llama_init_from_model: flash_attn    = 0
0.00.536.335 I llama_init_from_model: freq_base     = 10000.0
0.00.536.335 I llama_init_from_model: freq_scale    = 1
0.00.536.339 I ggml_metal_init: allocating
0.00.536.423 I ggml_metal_init: found device: Apple M4
0.00.536.437 I ggml_metal_init: picking default device: Apple M4
0.00.538.270 I ggml_metal_init: using embedded metal library
0.00.544.949 I ggml_metal_init: GPU name:   Apple M4
0.00.544.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.544.953 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.544.954 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.544.955 I ggml_metal_init: simdgroup reduction   = true
0.00.544.955 I ggml_metal_init: simdgroup matrix mul. = true
0.00.544.955 I ggml_metal_init: has residency sets    = true
0.00.544.955 I ggml_metal_init: has bfloat            = true
0.00.544.956 I ggml_metal_init: use bfloat            = true
0.00.544.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.544.958 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.854 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.221 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.616.229 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.616.251 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.429 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.620.431 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.620.431 I llama_init_from_model: graph nodes  = 967
0.00.620.431 I llama_init_from_model: graph splits = 2
0.00.620.438 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.620.559 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.620.559 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.498 I main: llama threadpool init, n_threads = 4
0.00.680.546 I 
0.00.680.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.561 I 
0.00.680.741 I sampler seed: 1234
0.00.680.746 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.680.766 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.680.766 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.680.766 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.443.197 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.443.198 I llama_perf_context_print:        load time =     670.93 ms
0.01.443.199 I llama_perf_context_print: prompt eval time =      57.58 ms /     7 tokens (    8.23 ms per token,   121.56 tokens per second)
0.01.443.200 I llama_perf_context_print:        eval time =     701.93 ms /    63 runs   (   11.14 ms per token,    89.75 tokens per second)
0.01.443.201 I llama_perf_context_print:       total time =     763.40 ms /    70 tokens
0.01.443.485 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.108s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.326 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.902 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.907 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.908 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.909 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.909 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.909 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.910 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.911 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.911 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.912 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.912 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.913 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.914 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.914 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.915 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.806 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.607 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.608 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.608 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.608 I llama_model_loader: - type  f32:  194 tensors
0.00.024.608 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.609 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.609 I print_info: file format = GGUF V3 (latest)
0.00.024.609 I print_info: file type   = Q5_K - Medium
0.00.024.610 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.471 I load: special tokens cache size = 25
0.00.038.654 I load: token to piece cache size = 0.2984 MB
0.00.038.658 I print_info: arch             = gptneox
0.00.038.659 I print_info: vocab_only       = 0
0.00.038.659 I print_info: n_ctx_train      = 2048
0.00.038.659 I print_info: n_embd           = 2048
0.00.038.659 I print_info: n_layer          = 24
0.00.038.662 I print_info: n_head           = 16
0.00.038.663 I print_info: n_head_kv        = 16
0.00.038.663 I print_info: n_rot            = 32
0.00.038.663 I print_info: n_swa            = 0
0.00.038.664 I print_info: n_embd_head_k    = 128
0.00.038.665 I print_info: n_embd_head_v    = 128
0.00.038.665 I print_info: n_gqa            = 1
0.00.038.666 I print_info: n_embd_k_gqa     = 2048
0.00.038.667 I print_info: n_embd_v_gqa     = 2048
0.00.038.667 I print_info: f_norm_eps       = 1.0e-05
0.00.038.668 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.669 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.669 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.669 I print_info: f_logit_scale    = 0.0e+00
0.00.038.670 I print_info: n_ff             = 8192
0.00.038.672 I print_info: n_expert         = 0
0.00.038.672 I print_info: n_expert_used    = 0
0.00.038.672 I print_info: causal attn      = 1
0.00.038.672 I print_info: pooling type     = 0
0.00.038.674 I print_info: rope type        = 2
0.00.038.674 I print_info: rope scaling     = linear
0.00.038.674 I print_info: freq_base_train  = 10000.0
0.00.038.679 I print_info: freq_scale_train = 1
0.00.038.679 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.679 I print_info: rope_finetuned   = unknown
0.00.038.680 I print_info: ssm_d_conv       = 0
0.00.038.680 I print_info: ssm_d_inner      = 0
0.00.038.680 I print_info: ssm_d_state      = 0
0.00.038.680 I print_info: ssm_dt_rank      = 0
0.00.038.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.680 I print_info: model type       = 1.4B
0.00.038.681 I print_info: model params     = 1.41 B
0.00.038.681 I print_info: general.name     = 1.4B
0.00.038.682 I print_info: vocab type       = BPE
0.00.038.682 I print_info: n_vocab          = 50304
0.00.038.682 I print_info: n_merges         = 50009
0.00.038.682 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.682 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.682 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.683 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.683 I print_info: LF token         = 187 'Ċ'
0.00.038.683 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.683 I print_info: max token length = 1024
0.00.038.684 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.549 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.564 I load_tensors: offloading output layer to GPU
0.00.614.565 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.601 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.614.603 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.616.129 I llama_init_from_model: n_seq_max     = 1
0.00.616.134 I llama_init_from_model: n_ctx         = 2048
0.00.616.135 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.616.135 I llama_init_from_model: n_batch       = 2048
0.00.616.136 I llama_init_from_model: n_ubatch      = 512
0.00.616.136 I llama_init_from_model: flash_attn    = 0
0.00.616.138 I llama_init_from_model: freq_base     = 10000.0
0.00.616.138 I llama_init_from_model: freq_scale    = 1
0.00.616.141 I ggml_metal_init: allocating
0.00.616.217 I ggml_metal_init: found device: Apple M4
0.00.616.230 I ggml_metal_init: picking default device: Apple M4
0.00.618.113 I ggml_metal_init: using embedded metal library
0.00.624.569 I ggml_metal_init: GPU name:   Apple M4
0.00.624.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.624.574 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.624.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.624.575 I ggml_metal_init: simdgroup reduction   = true
0.00.624.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.624.576 I ggml_metal_init: has residency sets    = true
0.00.624.576 I ggml_metal_init: has bfloat            = true
0.00.624.576 I ggml_metal_init: use bfloat            = true
0.00.624.577 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.579 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.059 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.229 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.697.237 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.697.260 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.702.044 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.702.047 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.702.047 I llama_init_from_model: graph nodes  = 967
0.00.702.047 I llama_init_from_model: graph splits = 2
0.00.702.057 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.702.183 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.702.183 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.413 I main: llama threadpool init, n_threads = 4
0.00.763.456 I 
0.00.763.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.473 I 
0.00.763.624 I sampler seed: 1234
0.00.763.629 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.672 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.675 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.675 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.604.991 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.604.991 I llama_perf_context_print:        load time =     753.39 ms
0.01.604.992 I llama_perf_context_print: prompt eval time =      51.50 ms /     7 tokens (    7.36 ms per token,   135.92 tokens per second)
0.01.604.993 I llama_perf_context_print:        eval time =     786.93 ms /    63 runs   (   12.49 ms per token,    80.06 tokens per second)
0.01.604.993 I llama_perf_context_print:       total time =     842.27 ms /    70 tokens
0.01.605.268 I ggml_metal_free: deallocating

real	0m1.623s
user	0m0.109s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.823 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.505 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.505 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.506 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.420 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.198 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.200 I llama_model_loader: - type  f32:  194 tensors
0.00.024.200 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.201 I print_info: file format = GGUF V3 (latest)
0.00.024.201 I print_info: file type   = Q6_K
0.00.024.202 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.289 I load: special tokens cache size = 25
0.00.038.600 I load: token to piece cache size = 0.2984 MB
0.00.038.603 I print_info: arch             = gptneox
0.00.038.603 I print_info: vocab_only       = 0
0.00.038.604 I print_info: n_ctx_train      = 2048
0.00.038.604 I print_info: n_embd           = 2048
0.00.038.604 I print_info: n_layer          = 24
0.00.038.607 I print_info: n_head           = 16
0.00.038.608 I print_info: n_head_kv        = 16
0.00.038.608 I print_info: n_rot            = 32
0.00.038.608 I print_info: n_swa            = 0
0.00.038.608 I print_info: n_embd_head_k    = 128
0.00.038.608 I print_info: n_embd_head_v    = 128
0.00.038.611 I print_info: n_gqa            = 1
0.00.038.612 I print_info: n_embd_k_gqa     = 2048
0.00.038.614 I print_info: n_embd_v_gqa     = 2048
0.00.038.614 I print_info: f_norm_eps       = 1.0e-05
0.00.038.615 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.615 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.615 I print_info: f_logit_scale    = 0.0e+00
0.00.038.616 I print_info: n_ff             = 8192
0.00.038.616 I print_info: n_expert         = 0
0.00.038.616 I print_info: n_expert_used    = 0
0.00.038.616 I print_info: causal attn      = 1
0.00.038.616 I print_info: pooling type     = 0
0.00.038.617 I print_info: rope type        = 2
0.00.038.617 I print_info: rope scaling     = linear
0.00.038.619 I print_info: freq_base_train  = 10000.0
0.00.038.619 I print_info: freq_scale_train = 1
0.00.038.619 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.619 I print_info: rope_finetuned   = unknown
0.00.038.620 I print_info: ssm_d_conv       = 0
0.00.038.620 I print_info: ssm_d_inner      = 0
0.00.038.620 I print_info: ssm_d_state      = 0
0.00.038.620 I print_info: ssm_dt_rank      = 0
0.00.038.620 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.620 I print_info: model type       = 1.4B
0.00.038.621 I print_info: model params     = 1.41 B
0.00.038.621 I print_info: general.name     = 1.4B
0.00.038.621 I print_info: vocab type       = BPE
0.00.038.622 I print_info: n_vocab          = 50304
0.00.038.622 I print_info: n_merges         = 50009
0.00.038.626 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.628 I print_info: LF token         = 187 'Ċ'
0.00.038.629 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.629 I print_info: max token length = 1024
0.00.038.629 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.662.563 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.567 I load_tensors: offloading output layer to GPU
0.00.662.567 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.590 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.662.591 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.664.203 I llama_init_from_model: n_seq_max     = 1
0.00.664.205 I llama_init_from_model: n_ctx         = 2048
0.00.664.205 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.664.205 I llama_init_from_model: n_batch       = 2048
0.00.664.206 I llama_init_from_model: n_ubatch      = 512
0.00.664.206 I llama_init_from_model: flash_attn    = 0
0.00.664.208 I llama_init_from_model: freq_base     = 10000.0
0.00.664.208 I llama_init_from_model: freq_scale    = 1
0.00.664.209 I ggml_metal_init: allocating
0.00.664.256 I ggml_metal_init: found device: Apple M4
0.00.664.268 I ggml_metal_init: picking default device: Apple M4
0.00.665.837 I ggml_metal_init: using embedded metal library
0.00.671.717 I ggml_metal_init: GPU name:   Apple M4
0.00.671.721 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.722 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.723 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.724 I ggml_metal_init: simdgroup reduction   = true
0.00.671.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.725 I ggml_metal_init: has residency sets    = true
0.00.671.725 I ggml_metal_init: has bfloat            = true
0.00.671.725 I ggml_metal_init: use bfloat            = true
0.00.671.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.727 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.926 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.200 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.744.206 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.230 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.309 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.311 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.311 I llama_init_from_model: graph nodes  = 967
0.00.748.311 I llama_init_from_model: graph splits = 2
0.00.748.316 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.433 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.812.541 I main: llama threadpool init, n_threads = 4
0.00.812.640 I 
0.00.812.655 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.812.655 I 
0.00.812.829 I sampler seed: 1234
0.00.812.833 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.812.853 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.812.853 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.812.853 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.682.598 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.01.682.599 I llama_perf_context_print:        load time =     803.04 ms
0.01.682.599 I llama_perf_context_print: prompt eval time =      54.14 ms /     7 tokens (    7.73 ms per token,   129.28 tokens per second)
0.01.682.600 I llama_perf_context_print:        eval time =     812.70 ms /    63 runs   (   12.90 ms per token,    77.52 tokens per second)
0.01.682.600 I llama_perf_context_print:       total time =     870.74 ms /    70 tokens
0.01.682.867 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.109s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.584 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.428 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.752 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.760 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.763 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.770 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.771 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.772 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.773 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.778 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.779 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.468 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.873 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.165 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.167 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.168 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.168 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.169 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.169 I llama_model_loader: - type  f32:  194 tensors
0.00.057.170 I llama_model_loader: - type  f16:   98 tensors
0.00.057.171 I print_info: file format = GGUF V3 (latest)
0.00.057.172 I print_info: file type   = all F32 (guessed)
0.00.057.173 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.198 I load: special tokens cache size = 25
0.00.078.560 I load: token to piece cache size = 0.2984 MB
0.00.078.563 I print_info: arch             = gptneox
0.00.078.563 I print_info: vocab_only       = 0
0.00.078.564 I print_info: n_ctx_train      = 2048
0.00.078.564 I print_info: n_embd           = 2048
0.00.078.564 I print_info: n_layer          = 24
0.00.078.567 I print_info: n_head           = 16
0.00.078.568 I print_info: n_head_kv        = 16
0.00.078.568 I print_info: n_rot            = 32
0.00.078.569 I print_info: n_swa            = 0
0.00.078.569 I print_info: n_embd_head_k    = 128
0.00.078.570 I print_info: n_embd_head_v    = 128
0.00.078.571 I print_info: n_gqa            = 1
0.00.078.571 I print_info: n_embd_k_gqa     = 2048
0.00.078.572 I print_info: n_embd_v_gqa     = 2048
0.00.078.573 I print_info: f_norm_eps       = 1.0e-05
0.00.078.573 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.573 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.573 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.573 I print_info: f_logit_scale    = 0.0e+00
0.00.078.574 I print_info: n_ff             = 8192
0.00.078.574 I print_info: n_expert         = 0
0.00.078.574 I print_info: n_expert_used    = 0
0.00.078.575 I print_info: causal attn      = 1
0.00.078.575 I print_info: pooling type     = 0
0.00.078.575 I print_info: rope type        = 2
0.00.078.575 I print_info: rope scaling     = linear
0.00.078.577 I print_info: freq_base_train  = 10000.0
0.00.078.577 I print_info: freq_scale_train = 1
0.00.078.578 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.578 I print_info: rope_finetuned   = unknown
0.00.078.578 I print_info: ssm_d_conv       = 0
0.00.078.578 I print_info: ssm_d_inner      = 0
0.00.078.578 I print_info: ssm_d_state      = 0
0.00.078.578 I print_info: ssm_dt_rank      = 0
0.00.078.578 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.579 I print_info: model type       = 1.4B
0.00.078.579 I print_info: model params     = 1.41 B
0.00.078.579 I print_info: general.name     = 1.4B
0.00.078.580 I print_info: vocab type       = BPE
0.00.078.580 I print_info: n_vocab          = 50304
0.00.078.580 I print_info: n_merges         = 50009
0.00.078.580 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.580 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.582 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.582 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.583 I print_info: LF token         = 187 'Ċ'
0.00.078.583 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.583 I print_info: max token length = 1024
0.00.078.585 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.225.417 I load_tensors: offloading 24 repeating layers to GPU
0.01.225.420 I load_tensors: offloading output layer to GPU
0.01.225.421 I load_tensors: offloaded 25/25 layers to GPU
0.01.225.451 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.225.453 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.226.401 I llama_init_from_model: n_seq_max     = 1
0.01.226.402 I llama_init_from_model: n_ctx         = 128
0.01.226.403 I llama_init_from_model: n_ctx_per_seq = 128
0.01.226.403 I llama_init_from_model: n_batch       = 128
0.01.226.403 I llama_init_from_model: n_ubatch      = 128
0.01.226.403 I llama_init_from_model: flash_attn    = 0
0.01.226.404 I llama_init_from_model: freq_base     = 10000.0
0.01.226.404 I llama_init_from_model: freq_scale    = 1
0.01.226.405 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.226.406 I ggml_metal_init: allocating
0.01.226.488 I ggml_metal_init: found device: Apple M4
0.01.226.495 I ggml_metal_init: picking default device: Apple M4
0.01.227.678 I ggml_metal_init: using embedded metal library
0.01.231.613 I ggml_metal_init: GPU name:   Apple M4
0.01.231.615 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.231.616 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.231.616 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.231.617 I ggml_metal_init: simdgroup reduction   = true
0.01.231.617 I ggml_metal_init: simdgroup matrix mul. = true
0.01.231.617 I ggml_metal_init: has residency sets    = true
0.01.231.617 I ggml_metal_init: has bfloat            = true
0.01.231.617 I ggml_metal_init: use bfloat            = true
0.01.231.619 I ggml_metal_init: hasUnifiedMemory      = true
0.01.231.619 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.242.868 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.244.633 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.244.636 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.244.649 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.246.371 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.246.372 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.246.373 I llama_init_from_model: graph nodes  = 967
0.01.246.373 I llama_init_from_model: graph splits = 2
0.01.246.374 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.246.374 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.283.014 I 
0.01.283.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.283.064 I perplexity: tokenizing the input ..
0.01.288.452 I perplexity: tokenization took 5.386 ms
0.01.288.473 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.420.030 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.421.437 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.421.450 I llama_perf_context_print:        load time =    1258.58 ms
0.01.421.451 I llama_perf_context_print: prompt eval time =     131.24 ms /   128 tokens (    1.03 ms per token,   975.33 tokens per second)
0.01.421.451 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.421.452 I llama_perf_context_print:       total time =     138.44 ms /   129 tokens
0.01.421.789 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.101s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.978 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.151 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.152 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.152 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.152 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.153 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.153 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.156 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.157 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.157 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.048 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.845 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.846 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.846 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.847 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.847 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.847 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.848 I llama_model_loader: - type  f32:  194 tensors
0.00.024.848 I llama_model_loader: - type q8_0:   98 tensors
0.00.024.849 I print_info: file format = GGUF V3 (latest)
0.00.024.850 I print_info: file type   = Q8_0
0.00.024.850 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.013 I load: special tokens cache size = 25
0.00.039.245 I load: token to piece cache size = 0.2984 MB
0.00.039.249 I print_info: arch             = gptneox
0.00.039.250 I print_info: vocab_only       = 0
0.00.039.250 I print_info: n_ctx_train      = 2048
0.00.039.250 I print_info: n_embd           = 2048
0.00.039.250 I print_info: n_layer          = 24
0.00.039.254 I print_info: n_head           = 16
0.00.039.255 I print_info: n_head_kv        = 16
0.00.039.255 I print_info: n_rot            = 32
0.00.039.255 I print_info: n_swa            = 0
0.00.039.256 I print_info: n_embd_head_k    = 128
0.00.039.256 I print_info: n_embd_head_v    = 128
0.00.039.257 I print_info: n_gqa            = 1
0.00.039.257 I print_info: n_embd_k_gqa     = 2048
0.00.039.258 I print_info: n_embd_v_gqa     = 2048
0.00.039.259 I print_info: f_norm_eps       = 1.0e-05
0.00.039.259 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.259 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.259 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.259 I print_info: f_logit_scale    = 0.0e+00
0.00.039.260 I print_info: n_ff             = 8192
0.00.039.260 I print_info: n_expert         = 0
0.00.039.260 I print_info: n_expert_used    = 0
0.00.039.260 I print_info: causal attn      = 1
0.00.039.260 I print_info: pooling type     = 0
0.00.039.261 I print_info: rope type        = 2
0.00.039.261 I print_info: rope scaling     = linear
0.00.039.261 I print_info: freq_base_train  = 10000.0
0.00.039.263 I print_info: freq_scale_train = 1
0.00.039.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.265 I print_info: rope_finetuned   = unknown
0.00.039.265 I print_info: ssm_d_conv       = 0
0.00.039.265 I print_info: ssm_d_inner      = 0
0.00.039.265 I print_info: ssm_d_state      = 0
0.00.039.265 I print_info: ssm_dt_rank      = 0
0.00.039.265 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.266 I print_info: model type       = 1.4B
0.00.039.266 I print_info: model params     = 1.41 B
0.00.039.266 I print_info: general.name     = 1.4B
0.00.039.267 I print_info: vocab type       = BPE
0.00.039.267 I print_info: n_vocab          = 50304
0.00.039.267 I print_info: n_merges         = 50009
0.00.039.267 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.267 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.267 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.267 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: LF token         = 187 'Ċ'
0.00.039.268 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.268 I print_info: max token length = 1024
0.00.039.269 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.954.212 I load_tensors: offloading 24 repeating layers to GPU
0.00.954.220 I load_tensors: offloading output layer to GPU
0.00.954.220 I load_tensors: offloaded 25/25 layers to GPU
0.00.954.248 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.954.251 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.955.430 I llama_init_from_model: n_seq_max     = 1
0.00.955.432 I llama_init_from_model: n_ctx         = 128
0.00.955.433 I llama_init_from_model: n_ctx_per_seq = 128
0.00.955.433 I llama_init_from_model: n_batch       = 128
0.00.955.434 I llama_init_from_model: n_ubatch      = 128
0.00.955.434 I llama_init_from_model: flash_attn    = 0
0.00.955.435 I llama_init_from_model: freq_base     = 10000.0
0.00.955.435 I llama_init_from_model: freq_scale    = 1
0.00.955.436 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.955.437 I ggml_metal_init: allocating
0.00.955.470 I ggml_metal_init: found device: Apple M4
0.00.955.480 I ggml_metal_init: picking default device: Apple M4
0.00.956.836 I ggml_metal_init: using embedded metal library
0.00.962.183 I ggml_metal_init: GPU name:   Apple M4
0.00.962.189 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.962.190 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.962.191 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.962.191 I ggml_metal_init: simdgroup reduction   = true
0.00.962.191 I ggml_metal_init: simdgroup matrix mul. = true
0.00.962.191 I ggml_metal_init: has residency sets    = true
0.00.962.192 I ggml_metal_init: has bfloat            = true
0.00.962.192 I ggml_metal_init: use bfloat            = true
0.00.962.193 I ggml_metal_init: hasUnifiedMemory      = true
0.00.962.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.978.719 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.982.174 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.982.189 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.982.230 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.985.529 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.985.531 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.985.531 I llama_init_from_model: graph nodes  = 967
0.00.985.531 I llama_init_from_model: graph splits = 2
0.00.985.535 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.985.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.012.935 I 
0.01.012.993 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.013.025 I perplexity: tokenizing the input ..
0.01.020.641 I perplexity: tokenization took 7.613 ms
0.01.020.663 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.146.310 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.147.729 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.147.742 I llama_perf_context_print:        load time =    1003.95 ms
0.01.147.743 I llama_perf_context_print: prompt eval time =     124.74 ms /   128 tokens (    0.97 ms per token,  1026.17 tokens per second)
0.01.147.744 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.147.744 I llama_perf_context_print:       total time =     134.81 ms /   129 tokens
0.01.148.076 I ggml_metal_free: deallocating

real	0m1.162s
user	0m0.079s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.779 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.031 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.036 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.037 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.038 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.038 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.039 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.039 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.040 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.040 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.041 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.041 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.042 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.042 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.043 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.044 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.044 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.994 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.045 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.903 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.903 I llama_model_loader: - type  f32:  194 tensors
0.00.025.903 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.904 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.904 I print_info: file format = GGUF V3 (latest)
0.00.025.905 I print_info: file type   = Q4_0
0.00.025.908 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.299 I load: special tokens cache size = 25
0.00.040.731 I load: token to piece cache size = 0.2984 MB
0.00.040.734 I print_info: arch             = gptneox
0.00.040.734 I print_info: vocab_only       = 0
0.00.040.734 I print_info: n_ctx_train      = 2048
0.00.040.734 I print_info: n_embd           = 2048
0.00.040.735 I print_info: n_layer          = 24
0.00.040.738 I print_info: n_head           = 16
0.00.040.740 I print_info: n_head_kv        = 16
0.00.040.741 I print_info: n_rot            = 32
0.00.040.741 I print_info: n_swa            = 0
0.00.040.741 I print_info: n_embd_head_k    = 128
0.00.040.741 I print_info: n_embd_head_v    = 128
0.00.040.742 I print_info: n_gqa            = 1
0.00.040.747 I print_info: n_embd_k_gqa     = 2048
0.00.040.747 I print_info: n_embd_v_gqa     = 2048
0.00.040.748 I print_info: f_norm_eps       = 1.0e-05
0.00.040.748 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.749 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.749 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.749 I print_info: f_logit_scale    = 0.0e+00
0.00.040.749 I print_info: n_ff             = 8192
0.00.040.750 I print_info: n_expert         = 0
0.00.040.750 I print_info: n_expert_used    = 0
0.00.040.750 I print_info: causal attn      = 1
0.00.040.750 I print_info: pooling type     = 0
0.00.040.750 I print_info: rope type        = 2
0.00.040.750 I print_info: rope scaling     = linear
0.00.040.751 I print_info: freq_base_train  = 10000.0
0.00.040.752 I print_info: freq_scale_train = 1
0.00.040.752 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.752 I print_info: rope_finetuned   = unknown
0.00.040.752 I print_info: ssm_d_conv       = 0
0.00.040.754 I print_info: ssm_d_inner      = 0
0.00.040.754 I print_info: ssm_d_state      = 0
0.00.040.754 I print_info: ssm_dt_rank      = 0
0.00.040.754 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.754 I print_info: model type       = 1.4B
0.00.040.755 I print_info: model params     = 1.41 B
0.00.040.755 I print_info: general.name     = 1.4B
0.00.040.755 I print_info: vocab type       = BPE
0.00.040.756 I print_info: n_vocab          = 50304
0.00.040.756 I print_info: n_merges         = 50009
0.00.040.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.758 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.758 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.758 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.758 I print_info: LF token         = 187 'Ċ'
0.00.040.759 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.759 I print_info: max token length = 1024
0.00.040.759 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.681.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.681.344 I load_tensors: offloading output layer to GPU
0.00.681.344 I load_tensors: offloaded 25/25 layers to GPU
0.00.681.362 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.681.363 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.682.300 I llama_init_from_model: n_seq_max     = 1
0.00.682.306 I llama_init_from_model: n_ctx         = 128
0.00.682.306 I llama_init_from_model: n_ctx_per_seq = 128
0.00.682.307 I llama_init_from_model: n_batch       = 128
0.00.682.307 I llama_init_from_model: n_ubatch      = 128
0.00.682.307 I llama_init_from_model: flash_attn    = 0
0.00.682.309 I llama_init_from_model: freq_base     = 10000.0
0.00.682.309 I llama_init_from_model: freq_scale    = 1
0.00.682.310 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.682.311 I ggml_metal_init: allocating
0.00.682.362 I ggml_metal_init: found device: Apple M4
0.00.682.373 I ggml_metal_init: picking default device: Apple M4
0.00.683.409 I ggml_metal_init: using embedded metal library
0.00.687.602 I ggml_metal_init: GPU name:   Apple M4
0.00.687.609 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.687.610 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.687.611 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.687.611 I ggml_metal_init: simdgroup reduction   = true
0.00.687.611 I ggml_metal_init: simdgroup matrix mul. = true
0.00.687.611 I ggml_metal_init: has residency sets    = true
0.00.687.612 I ggml_metal_init: has bfloat            = true
0.00.687.612 I ggml_metal_init: use bfloat            = true
0.00.687.613 I ggml_metal_init: hasUnifiedMemory      = true
0.00.687.615 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.703.653 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.374 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.705.378 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.705.392 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.005 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.707.007 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.707.007 I llama_init_from_model: graph nodes  = 967
0.00.707.007 I llama_init_from_model: graph splits = 2
0.00.707.009 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.707.009 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.037 I 
0.00.728.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.061 I perplexity: tokenizing the input ..
0.00.731.715 I perplexity: tokenization took 3.653 ms
0.00.731.724 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.867.460 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.868.944 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.868.961 I llama_perf_context_print:        load time =     718.26 ms
0.00.868.963 I llama_perf_context_print: prompt eval time =     135.52 ms /   128 tokens (    1.06 ms per token,   944.54 tokens per second)
0.00.868.963 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.868.966 I llama_perf_context_print:       total time =     140.92 ms /   129 tokens
0.00.869.327 I ggml_metal_free: deallocating

real	0m0.885s
user	0m0.071s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.101 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.335 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.339 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.346 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.346 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.347 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.347 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.347 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.348 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.349 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.350 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.350 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.352 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.352 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.251 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.282 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.022 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.024 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.024 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.024 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.024 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.025 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.030.025 I llama_model_loader: - type  f32:  194 tensors
0.00.030.026 I llama_model_loader: - type q4_1:   97 tensors
0.00.030.026 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.027 I print_info: file format = GGUF V3 (latest)
0.00.030.027 I print_info: file type   = Q4_1
0.00.030.032 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.038.270 I load: special tokens cache size = 25
0.00.044.357 I load: token to piece cache size = 0.2984 MB
0.00.044.361 I print_info: arch             = gptneox
0.00.044.361 I print_info: vocab_only       = 0
0.00.044.361 I print_info: n_ctx_train      = 2048
0.00.044.361 I print_info: n_embd           = 2048
0.00.044.362 I print_info: n_layer          = 24
0.00.044.364 I print_info: n_head           = 16
0.00.044.365 I print_info: n_head_kv        = 16
0.00.044.366 I print_info: n_rot            = 32
0.00.044.366 I print_info: n_swa            = 0
0.00.044.366 I print_info: n_embd_head_k    = 128
0.00.044.366 I print_info: n_embd_head_v    = 128
0.00.044.367 I print_info: n_gqa            = 1
0.00.044.368 I print_info: n_embd_k_gqa     = 2048
0.00.044.368 I print_info: n_embd_v_gqa     = 2048
0.00.044.369 I print_info: f_norm_eps       = 1.0e-05
0.00.044.369 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.369 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.371 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.372 I print_info: f_logit_scale    = 0.0e+00
0.00.044.372 I print_info: n_ff             = 8192
0.00.044.372 I print_info: n_expert         = 0
0.00.044.373 I print_info: n_expert_used    = 0
0.00.044.373 I print_info: causal attn      = 1
0.00.044.373 I print_info: pooling type     = 0
0.00.044.373 I print_info: rope type        = 2
0.00.044.373 I print_info: rope scaling     = linear
0.00.044.374 I print_info: freq_base_train  = 10000.0
0.00.044.374 I print_info: freq_scale_train = 1
0.00.044.374 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.374 I print_info: rope_finetuned   = unknown
0.00.044.375 I print_info: ssm_d_conv       = 0
0.00.044.375 I print_info: ssm_d_inner      = 0
0.00.044.375 I print_info: ssm_d_state      = 0
0.00.044.375 I print_info: ssm_dt_rank      = 0
0.00.044.375 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.375 I print_info: model type       = 1.4B
0.00.044.376 I print_info: model params     = 1.41 B
0.00.044.376 I print_info: general.name     = 1.4B
0.00.044.377 I print_info: vocab type       = BPE
0.00.044.377 I print_info: n_vocab          = 50304
0.00.044.377 I print_info: n_merges         = 50009
0.00.044.377 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.377 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.378 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.378 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.378 I print_info: LF token         = 187 'Ċ'
0.00.044.379 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.379 I print_info: max token length = 1024
0.00.044.379 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.681.222 I load_tensors: offloading 24 repeating layers to GPU
0.00.681.237 I load_tensors: offloading output layer to GPU
0.00.681.238 I load_tensors: offloaded 25/25 layers to GPU
0.00.681.275 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.681.276 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.682.668 I llama_init_from_model: n_seq_max     = 1
0.00.682.670 I llama_init_from_model: n_ctx         = 128
0.00.682.671 I llama_init_from_model: n_ctx_per_seq = 128
0.00.682.671 I llama_init_from_model: n_batch       = 128
0.00.682.671 I llama_init_from_model: n_ubatch      = 128
0.00.682.672 I llama_init_from_model: flash_attn    = 0
0.00.682.674 I llama_init_from_model: freq_base     = 10000.0
0.00.682.674 I llama_init_from_model: freq_scale    = 1
0.00.682.675 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.682.677 I ggml_metal_init: allocating
0.00.682.759 I ggml_metal_init: found device: Apple M4
0.00.682.773 I ggml_metal_init: picking default device: Apple M4
0.00.684.667 I ggml_metal_init: using embedded metal library
0.00.690.904 I ggml_metal_init: GPU name:   Apple M4
0.00.690.909 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.690.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.690.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.690.911 I ggml_metal_init: simdgroup reduction   = true
0.00.690.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.690.912 I ggml_metal_init: has residency sets    = true
0.00.690.913 I ggml_metal_init: has bfloat            = true
0.00.690.913 I ggml_metal_init: use bfloat            = true
0.00.690.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.690.916 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.710.088 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.632 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.713.636 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.713.662 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.141 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.717.142 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.717.143 I llama_init_from_model: graph nodes  = 967
0.00.717.144 I llama_init_from_model: graph splits = 2
0.00.717.147 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.717.147 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.635 I 
0.00.744.691 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.721 I perplexity: tokenizing the input ..
0.00.752.085 I perplexity: tokenization took 7.361 ms
0.00.752.104 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.004 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.890.328 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.890.347 I llama_perf_context_print:        load time =     735.53 ms
0.00.890.348 I llama_perf_context_print: prompt eval time =     136.01 ms /   128 tokens (    1.06 ms per token,   941.12 tokens per second)
0.00.890.348 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.890.349 I llama_perf_context_print:       total time =     145.72 ms /   129 tokens
0.00.890.715 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.080s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.736 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.004 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.026.010 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.014 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.014 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.015 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.015 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.016 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.016 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.017 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.017 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.017 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.018 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.020 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.021 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.587 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.590 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.591 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.034.591 I llama_model_loader: - type  f32:  194 tensors
0.00.034.592 I llama_model_loader: - type q5_0:   97 tensors
0.00.034.592 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.593 I print_info: file format = GGUF V3 (latest)
0.00.034.599 I print_info: file type   = Q5_0
0.00.034.600 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.042.712 I load: special tokens cache size = 25
0.00.049.454 I load: token to piece cache size = 0.2984 MB
0.00.049.457 I print_info: arch             = gptneox
0.00.049.458 I print_info: vocab_only       = 0
0.00.049.458 I print_info: n_ctx_train      = 2048
0.00.049.458 I print_info: n_embd           = 2048
0.00.049.458 I print_info: n_layer          = 24
0.00.049.462 I print_info: n_head           = 16
0.00.049.463 I print_info: n_head_kv        = 16
0.00.049.463 I print_info: n_rot            = 32
0.00.049.463 I print_info: n_swa            = 0
0.00.049.463 I print_info: n_embd_head_k    = 128
0.00.049.463 I print_info: n_embd_head_v    = 128
0.00.049.464 I print_info: n_gqa            = 1
0.00.049.465 I print_info: n_embd_k_gqa     = 2048
0.00.049.465 I print_info: n_embd_v_gqa     = 2048
0.00.049.466 I print_info: f_norm_eps       = 1.0e-05
0.00.049.466 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.466 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.466 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.466 I print_info: f_logit_scale    = 0.0e+00
0.00.049.467 I print_info: n_ff             = 8192
0.00.049.467 I print_info: n_expert         = 0
0.00.049.467 I print_info: n_expert_used    = 0
0.00.049.467 I print_info: causal attn      = 1
0.00.049.468 I print_info: pooling type     = 0
0.00.049.468 I print_info: rope type        = 2
0.00.049.468 I print_info: rope scaling     = linear
0.00.049.468 I print_info: freq_base_train  = 10000.0
0.00.049.468 I print_info: freq_scale_train = 1
0.00.049.469 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.469 I print_info: rope_finetuned   = unknown
0.00.049.469 I print_info: ssm_d_conv       = 0
0.00.049.469 I print_info: ssm_d_inner      = 0
0.00.049.469 I print_info: ssm_d_state      = 0
0.00.049.469 I print_info: ssm_dt_rank      = 0
0.00.049.470 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.470 I print_info: model type       = 1.4B
0.00.049.470 I print_info: model params     = 1.41 B
0.00.049.470 I print_info: general.name     = 1.4B
0.00.049.470 I print_info: vocab type       = BPE
0.00.049.471 I print_info: n_vocab          = 50304
0.00.049.473 I print_info: n_merges         = 50009
0.00.049.473 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.473 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.473 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.474 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.474 I print_info: LF token         = 187 'Ċ'
0.00.049.474 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.474 I print_info: max token length = 1024
0.00.049.475 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.727.989 I load_tensors: offloading 24 repeating layers to GPU
0.00.728.000 I load_tensors: offloading output layer to GPU
0.00.728.001 I load_tensors: offloaded 25/25 layers to GPU
0.00.728.027 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.728.029 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.729.515 I llama_init_from_model: n_seq_max     = 1
0.00.729.518 I llama_init_from_model: n_ctx         = 128
0.00.729.519 I llama_init_from_model: n_ctx_per_seq = 128
0.00.729.519 I llama_init_from_model: n_batch       = 128
0.00.729.520 I llama_init_from_model: n_ubatch      = 128
0.00.729.520 I llama_init_from_model: flash_attn    = 0
0.00.729.521 I llama_init_from_model: freq_base     = 10000.0
0.00.729.522 I llama_init_from_model: freq_scale    = 1
0.00.729.523 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.729.524 I ggml_metal_init: allocating
0.00.729.567 I ggml_metal_init: found device: Apple M4
0.00.729.580 I ggml_metal_init: picking default device: Apple M4
0.00.731.006 I ggml_metal_init: using embedded metal library
0.00.737.366 I ggml_metal_init: GPU name:   Apple M4
0.00.737.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.737.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.737.372 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.737.372 I ggml_metal_init: simdgroup reduction   = true
0.00.737.372 I ggml_metal_init: simdgroup matrix mul. = true
0.00.737.373 I ggml_metal_init: has residency sets    = true
0.00.737.373 I ggml_metal_init: has bfloat            = true
0.00.737.373 I ggml_metal_init: use bfloat            = true
0.00.737.374 I ggml_metal_init: hasUnifiedMemory      = true
0.00.737.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.754.016 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.757.655 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.757.658 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.757.684 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.760.832 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.760.834 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.760.834 I llama_init_from_model: graph nodes  = 967
0.00.760.835 I llama_init_from_model: graph splits = 2
0.00.760.837 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.760.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.448 I 
0.00.790.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.533 I perplexity: tokenizing the input ..
0.00.797.901 I perplexity: tokenization took 7.364 ms
0.00.797.921 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.946.786 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.948.114 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.948.126 I llama_perf_context_print:        load time =     780.70 ms
0.00.948.127 I llama_perf_context_print: prompt eval time =     147.98 ms /   128 tokens (    1.16 ms per token,   865.01 tokens per second)
0.00.948.127 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.948.128 I llama_perf_context_print:       total time =     157.68 ms /   129 tokens
0.00.948.471 I ggml_metal_free: deallocating

real	0m0.964s
user	0m0.080s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.950 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.493 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.507 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.507 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.508 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.509 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.509 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.510 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.511 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.511 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.514 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.401 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.321 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.323 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.323 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.323 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.324 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.324 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.325 I llama_model_loader: - type  f32:  194 tensors
0.00.028.325 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.325 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.326 I print_info: file format = GGUF V3 (latest)
0.00.028.327 I print_info: file type   = Q5_1
0.00.028.328 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.036.511 I load: special tokens cache size = 25
0.00.042.312 I load: token to piece cache size = 0.2984 MB
0.00.042.315 I print_info: arch             = gptneox
0.00.042.315 I print_info: vocab_only       = 0
0.00.042.315 I print_info: n_ctx_train      = 2048
0.00.042.316 I print_info: n_embd           = 2048
0.00.042.316 I print_info: n_layer          = 24
0.00.042.320 I print_info: n_head           = 16
0.00.042.321 I print_info: n_head_kv        = 16
0.00.042.321 I print_info: n_rot            = 32
0.00.042.321 I print_info: n_swa            = 0
0.00.042.322 I print_info: n_embd_head_k    = 128
0.00.042.322 I print_info: n_embd_head_v    = 128
0.00.042.322 I print_info: n_gqa            = 1
0.00.042.323 I print_info: n_embd_k_gqa     = 2048
0.00.042.324 I print_info: n_embd_v_gqa     = 2048
0.00.042.324 I print_info: f_norm_eps       = 1.0e-05
0.00.042.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.325 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.325 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.325 I print_info: f_logit_scale    = 0.0e+00
0.00.042.326 I print_info: n_ff             = 8192
0.00.042.326 I print_info: n_expert         = 0
0.00.042.327 I print_info: n_expert_used    = 0
0.00.042.327 I print_info: causal attn      = 1
0.00.042.327 I print_info: pooling type     = 0
0.00.042.327 I print_info: rope type        = 2
0.00.042.327 I print_info: rope scaling     = linear
0.00.042.329 I print_info: freq_base_train  = 10000.0
0.00.042.331 I print_info: freq_scale_train = 1
0.00.042.331 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.331 I print_info: rope_finetuned   = unknown
0.00.042.331 I print_info: ssm_d_conv       = 0
0.00.042.332 I print_info: ssm_d_inner      = 0
0.00.042.332 I print_info: ssm_d_state      = 0
0.00.042.332 I print_info: ssm_dt_rank      = 0
0.00.042.332 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.332 I print_info: model type       = 1.4B
0.00.042.333 I print_info: model params     = 1.41 B
0.00.042.333 I print_info: general.name     = 1.4B
0.00.042.333 I print_info: vocab type       = BPE
0.00.042.333 I print_info: n_vocab          = 50304
0.00.042.337 I print_info: n_merges         = 50009
0.00.042.337 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.337 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.338 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.338 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.339 I print_info: LF token         = 187 'Ċ'
0.00.042.339 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.339 I print_info: max token length = 1024
0.00.042.339 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.737.608 I load_tensors: offloading 24 repeating layers to GPU
0.00.737.625 I load_tensors: offloading output layer to GPU
0.00.737.626 I load_tensors: offloaded 25/25 layers to GPU
0.00.737.659 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.737.660 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.739.198 I llama_init_from_model: n_seq_max     = 1
0.00.739.201 I llama_init_from_model: n_ctx         = 128
0.00.739.202 I llama_init_from_model: n_ctx_per_seq = 128
0.00.739.203 I llama_init_from_model: n_batch       = 128
0.00.739.203 I llama_init_from_model: n_ubatch      = 128
0.00.739.203 I llama_init_from_model: flash_attn    = 0
0.00.739.206 I llama_init_from_model: freq_base     = 10000.0
0.00.739.206 I llama_init_from_model: freq_scale    = 1
0.00.739.207 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.739.209 I ggml_metal_init: allocating
0.00.739.264 I ggml_metal_init: found device: Apple M4
0.00.739.276 I ggml_metal_init: picking default device: Apple M4
0.00.740.947 I ggml_metal_init: using embedded metal library
0.00.747.408 I ggml_metal_init: GPU name:   Apple M4
0.00.747.414 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.747.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.747.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.747.417 I ggml_metal_init: simdgroup reduction   = true
0.00.747.417 I ggml_metal_init: simdgroup matrix mul. = true
0.00.747.417 I ggml_metal_init: has residency sets    = true
0.00.747.418 I ggml_metal_init: has bfloat            = true
0.00.747.418 I ggml_metal_init: use bfloat            = true
0.00.747.419 I ggml_metal_init: hasUnifiedMemory      = true
0.00.747.421 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.765.657 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.769.206 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.769.213 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.769.252 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.772.487 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.772.489 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.772.489 I llama_init_from_model: graph nodes  = 967
0.00.772.490 I llama_init_from_model: graph splits = 2
0.00.772.493 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.772.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.174 I 
0.00.806.239 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.268 I perplexity: tokenizing the input ..
0.00.813.043 I perplexity: tokenization took 6.774 ms
0.00.813.055 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.960.946 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.962.327 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.962.346 I llama_perf_context_print:        load time =     797.22 ms
0.00.962.347 I llama_perf_context_print: prompt eval time =     147.66 ms /   128 tokens (    1.15 ms per token,   866.84 tokens per second)
0.00.962.347 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.962.348 I llama_perf_context_print:       total time =     156.18 ms /   129 tokens
0.00.962.730 I ggml_metal_free: deallocating

real	0m0.977s
user	0m0.078s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.592 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.147 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.152 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.152 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.152 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.152 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.153 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.154 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.154 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.155 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.156 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.156 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.157 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.159 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.159 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.159 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.930 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.804 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.032.805 I llama_model_loader: - type  f32:  194 tensors
0.00.032.805 I llama_model_loader: - type q2_K:   49 tensors
0.00.032.805 I llama_model_loader: - type q3_K:   48 tensors
0.00.032.805 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.806 I print_info: file format = GGUF V3 (latest)
0.00.032.807 I print_info: file type   = Q2_K - Medium
0.00.032.807 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.041.012 I load: special tokens cache size = 25
0.00.046.842 I load: token to piece cache size = 0.2984 MB
0.00.046.844 I print_info: arch             = gptneox
0.00.046.845 I print_info: vocab_only       = 0
0.00.046.845 I print_info: n_ctx_train      = 2048
0.00.046.845 I print_info: n_embd           = 2048
0.00.046.845 I print_info: n_layer          = 24
0.00.046.848 I print_info: n_head           = 16
0.00.046.849 I print_info: n_head_kv        = 16
0.00.046.849 I print_info: n_rot            = 32
0.00.046.850 I print_info: n_swa            = 0
0.00.046.850 I print_info: n_embd_head_k    = 128
0.00.046.851 I print_info: n_embd_head_v    = 128
0.00.046.852 I print_info: n_gqa            = 1
0.00.046.852 I print_info: n_embd_k_gqa     = 2048
0.00.046.855 I print_info: n_embd_v_gqa     = 2048
0.00.046.855 I print_info: f_norm_eps       = 1.0e-05
0.00.046.855 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.856 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.856 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.856 I print_info: f_logit_scale    = 0.0e+00
0.00.046.857 I print_info: n_ff             = 8192
0.00.046.857 I print_info: n_expert         = 0
0.00.046.858 I print_info: n_expert_used    = 0
0.00.046.859 I print_info: causal attn      = 1
0.00.046.859 I print_info: pooling type     = 0
0.00.046.859 I print_info: rope type        = 2
0.00.046.859 I print_info: rope scaling     = linear
0.00.046.859 I print_info: freq_base_train  = 10000.0
0.00.046.860 I print_info: freq_scale_train = 1
0.00.046.860 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.860 I print_info: rope_finetuned   = unknown
0.00.046.860 I print_info: ssm_d_conv       = 0
0.00.046.860 I print_info: ssm_d_inner      = 0
0.00.046.861 I print_info: ssm_d_state      = 0
0.00.046.861 I print_info: ssm_dt_rank      = 0
0.00.046.861 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.861 I print_info: model type       = 1.4B
0.00.046.861 I print_info: model params     = 1.41 B
0.00.046.862 I print_info: general.name     = 1.4B
0.00.046.862 I print_info: vocab type       = BPE
0.00.046.862 I print_info: n_vocab          = 50304
0.00.046.862 I print_info: n_merges         = 50009
0.00.046.864 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.865 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.865 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.865 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.865 I print_info: LF token         = 187 'Ċ'
0.00.046.865 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.866 I print_info: max token length = 1024
0.00.046.867 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.386.557 I load_tensors: offloading 24 repeating layers to GPU
0.00.386.572 I load_tensors: offloading output layer to GPU
0.00.386.573 I load_tensors: offloaded 25/25 layers to GPU
0.00.386.602 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.386.604 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.388.353 I llama_init_from_model: n_seq_max     = 1
0.00.388.358 I llama_init_from_model: n_ctx         = 128
0.00.388.358 I llama_init_from_model: n_ctx_per_seq = 128
0.00.388.359 I llama_init_from_model: n_batch       = 128
0.00.388.359 I llama_init_from_model: n_ubatch      = 128
0.00.388.359 I llama_init_from_model: flash_attn    = 0
0.00.388.362 I llama_init_from_model: freq_base     = 10000.0
0.00.388.362 I llama_init_from_model: freq_scale    = 1
0.00.388.363 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.388.364 I ggml_metal_init: allocating
0.00.388.437 I ggml_metal_init: found device: Apple M4
0.00.388.450 I ggml_metal_init: picking default device: Apple M4
0.00.390.234 I ggml_metal_init: using embedded metal library
0.00.395.618 I ggml_metal_init: GPU name:   Apple M4
0.00.395.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.395.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.395.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.395.631 I ggml_metal_init: simdgroup reduction   = true
0.00.395.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.395.632 I ggml_metal_init: has residency sets    = true
0.00.395.632 I ggml_metal_init: has bfloat            = true
0.00.395.633 I ggml_metal_init: use bfloat            = true
0.00.395.634 I ggml_metal_init: hasUnifiedMemory      = true
0.00.395.638 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.416.689 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.420.296 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.420.303 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.420.352 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.423.768 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.423.770 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.423.771 I llama_init_from_model: graph nodes  = 967
0.00.423.771 I llama_init_from_model: graph splits = 2
0.00.423.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.423.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.451.682 I 
0.00.451.746 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.451.772 I perplexity: tokenizing the input ..
0.00.458.704 I perplexity: tokenization took 6.929 ms
0.00.458.725 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.592.251 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.593.589 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.593.601 I llama_perf_context_print:        load time =     438.08 ms
0.00.593.601 I llama_perf_context_print: prompt eval time =     132.57 ms /   128 tokens (    1.04 ms per token,   965.52 tokens per second)
0.00.593.602 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.593.603 I llama_perf_context_print:       total time =     141.92 ms /   129 tokens
0.00.593.980 I ggml_metal_free: deallocating

real	0m0.609s
user	0m0.081s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.757 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.390 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.403 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.404 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.404 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.408 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.408 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.411 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.411 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.151 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.210 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.950 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.952 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.952 I llama_model_loader: - type  f32:  194 tensors
0.00.024.953 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.953 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.953 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.953 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.954 I print_info: file format = GGUF V3 (latest)
0.00.024.955 I print_info: file type   = Q3_K - Medium
0.00.024.956 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.955 I load: special tokens cache size = 25
0.00.038.979 I load: token to piece cache size = 0.2984 MB
0.00.038.982 I print_info: arch             = gptneox
0.00.038.983 I print_info: vocab_only       = 0
0.00.038.983 I print_info: n_ctx_train      = 2048
0.00.038.983 I print_info: n_embd           = 2048
0.00.038.983 I print_info: n_layer          = 24
0.00.038.987 I print_info: n_head           = 16
0.00.038.988 I print_info: n_head_kv        = 16
0.00.038.988 I print_info: n_rot            = 32
0.00.038.988 I print_info: n_swa            = 0
0.00.038.989 I print_info: n_embd_head_k    = 128
0.00.038.989 I print_info: n_embd_head_v    = 128
0.00.038.989 I print_info: n_gqa            = 1
0.00.038.990 I print_info: n_embd_k_gqa     = 2048
0.00.038.991 I print_info: n_embd_v_gqa     = 2048
0.00.038.991 I print_info: f_norm_eps       = 1.0e-05
0.00.038.992 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.992 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.992 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.992 I print_info: f_logit_scale    = 0.0e+00
0.00.038.993 I print_info: n_ff             = 8192
0.00.038.993 I print_info: n_expert         = 0
0.00.038.993 I print_info: n_expert_used    = 0
0.00.038.996 I print_info: causal attn      = 1
0.00.038.996 I print_info: pooling type     = 0
0.00.038.997 I print_info: rope type        = 2
0.00.038.997 I print_info: rope scaling     = linear
0.00.038.997 I print_info: freq_base_train  = 10000.0
0.00.038.997 I print_info: freq_scale_train = 1
0.00.038.998 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.998 I print_info: rope_finetuned   = unknown
0.00.038.999 I print_info: ssm_d_conv       = 0
0.00.038.999 I print_info: ssm_d_inner      = 0
0.00.039.000 I print_info: ssm_d_state      = 0
0.00.039.000 I print_info: ssm_dt_rank      = 0
0.00.039.000 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.001 I print_info: model type       = 1.4B
0.00.039.001 I print_info: model params     = 1.41 B
0.00.039.001 I print_info: general.name     = 1.4B
0.00.039.002 I print_info: vocab type       = BPE
0.00.039.002 I print_info: n_vocab          = 50304
0.00.039.002 I print_info: n_merges         = 50009
0.00.039.002 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.002 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.003 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.003 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.003 I print_info: LF token         = 187 'Ċ'
0.00.039.003 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.004 I print_info: max token length = 1024
0.00.039.004 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.467.246 I load_tensors: offloading 24 repeating layers to GPU
0.00.467.261 I load_tensors: offloading output layer to GPU
0.00.467.262 I load_tensors: offloaded 25/25 layers to GPU
0.00.467.296 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.467.297 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.468.850 I llama_init_from_model: n_seq_max     = 1
0.00.468.853 I llama_init_from_model: n_ctx         = 128
0.00.468.854 I llama_init_from_model: n_ctx_per_seq = 128
0.00.468.854 I llama_init_from_model: n_batch       = 128
0.00.468.855 I llama_init_from_model: n_ubatch      = 128
0.00.468.855 I llama_init_from_model: flash_attn    = 0
0.00.468.858 I llama_init_from_model: freq_base     = 10000.0
0.00.468.858 I llama_init_from_model: freq_scale    = 1
0.00.468.859 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.468.865 I ggml_metal_init: allocating
0.00.468.979 I ggml_metal_init: found device: Apple M4
0.00.468.996 I ggml_metal_init: picking default device: Apple M4
0.00.470.857 I ggml_metal_init: using embedded metal library
0.00.476.722 I ggml_metal_init: GPU name:   Apple M4
0.00.476.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.476.727 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.476.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.476.729 I ggml_metal_init: simdgroup reduction   = true
0.00.476.729 I ggml_metal_init: simdgroup matrix mul. = true
0.00.476.730 I ggml_metal_init: has residency sets    = true
0.00.476.730 I ggml_metal_init: has bfloat            = true
0.00.476.730 I ggml_metal_init: use bfloat            = true
0.00.476.731 I ggml_metal_init: hasUnifiedMemory      = true
0.00.476.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.495.542 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.499.037 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.499.041 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.499.066 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.502.164 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.502.166 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.502.166 I llama_init_from_model: graph nodes  = 967
0.00.502.167 I llama_init_from_model: graph splits = 2
0.00.502.169 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.502.169 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.662 I 
0.00.533.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.751 I perplexity: tokenizing the input ..
0.00.540.711 I perplexity: tokenization took 6.957 ms
0.00.540.731 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.686.482 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.687.791 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.687.809 I llama_perf_context_print:        load time =     524.89 ms
0.00.687.809 I llama_perf_context_print: prompt eval time =     145.33 ms /   128 tokens (    1.14 ms per token,   880.75 tokens per second)
0.00.687.810 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.687.811 I llama_perf_context_print:       total time =     154.15 ms /   129 tokens
0.00.688.182 I ggml_metal_free: deallocating

real	0m0.702s
user	0m0.078s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.966 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.972 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.975 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.975 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.976 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.976 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.977 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.977 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.978 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.978 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.978 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.979 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.979 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.982 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.982 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.982 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.882 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.892 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.772 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.773 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.774 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.774 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.774 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.775 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.775 I llama_model_loader: - type  f32:  194 tensors
0.00.026.776 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.776 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.776 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.777 I print_info: file format = GGUF V3 (latest)
0.00.026.777 I print_info: file type   = Q4_K - Medium
0.00.026.781 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.873 I load: special tokens cache size = 25
0.00.040.783 I load: token to piece cache size = 0.2984 MB
0.00.040.787 I print_info: arch             = gptneox
0.00.040.787 I print_info: vocab_only       = 0
0.00.040.787 I print_info: n_ctx_train      = 2048
0.00.040.787 I print_info: n_embd           = 2048
0.00.040.787 I print_info: n_layer          = 24
0.00.040.791 I print_info: n_head           = 16
0.00.040.792 I print_info: n_head_kv        = 16
0.00.040.792 I print_info: n_rot            = 32
0.00.040.792 I print_info: n_swa            = 0
0.00.040.792 I print_info: n_embd_head_k    = 128
0.00.040.792 I print_info: n_embd_head_v    = 128
0.00.040.793 I print_info: n_gqa            = 1
0.00.040.794 I print_info: n_embd_k_gqa     = 2048
0.00.040.795 I print_info: n_embd_v_gqa     = 2048
0.00.040.795 I print_info: f_norm_eps       = 1.0e-05
0.00.040.796 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.796 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.796 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.796 I print_info: f_logit_scale    = 0.0e+00
0.00.040.797 I print_info: n_ff             = 8192
0.00.040.797 I print_info: n_expert         = 0
0.00.040.797 I print_info: n_expert_used    = 0
0.00.040.799 I print_info: causal attn      = 1
0.00.040.799 I print_info: pooling type     = 0
0.00.040.799 I print_info: rope type        = 2
0.00.040.799 I print_info: rope scaling     = linear
0.00.040.801 I print_info: freq_base_train  = 10000.0
0.00.040.801 I print_info: freq_scale_train = 1
0.00.040.801 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.802 I print_info: rope_finetuned   = unknown
0.00.040.802 I print_info: ssm_d_conv       = 0
0.00.040.802 I print_info: ssm_d_inner      = 0
0.00.040.803 I print_info: ssm_d_state      = 0
0.00.040.803 I print_info: ssm_dt_rank      = 0
0.00.040.803 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.804 I print_info: model type       = 1.4B
0.00.040.804 I print_info: model params     = 1.41 B
0.00.040.804 I print_info: general.name     = 1.4B
0.00.040.805 I print_info: vocab type       = BPE
0.00.040.805 I print_info: n_vocab          = 50304
0.00.040.805 I print_info: n_merges         = 50009
0.00.040.805 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.805 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.806 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.809 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.809 I print_info: LF token         = 187 'Ċ'
0.00.040.810 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: max token length = 1024
0.00.040.810 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.761.904 I load_tensors: offloading 24 repeating layers to GPU
0.00.761.918 I load_tensors: offloading output layer to GPU
0.00.761.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.761.952 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.761.953 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.763.623 I llama_init_from_model: n_seq_max     = 1
0.00.763.625 I llama_init_from_model: n_ctx         = 128
0.00.763.626 I llama_init_from_model: n_ctx_per_seq = 128
0.00.763.626 I llama_init_from_model: n_batch       = 128
0.00.763.626 I llama_init_from_model: n_ubatch      = 128
0.00.763.627 I llama_init_from_model: flash_attn    = 0
0.00.763.629 I llama_init_from_model: freq_base     = 10000.0
0.00.763.630 I llama_init_from_model: freq_scale    = 1
0.00.763.630 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.763.632 I ggml_metal_init: allocating
0.00.763.708 I ggml_metal_init: found device: Apple M4
0.00.763.722 I ggml_metal_init: picking default device: Apple M4
0.00.765.495 I ggml_metal_init: using embedded metal library
0.00.772.377 I ggml_metal_init: GPU name:   Apple M4
0.00.772.381 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.772.382 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.772.383 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.772.383 I ggml_metal_init: simdgroup reduction   = true
0.00.772.384 I ggml_metal_init: simdgroup matrix mul. = true
0.00.772.384 I ggml_metal_init: has residency sets    = true
0.00.772.384 I ggml_metal_init: has bfloat            = true
0.00.772.385 I ggml_metal_init: use bfloat            = true
0.00.772.386 I ggml_metal_init: hasUnifiedMemory      = true
0.00.772.387 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.789.632 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.793.102 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.793.107 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.793.142 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.796.262 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.796.264 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.796.264 I llama_init_from_model: graph nodes  = 967
0.00.796.264 I llama_init_from_model: graph splits = 2
0.00.796.267 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.796.268 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.823.523 I 
0.00.823.588 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.823.617 I perplexity: tokenizing the input ..
0.00.830.759 I perplexity: tokenization took 7.137 ms
0.00.830.782 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.965.940 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.967.280 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.967.295 I llama_perf_context_print:        load time =     814.61 ms
0.00.967.296 I llama_perf_context_print: prompt eval time =     134.28 ms /   128 tokens (    1.05 ms per token,   953.25 tokens per second)
0.00.967.296 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.967.297 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.00.967.653 I ggml_metal_free: deallocating

real	0m0.981s
user	0m0.080s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.182 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.647 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.655 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.789 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.902 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.902 I llama_model_loader: - type  f32:  194 tensors
0.00.028.903 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.903 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.904 I print_info: file format = GGUF V3 (latest)
0.00.028.904 I print_info: file type   = Q5_K - Medium
0.00.028.905 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.038.150 I load: special tokens cache size = 25
0.00.045.971 I load: token to piece cache size = 0.2984 MB
0.00.045.975 I print_info: arch             = gptneox
0.00.045.975 I print_info: vocab_only       = 0
0.00.045.975 I print_info: n_ctx_train      = 2048
0.00.045.975 I print_info: n_embd           = 2048
0.00.045.975 I print_info: n_layer          = 24
0.00.045.978 I print_info: n_head           = 16
0.00.045.979 I print_info: n_head_kv        = 16
0.00.045.979 I print_info: n_rot            = 32
0.00.045.980 I print_info: n_swa            = 0
0.00.045.982 I print_info: n_embd_head_k    = 128
0.00.045.982 I print_info: n_embd_head_v    = 128
0.00.045.983 I print_info: n_gqa            = 1
0.00.045.983 I print_info: n_embd_k_gqa     = 2048
0.00.045.984 I print_info: n_embd_v_gqa     = 2048
0.00.045.985 I print_info: f_norm_eps       = 1.0e-05
0.00.045.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.986 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.986 I print_info: f_logit_scale    = 0.0e+00
0.00.045.987 I print_info: n_ff             = 8192
0.00.045.987 I print_info: n_expert         = 0
0.00.045.987 I print_info: n_expert_used    = 0
0.00.045.987 I print_info: causal attn      = 1
0.00.045.987 I print_info: pooling type     = 0
0.00.045.987 I print_info: rope type        = 2
0.00.045.988 I print_info: rope scaling     = linear
0.00.045.988 I print_info: freq_base_train  = 10000.0
0.00.045.989 I print_info: freq_scale_train = 1
0.00.045.989 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.989 I print_info: rope_finetuned   = unknown
0.00.045.989 I print_info: ssm_d_conv       = 0
0.00.045.989 I print_info: ssm_d_inner      = 0
0.00.045.989 I print_info: ssm_d_state      = 0
0.00.045.990 I print_info: ssm_dt_rank      = 0
0.00.045.990 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.990 I print_info: model type       = 1.4B
0.00.045.990 I print_info: model params     = 1.41 B
0.00.045.990 I print_info: general.name     = 1.4B
0.00.045.991 I print_info: vocab type       = BPE
0.00.045.991 I print_info: n_vocab          = 50304
0.00.045.991 I print_info: n_merges         = 50009
0.00.045.992 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.992 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.992 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.992 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.992 I print_info: LF token         = 187 'Ċ'
0.00.045.993 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.993 I print_info: max token length = 1024
0.00.045.994 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.359 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.373 I load_tensors: offloading output layer to GPU
0.00.630.374 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.404 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.630.405 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.632.013 I llama_init_from_model: n_seq_max     = 1
0.00.632.020 I llama_init_from_model: n_ctx         = 128
0.00.632.020 I llama_init_from_model: n_ctx_per_seq = 128
0.00.632.021 I llama_init_from_model: n_batch       = 128
0.00.632.021 I llama_init_from_model: n_ubatch      = 128
0.00.632.022 I llama_init_from_model: flash_attn    = 0
0.00.632.023 I llama_init_from_model: freq_base     = 10000.0
0.00.632.024 I llama_init_from_model: freq_scale    = 1
0.00.632.024 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.632.027 I ggml_metal_init: allocating
0.00.632.099 I ggml_metal_init: found device: Apple M4
0.00.632.118 I ggml_metal_init: picking default device: Apple M4
0.00.633.836 I ggml_metal_init: using embedded metal library
0.00.640.323 I ggml_metal_init: GPU name:   Apple M4
0.00.640.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.328 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.328 I ggml_metal_init: simdgroup reduction   = true
0.00.640.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.329 I ggml_metal_init: has residency sets    = true
0.00.640.329 I ggml_metal_init: has bfloat            = true
0.00.640.329 I ggml_metal_init: use bfloat            = true
0.00.640.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.884 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.457 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.661.467 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.661.513 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.664.697 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.664.699 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.664.700 I llama_init_from_model: graph nodes  = 967
0.00.664.700 I llama_init_from_model: graph splits = 2
0.00.664.703 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.664.703 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.483 I 
0.00.694.544 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.574 I perplexity: tokenizing the input ..
0.00.701.848 I perplexity: tokenization took 7.27 ms
0.00.701.870 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.528 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.844.858 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.844.875 I llama_perf_context_print:        load time =     682.29 ms
0.00.844.876 I llama_perf_context_print: prompt eval time =     140.70 ms /   128 tokens (    1.10 ms per token,   909.71 tokens per second)
0.00.844.879 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.844.879 I llama_perf_context_print:       total time =     150.40 ms /   129 tokens
0.00.845.240 I ggml_metal_free: deallocating

real	0m0.877s
user	0m0.083s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.895 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.490 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.496 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.496 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.497 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.497 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.498 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.498 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.499 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.501 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.502 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.423 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.392 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.394 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.394 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.395 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.396 I llama_model_loader: - type  f32:  194 tensors
0.00.024.396 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.397 I print_info: file format = GGUF V3 (latest)
0.00.024.397 I print_info: file type   = Q6_K
0.00.024.398 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.350 I load: special tokens cache size = 25
0.00.038.433 I load: token to piece cache size = 0.2984 MB
0.00.038.436 I print_info: arch             = gptneox
0.00.038.437 I print_info: vocab_only       = 0
0.00.038.437 I print_info: n_ctx_train      = 2048
0.00.038.437 I print_info: n_embd           = 2048
0.00.038.437 I print_info: n_layer          = 24
0.00.038.441 I print_info: n_head           = 16
0.00.038.442 I print_info: n_head_kv        = 16
0.00.038.442 I print_info: n_rot            = 32
0.00.038.442 I print_info: n_swa            = 0
0.00.038.442 I print_info: n_embd_head_k    = 128
0.00.038.442 I print_info: n_embd_head_v    = 128
0.00.038.443 I print_info: n_gqa            = 1
0.00.038.446 I print_info: n_embd_k_gqa     = 2048
0.00.038.446 I print_info: n_embd_v_gqa     = 2048
0.00.038.447 I print_info: f_norm_eps       = 1.0e-05
0.00.038.448 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.448 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.448 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.448 I print_info: f_logit_scale    = 0.0e+00
0.00.038.449 I print_info: n_ff             = 8192
0.00.038.449 I print_info: n_expert         = 0
0.00.038.450 I print_info: n_expert_used    = 0
0.00.038.450 I print_info: causal attn      = 1
0.00.038.450 I print_info: pooling type     = 0
0.00.038.451 I print_info: rope type        = 2
0.00.038.452 I print_info: rope scaling     = linear
0.00.038.452 I print_info: freq_base_train  = 10000.0
0.00.038.452 I print_info: freq_scale_train = 1
0.00.038.453 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.453 I print_info: rope_finetuned   = unknown
0.00.038.453 I print_info: ssm_d_conv       = 0
0.00.038.453 I print_info: ssm_d_inner      = 0
0.00.038.453 I print_info: ssm_d_state      = 0
0.00.038.453 I print_info: ssm_dt_rank      = 0
0.00.038.453 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.454 I print_info: model type       = 1.4B
0.00.038.454 I print_info: model params     = 1.41 B
0.00.038.454 I print_info: general.name     = 1.4B
0.00.038.455 I print_info: vocab type       = BPE
0.00.038.458 I print_info: n_vocab          = 50304
0.00.038.458 I print_info: n_merges         = 50009
0.00.038.459 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.459 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.459 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.459 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.460 I print_info: LF token         = 187 'Ċ'
0.00.038.462 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.462 I print_info: max token length = 1024
0.00.038.462 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.428.793 I load_tensors: offloading 24 repeating layers to GPU
0.00.428.796 I load_tensors: offloading output layer to GPU
0.00.428.797 I load_tensors: offloaded 25/25 layers to GPU
0.00.428.821 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.428.823 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.430.356 I llama_init_from_model: n_seq_max     = 1
0.00.430.359 I llama_init_from_model: n_ctx         = 128
0.00.430.359 I llama_init_from_model: n_ctx_per_seq = 128
0.00.430.359 I llama_init_from_model: n_batch       = 128
0.00.430.360 I llama_init_from_model: n_ubatch      = 128
0.00.430.360 I llama_init_from_model: flash_attn    = 0
0.00.430.361 I llama_init_from_model: freq_base     = 10000.0
0.00.430.362 I llama_init_from_model: freq_scale    = 1
0.00.430.362 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.430.364 I ggml_metal_init: allocating
0.00.430.419 I ggml_metal_init: found device: Apple M4
0.00.430.432 I ggml_metal_init: picking default device: Apple M4
0.00.431.883 I ggml_metal_init: using embedded metal library
0.00.437.738 I ggml_metal_init: GPU name:   Apple M4
0.00.437.742 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.437.743 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.437.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.437.744 I ggml_metal_init: simdgroup reduction   = true
0.00.437.744 I ggml_metal_init: simdgroup matrix mul. = true
0.00.437.745 I ggml_metal_init: has residency sets    = true
0.00.437.745 I ggml_metal_init: has bfloat            = true
0.00.437.745 I ggml_metal_init: use bfloat            = true
0.00.437.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.437.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.454.591 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.458.012 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.458.016 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.458.045 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.461.185 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.461.187 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.461.187 I llama_init_from_model: graph nodes  = 967
0.00.461.188 I llama_init_from_model: graph splits = 2
0.00.461.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.461.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.956 I 
0.00.497.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.497.046 I perplexity: tokenizing the input ..
0.00.503.590 I perplexity: tokenization took 6.542 ms
0.00.503.607 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.643.744 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.645.160 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.645.173 I llama_perf_context_print:        load time =     488.05 ms
0.00.645.174 I llama_perf_context_print: prompt eval time =     139.59 ms /   128 tokens (    1.09 ms per token,   916.96 tokens per second)
0.00.645.175 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.645.175 I llama_perf_context_print:       total time =     148.22 ms /   129 tokens
0.00.645.563 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.077s
sys	0m0.113s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.343 I build: 4696 (e598697d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.439 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.115 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.121 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.123 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.127 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.127 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.128 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.130 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.136 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.976 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.837 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.839 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.840 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.840 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.840 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.841 I llama_model_loader: - type  f32:  194 tensors
0.00.051.841 I llama_model_loader: - type  f16:   98 tensors
0.00.051.842 I print_info: file format = GGUF V3 (latest)
0.00.051.843 I print_info: file type   = all F32 (guessed)
0.00.051.849 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.889 I load: special tokens cache size = 25
0.00.070.388 I load: token to piece cache size = 0.2984 MB
0.00.070.390 I print_info: arch             = gptneox
0.00.070.391 I print_info: vocab_only       = 0
0.00.070.391 I print_info: n_ctx_train      = 2048
0.00.070.391 I print_info: n_embd           = 2048
0.00.070.391 I print_info: n_layer          = 24
0.00.070.394 I print_info: n_head           = 16
0.00.070.394 I print_info: n_head_kv        = 16
0.00.070.395 I print_info: n_rot            = 32
0.00.070.395 I print_info: n_swa            = 0
0.00.070.395 I print_info: n_embd_head_k    = 128
0.00.070.395 I print_info: n_embd_head_v    = 128
0.00.070.396 I print_info: n_gqa            = 1
0.00.070.397 I print_info: n_embd_k_gqa     = 2048
0.00.070.397 I print_info: n_embd_v_gqa     = 2048
0.00.070.398 I print_info: f_norm_eps       = 1.0e-05
0.00.070.398 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.399 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.399 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.399 I print_info: f_logit_scale    = 0.0e+00
0.00.070.399 I print_info: n_ff             = 8192
0.00.070.400 I print_info: n_expert         = 0
0.00.070.400 I print_info: n_expert_used    = 0
0.00.070.400 I print_info: causal attn      = 1
0.00.070.400 I print_info: pooling type     = 0
0.00.070.400 I print_info: rope type        = 2
0.00.070.400 I print_info: rope scaling     = linear
0.00.070.401 I print_info: freq_base_train  = 10000.0
0.00.070.401 I print_info: freq_scale_train = 1
0.00.070.401 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.401 I print_info: rope_finetuned   = unknown
0.00.070.404 I print_info: ssm_d_conv       = 0
0.00.070.404 I print_info: ssm_d_inner      = 0
0.00.070.404 I print_info: ssm_d_state      = 0
0.00.070.404 I print_info: ssm_dt_rank      = 0
0.00.070.404 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.404 I print_info: model type       = 1.4B
0.00.070.405 I print_info: model params     = 1.41 B
0.00.070.405 I print_info: general.name     = 1.4B
0.00.070.405 I print_info: vocab type       = BPE
0.00.070.405 I print_info: n_vocab          = 50304
0.00.070.405 I print_info: n_merges         = 50009
0.00.070.406 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.406 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.406 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.406 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.406 I print_info: LF token         = 187 'Ċ'
0.00.070.407 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.407 I print_info: max token length = 1024
0.00.070.407 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.339.712 I load_tensors: offloading 24 repeating layers to GPU
0.01.339.718 I load_tensors: offloading output layer to GPU
0.01.339.720 I load_tensors: offloaded 25/25 layers to GPU
0.01.339.745 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.339.748 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.340.772 I llama_init_from_model: n_seq_max     = 1
0.01.340.773 I llama_init_from_model: n_ctx         = 128
0.01.340.774 I llama_init_from_model: n_ctx_per_seq = 128
0.01.340.774 I llama_init_from_model: n_batch       = 128
0.01.340.774 I llama_init_from_model: n_ubatch      = 128
0.01.340.775 I llama_init_from_model: flash_attn    = 0
0.01.340.775 I llama_init_from_model: freq_base     = 10000.0
0.01.340.775 I llama_init_from_model: freq_scale    = 1
0.01.340.776 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.340.777 I ggml_metal_init: allocating
0.01.340.808 I ggml_metal_init: found device: Apple M4
0.01.340.815 I ggml_metal_init: picking default device: Apple M4
0.01.341.803 I ggml_metal_init: using embedded metal library
0.01.345.558 I ggml_metal_init: GPU name:   Apple M4
0.01.345.560 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.345.561 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.345.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.345.562 I ggml_metal_init: simdgroup reduction   = true
0.01.345.562 I ggml_metal_init: simdgroup matrix mul. = true
0.01.345.562 I ggml_metal_init: has residency sets    = true
0.01.345.562 I ggml_metal_init: has bfloat            = true
0.01.345.562 I ggml_metal_init: use bfloat            = true
0.01.345.563 I ggml_metal_init: hasUnifiedMemory      = true
0.01.345.564 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.356.109 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.357.774 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.357.776 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.357.791 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.359.463 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.359.464 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.359.465 I llama_init_from_model: graph nodes  = 967
0.01.359.465 I llama_init_from_model: graph splits = 2
0.01.359.466 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.359.467 I 
0.01.359.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.359.494 I compute_imatrix: tokenizing the input ..
0.01.363.554 I compute_imatrix: tokenization took 4.059 ms
0.01.363.556 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.630.849 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.633.398 I llama_perf_context_print:        load time =    1608.41 ms
0.01.633.399 I llama_perf_context_print: prompt eval time =     265.57 ms /   128 tokens (    2.07 ms per token,   481.98 tokens per second)
0.01.633.400 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.633.400 I llama_perf_context_print:       total time =    1610.95 ms /   129 tokens
0.01.633.882 I ggml_metal_free: deallocating

real	0m1.817s
user	0m0.123s
sys	0m0.249s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4696 (e598697d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120607ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120608380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120608930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120608ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120609490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120609a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120609ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12060a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12060ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12060b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12060b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12060ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12060c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12060cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12060d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12060dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12060e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12060ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12060f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12060f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1206100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1206107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120610ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120611780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120611ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120612160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120612770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1206133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120613920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120613be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120614080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120614340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120614bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120615110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1206153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120615870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120615d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1206161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120616650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120616af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120616f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120617430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1206178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120617d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120618030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120618640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120618c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120619570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120619b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12061a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12061a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12061adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12061b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12061b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12061c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12061c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12061cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12061cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12061d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12061dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12061de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12061e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12061e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12061ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12061f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12061f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12061fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12061fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120620380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120620820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120620cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120621160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120621600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120621b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1206220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1206225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120622b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120623090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1206235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120623b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120624080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1206245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120624b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120625070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1206255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120625b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120626060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1206265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120626b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120627050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1206275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120627af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120628040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120628590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120628ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120629030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120629580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120619260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1206299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12062a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12062a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12062ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12062b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12062b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12062bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12062c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12062c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12062cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12062d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12062d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12062dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12062e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12062e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12062eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12062eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12062f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12062f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12062fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120630270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120630710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120630bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120631050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1206314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120631990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120631e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1206322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120632770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120632c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1206330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120633550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1206339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120633e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120634330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1206347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120634c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120635110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1206355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120635a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120635ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120636390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120636830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120637170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120637610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120637ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120637f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1206383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120638890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120638d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1206391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120639670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120639b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120639fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12063a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12063a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12063ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12063b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12063b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12063bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12063c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12063c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12063c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12063cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12063d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12063d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12063dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12063e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12063e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12063e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12063ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12063f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12063f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12063fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1206400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120640570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120640a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120640eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120641350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1206417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120641c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120642130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1206425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120642a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120642f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1206433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120643850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120643cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120644190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120644630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120644ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120644f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120645410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1206458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120645e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120646350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1206468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120646df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1206470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1206476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120647cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1206482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120648ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120648f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120649230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120649840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120649e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12064a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12064aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12064af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12064b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12064bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12064c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12064c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12064cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12064d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12064d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12064dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12064e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12064e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12064eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12064f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12064f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12064fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1206500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120650630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120650b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1206510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120651620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120651b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1206520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120652610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120652b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1206530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120653600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120653b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1206540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1206545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120654b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120655090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1206555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120655b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120656080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1206565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120656b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120657070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1206575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120657b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120658060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1206585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120658b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120659050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1206595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120659af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12065a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12065a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12065aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12065b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12065b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12065bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12065c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12065c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12065cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12065d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12065d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12065dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12065e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12065e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12065e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12065ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12065f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12065f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12065fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120660110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1206605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120660a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120660ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120661390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120661830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120661cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120662170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120662610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120662ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120663000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120663720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120663e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120664560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120664c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120664f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120665730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1206659f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120666000 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.781.369 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.781.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x113b04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x113b05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x113b056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x113b05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x113b05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x113b06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x113b06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x113b06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x113b07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x113b075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x113b07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x113b08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x113b08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x113b093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x113b09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x113b0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x113b0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x113b0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x113b0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x113b0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x113b0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x113b0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x113b0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x113b0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x113b0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x113b0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x113b0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x113b0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x113b0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x113b0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x113b0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x113b0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x113b10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x113b106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x113b10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x113b10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x113b11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x113b118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x113b11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x113b12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x113b12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x113b12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x113b12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x113b13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x113b137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x113b13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x113b140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x113b14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x113b14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x113b14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x113b15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x113b156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x113b15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x113b15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x113b16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x113b16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x113b16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x113b17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x113b17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x113b17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x113b18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x113b184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x113b18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x113b18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x113b19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x113b19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x113b19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x113b19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x113b1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x113b1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x113b1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x113b1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x113b1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x113b1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x113b1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x113b1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x113b1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x113b1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x113b1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x113b1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x113b1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x113b1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x113b1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x113b1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x113b1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x113b1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x113b1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x113b1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x113b1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x113b20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x113b20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x113b209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x113b20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x113b212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x113b21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x113b21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x113b22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x113b22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x113b228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x113b22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x113b231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x113b23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x113b23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x113b23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x113b24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x113b24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x113b24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x113b250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x113b25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x113b259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x113b25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x113b262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x113b26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x113b26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x113b26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x113b27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x113b278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x113b27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x113b281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x113b28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x113b28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x113b28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x113b29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x113b297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x113b29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x113b2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x113b2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x113b2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x113b2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x113b2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x113b2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x113b2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x113b2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x113b2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x113b2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x113b2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x113b2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x113b2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x113b2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x113b2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x113b2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x113b2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x113b2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x113b2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x113b2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x113b2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x113b2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x113b30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x113b306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x113b30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x113b30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x113b31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x113b31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x113b31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x113b32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x113b325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x113b32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x113b32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x113b33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x113b337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x113b33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x113b34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x113b344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x113b34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x113b34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x113b35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x113b35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x113b36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x113b363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x113b36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x113b36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x113b37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x113b375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x113b37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x113b37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x113b38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x113b38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x113b38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x113b39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x113b394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x113b39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x113b39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x113b3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x113b3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x113b3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x113b3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x113b3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x113b3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x113b3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x113b3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x113b3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x113b3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x113b3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x113b3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x113b3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x113b3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x113b3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x113b3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x113b3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x113b3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x113b3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x113b3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x113b3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x113b400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x113b40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x113b409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x113b40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x113b41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x113b417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x113b41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x113b42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x113b42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x113b430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x113b43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x113b43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x113b441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x113b447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x113b44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x113b45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x113b458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x113b45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x113b46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x113b46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x113b46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x113b475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x113b47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x113b48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x113b486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x113b48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x113b49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x113b49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x113b49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x113b4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x113b4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x113b4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x113b4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x113b4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x113b4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x113b4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x113b4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x113b4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x113b4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x113b4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x113b4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x113b4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x113b4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x113b4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x113b4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x113b4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x113b50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x113b50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x113b510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x113b516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x113b51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x113b52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x113b527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x113b52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x113b53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x113b53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x113b53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x113b544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x113b54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x113b55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x113b555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x113b55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x113b56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x113b56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x113b56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x113b571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x113b576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x113b57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x113b580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x113b585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x113b58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x113b58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x113b594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x113b599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x113b59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x113b5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x113b5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x113b5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x113b5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x113b5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x113b5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x113b5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x113b5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x113b5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x113b5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x113b5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x113b5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x113b5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120665cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1206494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120647370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120647f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12061b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12061aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12061d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120649b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120612420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120618f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120619830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120619e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1206182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12061a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120611420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120629cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120665200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120614600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1206148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12064a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1206485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120612a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120612cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120612fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120666460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120666720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1206669e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120666ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120666f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120667220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1206674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1206677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120667a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120667d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120667fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1206682a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120668560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120668820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120668ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120668da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120669060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120669320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1206695e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1206698a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120669b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120669e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12066a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12066a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12066a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12066a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12066abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12066aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12066b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12066b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12066b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12066b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12066bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12066bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12066c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12066c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12066c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12066ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12066cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12066cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12066d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12066d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12066d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12066daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12066dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12066e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12066e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12066e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12066e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12066eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12066ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12066f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12066f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12066f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12066f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12066fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12066fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120670120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1206703e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1206706a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120670960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120670c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120670ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1206711a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120671460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120671720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1206719e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120671ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120671f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120672220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1206724e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1206727a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120672a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120672d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120672fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1206732a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120673560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120673820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120673ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120673da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120674060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120674320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1206745e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1206748a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120674b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120674e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1206750e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1206753a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120675660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120675920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120675be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120675ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120676160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120676420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1206766e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1206769a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120676c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120676f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1206771e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1206774a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120677760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120677a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120677ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120677fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120678260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120678520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1206787e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120678aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120678d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120679020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1206792e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1206795a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120679860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120679b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120679de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12067a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12067a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12067a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12067a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12067aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12067ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12067b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12067b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12067b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12067b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12067bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12067bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12067c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12067c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12067c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12067c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12067cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12067cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12067d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12067d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12067d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12067da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12067dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12067dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12067e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12067e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12067e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12067eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12067eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12067f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12067f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12067f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12067f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12067fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12067fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1206800e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1206803a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120680660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120680920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120680be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120680ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120681160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120681420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1206816e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1206819a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120681c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120681f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1206821e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1206824a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120682760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120682a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120682ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120682fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120683260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120683520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1206837e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120683aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120683d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120684020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1206842e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1206845a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120684860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120684b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120684de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1206850a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120685360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120685620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1206858e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120685ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120685e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120686260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120686a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120686cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120686f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120687400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120687870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120687ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120688150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1206885c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120688a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120688ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120689310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120689780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120689bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12068a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12068a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12068a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12068adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12068b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12068b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12068bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12068bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12068c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12068c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12068ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12068d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12068d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12068da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12068de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12068e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12068e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12068ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12068f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12068f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12068f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12068fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120690200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120690670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120690ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120690f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1206913c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120691830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120691ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120692110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120692580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1206929f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120692e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1206932d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120693740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120693bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120694020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120694490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120694900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120694d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1206951e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120695650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120695ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120695f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1206963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120696810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120696c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1206970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120697560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1206979d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120697e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1206982b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120698720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120698b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120699000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120699470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1206998e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120699d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12069a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12069a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12069b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12069b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12069bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12069c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12069c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12069d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12069d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12069d980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.826s
user	0m0.278s
sys	0m0.348s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4696 (e598697d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1231076d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123107de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123108390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123108940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123108ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1231094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123109a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12310a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12310a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12310aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12310afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12310b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12310bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12310c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12310cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12310d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12310ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12310e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12310ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12310f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12310fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123110220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123110940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1231111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123111900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123111bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1231121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123112e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123113380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123113640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123113ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123113da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123114630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123114b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123114e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1231152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123115770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123115c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1231160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123116550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1231169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123116e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123117330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1231177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123117a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1231180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1231186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123118fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1231195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123119bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12311a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12311a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12311ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12311b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12311bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12311c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12311c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12311c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12311ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12311d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12311d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12311dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12311e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12311e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12311eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12311f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12311f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12311f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12311fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123120280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123120720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123120bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123121060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1231215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123121b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123122050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1231225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123122af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123123040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123123590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123123ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123124030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123124580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123124ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123125020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123125570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123125ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123126010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123126560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123126ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123127000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123127550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123127aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123127ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123128540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123128a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123128fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123118cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123129450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123129c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12312a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12312a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12312abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12312b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12312b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12312bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12312c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12312c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12312cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12312d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12312d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12312dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12312e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12312e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12312ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12312eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12312f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12312f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12312fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123130170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123130610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123130ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123130f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1231313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123131890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123131d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1231321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123132670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123132b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123132fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123133450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1231338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123133d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123134230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1231346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123134b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123135010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1231354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123135950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123135df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123136290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123136730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123136bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123137070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123137510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1231379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123137e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1231382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123138790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123138c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1231390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123139570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123139a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123139eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12313a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12313a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12313ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12313b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12313b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12313ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12313bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12313c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12313c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12313ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12313d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12313d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12313dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12313df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12313e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12313e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12313ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12313f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12313f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12313fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12313ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123140470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123140910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123140db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123141250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1231416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123141b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123142030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1231424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123142970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123142e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1231432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123143750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123143bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123144090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123144530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1231449d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123144e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123145310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123145860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123145db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123146300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123146850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123146b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123147120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123147730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123147d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123148530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1231489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123148c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1231492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1231498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12314a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12314a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12314a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12314ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12314b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12314bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12314c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12314c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12314cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12314d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12314d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12314db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12314e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12314e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12314eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12314f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12314f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12314fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123150090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1231505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123150b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123151080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1231515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123151b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123152070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1231525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123152b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123153060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1231535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123153b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123154050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1231545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123154af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123155040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123155590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123155ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123156030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123156580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123156ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123157020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123157570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123157ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123158010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123158560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123158ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123159000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123159550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123159aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123159ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12315a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12315aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12315afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12315b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12315ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12315bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12315c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12315ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12315cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12315d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12315da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12315dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12315e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12315e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12315ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12315f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12315f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12315fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123160010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1231604b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123160950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123160df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123161290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123161730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123161bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123162070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123162510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123162a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123163180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1231638a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123163fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1231646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1231649a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123165190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123165450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123165a60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.749 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.753 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121f05690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121f05b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121f05f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121f063e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121f06850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121f06cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121f07130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121f075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121f07a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121f07e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121f082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121f089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121f094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121f09c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121f0a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121f0abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121f0b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121f0b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121f0c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121f0c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121f0d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121f0d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121f0de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121f0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121f0ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121f0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121f0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121f0f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121f0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121f0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121f103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121f108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121f10d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121f11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121f11490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121f11900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121f11d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121f121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121f12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121f12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121f12f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121f133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121f13810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121f13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121f140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121f14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121f149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121f14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121f152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121f15720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121f15b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121f16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121f16470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121f168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121f16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121f171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121f17730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121f17c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121f180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121f18510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121f18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121f18df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121f19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121f196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121f19b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121f19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121f1a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121f1a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121f1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121f1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121f1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121f1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121f1bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121f1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121f1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121f1cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121f1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121f1d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121f1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121f1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121f1e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121f1e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121f1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121f1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121f1f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121f1f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121f1fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121f20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121f205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121f20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121f20ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121f21310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121f21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121f21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121f22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121f224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121f22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121f22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121f23220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121f23690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121f23b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121f23f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121f243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121f24850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121f24cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121f25130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121f255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121f25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121f25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121f262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121f26760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121f26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121f27040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121f274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121f27920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121f27d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121f28200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121f28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121f28ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121f28f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121f293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121f29830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121f29ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121f2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121f2a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121f2a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121f2ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121f2b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121f2b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121f2bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121f2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121f2c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121f2c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121f2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121f2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121f2d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121f2dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121f2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121f2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121f2e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121f2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121f2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121f2f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121f2f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121f2fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121f302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121f30720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121f30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121f31000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121f31470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121f318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121f31d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121f321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121f32630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121f32aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121f32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121f33380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121f337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121f33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121f340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121f34540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121f349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121f34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121f35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121f35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121f35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121f367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121f36a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121f37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121f37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121f37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121f37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121f38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121f387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121f38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121f390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121f39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121f39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121f39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121f3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121f3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121f3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121f3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121f3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121f3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121f3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121f3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121f3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121f3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121f3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121f3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121f3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121f3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121f3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121f3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121f3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121f3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121f3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121f3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121f3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121f3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121f404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121f40a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121f40e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121f412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121f41750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121f41bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121f420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121f425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121f43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121f43420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121f439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121f43fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121f44560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121f44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121f450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121f456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121f45c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121f46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121f467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121f46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121f47360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121f47920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121f47ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121f484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121f48a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121f49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121f495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121f49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121f4a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121f4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121f4ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121f4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121f4b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121f4be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121f4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121f4c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121f4cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121f4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121f4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121f4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121f4e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121f4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121f4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121f4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121f4fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121f50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121f508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121f50ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121f51460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121f51a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121f51fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121f525a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121f52b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121f53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121f536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121f53ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121f54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121f54820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121f54de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121f553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121f55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121f55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121f564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121f56aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121f57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121f57620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121f57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121f58020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121f58520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121f58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121f58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121f59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121f59920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121f59e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121f5a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121f5a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121f5ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121f5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121f5b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121f5bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121f5c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121f5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121f5d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121f5d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121f5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121f5e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121f5eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121f5ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121f5f410 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121e0b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121e0b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121e0bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121e0c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121e0ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121e0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121e0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121e0f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121e0fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121e10160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121e105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121e10c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121e11770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121e11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121e12730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121e12e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121e13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121e13c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121e143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121e14b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121e152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121e159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121e160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121e16800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121e16f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121e171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121e174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121e17910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121e17d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121e181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121e18660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121e18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121e19000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121e192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121e19730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121e19ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121e1a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121e1a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121e1a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121e1ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121e1b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121e1b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121e1bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121e1bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121e1c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121e1c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121e1cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121e1d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121e1d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121e1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121e1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121e1e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121e1e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121e1eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121e1eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121e1f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121e1f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121e1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121e20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121e207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121e20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121e21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121e21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121e21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121e21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121e22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121e226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121e22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121e22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121e23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121e23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121e23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121e24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121e245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121e24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121e24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121e25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121e25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121e25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121e26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121e264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121e26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121e26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121e27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121e276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121e27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121e27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121e283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121e28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121e28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121e29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121e295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121e29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121e29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121e2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121e2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121e2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121e2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121e2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121e2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121e2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121e2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121e2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121e2cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121e2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121e2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121e2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121e2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121e2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121e2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121e2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121e2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121e2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121e2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121e2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121e302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121e30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121e30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121e30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121e31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121e318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121e31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121e321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121e32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121e32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121e32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121e33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121e337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121e33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121e340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121e34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121e349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121e34e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121e35280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121e356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121e35b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121e35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121e36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121e368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121e36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121e37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121e37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121e37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121e37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121e38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121e387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121e38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121e390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121e39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121e39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121e39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121e3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121e3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121e3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121e3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121e3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121e3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121e3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121e3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121e3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121e3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121e3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121e3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121e3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121e3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121e3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121e3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121e3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121e3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121e3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121e3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121e3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121e3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121e40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121e40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121e40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121e41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121e415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121e41a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121e41ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121e42310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121e42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121e42bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121e43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121e434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121e43940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121e43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121e44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121e44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121e44b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121e44f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121e453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121e45850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121e45cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121e46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121e465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121e46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121e46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121e472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121e47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121e47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121e48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121e484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121e48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121e48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121e49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121e49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121e49ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121e49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121e4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121e4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121e4b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121e4b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121e4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121e4bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121e4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121e4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121e4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121e4cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121e4d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121e4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121e4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121e4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121e4e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121e4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121e4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121e4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121e4f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121e4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121e50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121e504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121e50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121e50d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121e511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121e51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121e51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121e51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121e523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121e52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121e52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121e53100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121e53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121e539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121e53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121e542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121e54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121e54ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121e55010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121e55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121e558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121e55d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121e561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121e56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121e56ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121e56f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121e57390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121e57800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121e57c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121e580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121e58550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121e589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121e58e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121e592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121e59710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121e59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121e59ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121e5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121e5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121e5ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121e5b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121e5b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121e5ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121e5bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121e5c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121e5c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121e5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121e5d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121e5d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121e5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121e5de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121e5e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121e5e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121e5eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121e5f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121e5fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121e60410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121e60b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121e60df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121e61260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121e61860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121e61e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.943s
user	0m0.230s
sys	0m0.180s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
